00:00 - in this course you will learn about
00:01 - microservice architecture and
00:04 - distributed systems using a Hands-On
00:07 - approach a microservices architecture is
00:10 - a type of application architecture where
00:13 - the application is developed as a
00:15 - collection of services Giorgio from
00:18 - Canton coding teaches this course he
00:21 - does a great job teaching how to combine
00:23 - a bunch of different Technologies into a
00:25 - single application hey what's up
00:27 - everybody and welcome to this video on
00:30 - microservice architectures where we will
00:32 - be applying this architecture to an
00:35 - application that will convert video
00:37 - files to MP3 files in this Hands-On
00:40 - tutorial we'll be making use of python
00:43 - rabbitmq mongodb Docker kubernetes and
00:47 - MySQL to build this microservice
00:50 - architecture which is admittedly a lot
00:53 - but don't worry I'll walk you through
00:55 - every step so let's go over what this
00:58 - application is going to to look like
01:00 - from a top-down perspective so when a
01:03 - user uploads a video to be converted to
01:06 - MP3 that request will first hit our
01:08 - Gateway our Gateway will then store the
01:11 - video in mongodb and then put a message
01:14 - on this queue here which is our rapidm
01:16 - queue letting Downstream Services know
01:19 - that there is a video to be processed in
01:21 - mongodb the video to MP3 converter
01:24 - service will consume messages from the
01:26 - queue it will then get the ID of the
01:28 - video from the message pull that video
01:31 - from mongodb convert the video to MP3
01:34 - then store the MP3 on mongodb then put a
01:38 - new message on the Queue to be consumed
01:39 - by the notification service that says
01:42 - that the conversion job is done the
01:44 - notification service consumes those
01:46 - messages from the queue and sends an
01:48 - email notification to the client
01:50 - informing the client that the MP3 for
01:52 - the video that he or she uploaded is
01:55 - ready for download the client will then
01:57 - use a unique ID acquired from the
01:59 - notification education plus his or her
02:01 - JWT to make a request to the API Gateway
02:04 - to download the MP3 and the API Gateway
02:08 - will pull the MP3 from mongodb and serve
02:11 - it to the client and that is the overall
02:14 - conversion flow and how rapid mq is
02:17 - integrated with the overall system okay
02:20 - so the first thing that we're going to
02:22 - need to do before we get started with
02:24 - writing any code or setting up any of
02:26 - our services is we're going to need to
02:29 - install a few things now the Links for
02:32 - all of these pages are going to be in
02:35 - the description and it's critical that
02:37 - we correctly install all of these
02:39 - prerequisites prior to moving forward
02:41 - with the course so please take the time
02:44 - to make sure that you've correctly
02:46 - installed all of these things so to
02:49 - start we're going to install Docker and
02:52 - I'm using Max so everything that I do
02:55 - like everything that I install is going
02:56 - to be for Mac so if you have Windows or
02:59 - a Linux system you're going to need to
03:01 - do some additional research to figure
03:03 - out how to install on those platforms
03:05 - because I'm not going to spend the time
03:07 - diving too deep into that but if you're
03:09 - able to successfully install everything
03:11 - on this list then that's probably one of
03:14 - the most difficult parts of this course
03:15 - and the rest of it should be a pretty
03:17 - smooth sailing so to start we're going
03:19 - to install Docker so I would hit this
03:22 - install Docker desktop and I'm using an
03:25 - M1 MacBook Pro so I would install with
03:28 - apple chip if you're using an Intel
03:30 - MacBook Pro or MacBook then you would
03:32 - use the Intel chip installation and of
03:34 - course you're just going to click the
03:36 - link and install it that way
03:39 - and following the successful
03:41 - installation of Docker you should be
03:43 - able to do Docker version and get a
03:46 - Docker version here now once you've
03:48 - gotten Docker installed we'll move on to
03:51 - installing the kubernetes manline tool
03:54 - and as you can see the kubernetes
03:56 - command line tool allows you to run
03:57 - commands against kubernetes clusters and
04:00 - we're going to be deploying our services
04:02 - within a kubernetes cluster so that's
04:04 - why we need this command line tool so
04:07 - again I'm going to choose to install for
04:10 - Mac
04:11 - and there are a couple of options to
04:14 - install for Mac you can use Homebrew or
04:16 - you can install the binary using curl I
04:19 - believe I installed the binary using
04:21 - curl and I selected the Apple chip and I
04:25 - just ran this command
04:29 - and I didn't validate the binary
04:35 - but you do have to make the binary
04:37 - executable by running this command
04:40 - and of course you're going to need to
04:41 - move the binary to a location that's
04:44 - within your path now I'm not going to
04:46 - get into details on how to do this this
04:48 - documentation is pretty detailed and
04:50 - installing binaries and adding them to
04:53 - your path isn't within the scope of this
04:54 - video so if that's a little bit too
04:56 - advanced for you you can just use the
04:58 - Homebrew installation which pretty much
05:00 - automates all of this for you
05:03 - and once you've finished with the
05:04 - installation you should be able to type
05:06 - Cube CTL or cube cuddle or whatever you
05:09 - like to call it
05:11 - and if you type that you should get the
05:14 - output that provides information about
05:16 - this command line utility
05:19 - now following the installation of
05:21 - cubecto you can move on to installing
05:24 - minicube and minicube is a local
05:26 - kubernetes focusing on making it easy to
05:29 - learn and develop for kubernetes so
05:31 - basically this is going to allow us to
05:34 - have a kubernetes cluster on our local
05:37 - machine so this way we can make a
05:40 - microservice architecture on our local
05:42 - machine without having to actually have
05:44 - a kubernetes cluster deployed to like a
05:47 - production environment or something like
05:48 - that so this goes on to explain the
05:51 - requirements to use minicube
05:54 - and the installation instructions are
05:57 - here and you're just going to select for
06:00 - your operating system so I would select
06:03 - Mac OS and I would select arm 64 as the
06:06 - architecture and of course we'll select
06:08 - stable and I used the binary download
06:12 - for this one as well but again if you're
06:14 - not familiar with how to install
06:16 - binaries and configure your path just go
06:19 - with the Homebrew installation
06:22 - and then once it's installed we should
06:24 - be able to start a cluster from our
06:27 - local machine
06:29 - and your Cube CTL will automatically be
06:33 - configured to work with mini cubes so
06:36 - you don't need to do anything there so
06:38 - once mini cube is installed you should
06:39 - be able to do mini Cube start
06:45 - and everything should start up for you
06:47 - and there's actually another thing that
06:50 - I forgot to add to the list of what we
06:52 - need to install so we're going to
06:53 - install this K9s
06:58 - and we're just going to use this to sort
07:01 - of help manage our kubernetes cluster so
07:04 - the installation instructions for this
07:06 - are just here in the documentation you
07:09 - can just do Brew install canines of
07:11 - course if you're on Mac but of course
07:13 - they have the installation instructions
07:14 - for other operating systems as well so
07:17 - just figure out which one of these is
07:19 - best for you
07:21 - and once you've finished installing
07:23 - canines you should be able to type
07:25 - canines in the command line interface
07:27 - and it should pull up our cluster which
07:30 - is just our mini Cube cluster which you
07:32 - can see here and you can quit by just
07:34 - using Ctrl C
07:35 - and let's go ahead and clear this now
07:38 - the next thing that we're going to need
07:39 - to install is python3 more specifically
07:42 - because we're going to use Python 3 to
07:44 - create our first service a very simple
07:47 - authentication Service and all of these
07:49 - services are going to be relatively
07:51 - simple because the focus of this
07:53 - tutorial isn't necessarily the services
07:55 - but how all of the services
07:57 - intercommunicate and how a distributed
08:00 - system is integrated as a whole so we're
08:03 - not going to spend too much time on
08:05 - creating large services for this
08:08 - microservice architecture anyways to
08:10 - download python if you don't already
08:12 - have it you're just going to select for
08:14 - Mac of course you're just going to
08:15 - select this button here and just follow
08:18 - the installation instructions from there
08:20 - and lastly we're going to need to
08:22 - install MySQL because this is going to
08:25 - be the database that we use for our auth
08:28 - service so I installed MySQL using
08:31 - Homebrew so you can just do Brew install
08:33 - MySQL so let's just go ahead and copy
08:35 - this
08:36 - and we can just paste it
08:41 - now as you can see here it says we've
08:43 - installed your mySQL database without a
08:45 - root password and to secure it run
08:47 - mysql's secure installation but we're
08:49 - not going to do any of this because
08:50 - again that's not the focus of this
08:53 - tutorial so yeah in a production
08:55 - environment you're going to want to make
08:57 - sure you're securing your MySQL
09:00 - installation and you're going to want to
09:02 - follow all of the best security
09:04 - practices when working with and
09:06 - installing a database server but in our
09:09 - case we're just going to leave the
09:11 - installation the way it is which again
09:13 - isn't something that you're going to
09:14 - want to do in a production environment
09:15 - this is just so that we can get to the
09:19 - actual meat and potatoes of the tutorial
09:21 - so we can access our mySQL database by
09:26 - just running MySQL with the user root
09:28 - and we don't even need a password so we
09:30 - should just be able to do MySQL U root
09:33 - and this will give us access to the
09:35 - database
09:38 - and I think that's it for the things
09:40 - that we need to install for now we're
09:41 - probably going to need to install some
09:43 - more things later but for our first
09:45 - service I think that that's all that we
09:47 - need so as mentioned before we're going
09:49 - to start with our auth service that's
09:51 - going to be this first service that we
09:53 - create and deploy to our cluster on our
09:56 - local environment and just quick note
09:57 - everything's going to be deployed on our
10:00 - local environment in our mini Cube
10:02 - cluster we're not going to deploy
10:04 - anything to a server I might create
10:06 - another tutorial on how to actually
10:08 - deploy this to a server or to a
10:11 - production like environment but for now
10:13 - we're only focused on the actual
10:15 - architecture so everything's going to be
10:17 - done on our local system within this
10:19 - mini Cube cluster so to start we're
10:22 - going to want to create a directory so
10:24 - we'll make dur and we'll call this dir
10:27 - maybe something like system design
10:32 - and we'll just go ahead and change
10:33 - directory into this system design
10:35 - directory and we're going to be writing
10:38 - code for multiple services and actually
10:41 - all of the services are going to be
10:43 - written in Python and if you aren't
10:44 - familiar with python don't worry I
10:47 - explained enough when writing the code
10:49 - where it shouldn't really matter which
10:51 - language you're most comfortable with
10:52 - and since we're focused on the
10:55 - architecture as a whole we're not
10:57 - necessarily going to be writing any
10:58 - complicated code anyway so anyways we're
11:01 - going to make a directory for Python and
11:03 - this is going to contain our python
11:05 - services and we'll put a source
11:08 - directory but we don't need a bin
11:10 - directory so we can just do let's just
11:13 - make a python directory change directory
11:16 - to python
11:18 - make directory source
11:22 - So within this system design python
11:25 - Source directory we're going to create
11:27 - the directory for our auth service so
11:30 - we'll make their auth and this author is
11:33 - going to contain our auth service code
11:35 - so let's CD auth
11:37 - and from here to start we're going to
11:40 - want to create a virtual environment
11:44 - and we're going to write all of the code
11:46 - for the service in one file that we're
11:48 - going to call server.pi and the reason
11:51 - we're doing this in one file is because
11:53 - it's going to be less than 700 lines of
11:55 - code and like I said before the service
11:57 - is going to be relatively small it's
11:59 - just going to be a very simple auth
12:01 - service and actually I forgot to
12:03 - activate our virtual environment so
12:06 - let's go ahead and do that and we should
12:07 - see that our virtual environment is
12:10 - running if we run this command here and
12:13 - I'm going to need to install a couple of
12:15 - things for my Vim configuration I'll go
12:18 - ahead and install pylent
12:21 - and pip install Jedi let's just go ahead
12:25 - and run this command that they're
12:26 - suggesting to upgrade pip
12:31 - and let's open server.pi and the start
12:35 - of our file is just going to be to
12:37 - import JWT which is Json web token and
12:41 - we're going to get into why we're
12:42 - importing that soon we're going to
12:45 - import date time and Os and we're also
12:49 - going to import from flask
12:52 - import flask and request and we're also
12:56 - going to import from flask
12:59 - mysqldb Imports
13:02 - my SQL so basically this is going to
13:07 - allow us to query our mySQL database
13:10 - this is going to be our actual server
13:13 - we're going to be using flask to create
13:15 - our server and this is going to be what
13:18 - we're going to use for our actual auth
13:21 - we're going to use Json web tokens and
13:25 - date time is so that we can set an
13:27 - expiration date on our token and Os is
13:30 - just going to be used so that we can use
13:33 - environment variables to configure our
13:35 - MySQL connection and you will see what I
13:38 - mean by that soon
13:40 - so let's just go ahead and save this and
13:42 - we're going to need to install a couple
13:44 - of things so let's just cat server.pi so
13:48 - we can see what we need to install and
13:49 - then we can just do pip install I
13:51 - believe JWT is like Pi JWT
13:55 - and we're going to need to pip install
13:57 - flask
14:01 - and pip install flask MySQL DB
14:08 - and we can open this file back up again
14:10 - and to start we're just going to create
14:13 - a server which is just going to be a
14:15 - flask object
14:18 - so we're going to instantiate this flask
14:21 - object and we're going to create a MySQL
14:23 - object which is going to be an instance
14:26 - of this MySQL which we passed the server
14:29 - now for the purposes of this video we
14:31 - don't necessarily need to understand the
14:34 - magic Behind These two lines of code
14:36 - here
14:37 - if we go ahead and save this and we go
14:39 - to the definition here we can get a
14:41 - general understanding of what this flask
14:44 - object is doing but the main thing that
14:46 - we need to be concerned with is once it
14:48 - is created it will act as a central
14:50 - registry for the view functions the URL
14:53 - rules template configuration and much
14:55 - more so basically this is just going to
14:57 - configure our server so that requests to
15:00 - specific routes can interface with our
15:02 - code and this MySQL object is basically
15:05 - just going to make it so that our
15:08 - application can connect to our mySQL
15:10 - database and basically query the
15:12 - database so following this we're going
15:14 - to want to set up our config so our
15:17 - server object has a config attribute
15:20 - which is essentially a dictionary which
15:23 - we can use to store configuration
15:25 - variables so for example we can set the
15:29 - configuration for our MySQL host
15:33 - and we can set it equal to OS dot
15:37 - environ.get which is just going to get
15:40 - the MySQL host from our environment so
15:43 - what do I mean by that so if we save
15:45 - this and we do export MySQL host equals
15:49 - localhost and we go into server.pi
15:54 - this code here this OS dot environ.get
15:57 - MySQL host is going to resolve to
16:00 - localhost that we set in our environment
16:03 - within our shell so if we were to go and
16:06 - print this
16:07 - server.config
16:10 - MySQL host
16:13 - and if we were to just Python 3
16:15 - server.pi
16:17 - you'd see that it prints out the Local
16:19 - Host that we set in our environment
16:21 - variable
16:25 - So This Server is our application
16:29 - and server.config is just the
16:31 - configuration for our server or our
16:33 - application
16:34 - so we're going to create a couple of
16:36 - these
16:40 - all with different variables of course
16:42 - so this one's going to be
16:45 - MySQL user
16:48 - and the same here for the environment
16:51 - and this one will be MySQL password
16:59 - and this one would be MySQL DB
17:05 - and MySQL
17:08 - port
17:09 - and we don't need that one so this is
17:11 - going to be the configuration for our
17:13 - application and these are going to be
17:15 - the variables that we use to connect to
17:17 - our mySQL database and the next thing
17:20 - that we want to do is create our first
17:22 - route
17:23 - which is going to have the path login
17:26 - and the methods for login are just going
17:30 - to be post
17:31 - and this route is going to route to this
17:34 - function login
17:36 - and we can just write the code for the
17:38 - login function we're going to set off a
17:40 - variable called auth equal to request
17:42 - dot authorization
17:46 - and this request is the request that
17:49 - we're importing here and with this
17:51 - authorization attribute provides is the
17:54 - credentials from a basic authorization
17:56 - header so when we send a request to this
17:59 - login route we're going to need to
18:01 - provide a basic authorization header
18:04 - which will contain essentially a
18:05 - username and a password and this request
18:08 - object has an attribute that gives us
18:10 - access to that so once we instantiate
18:13 - this object we'd be able to do
18:15 - auth.username to get the username from
18:18 - the basic authorization header and
18:22 - auth.password to get the password from
18:25 - the basic authorization header
18:28 - and you'll see what I mean when we send
18:30 - the actual request but if we don't
18:32 - provide that header within the request
18:33 - then this auth is going to be none so
18:36 - that means that the request is going to
18:38 - be invalid so we're going to do if not
18:40 - off so if the header doesn't exist
18:43 - within the request we're going to return
18:45 - missing credentials and we're going to
18:49 - turn a status 401 which is just the
18:52 - standard status code that we would
18:53 - return in this case
18:55 - and the next thing that we're going to
18:57 - do is we're just going to check DB for
19:00 - username and password
19:03 - so the way that this login route is
19:05 - going to work is basically we're going
19:08 - to check a database so this off service
19:11 - is going to have its own mySQL database
19:13 - and we're going to check a user table
19:15 - within that database which we're going
19:17 - to create and the user table should
19:19 - contain a username and password for the
19:22 - users that are trying to log in or
19:24 - trying to access the API so actually
19:27 - before we do this part we want to go
19:29 - ahead and create a database and a user
19:32 - table and a user that we can use to
19:34 - access the API so we'll go ahead and
19:37 - save this and let's just clear and we
19:40 - just want to go ahead and create a file
19:42 - called
19:43 - init.sql So within this file we're
19:46 - basically going to create a user for our
19:49 - auth service and we're going to give
19:51 - that user a username and a password and
19:53 - then we're going to create a database a
19:56 - mySQL database called auth which is
19:58 - going to be the database for our auth
20:00 - service and we're going to Grant the
20:02 - user that we create privileges to the
20:04 - database and we're going to create a
20:06 - table within that database called user
20:08 - not to be mistaken with the user that
20:10 - we're creating within the mySQL database
20:13 - and that user table is going to be what
20:15 - we use to store users that we want to
20:18 - give access to our API and I know I'm
20:21 - using the word user a lot and the users
20:24 - that I'm mentioning here aren't
20:25 - interchangeable so let's just go ahead
20:27 - and get into it so I can show you what I
20:29 - mean so first we want to create user now
20:33 - this user is the user for our actual
20:36 - database so this isn't the user that
20:38 - we're trying to give access to our API
20:41 - and we're just going to call it auth
20:42 - user because it's the user for the auth
20:44 - service to access our database
20:48 - and we're going to say identified by and
20:51 - we're going to give it a simple password
20:53 - auth123
20:55 - so this here is creating the user to
20:57 - access the mySQL database so this isn't
21:00 - the user to access the API this is a SQL
21:03 - script so we're basically just writing
21:06 - out some SQL queries and statements in
21:09 - this script to build our database
21:10 - essentially so then we want to create
21:13 - database
21:15 - and we're going to call the database off
21:17 - and we want to give this user up here
21:20 - access to the auth database and all of
21:23 - its tables so we're going to do Grant
21:25 - all
21:27 - privileges on off and all tables to
21:33 - auth user which is the user that we just
21:35 - created at localhost
21:38 - and we want to use the auth database
21:40 - when we create the table and then we'll
21:43 - create a table called user
21:47 - and the primary key is just going to be
21:49 - ID int not null Auto increment
21:55 - primary key
21:57 - and actually there's a typo here
22:00 - Auto increment and the next column is
22:04 - going to just be email and it's going to
22:06 - be varchar and we'll just put the
22:08 - maximum 255 not null
22:11 - and lastly we're going to have a
22:13 - password which is again a parchar
22:18 - and we'll just allow it to be long
22:22 - and after we create the table we just
22:25 - want to insert the user that we're going
22:28 - to use to test our application
22:31 - so we're going to say I'm going to give
22:33 - the user an email and a password
22:35 - and the values are going to be
22:39 - I'll just use my name you can use yours
22:41 - if you want or whatever
22:44 - admin123 for the password
22:46 - and can't forget this in my colon so
22:50 - so this is going to create our initial
22:53 - user
22:54 - or the user that will represent our auth
22:57 - service so basically we're going to use
23:00 - this credential to access the database
23:02 - via our auth service and then we're
23:05 - going to create the database
23:07 - and then we're going to Grant
23:08 - permissions for our auth service to make
23:11 - changes to this auth service database
23:14 - and we're going to go ahead and create
23:16 - the table here as well as our initial
23:18 - user
23:20 - and this is going to be the user that
23:21 - goes into the database which will have
23:24 - access to our off service API
23:28 - so we can just go ahead and save that
23:30 - and now we can just go ahead and run
23:32 - this script to create our initial
23:34 - database
23:35 - so just before we do that we'll just go
23:37 - ahead and
23:39 - go into our database and we can do show
23:44 - databases here and you'll see that we
23:47 - don't have the auth database yet
23:50 - so we can go ahead and exit that and
23:52 - clear and now we're going to do the same
23:55 - thing that we would do to log in but
23:57 - this time we're just going to pass in
23:59 - our init.sql file and it appears we have
24:03 - an error in our syntax near Auto
24:06 - increment primary key
24:08 - so let's go ahead and go back in here
24:11 - and I spelled increment wrong
24:16 - should be increment
24:19 - and let's just try that again
24:22 - and actually it's failing now because
24:24 - just now when we try to run the script
24:26 - the first time it already created the
24:29 - user but then the script failed so we
24:32 - already have this user so it's trying to
24:34 - create the same user again so let's just
24:36 - go ahead and delete their user
24:40 - drop database auth as well
24:43 - and drop
24:45 - user
24:48 - so now we dropped the database and we
24:50 - dropped the user that we created in that
24:52 - script because we want the whole script
24:53 - to run not just part of it so we can go
24:56 - ahead and clear this and let's just run
24:58 - it one more time so on SQL
25:01 - you root
25:04 - innate.sql and the script ran
25:06 - successfully
25:08 - so let's go in here
25:10 - and show databases and you can see now
25:13 - that we have this auth database and if
25:16 - we use auth
25:18 - and we show tables
25:21 - you can see that we have this user table
25:23 - and we can also describe user and it
25:27 - shows all of the fields for the user
25:29 - table which include the primary key
25:31 - which is the ID the email and the
25:33 - password and we can even select all from
25:37 - user
25:39 - oops
25:41 - select all from user and you see that we
25:44 - have the one user that we created in the
25:47 - script I named it my name for the email
25:50 - and admin123 for the password you can do
25:53 - whatever you want for this part as long
25:54 - as you make sure to use those
25:56 - credentials when you actually try to
25:58 - make requests to our API
26:00 - so we can go ahead and exit that and now
26:03 - we can go back into writing our code for
26:05 - our server.pi file so here we're going
26:08 - to check the DB for the username and the
26:11 - password that we pass in our basic
26:14 - authorization header in our request to
26:16 - this login endpoint and we're going to
26:18 - do that by making use of this flask
26:21 - MySQL DB here so this is basically just
26:24 - going to allow us to interface with our
26:26 - mySQL database
26:28 - and that's why we added this application
26:31 - config here which has all of the
26:33 - configuration variables to connect to
26:35 - the database that we just created
26:39 - and we're going to need to set
26:41 - environment variables for our host and
26:44 - our user and our password in our
26:46 - database which are all things that we
26:48 - just created
26:49 - and our host is going to be localhost
26:51 - and the port is going to be the default
26:54 - port for MySQL
26:56 - so we'll go here and we're going to
26:59 - create a cursor by doing
27:01 - mysql.connection.cursor
27:06 - and we're going to use that cursor to
27:08 - execute queries so we're going to say
27:10 - the result of the query is going to be
27:13 - equal to
27:14 - cursor.execute and within this execute
27:18 - method we're going to put in our query
27:21 - so we can just do it this way
27:24 - so we'll do select email we want to
27:26 - select the email and the password from
27:29 - our user table where
27:31 - email
27:33 - equals
27:34 - whatever email is getting passed in the
27:37 - request
27:38 - so we're going to do off we're going to
27:40 - take it from that alt dictionary or
27:42 - object we're going to take the username
27:44 - and we need to pass it in as a tuple
27:48 - so remember this auth object here gives
27:50 - us access to the username and the
27:52 - password from the basic authorization
27:54 - header
27:56 - so what we're doing here is we're
27:58 - selecting from our user table in our
28:01 - auth database the email that's equal to
28:04 - the username that's passed into this
28:06 - basic authorization header so we're
28:08 - going to be using email for our username
28:13 - and if the user exists within our
28:15 - database then we should have a result so
28:18 - if result is greater than zero because
28:20 - results going to be an array of rows I
28:23 - believe so if result is greater than
28:25 - zero then that means that we have at
28:27 - least one row with that username and in
28:30 - this situation we should only have one
28:31 - row with that username because the
28:34 - username should be unique but actually I
28:37 - forgot to set the column for the
28:39 - username to be unique so maybe we should
28:41 - go ahead and do that
28:44 - so we should be able to just go over
28:46 - here and add unique for the email and
28:50 - then save it and once again we need to
28:53 - drop our database
28:55 - we're going to drop the user
28:59 - and we're also going to drop the
29:01 - database
29:04 - and now once again we can run MySQL U
29:07 - root init.sql okay so back into our
29:12 - server.pi file
29:14 - so anyways at this point if resulted is
29:17 - greater than zero then that means the
29:18 - user exists within our database
29:22 - and if that's the case we're going to
29:24 - set the row that contains our user data
29:27 - to
29:28 - cursor.fetch one
29:30 - and this is basically going to resolve
29:32 - to a tuple which is going to contain our
29:35 - email so we'll do user row 0 that's
29:39 - going to be the email and our password
29:41 - which is going to be
29:44 - user Row one
29:46 - and next we just want to check to see if
29:49 - the username and the password returned
29:52 - in the row is equal to the credentials
29:54 - passed in the request and if that's not
29:56 - the case we'll say that the credential
29:58 - is invalid and if it is the case then
30:00 - we're going to return a Json web token
30:03 - so we'll say if auth.username not equal
30:06 - to email or auth.password because we
30:11 - need both of them to be equal to
30:14 - what we get from the database not equal
30:16 - to password then if that's the case
30:19 - we're going to return invalid
30:21 - credentials and we're going to return a
30:24 - 401 status code
30:26 - else will return and we're going to
30:29 - create this function we haven't created
30:31 - it yet but we'll return the results of
30:33 - the function called create JWT and in
30:37 - this function we'll pass the auth
30:39 - username and we'll need to pass in a
30:42 - secret for the JWT so we'll just have
30:45 - that in our environment
30:48 - and we'll just call it JWT Secret
30:52 - and we're going to pass in true and I'll
30:55 - get to what this means in a second but
30:57 - we're not creating this create JWT
30:59 - function yet so just bear with me for a
31:02 - bit and lastly we get to
31:07 - if result is not greater than zero so if
31:10 - result's not greater than zero then that
31:12 - means the user doesn't exist in our
31:14 - database and if the user doesn't exist
31:16 - in our database then that means the user
31:18 - doesn't have access so we'll just return
31:21 - invalid credentials as well for this one
31:25 - and a 401.
31:33 - and that is going to be it for our login
31:37 - route and the login function so now we
31:40 - want to go ahead and create this create
31:43 - JWT method or create JWT function
31:46 - actually so I'm going to go ahead and
31:49 - explain a little bit about what a JWT
31:52 - actually is first so we're going to go
31:54 - over the overall flow using basic
31:56 - authentication and jwts and I will try
32:00 - to clear up any compiled confusion that
32:02 - you might have up to this point in the
32:04 - tutorial so let's visualize the flow
32:07 - from a top-down perspective to get an
32:09 - overall understanding of what our code
32:12 - is doing so as mentioned before our
32:15 - micro services are going to be running
32:17 - in a kubernetes cluster and that
32:19 - clusters internal network is not going
32:21 - to be accessible to or from the outside
32:24 - world or the open internet our client is
32:27 - going to be making requests from outside
32:30 - of the cluster with the intention of
32:32 - making use of our distributed system
32:34 - deployed within our private kubernetes
32:36 - cluster via our systems Gateway so our
32:39 - Gateway service is going to be the entry
32:42 - point to the overall application and the
32:44 - Gateway service is going to be the
32:46 - service that receives requests from the
32:48 - client and it is also going to be the
32:50 - service that communicates with the
32:52 - necessary internal services to fulfill
32:55 - the requests received from the client
32:57 - our Gateway is also going to be where we
33:00 - Define the functionality of our overall
33:02 - application for example if we want to
33:05 - add functionality to upload a file we
33:08 - need to Define an upload endpoint in our
33:10 - Gateway service source that initiates
33:13 - all of the necessary internal services
33:15 - to make that happen so if our internal
33:18 - Services live within a private Network
33:20 - how do we determine when we should allow
33:23 - requests in from the open internet this
33:25 - is where our auth service comes in we
33:28 - can give clients access to our
33:30 - application by creating credentials for
33:32 - them with in our office database any
33:35 - user password combination that exists
33:37 - within our MySQL DBS user table is a
33:41 - user password combination that will be
33:43 - granted access to our application's
33:45 - endpoints this is where the
33:47 - authentication scheme called basic
33:49 - authentication or basic access
33:51 - authentication comes in this
33:53 - authentication scheme requires the
33:55 - client to provide a username and
33:58 - password in their request which should
33:59 - be contained within a header field of
34:02 - the form authorization basic credentials
34:05 - where credentials is the base64 encoding
34:08 - of the username and password joined by a
34:11 - single colon in the context of our off
34:14 - flow we are going to make use of this
34:16 - authentication scheme by taking the
34:18 - username and password from the
34:20 - authorization header when a client sends
34:22 - a request to our login endpoint and
34:25 - comparing them to what we have in our
34:27 - mysqldb if we find a match for the
34:30 - credentials we know that the user has
34:32 - access so we will return a Json web
34:34 - token to the client which the client
34:36 - will use for subsequent requests to our
34:39 - gateways upload and download endpoints
34:41 - which brings us to the other critical
34:44 - part of our off flow Json web tokens so
34:47 - what are Json web tokens a Json web
34:50 - token is basically just two Json
34:52 - formatted strings and a signature which
34:55 - comprise three parts each part being
34:58 - base64 encoded all three parts are
35:00 - merged together separated by a single
35:03 - dot which is how we end up with
35:05 - something that looks like this but let's
35:07 - break this down so what are these three
35:09 - parts well the first part is the header
35:12 - the header contains a key value pair for
35:14 - both the signing algorithm and the type
35:16 - of token which is of course JWT the
35:19 - signing algorithm is the algorithm that
35:22 - was used to sign the token which will
35:24 - allow us to later verify that the sender
35:26 - of the token is who it says it is and to
35:29 - ensure that the message wasn't changed
35:31 - along the way now there are both
35:33 - asymmetric signing algorithms with two
35:35 - keys a public and private key and there
35:38 - are symmetric signing algorithms which
35:40 - use just one private key we aren't going
35:42 - to go into detail about signing
35:44 - algorithms because it is not within the
35:46 - scope of this course but what you do
35:48 - need to know is that our auth service is
35:50 - going to be using the symmetric signing
35:52 - algorithm
35:53 - hs-256 for example our auth service is
35:57 - going to be the only entity that knows
35:59 - our single private key and when a user
36:02 - logs in using basic auth or auth service
36:04 - will create a JWT or Json web token and
36:08 - sign it using that private key that JWT
36:11 - will then be returned to the user or the
36:14 - client that way when the user makes
36:16 - following requests to our API it will
36:19 - send its JWT in the requests and our
36:22 - auth service can validate the token
36:24 - using the single private key if the
36:26 - token has been tampered with in any way
36:28 - or was signed using another key then our
36:31 - auth service will know that the token is
36:33 - invalid it's important that we know that
36:36 - the Json formatted data in the token
36:38 - hasn't been tampered with because that
36:40 - data is going to contain the access
36:42 - permissions for the user so without this
36:44 - signing algorithm if the client were to
36:46 - alter the Json data and upgrade its
36:49 - permissions to increase its permissions
36:51 - allowing itself access to resources that
36:53 - shouldn't be available to that
36:55 - particular user then at that point our
36:57 - entire system would be compromised this
37:00 - brings me to the next part of our JWT
37:03 - the payload the payload contains the
37:05 - claims for the user or the bearer of the
37:08 - token what are claims you ask or maybe
37:10 - you didn't ask but I'll explain it
37:12 - anyway simply put claims are just pieces
37:15 - of information about the user for the
37:17 - most part these claims are defined by us
37:19 - although there are predefined claims as
37:22 - well for things like the issuer of the
37:24 - token the expiration of the token Etc
37:26 - the claims that we're going to Define on
37:28 - our own are who the user is for example
37:31 - the username and whether or not the user
37:34 - has admin privileges which in our case
37:36 - is just going to be true or false now
37:38 - the last part of the token is the
37:40 - signature the signature is created by
37:43 - taking the base64 encoded header the
37:46 - encoded payload and our private key and
37:49 - signing them using the signing algorithm
37:51 - which would in our case be hs256 at the
37:55 - end of all this we are left with a token
37:57 - that looks like this and now whenever
38:00 - the client makes requests to our API and
38:02 - provides this token within the request
38:04 - we can determine if the client's token
38:06 - was indeed signed with our private key
38:08 - and our signing algorithm and if so we
38:11 - can determine the client's access level
38:13 - by checking the claims in the payload
38:15 - portion of the token so in our case
38:17 - we're simply going to allow the client
38:19 - access to all of our endpoints if a
38:22 - claim that we're going to call admin has
38:24 - a value of true when we decode the
38:26 - payload portion of the token and that's
38:28 - going to be our off flow so we're going
38:31 - to Define and create JWT and it's going
38:35 - to take in a username a secret
38:38 - and auth's
38:42 - and auth is just going to tell us
38:44 - whether or not the user is an
38:46 - administrator so we're going to keep the
38:48 - permissions simple we're going to either
38:50 - have true or false either the user is an
38:53 - administrator or the user isn't an
38:55 - administrator
38:57 - and this is just going to return JWT dot
39:00 - encode now this JWT comes from up here
39:04 - we're importing this JWT here and it's
39:07 - from this Pi JWT module
39:11 - So within this in code we're going to
39:13 - need to pass a dictionary containing our
39:16 - claims a secret and an algorithm so
39:19 - we'll start with the dictionary
39:22 - it's going to contain username and the
39:24 - username is going to be the username
39:26 - that we pass into the function and it's
39:28 - going to have expiration and expiration
39:31 - is going to be date time dot date time
39:35 - dot now and time zone is going to be
39:39 - equal to date time Dot
39:42 - timezone.utc
39:45 - and we'll just continue this on the next
39:47 - line and we're going to add that to date
39:49 - time dot time Delta
39:53 - days
39:54 - equals one
39:56 - this is just going to set the expiration
39:58 - of this token to one day so this token
40:01 - is going to expire in 24 hours
40:05 - and this IAT is just issued at so like
40:08 - this is when the token is issued so
40:10 - we're just going to do date time
40:12 - date time
40:15 - UTC now
40:17 - and actually we could have done the same
40:19 - thing above for it now but we'll just
40:21 - leave it
40:22 - and then we're going to have whether or
40:23 - not this user is an administrator
40:27 - and that's just going to be
40:30 - the bull that we pass here for alts
40:34 - and next we need to pass in the secret
40:36 - and we also need to pass in the
40:39 - algorithm and this is basically just the
40:42 - signing algorithm for our JWT
40:46 - and we'll just use hs256
40:50 - which I believe is the default but a
40:53 - little verbosity never hurt anybody
41:01 - so actually
41:05 - so now we have our login route and our
41:09 - login function and we have the function
41:12 - to create the Json web token
41:17 - so essentially the flow is going to be a
41:20 - user is going to make a request to our
41:23 - login route using his or her credentials
41:26 - a username and a password and then we're
41:29 - going to check to
41:31 - see if that user data exists within our
41:34 - database if it does then we can consider
41:36 - the user to be authenticated and we'll
41:39 - return a Json web token which is going
41:42 - to be used by that user to make requests
41:45 - to the API and the endpoints that that
41:48 - user will have access to will be
41:50 - determined by this permissions here so
41:53 - whether or not the user is an admin
41:55 - we're going to keep it simple if the
41:57 - user is an admin will make the user have
41:59 - access to all the endpoints and yeah
42:01 - that's going to be the flow for logging
42:04 - in so we also need to create an endpoint
42:07 - for this auth service to actually
42:09 - validate the jwts
42:15 - so the way that we're going to do it is
42:17 - we're going to basically we're using
42:20 - this secret when we create the JWT and
42:23 - that same secret is going to be used to
42:25 - actually decode the token as well so
42:28 - that's how we know that this is a valid
42:30 - token for our API okay so while we're
42:33 - down here let's just go ahead and
42:36 - configure our entry point so we'll just
42:39 - do if name equals Main and all this
42:44 - means is basically when we run this file
42:48 - using the python command then this name
42:51 - variable will resolve to Main
42:54 - so let's just leave here and clear this
42:56 - so if we do Python 3 server.pi whenever
43:00 - we run this file this way the name
43:01 - variable resolves to Main
43:05 - so if we go back in here
43:07 - we can actually just print name
43:12 - and if we run this we'll see that the
43:15 - result is Main
43:17 - so that's all this is for so whenever we
43:20 - run our file using the python command we
43:23 - want our server to start so we'll do
43:25 - server run and we want our server to run
43:28 - on Port 5000 and we want to configure
43:32 - the host parameter like so which
43:34 - essentially is going to allow our
43:37 - application to listen to any IP address
43:41 - on our host
43:45 - so essentially if we don't set this host
43:48 - parameter like so the default is going
43:50 - to be localhost which means that our API
43:53 - wouldn't be available externally
43:55 - and as you can see here in this flask
43:58 - documentation configuring our host
44:01 - parameter this way tells our operating
44:04 - system to listen on all public IPS
44:06 - otherwise the server is only accessible
44:09 - from our own computer or from localhost
44:12 - so let me try to explain what I mean by
44:15 - that so basically any server is going to
44:18 - need an IP address to allow access from
44:21 - outside of the server so in our case our
44:24 - server would be a Docker container and
44:27 - our application will be running within
44:29 - that container when we spin up our
44:31 - Docker container it will be given its
44:34 - own IP address we can then use that IP
44:37 - address to send requests to our Docker
44:40 - container which in this case is our
44:42 - server and keep in mind when I'm
44:44 - referring to an IP address assigned to a
44:47 - Docker container I'm referring to the IP
44:49 - address assigned to that container
44:51 - within a Docker Network so when we spin
44:54 - up our Docker container it will be given
44:56 - its own IP address and we can use that
44:59 - IP address to send requests to our
45:01 - Docker container which in this case is
45:04 - our server but that alone isn't enough
45:06 - to enable our flask application to
45:09 - receive those requests we need to tell
45:11 - our flask application to listen on our
45:14 - container's IP address so that when
45:16 - request gets into our containers IP our
45:19 - application can receive those requests
45:22 - so this is where the host config comes
45:24 - in the host is the server that is
45:27 - hosting our application in our case the
45:30 - server that is hosting our flask
45:31 - application is the docker container that
45:34 - it is running in so we need to tell our
45:36 - flask app to listen on our Docker
45:39 - containers IP address but a Docker
45:42 - container's IP address is subject to
45:44 - change so instead of setting it to the
45:46 - static IP address of our Docker
45:48 - container we set it to this
45:50 - 0.0.0.0 IP address which is kind of like
45:54 - a wild card that tells our our flask app
45:56 - to listen on any and all of our Docker
45:59 - containers IP addresses that it can find
46:01 - if we don't configure this it will
46:03 - default to just localhost which is the
46:06 - loopback address and localhost is only
46:09 - accessible from within the host
46:10 - therefore outside requests sent to our
46:13 - Docker container would never actually
46:15 - make it to our flask app because the
46:18 - loopback address isn't publicly
46:19 - accessible so when we set host to
46:23 - 0.0.0.0 we are telling our flask app to
46:26 - listen on all of our Docker containers
46:28 - IPS including the loopback address or
46:31 - localhost and any other IP address
46:33 - available on the docker container for
46:35 - example if we connect our Docker
46:37 - container to two separate Docker
46:39 - networks Docker will assign a different
46:41 - IP address to our container for each
46:44 - Docker Network that means that with the
46:47 - 0.0.0.0 host configuration our flask app
46:51 - will listen to requests coming to both
46:53 - of the IP addresses assigned to the
46:55 - container here it's also possible to set
46:58 - the host config to a specific IP address
47:00 - that way our flask app will only listen
47:03 - to requests going to that IP address and
47:05 - would no longer listen to requests going
47:07 - to localhost or the other IP address
47:10 - from the other Docker Network
47:12 - so now that we've finished creating our
47:14 - login route we want to actually create
47:17 - another route to validate jwts
47:21 - and this route is going to be used by
47:24 - our API Gateway to validate jwt's synth
47:28 - within request from the client to both
47:31 - upload and receive or download MP3s or
47:36 - to upload videos and download the MP3
47:38 - version of those videos and you'll see
47:40 - what I mean by that later on in the
47:42 - tutorial when we actually start to
47:44 - implement that so we're going to do
47:47 - another route and this one's going to be
47:50 - validate
47:51 - and methods are going to be
47:54 - post
47:56 - and we're going to define a function
47:58 - called validate
47:59 - and we're going to want to pull the
48:01 - encoded JWT from our request and we're
48:05 - going to require the JWT to be in a
48:08 - headers or a header called authorization
48:13 - and if the JWT is not present in the
48:16 - authorization header we want to return
48:18 - an error
48:20 - so we'll say encoded JWT if not encoded
48:23 - JWT we'll just return missing
48:27 - credentials
48:29 - and a 401
48:31 - so if you remember from the explanation
48:34 - of the basic authentication scheme for
48:37 - that scheme we would need to have the
48:39 - word basic in our authorization header
48:41 - that contained our base64 encoded
48:44 - username and password separated by a
48:46 - colon well for our JWT we instead need
48:50 - to have the word Bearer in the
48:52 - authorization header that includes the
48:54 - token so let me quickly go over the
48:56 - format for the authorization header so
48:59 - that you understand what's happening so
49:01 - if we look at the documentation for
49:02 - authorization headers on mozilla.org we
49:06 - see that the format for the header is
49:08 - first type and then credentials here
49:11 - type represents the authentication
49:13 - scheme and credentials represents the
49:16 - credentials necessary specific to that
49:18 - type so from the perspective of the
49:20 - server handling the authorization header
49:23 - the type tells us what type of
49:25 - credential is contained within the
49:27 - header so if the type is basic from the
49:30 - server perspective we know that we are
49:32 - dealing with a credential which is a
49:34 - base64 encoded username and password
49:36 - separated by a colon if the type is
49:40 - Bearer we know that we are dealing with
49:42 - a bearer token which essentially means
49:44 - that we can assume the party in
49:46 - possession of the token or the bearer of
49:48 - the token has access to the tokens
49:51 - Associated resources now in the code for
49:54 - our validation endpoint to save some
49:57 - time we're just going to assume the
49:59 - authorization header contains a bearer
50:01 - token therefore we aren't going to check
50:03 - or validate the word that represents the
50:06 - type that comes before the credential in
50:08 - the header but in an actual production
50:10 - environment you are definitely going to
50:12 - want to spend the extra time to check
50:13 - the type or the authentication scheme
50:16 - present within the authorization header
50:19 - and within this authorization header
50:21 - we're going to require the token to be
50:23 - formatted as bear authentication so
50:26 - basically we're going to want the header
50:28 - that's synt with the JWT to look like
50:30 - this so it's going to have this Bearer
50:33 - as part of the string and then the token
50:35 - and as a result of that if the encoded
50:39 - JWT is present we're going to need to
50:42 - split the string so we're going to set
50:44 - encoded JWT equal to encoded JWT dot
50:48 - split and we're going to need to split
50:51 - it based on a space because there's
50:54 - going to be the word bear
50:56 - and then a space and then there's going
50:58 - to be the token so the array that
51:00 - results from this split is going to have
51:02 - the item with the word bear and it's
51:04 - going to have an element with the token
51:06 - so we're going to need the first index
51:09 - or the item at the first index of the
51:12 - array not the zero
51:13 - and then we're going to try and we're
51:17 - going to do decoded equals JWT dot
51:21 - decode
51:24 - and to this decode method we're going to
51:27 - need to pass the encoded JWT and we also
51:30 - need to pass our JWT secret the one that
51:33 - was used when we actually encoded the
51:36 - JWT which is going to be in an
51:38 - environment variable
51:43 - and we're also going to need the
51:45 - algorithm and the algorithm that we used
51:48 - was hs256
51:52 - and if that fails
51:54 - we're just going to return not
51:57 - authorized
51:59 - and a 403. but if it doesn't fail we
52:02 - will return the decoded token
52:06 - and a 200. and that's pretty much going
52:10 - to be it for our auth service so we can
52:13 - just
52:14 - go ahead and save this
52:23 - so now we have our actual service and we
52:27 - have our init script for our database
52:29 - and now we're going to need to start
52:31 - writing all of our infrastructure code
52:33 - for the actual deployment so we're
52:35 - basically going to deploy all of our
52:37 - services within a kubernetes cluster as
52:40 - you already know so we need to create
52:42 - Docker images that we're going to push
52:45 - to a repository and our kubernetes
52:48 - configuration is going to pull from the
52:51 - repositories for our Docker images and
52:54 - create our deployments within our
52:55 - cluster and I know that this sounds kind
52:57 - of confusing but we're going to walk
52:59 - through everything step by step so don't
53:01 - worry so we're going to start by making
53:03 - a Docker file and as our base image we
53:06 - want to use a python image so we'll use
53:08 - this python 310 slim bullseye
53:15 - and after we write out this Docker file
53:17 - I'll go over what all the lines mean in
53:20 - a little bit more detail but for now I'm
53:22 - just going to vaguely go over what we're
53:23 - doing so first we're going to run our
53:27 - apt-get update
53:30 - and we also need to apt-kit install a
53:33 - couple of dependencies
53:47 - so we're going to need build Essentials
53:50 - and default
53:52 - live MySQL
53:55 - client Dev
53:57 - and then we want to pip install upgrade
54:01 - pip
54:07 - and following that we want to set our
54:10 - working directory to app and we want to
54:13 - copy first just our requirements dot txt
54:17 - which we haven't created yet
54:21 - and the reason we're copying this
54:23 - separately from the rest of the
54:25 - application is because we want to make
54:27 - sure our requirements are in a separate
54:30 - layer so that if our application code
54:32 - changes we can still use the cached
54:35 - requirements layer we don't need to
54:36 - reinstall or recreate the layer and I'll
54:39 - probably explain that in a little bit
54:41 - more detail later on
54:43 - so then we want to run pip install our
54:46 - requirements
54:56 - and then we can copy over the rest of
54:58 - our application
55:00 - and our app is going to be running on
55:03 - Port 5000 so we'll expose that port and
55:06 - finally we need to create the command
55:08 - which is going to be python3
55:12 - server.pi
55:14 - so it's the same as when we're actually
55:17 - running this python3 server.pi from the
55:19 - command line
55:21 - and that is it for that so we'll save
55:24 - that now let's get into explaining the
55:26 - contents of our Docker file so basically
55:29 - when we build a Docker image we're
55:32 - building it on top of a base image which
55:34 - in our case is this base image python
55:37 - 310 slim Bullseye so we can think of an
55:40 - image as a file system snapshot for
55:43 - example the base image that we are
55:45 - building our image on top of is
55:47 - essentially a snapshot of a file system
55:49 - that contains all of the necessary
55:51 - dependencies to run python applications
55:54 - for instance we wouldn't be able to run
55:56 - a DOT Pi file on an OS that doesn't have
55:59 - python installed right so a base python
56:02 - image will have things pre-installed so
56:04 - that we don't need to worry about that
56:06 - so based on that understanding let's go
56:09 - a little deeper it's important to keep
56:11 - in mind when writing Docker files that
56:13 - each instruction in a Docker file
56:15 - results in a single New Image layer
56:17 - being created that means that the next
56:19 - instructions image layer will be built
56:21 - on top of the previous instructions
56:24 - image layer so that means that this from
56:26 - instruction creates a layer and then
56:28 - this run instruction creates a new layer
56:31 - on top of the previous layer that was
56:33 - built from the from instruction and this
56:35 - continues on until we reach the end of
56:38 - our Docker file this is important to
56:40 - understand because if we need to build
56:42 - our Docker image again Docker is smart
56:45 - enough to use cached image layers if
56:48 - nothing within the layer has changed and
56:50 - none of its preceding layers have
56:51 - changed so what do I mean by that so
56:54 - let's say that the dependencies for our
56:56 - application change resulting in our
56:58 - requirements.txt file changing when we
57:01 - rebuild the image we won't need to
57:03 - rebuild every layer again we only need
57:06 - to rebuild the layer that changes and
57:08 - every layer after it because of course
57:10 - every layer after it is based on its
57:13 - preceding layer therefore if the
57:14 - preceding layer changes so does it and
57:17 - the reason it's important to understand
57:18 - this is because optimizing your Docker
57:21 - file to use cached layers efficiently
57:23 - will significantly decrease the build
57:25 - time of your image and that might not
57:27 - seem so beneficial in this context but
57:30 - when we are talking about deploying
57:32 - production applications using CI CD
57:34 - pipelines the build speed is something
57:36 - that we want to consider now if you
57:38 - don't know what a CI CD pipeline is
57:40 - don't worry it's not necessary to
57:42 - understand that for this tutorial
57:43 - anyways with the understanding that each
57:46 - instruction results in its own layer and
57:49 - that if one layer changes every layer
57:50 - after that layer will also need to be
57:52 - rebuilt we can see why it's beneficial
57:54 - to separate the copy instructions for
57:57 - our requirements.txt from the copy
58:00 - instructions of the rest of our
58:01 - application because with this
58:03 - configuration as you can see if the
58:06 - dependencies for our application change
58:08 - resulting in our requirements.txt file
58:11 - changing we need to create a new layer
58:14 - to build onto with the new requirements
58:16 - being installed in this run pip install
58:19 - layer and of course every layer after
58:21 - that layer will need to be rebuilt as
58:23 - well but if we only make a code change
58:26 - and our requirements don't change we
58:28 - don't want to have to build the layer
58:30 - that installs our dependencies again
58:32 - because this is probably one of the most
58:34 - time consuming layers to build so as you
58:37 - can see we are copying the rest of our
58:39 - application to the app directory here
58:41 - and by the way this dot just means the
58:43 - current directory that we ran the docker
58:46 - builds command in so if we copy dot to
58:49 - our app directory which is our working
58:51 - directory or copying everything in the
58:54 - directory where we ran the docker build
58:56 - command in on our local machine in other
58:59 - words the directory that contains our
59:01 - Docker file on our local machine and if
59:04 - any of the code has changed since our
59:06 - source files are contained within that
59:08 - directory Docker will detect that there
59:11 - was a change and rebuild this layer and
59:13 - every layer following this layer will
59:14 - need to be rebuilt as well but as you
59:16 - can see the layers following this layer
59:18 - don't include the time consuming pip
59:20 - install command so that's just a quick
59:23 - example of why optimizing your Docker
59:25 - file to be more layer efficient is
59:27 - beneficial so now back to the rest of
59:29 - our instructions so after we build our
59:32 - base layer using the python 310 slim
59:35 - Bullseye image we then move on to
59:38 - installing our OS dependencies and all
59:41 - of these flags in purple here are to
59:43 - avoid installing unnecessary additional
59:45 - packages as well as avoiding taking up
59:48 - additional space with caching and stuff
59:50 - like that because we want our container
59:52 - to be as light as possible and we also
59:54 - don't want to introduce potential
59:56 - vulnerabilities present on packages that
59:58 - we don't even need and the reason we are
60:01 - combining all of these commands into one
60:03 - command is so that we can keep them all
60:05 - in the same run instruction therefore
60:07 - keeping them contained to one image
60:09 - layer because remember every Docker
60:12 - instruction creates a new image layer
60:13 - and then here we're just creating a
60:16 - directory to work in and this dirt is
60:19 - where our application source is going to
60:21 - live and these instructions we've
60:23 - already gone over and this expose
60:26 - instruction doesn't actually do much of
60:28 - anything other than serve as
60:30 - documentation to anybody that builds
60:32 - this image and it essentially lets them
60:34 - know what port is intended to be
60:36 - published so our app listens on Port
60:39 - 5000 so that is the port that we have
60:41 - here in the expose instruction and
60:44 - lastly we have our Command instruction
60:46 - and this instruction is the instruction
60:48 - that is going to be used to run our
60:50 - container this instruction sets the
60:53 - command to be executed when running the
60:55 - image so for example when we run our
60:57 - image the Python 3 command will be run
61:00 - on our server.pi file which is going to
61:03 - run our auth server in this case
61:05 - okay now let's go ahead and build this
61:08 - Docker file
61:15 - oh and I forgot to create our
61:18 - requirements.txt so we're going to do uh
61:21 - pip 3 freeze and we're going to freeze
61:23 - our requirements the current
61:25 - requirements for our application into a
61:27 - file called requirements.txt and if we
61:30 - go into this file you see it has all of
61:33 - the requirements that we needed to
61:35 - install for our application like it has
61:37 - this MySQL DB that we're using and of
61:39 - course flask and any of their
61:42 - dependencies as well
61:44 - so doing pip 3 freeze it basically
61:47 - freezes our dependencies into a file so
61:50 - that we know what dependencies we need
61:52 - to install to run this application
61:54 - so let's go ahead and try this again
62:00 - and now it says unable to locate package
62:03 - build Essentials so let's go back into
62:06 - our Docker file and that is a typo it's
62:09 - actually just build essential
62:12 - and let's try again
62:21 - foreign and now our image is finished
62:25 - being built so once we've finished
62:27 - building our image we actually want to
62:29 - create a Docker registry or a repository
62:32 - so you can just go to hub.docker.com
62:39 - and create an account here I already
62:42 - have an account so I will just sign in
62:44 - okay so once you've successfully created
62:47 - an account and logged into your Docker
62:49 - Hub account you should end up at a page
62:51 - that looks like this and from here you
62:53 - just want to click this repositories Tab
62:56 - and what we're going to do is we're
62:58 - going to create the repositories that
63:00 - we're going to push our container images
63:02 - to and then our kubernetes configuration
63:06 - is going to pull from this repository
63:08 - the repositories that we create for each
63:11 - individual service now your repositories
63:13 - are going to have the same suffix as
63:16 - mine because we're going to create the
63:19 - repository name using the same suffix
63:21 - but the prefix to your repository is
63:23 - going to be different mine's going to
63:25 - have this prefix and yours is going to
63:27 - be whatever the name of your account is
63:30 - so for example if we create a repository
63:33 - here
63:34 - and we name the repository auth because
63:37 - it's going to be the repository for our
63:39 - auth services images when we actually
63:41 - push to this repository from our command
63:44 - line I'm going to push to sweezytech
63:47 - auth and you're going to push to
63:48 - whatever your username is for your
63:51 - account and you'll see what I mean by
63:53 - that in a second so we're going to go
63:54 - ahead and create this repository for our
63:56 - auth services images and we're just
63:58 - going to make it public because we only
64:00 - can make one private one and then if we
64:02 - do a private one we're going to have to
64:04 - configure credentials within minicube
64:06 - which is a little bit more complicated
64:08 - so we're just going to do public and
64:10 - throughout this tutorial it will be
64:12 - possible for you to push and pull from
64:15 - my repository which can cause lots of
64:17 - issues for you as you follow along with
64:20 - this tutorial because my images may or
64:22 - may not be in the same state that they
64:24 - are at the part of the tutorial that
64:26 - you're on so just make sure you take the
64:29 - time to create this account and create
64:30 - your own repositories so we can just go
64:33 - ahead and create this and now as you can
64:35 - see here it tells you how to push to
64:37 - this repository and as you can see this
64:40 - is going to look different for you than
64:42 - it does for me specifically this part is
64:45 - going to look different for you
64:48 - so anyways now what we want to do is we
64:51 - want to tag the image that we just
64:53 - created here we just built an image
64:55 - using this Docker builds command it
64:57 - built the image based on our Docker file
64:59 - that we just created now we want to tag
65:02 - this image so we'll do Docker tag and
65:05 - we're going to just use this Shaw here
65:11 - you don't actually need to put in the
65:13 - whole thing but I'm just going to put in
65:14 - the whole thing and you're just going to
65:16 - tag it using your username from your
65:18 - Docker Hub account
65:19 - slash auth and then we're going to tag
65:22 - it as latest because it's going to be
65:24 - the most recent version of our image
65:27 - now if we do Docker image LS you can see
65:31 - that we have our tag here and you can
65:34 - compare this part of the image ID to the
65:36 - first part of this shot and just ignore
65:39 - all of these other images that I have
65:41 - you'll likely have whatever images you
65:44 - have on your system as well but none of
65:46 - that matters as long as you have this
65:48 - image tag here then you're fine so let's
65:51 - just go ahead and clear that and now
65:52 - that we've tagged it we can push it to
65:55 - our repository so we'll just do Docker
65:57 - push and again you're just going to use
66:01 - your username for your Docker Hub
66:03 - account and then auth and then latest
66:05 - and then just push that
66:12 - and once that's finished you can go to
66:15 - the repository and you can just refresh
66:18 - the page
66:21 - and you should see your image tag here
66:24 - so that means we've successfully pushed
66:27 - this image to our repository and now
66:30 - whenever we want to pull this image we
66:32 - could just do Docker pool and we could
66:34 - just do
66:38 - the name of our image and the tag or the
66:41 - URL for our image in the tag
66:43 - and we'd be able to pull it but that's
66:46 - not actually how we're going to be
66:48 - pulling these images our kubernetes
66:50 - configuration is actually going to be
66:51 - pulling the images so let's go ahead and
66:53 - clear
66:54 - and now we're going to make a directory
66:57 - called Manifest this directory is going
66:59 - to contain all of our kubernetes
67:01 - configurations so let's change directory
67:04 - to manifest
67:06 - and for all of these configuration files
67:09 - I'm going to go over them in detail
67:11 - after we write them out so if you're
67:13 - confused about the infrastructure code
67:15 - that we're writing just hang in there so
67:18 - all of our configuration files are going
67:19 - to be yaml files and we'll start with a
67:22 - file called off deploy.yaml and within
67:26 - this file we'll do API version apps V1
67:31 - foreign
67:32 - is deployment now again this is the
67:36 - configuration for our kubernetes cluster
67:39 - and our service and if you're not
67:40 - familiar with kubernetes a lot of these
67:42 - configurations you probably won't
67:44 - understand I will try to go into more
67:47 - detail for these in a little bit so
67:50 - we'll do metadata name of our service or
67:54 - our deployment is going to be off
67:56 - and labels
67:58 - app is going to be auth as well
68:02 - and then we're going to do our spec
68:04 - in replicas we're going to want two
68:07 - replicas or two instances of our service
68:10 - and selector we're going to match labels
68:14 - and the app will be off
68:17 - strategy
68:19 - type
68:20 - rolling update and the configuration for
68:24 - Rolling update
68:25 - we're going to do Max surge equals three
68:29 - then we want to do template
68:32 - metadata for template labels
68:36 - app off
68:39 - and we'll do spec again
68:42 - containers
68:45 - name auth and this is where we're going
68:48 - to configure it to pool our image we're
68:51 - going to set image to
68:54 - remember this should be your username
68:56 - and then auth
68:58 - and then ports
69:00 - the container port
69:03 - is going to be 5000 because our
69:06 - application is running on Port 5000 so
69:08 - we'll just do the same for container
69:10 - Port as well and then we want to get our
69:12 - environment variables from config map
69:15 - file we're going to create this file
69:17 - after this so we're going to do config
69:19 - map ref and we're going to name the
69:23 - config map that we're going to create
69:24 - auth config map and again we haven't
69:27 - created this yet we're going to create
69:28 - it soon and here we're going to do
69:31 - Secret ref and we're going to store our
69:34 - secrets in a secret we're going to name
69:37 - it auth secret
69:40 - and we're going to create this file as
69:42 - well
69:44 - and let's just check formatting and
69:47 - looks fine
69:50 - so we can just go ahead and save that so
69:52 - now let's create the config map so we'll
69:55 - just do Vim
69:57 - configmap.yaml this config map is going
69:59 - to set environment variables within our
70:02 - container so for this one we'll do API
70:04 - version
70:06 - B1 kind config map
70:10 - metadata name is going to be auth config
70:13 - map and data are going to be our
70:16 - environment variables so we'll do MySQL
70:19 - post and since we're going to be using
70:23 - our local MySQL server we're going to
70:26 - need to reference that server from
70:29 - within our kubernetes cluster and
70:31 - luckily minicube gives us a way to
70:33 - access our host the Clusters hosts via
70:36 - this host dot miniq dot internal because
70:40 - basically within the cluster we're kind
70:43 - of in our own isolated Network so since
70:47 - our MySQL server is just deployed to our
70:50 - local host from within the cluster we
70:52 - wouldn't be able to just use localhost
70:54 - we need to access the system that's
70:57 - hosting the cluster and that's what this
70:59 - host mini Cube internal is for
71:02 - so our MySQL user is going to be off
71:06 - user we created and our MySQL DB
71:10 - is going to be auth which we also
71:12 - created and default port for MySQL is
71:16 - 3306 and actually we should do it as a
71:20 - string so these are going to be the
71:22 - environment variables that will
71:24 - automatically be exported within our
71:26 - Shale when we do the deployment so in
71:29 - other words if we were to run the
71:31 - environment command within that
71:33 - container all of these variables and
71:36 - their values are going to be present
71:37 - within the container
71:39 - so that's what this config map file is
71:41 - for and configmap is for environment
71:44 - variables that aren't necessarily
71:46 - sensitive data like passwords so we're
71:48 - also going to need to do a similar file
71:50 - for our secrets or sensitive data like
71:53 - our password to our database and of
71:55 - course in a production environment you
71:57 - would never push your secrets
72:00 - configuration to like a git repository
72:03 - or something because then your passwords
72:05 - would be easily visible at the
72:07 - repository so just keep that in mind
72:09 - when we're creating this file so we're
72:11 - just going to call it secret.yaml
72:14 - and API version once again V1 and this
72:18 - time kind is going to be secret and
72:21 - metadata we're going to do name auth
72:24 - Secret
72:25 - and we're going to do string data
72:28 - and we'll do MySQL password is going to
72:31 - be auth123 and our JWT secret is another
72:36 - secret that we need for this application
72:38 - and we're just going to make it a random
72:40 - name sarcasm
72:42 - and we need to set type to opaque
72:47 - and this is going to be our environment
72:50 - variables for our secrets and we can
72:53 - just go ahead and check if the
72:55 - formatting is okay and save that and
72:57 - lastly we need to create our
73:00 - service.yaml and we'll do API version
73:04 - V1 and kind is going to be service
73:09 - metadata will be name is auth that's
73:13 - going to be the name of the overall
73:14 - service and spec
73:17 - we'll do selector
73:20 - app off and we'll do type
73:24 - cluster IP and this cluster IP basically
73:27 - just means that the IP address assigned
73:30 - to this service is only going to be
73:32 - accessible within our cluster but again
73:35 - I'll go into a little bit more details
73:36 - soon
73:38 - and our Port is going to be 5000
73:42 - our Target Port 5000 as well
73:45 - and protocol is going to be TCP
73:51 - and then we can just save that so once
73:54 - we have all of our info code for our
73:57 - kubernetes deployment we can actually
73:59 - start to deploy this off service to our
74:02 - cluster
74:03 - so let's go ahead and take a look at
74:05 - canines So currently we have nothing in
74:08 - canines there's no cluster and there's
74:10 - no context because our mini Cube isn't
74:13 - running right now
74:14 - so we can actually do
74:17 - any Cube start
74:23 - and once mini cube is started we should
74:25 - be able to go back into canines
74:28 - and if we change the namespace to All by
74:32 - hitting zero
74:34 - here
74:38 - we can see
74:40 - that we have our mini Cube pods running
74:43 - within the cube system namespace
74:47 - and you can see here our cluster is mini
74:49 - Cube so let's go ahead and Ctrl C out of
74:52 - there and we can clear this
74:55 - okay so let me just briefly go over what
74:57 - we're doing here so basically within
75:00 - this manifest directory we wrote the
75:02 - infrastructure code for our auth
75:05 - deployment so if we change directory
75:07 - back to our main directory we wrote the
75:10 - code for our auth service and we created
75:13 - a Docker file to build that source code
75:16 - into a Docker image and we then pushed
75:19 - that Docker image to a repository on the
75:22 - internet and within our manifest
75:25 - infrastructure code
75:30 - we're actually pulling that image from
75:33 - the internet and deploying it to
75:35 - kubernetes and that image contains our
75:38 - code
75:41 - so all of these files within this
75:43 - manifest directory when applied will
75:46 - interface with the kubernetes API which
75:49 - is the API for our kubernetes cluster to
75:52 - interface with our kubernetes cluster so
75:54 - these files are going to interface with
75:56 - that API to create our service and its
75:59 - corresponding resources like its config
76:01 - map and its secret and to do that all we
76:04 - need to do is do Cube CTL
76:07 - apply and then we're going to use this F
76:10 - flag for file and we're just going to
76:12 - apply all the files in the current
76:14 - directory this manifest directory
76:18 - and as you can see here our config map
76:21 - resource was created and our secret was
76:24 - created and our service was created but
76:26 - we actually had an error here for our
76:28 - deployment so it's saying that let's see
76:32 - a known field template
76:34 - so this seems like there's an issue with
76:37 - our auth deploy.yaml file let's go ahead
76:39 - and have a look
76:40 - so a template shouldn't be unknown
76:43 - so the spacing is really important in
76:47 - yaml files so we'll put this back one
76:53 - because strategy is actually part of
76:56 - spec
76:58 - but the way we had it before
77:01 - we had strategy a part of selector
77:04 - so we need to make sure that spacing is
77:06 - correct in these files
77:10 - so we'll go ahead and
77:13 - put this back
77:20 - and let's give it a try now
77:22 - so we can just apply again and it will
77:25 - only apply the files that have changed
77:29 - and now we're getting unknown field name
77:31 - so I'm assuming that it's an issue with
77:34 - the spacing again so let's just go over
77:36 - this
77:40 - so the issue here is that our config map
77:43 - reference the name shouldn't be at the
77:45 - same spacing it should be
77:48 - there and the same for the secret ref
77:51 - name
77:52 - so we can go ahead and save and let's
77:55 - try to apply again and now we were able
77:58 - to create our deployment as well so now
78:02 - that we've created these resources we
78:04 - can go into K9s
78:08 - and we can see that we have two
78:10 - instances of our auth service being
78:14 - created
78:16 - and now both instances are running and
78:20 - if we go into the logs by just pressing
78:22 - enter on first the Pod and then the
78:26 - container we can see the logs within the
78:28 - container and we see that our server is
78:30 - running
78:31 - and that's for both of these replicas
78:35 - and also within canines we can use the
78:38 - shell within the container by just
78:40 - entering on the Pod and then pressing s
78:43 - on the container to access the shell
78:46 - so now we're in a shell within our
78:49 - container and within this shell we can
78:51 - do environment and we can see our
78:54 - environment variables so you can see we
78:56 - have our secret here our MySQL password
78:59 - and we have our MySQL user here actually
79:03 - we can just environment grep MySQL and
79:07 - we can see all of our MySQL environment
79:09 - variables from both our config map and
79:12 - our secret
79:13 - and we can just exit the shell here and
79:16 - leave canines okay so to explain our
79:19 - kubernetes configuration I will need to
79:22 - explain a little bit more about
79:24 - kubernetes in general so throughout this
79:26 - course there's been a lot of mention of
79:28 - deploying our micro services to a
79:30 - kubernetes cluster but what does that
79:33 - actually mean let's first briefly go
79:35 - over what kubernetes is in simple terms
79:38 - kubernetes eliminates many of the manual
79:41 - processes involved in deploying and
79:43 - scaling containerized applications for
79:46 - example if we configure a service to
79:48 - have four pods kubernetes will keep
79:51 - track of how many pods are up and
79:52 - running and if any of the pods go down
79:55 - for any reason kubernetes will
79:57 - automatically scale the deployment so
79:59 - that the number of PODS matches the
80:00 - configured amount so there's no need to
80:03 - manually deploy individual pods when a
80:06 - pod crashes kubernetes also makes
80:08 - manually scaling pods more streamlined
80:11 - for example say I have a service that
80:13 - load balances requests to individual
80:16 - pods using round robin and that service
80:18 - is experiencing more traffic than the
80:20 - number of available pods can handle as a
80:23 - result of this I decide to scale my
80:25 - service up from two to five pods without
80:28 - kubernetes in a situation like this I'd
80:30 - likely need to go manually deploy each
80:33 - individual additional pod and then I'd
80:36 - need to reconfigure the load balancer to
80:38 - include the new pods in the round robin
80:40 - algorithm but kubernetes can handle all
80:43 - of this for you and it's as simple as
80:45 - running this command with this simple
80:47 - command kubernetes will scale up your
80:50 - service which includes maintaining the
80:52 - newly scaled number of PODS if a pod
80:54 - happens to crash and it will auto
80:56 - configure the load balancer to include
80:58 - the new pods basically with kubernetes
81:01 - weekend cluster together a bunch of
81:03 - containerized services and easily
81:05 - orchestrate the deployment and
81:07 - management of these services within the
81:09 - cluster using what we call kubernetes
81:12 - Objects which are persisted did entities
81:14 - in the kubernetes system I know that
81:17 - sounds a bit complicated so let me
81:18 - explain so for this part of the
81:20 - explanation let's go to the kubernetes
81:23 - documentation
81:24 - so it's explained here that a kubernetes
81:26 - object is a record of intent once you
81:29 - create the object the kubernetes system
81:32 - will constantly work to ensure that
81:34 - object exists by creating an object
81:37 - you're effectively telling the
81:38 - kubernetes system what you want your
81:40 - cluster's workload to look like this is
81:43 - your cluster's desired State now this
81:45 - sounds complicated but we've actually
81:47 - already done this multiple times for
81:50 - example we created a deployment object
81:52 - here in this yaml file this file is the
81:55 - above mentioned record of intent we are
81:58 - telling kubernetes that we want this
82:00 - deployment object to exist in our
82:02 - cluster in the state specified in our
82:05 - spec here for example we want two
82:08 - replicas to be deployed once this
82:10 - configuration is applied as explained
82:12 - here the kubernetes control plane
82:15 - continually and actively manages every
82:17 - object's actual state to match the
82:20 - desired State you supplied that means
82:22 - that kubernetes will keep track of the
82:25 - actual status or state of your
82:26 - deployment and make sure that it matches
82:29 - your record of intent in other words
82:31 - your yaml specification
82:33 - so bringing it all together we can say
82:35 - that our kubernetes cluster is comprised
82:38 - of a bunch of objects that we've
82:40 - configured that describe our cluster's
82:42 - intended state from there kubernetes
82:45 - will continually compare the current
82:47 - status or state of those objects to the
82:50 - specification or desired state from our
82:52 - original configuration and if that
82:55 - comparison ever differs kubernetes will
82:57 - automatically make adjustments to match
82:59 - the current status with our original
83:01 - record of intent in other words our
83:04 - original specification so how do we
83:06 - communicate with kubernetes to configure
83:09 - and or create these objects well let's
83:12 - once again take a look at the
83:13 - documentation it's explained here that
83:16 - to work with kubernetes objects whether
83:18 - to create modify or delete them you will
83:21 - need to use the kubernetes API when you
83:24 - use the cube CTL command line interface
83:27 - for example the CLI makes the necessary
83:30 - kubernetes API calls for you so
83:32 - basically the Q CTL CLI that we
83:35 - installed is interfacing with the
83:37 - kubernetes API to essentially run crud
83:40 - operations on our clusters objects in
83:43 - our case we are running our cluster
83:45 - locally using mini Cube so the end point
83:47 - for the kubernetes API in this case is
83:49 - on our local machine but in the real
83:52 - world your cluster will usually be
83:54 - deployed on some server and on your
83:56 - local machine you'll have a kubernetes
83:59 - configuration for the cluster on that
84:01 - server which will enable your local Cube
84:03 - CTL CLI to interface with the remote
84:06 - server but we don't need to go into the
84:09 - details for that in this video so now
84:11 - that we have a general understanding of
84:13 - what kubernetes is and how it is working
84:15 - we can now get into explaining our
84:17 - actual yaml configuration files so if we
84:20 - have a look at the documentation here we
84:23 - see that there are some required fields
84:25 - necessary when creating kubernetes
84:26 - objects using the dot yaml files those
84:29 - are API version kind metadata and spec
84:33 - we can also see a description for each
84:36 - field API version is which version of
84:38 - the kubernetes API we are using to
84:40 - create this object kind is what kind of
84:43 - object we want to create for example
84:46 - deployment config map secret
84:49 - Etc metadata is just data that helps
84:52 - uniquely identify the object and lastly
84:55 - spec is the desired state or record of
84:58 - intent for the object which we explained
85:00 - before as mentioned here spec format is
85:03 - different for every kubernetes object
85:05 - type for example the spec format for an
85:08 - object of kind deployment will be
85:11 - different from the spec format for an
85:13 - object of kind service and to see how to
85:16 - configure the spec for specific types of
85:19 - objects we can use the kubernetes API
85:21 - reference
85:22 - so let's go over the spec format for our
85:26 - deployment object configuration so first
85:29 - as you can see here we have all of the
85:31 - required fields we have API version
85:34 - which is the required field here we have
85:37 - kind which is the required field here
85:39 - and we have metadata which is the
85:42 - required field here and as mentioned in
85:44 - this dock the precise format of the
85:46 - object spec is different for every
85:49 - kubernetes object so anything within
85:51 - this spec block is our deployment spec
85:54 - so to see the actual format for the spec
85:57 - for a deployment we can just go to the
86:00 - kubernetes API reference here
86:04 - and a deployment is a workload resource
86:07 - so we would click this workload resource
86:10 - and we see we have deployment here so we
86:13 - can select deployment
86:15 - so this here is basically giving us the
86:18 - overall configuration for a deployment
86:20 - so it's showing the API version and the
86:23 - kind and the metadata as well and it
86:26 - also tells us the object metadata
86:29 - because as you can see here our metadata
86:31 - has its own nested fields
86:34 - so if we were to hit that object
86:36 - metadata we'd get a description for each
86:39 - field that we have within our metadata
86:41 - so name and it tells us that name must
86:44 - be unique within the namespace and that
86:47 - it's required when creating a resource
86:49 - and if we scroll down here we see that
86:52 - we have additional fields that we're not
86:54 - using currently but that we can use
86:56 - within our metadata block
86:59 - and then we have labels which we're
87:01 - using here and it tells us the format
87:05 - for the labels is it's a map of string
87:08 - keys and string values so for example
87:11 - this app would be the key and auth would
87:14 - be the value for our labels
87:17 - so let's go back and get into the spec
87:20 - configuration so as you can see here is
87:22 - spec and then there's a deployment spec
87:25 - link that we can select that will give
87:27 - us the details of the actual spec format
87:30 - for a deployment so we have our spec
87:32 - here and any of the fields nested within
87:36 - this spec block are going to be present
87:38 - here so we can get a detailed
87:40 - explanation of each individual field so
87:42 - for example the selector here is the
87:45 - selector that we have here and if we
87:48 - click label selector
87:50 - we can find this matched labels that
87:53 - we're using here down here
87:55 - and basically what match labels is doing
87:58 - it says here a label selector is a label
88:00 - query over a set of resources and to
88:03 - understand what I mean by that let's go
88:05 - back to what a selector is here so as
88:08 - you can see here it says label selector
88:10 - for pods existing replica sets whose
88:12 - pods are selected by this will be the
88:14 - ones affected by this deployment and it
88:16 - must match the Pod templates labels so
88:19 - as you can see within our template which
88:22 - we're going to get to for our replicas
88:24 - we're setting the label with key app and
88:28 - value auth so we're going to get into it
88:31 - but this template is basically going to
88:33 - be the configuration for each individual
88:35 - pod and our deployment is basically
88:39 - going to know what pods are part of the
88:42 - overall deployment because this selector
88:44 - is going to match the labels that are
88:47 - assigned to each individual pod in our
88:50 - template here
88:52 - so simply put our deployment knows what
88:55 - pods are part of the deployment because
88:57 - based on this template each pod is going
89:00 - to be deployed with a label that's a key
89:02 - value pair where the key is going to be
89:05 - app and the value is going to be auth
89:07 - and our deployment is going to select
89:09 - pods using the same label app off as the
89:13 - key and value
89:15 - and then we can go here to replicas and
89:18 - as you can see here replicas is the
89:21 - number of desired pods and you already
89:23 - saw how this replicas is working you saw
89:26 - that when we actually applied our
89:28 - configuration there were two auth pods
89:30 - deployed so if we were to increase this
89:32 - to say four then four auth pods would be
89:34 - deployed when we apply the configuration
89:36 - and then we can head over here to
89:38 - strategy which is here and this is just
89:42 - the deployment strategy to use to
89:44 - replace existing pods with new ones and
89:46 - basically this is the difference between
89:48 - here killing all of the existing pods
89:51 - before creating new ones which would
89:54 - essentially mean that our service is
89:56 - unavailable during the creation of new
89:58 - pods or replacing our old replica sets
90:02 - by new ones using rolling update which
90:05 - basically gradually scales down the old
90:07 - replica sets and scales up new ones and
90:10 - in our case we're actually configuring
90:12 - the max surge here to three which is
90:15 - this here and this is the maximum number
90:18 - of PODS that can be scheduled above the
90:20 - desired number of PODS so for example if
90:23 - our desired number of PODS is two and we
90:26 - need to do an update a rolling update it
90:29 - might be necessary to exceed the number
90:31 - of replicas while some pods are shutting
90:34 - down and newer pods are spinning up so
90:37 - this Max surge is just to give us some
90:39 - extra Headroom when we're actually
90:41 - needing to update our deployment and
90:44 - lastly we get into our template here
90:46 - and simply put template describes the
90:50 - pods that will be created we've already
90:52 - gone over this a little bit but let's
90:53 - actually go into the Pod template spec
90:57 - so everything nested under this template
91:00 - field is within the Pod template spec so
91:03 - we have here our metadata and metadata
91:06 - is the same for all of the types so
91:08 - object metadata is going to be the same
91:10 - regardless of the type
91:12 - so if we click this we see that it's
91:14 - still just name and all of the other
91:16 - fields that are possible within metadata
91:22 - and same thing with spec but this time
91:26 - the spec here isn't the same spec as
91:28 - deployment because remember each type
91:31 - has its own spec format so this spec is
91:34 - going to be pod spec here so if we
91:37 - select pod spec
91:39 - we get a different set of fields and
91:41 - descriptions for a spec for a template
91:44 - which is a pod spec because the template
91:47 - is the template for pods so as you can
91:49 - see here we have containers then we have
91:51 - containers here as well
91:53 - and this is where we Define our
91:56 - container so we need to name the
91:58 - container and we need to set an image
92:00 - for our container as well and remember
92:03 - we're pulling our image for our
92:05 - container from our Docker repository and
92:08 - that's the image that's going to be used
92:11 - for our container within our pod so
92:13 - that's image here and then we have ports
92:19 - which we can find here and this ports is
92:22 - actually similar to the expose
92:24 - instruction that we did in our Docker
92:26 - file doesn't actually serve as anything
92:28 - other than documentation so as you can
92:30 - see here it says list of ports to expose
92:33 - from the container exposing a port here
92:36 - gives the system additional information
92:37 - about the network connection a container
92:39 - uses but is primarily informational so
92:42 - not specifying a port here does not
92:44 - prevent that Port from being exposed so
92:47 - any port which is listening on the
92:49 - default
92:50 - 0.0.0.0 address inside a container will
92:53 - be accessible from the network so we've
92:55 - already gone over all of this when we
92:57 - were configuring our Docker file so you
93:00 - should be familiar so yeah this
93:02 - container Port 5000 is similar to our
93:05 - expose instruction in our Docker file it
93:08 - serves as documentation essentially and
93:10 - let's see if we can just search for
93:13 - environment from
93:16 - foreign
93:18 - so we have environment from here which
93:21 - is this configuration here and it's just
93:24 - list of resources to populate
93:26 - environment variables in the container
93:28 - so as I explained to you and showed to
93:30 - you our config map is where we Define
93:33 - our environment variables for our
93:36 - container which you were able to see
93:37 - when we went into the shell for our
93:40 - container so we're using an additional
93:42 - resource config map
93:45 - which can be seen here and it's the
93:48 - config map to select from for the
93:50 - environment variables for the container
93:52 - and down here we also have our secret
93:55 - ref or this one for that matter the
93:58 - contents of the target secret data field
94:00 - will represent the key value pairs as
94:02 - environment variables so essentially the
94:04 - secrets are being stored as environment
94:06 - variables as well and the secrets come
94:08 - from our secrets configuration and both
94:11 - this config map and the secret are their
94:15 - own individual kubernetes objects as
94:17 - well so basically whenever you see kind
94:20 - and a name that means that's an
94:22 - individual kubernetes object so as you
94:25 - can see for our configuration file for
94:27 - our config map it has its own kind and
94:30 - config map type so this is going to
94:32 - create another object in our kubernetes
94:36 - cluster and as you can see each object
94:39 - configuration is going to have
94:41 - essentially similar Fields overall like
94:45 - they're all going to need an API version
94:47 - a kind metadata but some things might
94:51 - differ like this here is a data field
94:54 - which we don't use in our actual
94:57 - deployment object configuration and
95:00 - here's another example when we created
95:02 - Our Kind service we still have the
95:05 - metadata field with its nested field
95:07 - name and again we have the spec for the
95:11 - kind service but of course this spec
95:13 - format is going to be specific to the
95:16 - kind service so it'll be different from
95:19 - our deployment spec format and this API
95:23 - reference documentation is very
95:24 - important and I will have a link to it
95:27 - in the description of this video so now
95:30 - we can start to write the code for our
95:32 - Gateway service so we can just change
95:35 - directory
95:38 - back to our python Source directory and
95:41 - right now we only have our auth service
95:43 - but we can also make their Gateway
95:47 - and we'll go ahead and change directory
95:50 - into Gateway and to start we want to
95:52 - create a virtual environment as usual
95:57 - and then we want to start our virtual
95:59 - environment
96:00 - and as you can see our virtual
96:02 - environment variable is the Gateway
96:05 - virtual environment now the next thing
96:07 - that we want to do is create a file
96:09 - called server.pi so our Gateway service
96:12 - is going to have a few dependencies so
96:15 - we're going to need to import
96:19 - and actually let me go ahead and install
96:21 - some Vim dependencies
96:34 - so we're going to need to import OS
96:39 - we're going to need to import grid FS
96:41 - pica
96:43 - and Json and we're also going to need to
96:46 - import flask
96:49 - and request and we're going to need to
96:52 - import flask Pi
96:58 - and we're going to create an auth
97:00 - package
97:02 - and we're going to create a validate
97:04 - module within that package and we're
97:07 - also going to create an auth service
97:09 - package and we're going to import module
97:12 - access from that package
97:15 - and we're also going to need to create a
97:18 - storage package and we'll import util
97:22 - from the storage package
97:25 - so these we haven't created yet but
97:27 - we're going to create soon
97:29 - and flask by we're going to use
97:32 - mongodb to store our files
97:36 - and this grid FS is basically going to
97:40 - allow us to store larger files in
97:43 - mongodb and I'll explain a little bit
97:45 - more about that when we get to it and
97:47 - this Pika here is going to be what we
97:50 - use to interface with our queue we're
97:53 - going to be using a rabbitmq service to
97:56 - store our messages and I'll go over that
97:59 - more once we get to it as well
98:02 - so to start we just want to create our
98:06 - server so server is going to be equal to
98:08 - flask
98:12 - so in our server config
98:16 - we can do
98:18 - URI
98:20 - and set it equal to
98:23 - mongodb at
98:26 - host.minicube dot internal
98:29 - at Port
98:31 - 27017 which is the default mongodb port
98:36 - and at database that we're going to call
98:39 - videos so if you remember this host mini
98:41 - Cube internal just gives us access to
98:44 - our local host from within our
98:46 - kubernetes cluster and this is just a
98:49 - mongodb URI that's going to be the
98:52 - endpoint to interface with our mongodb
98:55 - so our config is going to have the
98:57 - configuration to our mongodb that's on
99:00 - the Local Host that we haven't yet
99:02 - installed but we will
99:04 - then we're just going to create a
99:06 - variable called and we're going to
99:08 - do PI
99:10 - server
99:12 - and I will explain what this is doing in
99:15 - a second but first let's just go ahead
99:16 - and install all of our dependencies
99:19 - so we can go ahead and save this and
99:22 - we'll just cat server.pi so we can see
99:24 - what we need to install and we'll do
99:26 - pip3 install
99:29 - pika
99:30 - and pip3 install flask
99:34 - and let's cut that again so we can see
99:38 - pip3 install I believe it's high
99:42 - and once we've installed those
99:44 - dependencies we can go back in here
99:48 - foreign
99:53 - [Music]
99:54 - flask Pi
99:57 - which is flask
99:59 - hi
100:02 - and now let's go back in here and see
100:06 - and we have a type over here this should
100:08 - be a small l
100:11 - so now here what this line of code is
100:13 - doing
100:14 - so what we need to know about this line
100:16 - of code is that this Pi is going
100:19 - to wrap our flask server which is going
100:22 - to allow us to interface with our
100:24 - mongodb so if we go to the definition
100:27 - here we see that it manages mongodb
100:30 - connections for our flask app
100:33 - so this essentially is abstracting the
100:36 - handling of the mongodb connections away
100:38 - from us so beyond that we don't really
100:40 - need to understand what's actually
100:41 - happening for the purposes of our use
100:45 - case
100:45 - and once we've created that
100:48 - variable or the pi instance we're
100:52 - going to create FS for grid fs and we're
100:56 - going to create an instance of this grid
100:58 - FS class and we need to pass in our DB
101:03 - from our database
101:06 - which is going to be this video's DB
101:10 - so grid FS is going to wrap our mongodb
101:14 - which is going to enable us to use
101:16 - mongodb's grid FS so let me quickly
101:19 - explain what grid FS is okay so we're
101:23 - going to very quickly go over what grid
101:25 - FS is in relation to our mongodb so
101:29 - we're using mongodb to store our files
101:33 - files being both our MP3 files and our
101:36 - video files and if we go to mongodb's
101:40 - limits and thresholds documentation we
101:43 - can see that a binary Json document size
101:46 - has a maximum size of 16 megabytes and
101:50 - it's explained here that the maximum
101:53 - document size helps ensure that a single
101:55 - document cannot use an excessive amount
101:57 - of Ram or during transmission excessive
102:00 - amounts of bandwidth so they're
102:03 - basically saying that handling files
102:06 - over 16 megabytes in memory will result
102:09 - in Prof performance degradation so what
102:12 - they provide as an alternative is grid
102:15 - FS which essentially allows us to work
102:18 - with files larger than 16 megabytes by
102:21 - sharding the files for example if we go
102:24 - to the gridfs documentation here we see
102:27 - that grid FS is a specification for
102:29 - storing and retrieving files that exceed
102:31 - the binary Json document size limit of
102:34 - 16 megabytes and it says instead of
102:36 - storing a file in a single document
102:38 - gridfs divides files into parts or
102:41 - chunks and it stores each chunk as a
102:44 - separate document so in this case we'd
102:46 - no longer be dealing with files larger
102:49 - than 16 megabytes in memory because a
102:52 - file larger than that size would be
102:54 - separated into chunks and we're only
102:55 - dealing with the individual chunks at
102:57 - that point which avoids the performance
103:00 - degradation issue so when using grid FS
103:03 - gridfs uses two collections to store
103:07 - files and you can just think of
103:09 - collections in mongodb as tables so it's
103:13 - explained here that one of those
103:14 - collections stores the file chunks and
103:16 - then there's another collection that
103:18 - stores the files metadata so this
103:21 - collection that stores the file's
103:23 - metadata basically contains the
103:25 - information necessary to reassemble the
103:28 - chunks to create or reform the original
103:32 - file and if you're interested in more
103:34 - details about this gridfs or mongodb for
103:38 - that matter has very good documentation
103:40 - so you can come and read additionally
103:44 - about this but you'll see later on in
103:47 - the tutorial these are the two
103:49 - collections that we'll be working with
103:50 - to work with these files and the reason
103:53 - we're actually using gridfs is because
103:56 - working with video files there's a high
103:59 - probability that will eventually be
104:01 - working with files larger than 16
104:03 - megabytes so this is essentially going
104:05 - to Future proof our application but for
104:08 - the purposes of this tutorial you don't
104:10 - really need to know much beyond what I
104:12 - just explained but again if you're the
104:14 - type of person that likes to dive deeper
104:16 - into these types of details like me then
104:19 - I recommend reading this page as well
104:22 - which is actually pretty interesting now
104:25 - the next thing that we're going to want
104:26 - to do is configure our rabbitmq
104:29 - connection
104:30 - so we'll just create a variable called
104:32 - connection and we're going to use Pica
104:35 - dot blocking connection
104:38 - which is essentially going to make our
104:41 - communication with our rapidm QQ
104:43 - synchronous and again the details of how
104:46 - this is working are abstracted away from
104:48 - us so we don't need to worry too much
104:50 - about that so in here we're going to add
104:53 - our connection parameters
104:58 - and we're going to pass to our
105:01 - connection parameters the host for our
105:04 - rapidm QQ and we're going to deploy our
105:07 - queue as a stateful set in our
105:09 - kubernetes cluster and it's going to be
105:11 - accessible via just the name rabbitmq
105:14 - and we haven't configured this yet we're
105:16 - going to configure it later but just
105:18 - know that this rabbit mq string is
105:20 - referencing our rabbitmq host and once
105:24 - we create this instance of a blocking
105:26 - connection we want to create a channel
105:28 - with just the connection that we just
105:31 - created dot Channel
105:35 - so let's briefly go over how rabbitmq is
105:39 - going to integrate with our overall
105:41 - architecture so we already know how the
105:44 - off flow works so we don't need to go
105:46 - over that again but now let's go over
105:48 - how rabbitmq integrates with our overall
105:51 - architecture so when a user uploads a
105:54 - video to be converted to MP3 that
105:56 - request will first hit our Gateway our
105:59 - Gateway will then store the video in
106:01 - mongodb and then put a message on this
106:04 - queue here which is our rapid in Q
106:06 - letting Downstream Services know that
106:08 - there is a video to be processed in
106:10 - mongodb the video to MP3 converter
106:13 - service will consume messages from the
106:16 - queue it will then get the ID of the
106:18 - video from the message pull that video
106:20 - from mongodb convert the video to MP3
106:23 - then store the MP3 on mongodb then put a
106:27 - new message on the Queue to be consumed
106:29 - by the notification service that says
106:31 - that the conversion job is done the
106:34 - notification service this consumes those
106:36 - messages from the queue and sends an
106:38 - email notification to the client
106:40 - informing the client that the MP3 for
106:42 - the video that he or she uploaded is
106:45 - ready for download the client will then
106:47 - use a unique ID acquired from the
106:49 - notification plus his or her JWT to make
106:52 - a request to the API Gateway to download
106:55 - the MP3 and the API Gateway will pull
106:58 - the MP3 from mongodb and serve it to the
107:01 - client and that is the overall
107:04 - conversion flow and how rapidmq is
107:07 - integrated with the overall system okay
107:09 - so now that we have a clear
107:11 - understanding of the overall flow of our
107:13 - system we can now use that understanding
107:16 - to familiarize ourself with some key
107:19 - terms when considering microservice
107:21 - architectures those terms are
107:23 - asynchronous and synchronous
107:25 - inter-service communication and strong
107:27 - and eventual consistency let's start
107:30 - with synchronous inter-service
107:32 - communication because understanding that
107:34 - will make it easy easier to understand
107:36 - everything else so synchronous
107:38 - inter-service communication put simply
107:41 - means that the client service sending
107:43 - the request awaits the response from the
107:45 - service that it is sending the request
107:47 - to the client service can't do anything
107:49 - while it waits for this response so it
107:51 - is essentially blocked so this request
107:54 - is considered a blocking request for
107:57 - example our Gateway service communicates
107:59 - with our auth service synchronously so
108:02 - when the Gateway service sends an HTTP
108:05 - post request to our auth service to log
108:08 - in a user and retrieve a JWT for that
108:11 - user our Gateway service is blocked
108:13 - until the auth service either Returns
108:15 - the JWT or an error so communication
108:18 - between our API Gateway and our off
108:21 - service is synchronous which makes those
108:24 - two Services tightly coupled now on the
108:27 - other end of the spectrum we have
108:28 - asynchronous inter-service communication
108:30 - so with asynchronous inter-service
108:33 - communication the client service does
108:35 - not need to await the response of the
108:38 - downstream service therefore this is
108:40 - considered a non-blocking request this
108:42 - is achieved in our case by using a cue
108:45 - for example in our architecture our
108:48 - Gateway service needs to communicate
108:50 - with our converter service but if our
108:52 - Gateway were to communicate with our
108:54 - converter service in a synchronous
108:56 - manner the performance of our Gateway
108:58 - would take a hit because if the Gateway
109:00 - were to get many requests to convert
109:03 - large videos the processes that make
109:05 - requests to the converter service would
109:07 - be blocked until the converter service
109:09 - finishes processing the videos so let's
109:12 - say that hypothetically our Gateway
109:14 - service processes one request per thread
109:17 - concurrently or at the same time if we
109:19 - have two processes with four threads
109:21 - each that's eight concurrent requests if
109:24 - our Gateway were to get more than eight
109:26 - requests to process large videos the
109:28 - entirety of our gateways threads would
109:31 - be blocked awaiting the completion of
109:33 - the processing of each request so in
109:35 - this case synchronous communication
109:37 - between our Gateway and our converter
109:39 - service would not be scalable and this
109:42 - is where our queue comes into the
109:43 - picture as explained before our Gateway
109:46 - doesn't communicate directly with our
109:48 - converter service therefore it does not
109:51 - depend on the converter Services
109:52 - response this means that in our current
109:55 - architecture our Gateway and our
109:58 - converter service are Loosely coupled
110:00 - this decoupling is done by using the
110:03 - queue our Gateway just stores the video
110:05 - on mongodb and throws a message on the
110:07 - queue for a downstream service to
110:09 - process the video at its convenience so
110:11 - in this case the only thing holding up
110:14 - the threads on our Gateway service is
110:16 - the uploading of the video to mongodb
110:18 - and putting the message on the Queue
110:20 - which means that our Gateway Services
110:21 - threads will be freed up much quicker
110:24 - allowing for our gateway to handle more
110:26 - incoming requests so with the current
110:28 - architecture our Gateway service is
110:30 - asynchronously communicating with our
110:32 - converter service that is it sends
110:34 - requests to the converter service in the
110:37 - form of messages on the Queue but it
110:39 - doesn't need to wait for nor does it
110:41 - care about a response from the converter
110:43 - service it essentially just sends and
110:45 - forgets the message and this same thing
110:47 - is happening with the communication
110:49 - between the converter service and the
110:51 - notification service now let's get into
110:53 - strong consistency versus eventual
110:56 - consistency let's start with an example
110:58 - of what our application flow would look
111:00 - like if it were strongly consistent so
111:03 - let's say that hypothetically whenever a
111:05 - user uploads a video to our gateway to
111:07 - be converted to an MP3 we make a
111:10 - synchronous request to our converter
111:11 - service waiting for the conversion to
111:14 - complete and then in response the
111:15 - converter sends an ID for the MP3 back
111:18 - to the user once the conversion is
111:20 - complete at the point that the user
111:22 - received that ID it's certain that the
111:24 - video has been processed and converted
111:26 - into an MP3 and that the data is
111:28 - consistent with the update so at that
111:30 - point if the user were to request to
111:33 - download the data based on that ID the
111:36 - user is guaranteed to get the most
111:37 - recent update of that data so that is
111:40 - strong consistency eventual consistency
111:43 - on the other hand is a bit different so
111:45 - let's use our actual architecture as an
111:48 - example in this case for the sake of
111:50 - example let's say that hypothetically
111:52 - when our Gateway uploads the video to
111:55 - mongodb and puts the message on the
111:57 - queue for it to be processed we return a
111:59 - download ID to the user at that moment I
112:02 - know that we don't return a download ID
112:04 - to the user at that moment in our actual
112:06 - application this is just to help you to
112:08 - understand if the user were to upload a
112:11 - video that takes one minute to process
112:12 - but immediately after receiving the ID
112:15 - the user tried to download the MP3 the
112:17 - MP3 would not yet be available because
112:19 - it would still be processing in that
112:21 - case but the MP3 eventually will be
112:24 - available so if the user were to wait
112:26 - one minute and then request to download
112:28 - the MP3 with that same ID at that point
112:31 - the MP3 would be available therefore the
112:34 - data is eventually consistent and that
112:37 - is eventual consistency
112:39 - okay so the first route that we're going
112:42 - to make for our Gateway is going to be
112:44 - our login route
112:52 - and we're going to define a function
112:54 - called login
112:56 - and what this route is going to do is
112:58 - it's going to communicate with our auth
113:01 - service to log the user in and assign a
113:05 - token to that user so we'll set token
113:08 - error equal to
113:11 - access Dot Login
113:14 - and we're going to pass in the request
113:16 - and this request is from flask and we're
113:19 - importing it here
113:26 - and we're going to create a module
113:28 - called access that's going to contain
113:30 - this login function so let's just go
113:33 - ahead and save this and let's clear and
113:36 - in our Gateway directory we want to
113:39 - create another directory that we'll call
113:41 - auth service
113:43 - and we'll CD into that directory and
113:46 - we'll create a file called init.pi which
113:50 - is essentially going to mark this
113:52 - directory as a package
113:56 - and we'll also create a file called
113:58 - access.pi which is going to be the
114:00 - module that contains our login function
114:03 - and we're going to need to import OS and
114:08 - we also need to import requests and this
114:11 - request is different from the request
114:13 - that we import from flask this request
114:16 - is going to be the module that we use to
114:20 - make HTTP calls to our auth service
114:24 - so we can go ahead and Define log in and
114:27 - we take in request which is not to be
114:29 - confused with this requests
114:32 - and we're going to set auth equal to
114:34 - request Dot
114:37 - authorization and as we write the code
114:39 - for this just pay close attention to
114:42 - requests versus requests with an S at
114:45 - the end because they're different so
114:47 - this request object has this
114:49 - authorization attribute so if when we
114:52 - create this variable this variable
114:55 - resolves To None So if not auth that
114:58 - means that there's no authorization
115:00 - parameters in our request so that means
115:03 - that we need to return none for our
115:06 - token and we need to return for our
115:09 - error missing credentials
115:13 - and a 401
115:17 - so if we go ahead and save this
115:20 - and quit and go back into our
115:24 - server.pi file
115:29 - we see that we're setting token and
115:32 - error equal to
115:34 - access.login so access.login needs to
115:37 - return a tuple and the first item in the
115:39 - Tuple will go to token and the second
115:41 - item in the Tuple will go to error so if
115:44 - we go back to the definition in this
115:46 - case when we're missing credentials
115:48 - we're going to return none for the token
115:50 - and we're going to return an error but
115:52 - upon the successful login of a user
115:55 - we're going to return none for the error
115:57 - and we're going to return an actual
115:59 - token
116:00 - so we'll set basic auth equal to
116:04 - auth.username and auth.password
116:09 - and we've already gone over basic auth
116:12 - so you should already be familiar with
116:14 - this
116:15 - and we're going to set our response
116:18 - equal to
116:19 - requests dot post
116:24 - now this request is going to be the
116:27 - request that's going to make the HTTP
116:29 - call to our auth service so let's save
116:32 - this actually and we need to install
116:36 - requests
116:40 - and let's go back into our access file
116:44 - so this request.post is going to make a
116:47 - post request to our auth service and the
116:50 - parameters or the arguments that we need
116:51 - to pass are the string the URL endpoint
116:55 - string and our auth header and the way
116:57 - that we do that is we can just create a
117:00 - formatted string
117:03 - and we'll do OS Dot environ.get
117:07 - and we'll just get our auth service
117:11 - address which we're going to create so
117:14 - we're going to create this off service
117:16 - address environment variable which is
117:18 - going to be the address for our auth
117:21 - service and we're going to need to
117:23 - access the login endpoint
117:26 - and to create the basic auth header
117:28 - we're just going to do auth equals basic
117:30 - auth
117:31 - and once this request completes this
117:35 - response is going to contain the result
117:38 - and we're going to check if response dot
117:41 - status code
117:43 - equals 200
117:46 - that means that we're good we're going
117:48 - to return response dot text which is
117:50 - going to be our token and none for the
117:52 - error otherwise
117:55 - we'll return none if we don't get a 200
117:59 - response it means we didn't get our
118:00 - token so we'll return none and we'll
118:02 - just return response.txt
118:05 - and response dot status code
118:11 - and we can go ahead and save that
118:14 - and this should be
118:16 - double equal
118:19 - and let's format
118:27 - and this is spelled wrong
118:31 - and we can save this file and let's just
118:34 - clear this
118:36 - let's just change directory back to our
118:39 - main directory and go back into
118:41 - server.pi
118:44 - so once we call this access.login we're
118:47 - going to check if not error and that's
118:50 - because if there is an error this is
118:52 - going to contain an error but if there
118:54 - is no error this is going to be null or
118:57 - none and if that's the case we can just
118:59 - return our token
119:01 - otherwise if there is an error then
119:03 - we're just going to return the error
119:06 - and that's going to be it for our login
119:08 - function
119:13 - now the next route that we want to
119:15 - create is our upload route and this is
119:19 - going to be the route that we use to
119:21 - upload our video that we want to convert
119:23 - into an MP3
119:25 - so we're just going to call it upload
119:28 - and methods will be just post
119:33 - so we'll Define a function called upload
119:37 - and now for this route we need to make
119:40 - sure that the user has a token from our
119:42 - login route so we need to validate our
119:45 - user so we're going to set access and
119:49 - error equal to validate dot token
119:53 - and we're going to create this validate
119:55 - module as well and we're going to pass
119:57 - in the request so let's go ahead and
119:59 - create this validate module with this
120:02 - token function
120:04 - so we'll go ahead and save and let's
120:06 - clear and we'll make dur and this dirt
120:09 - will just call auth the other one we
120:11 - call it auth service because we're
120:13 - communicating with the auth service on
120:16 - behalf of the user or the client but
120:19 - auth is going to just be used internally
120:21 - so our Gateway is going to use this auth
120:24 - package to validate tokens that it
120:26 - receives from the client so we'll make
120:28 - their auth
120:30 - and let's change their auth and once
120:32 - again we need to create this init.pi
120:34 - file to mark this directory as a package
120:37 - and we'll create a file called
120:39 - validate.pi and we will import OS and
120:43 - requests once again
120:45 - and we'll Define a function called token
120:49 - which takes in a request because we're
120:51 - validating a token
120:55 - so remember the flow is the client's
120:57 - going to access our internal services or
121:01 - our endpoints by first logging in and
121:04 - getting a JWT and then for all
121:06 - subsequent requests the client is going
121:08 - to have an authorization header
121:10 - containing that JWT which tells our API
121:14 - Gateway that that client has access to
121:17 - the endpoints of our overall application
121:20 - so this validate token function that
121:23 - we're creating here is going to be the
121:24 - function that validates that JWT sent by
121:27 - the client so we need to first check to
121:30 - see if the client has the authorization
121:32 - header in his request or his or her
121:35 - request
121:37 - foreign
121:41 - is not in request.headers we're going to
121:45 - return none for our access and we're
121:48 - going to return an error that says
121:50 - missing credentials
121:52 - and it's going to be a 401. otherwise
121:55 - we're going to set our token equal to
121:58 - request dot headers
122:01 - authorization
122:03 - and if that token does not exist we'll
122:07 - return none as well and the same error
122:10 - missing credentials
122:13 - 401
122:14 - but if the token does exist and the
122:18 - authorization header exists we'll set
122:19 - response equal to request dot post
122:23 - and this should be requests so don't
122:27 - make the same mistake I did and once
122:29 - again we're going to send a post request
122:32 - via HTTP to our auth service so once
122:36 - again we'll do a formatted string
122:40 - and we're going to get our host from the
122:43 - environment
122:46 - using the environment variable auth
122:49 - service address
122:52 - and we're going to access the validate
122:56 - endpoint
122:57 - and headers will be equal to
123:00 - authorization
123:02 - token
123:04 - so we're basically just passing along
123:06 - the authorization token to our validate
123:09 - request of our auth service
123:12 - and we want to check the response if
123:14 - response dot status code
123:17 - equals 200 then we're good so that means
123:20 - we're going to return
123:22 - response.txt and none and response.text
123:26 - is going to contain the body which will
123:29 - be the axis that the bearer of this
123:32 - token has and you'll see what I mean by
123:34 - that when we parse this
123:37 - else if the response from our auth
123:40 - service isn't 200 we're going to return
123:42 - none and we're going to return an error
123:45 - so response Dot txt and response dot
123:50 - status code
123:54 - and we can save that and let's go back
123:57 - to our root directory and back into our
124:01 - server.pi file okay so let's take a
124:03 - second to understand what's happening
124:05 - here with our validation before moving
124:08 - forward so let's quickly go back into
124:11 - our auth service and into the server.pi
124:15 - file
124:17 - and let's go over our login route so if
124:20 - you remember our login route here is
124:23 - going to take a username and a password
124:25 - or an email and a password and return a
124:28 - token and the token is going to be a
124:31 - Json web token and that token is being
124:34 - created here in this create JWT function
124:37 - and as you can see in this function
124:40 - we're encoding a payload and that
124:43 - payload is here and this payload
124:46 - contains our claims which is basically
124:49 - these data points within the payload so
124:52 - our username is a claim the expiration
124:55 - date is a claim and also contained
124:58 - within this payload is this claim here
125:00 - for admin which is just a bull that's
125:02 - going to be true or false and as I said
125:05 - earlier we're just going to allow
125:06 - anybody with admin equal to True access
125:09 - to all of the endpoints of our services
125:13 - so the token that's returned to the
125:17 - logged in client is going to contain
125:19 - this payload but the payload is going to
125:21 - be encrypted
125:23 - and when that client sends their token
125:25 - in their request and we validate it in
125:28 - using the validate endpoint for our off
125:32 - service we're going to first check to
125:34 - see if the token exists in the request
125:37 - and then we're going to decode that
125:40 - token and when we're decoding the token
125:42 - we're using the same key that we signed
125:45 - the token with this JWT secret which is
125:48 - how we know that this is a valid token
125:50 - because our auth service is the service
125:52 - that signed the token using this key and
125:55 - when we decode the token we're using the
125:57 - same key the service is key so if
125:59 - somebody were to send a token that was
126:01 - signed with a different secret key then
126:03 - of course it wouldn't work and when we
126:05 - decode this token with this decoded
126:07 - variable is going to include that
126:10 - payload that tells us who the user is
126:12 - via their username which is their email
126:14 - and their privileges which is the auth's
126:18 - claim which is true or false so now
126:20 - let's go back into our Gateway code code
126:24 - and here when a user tries to upload
126:27 - they need to have a token header which
126:30 - we're going to validate using our
126:33 - function to validate the token and the
126:35 - Gateway is just going to forward this
126:38 - token to our auth services validate
126:40 - endpoint and the response that we expect
126:43 - from our auth service is that decoded
126:46 - body
126:47 - so here when we get a successful
126:50 - response a 200 response what we're
126:52 - returning here in this response.txt is
126:55 - going to be the body of that token
126:57 - containing the claims so it's
126:58 - essentially it's going to be the decoded
127:01 - token where the body is visible and it's
127:03 - going to be a string which is going to
127:05 - be Json formatted
127:08 - and that means that here this access
127:11 - variable is going to resolve to that
127:14 - Json string that contains our payload
127:17 - with our claims so here we'll set access
127:20 - equal to Json dot loads
127:24 - access
127:27 - and if we go to the definition of this
127:29 - you see that it deserializes an instance
127:33 - containing a Json document to a python
127:36 - object so we're essentially just
127:37 - converting this Json string to a python
127:40 - object so that we can work with it in
127:42 - code
127:43 - and what's being converted into a python
127:46 - object is this here
127:50 - so our python object is going to look
127:52 - like this once we've decoded the Json
127:57 - so let's just go back
127:59 - and we can go ahead and close this
128:01 - so this access is going to contain that
128:04 - object and that object has the admin
128:07 - claim which is going to be a bull which
128:09 - is true or false and actually just to be
128:11 - clear let me show what I mean by that
128:17 - so this admin claim here is going to
128:20 - contain the auths which is a wool which
128:22 - is true or false and if it's true we'll
128:25 - give the user access to all of the
128:27 - endpoints
128:29 - so we're going to check for that claim
128:31 - so we're going to say if access which is
128:34 - the object that was converted from the
128:36 - Json so we're going to say if access
128:38 - admin which is essentially saying if the
128:42 - admin claim resolves to true then we're
128:44 - going to give the user access
128:47 - so let's go ahead and close that again
128:50 - so if the user does have access we're
128:53 - going to make sure that there's a file
128:56 - to be uploaded in the first place
128:58 - so if there's a file being uploaded the
129:01 - request should contain a dictionary in
129:04 - this request.files so we're going to say
129:06 - if the length of request.files is
129:09 - greater than one because we're only
129:10 - going to allow the uploading of one file
129:12 - at a time for now
129:13 - or the length of request.files is less
129:18 - than one because we want there to be
129:20 - exactly one file so we don't want more
129:22 - than one file then we don't want less
129:23 - than one file so if one of these is true
129:26 - then we're going to return
129:28 - exactly one file required
129:32 - and a 400.
129:35 - and this
129:36 - request.files dictionary is going to
129:39 - have a key for the file which will be
129:42 - defined when we send the request and the
129:44 - actual file as the value
129:48 - so that means that
129:50 - we should iterate through the key values
129:52 - in the request.files dictionary so we'll
129:55 - do for key which we don't need to use
129:57 - and file which is the value in
129:59 - request.files.items
130:03 - for every file we're going to upload it
130:05 - where we're going to do an upload and
130:08 - there should only be one file so this
130:09 - should only happen once
130:11 - so we'll do util dot upload and we
130:14 - haven't created this function yet but we
130:16 - will and this function is going to take
130:18 - us parameters to file our grid FS
130:21 - instance our rabbitmq Channel and the
130:26 - axis which was just explained above and
130:29 - this function is going to return an
130:30 - error if something goes wrong but if
130:32 - nothing goes wrong then it won't return
130:34 - anything it'll return none
130:37 - so to check if something went wrong
130:39 - we're just going to check if error and
130:41 - if there is an error we're just going to
130:43 - return error
130:44 - and after this for Loop completes if
130:47 - we've never returned an error then that
130:49 - means it was successful so we'll just
130:51 - return
130:52 - success
130:54 - with a 200 and that's what's going to be
130:57 - happening if the user is authorized but
131:00 - if the user isn't authorized so we need
131:03 - to go down here and do else
131:05 - so this is the block that's going to get
131:07 - executed if the user isn't authorized
131:09 - and in that case we're just going to
131:11 - return not
131:13 - authorized
131:15 - and a 401
131:22 - and that's going to be our upload route
131:24 - and remember we still need to go and
131:26 - create this upload function but I'm
131:28 - going to get to that in a second because
131:30 - that function is going to be a little
131:32 - bit involved
131:34 - so before we do that let's just finish
131:36 - up our template here for our Gateway
131:39 - server so we'll do the final endpoint
131:41 - which is going to be server.route
131:43 - and this is going to be the download
131:45 - endpoint which is the endpoint that is
131:47 - going to be used to download the MP3
131:50 - that was created from the video and this
131:53 - is going to be methods and we're only
131:55 - going to do git and we're going to
131:57 - Define download
132:00 - and for now we'll just pass this is just
132:02 - a template portion for this function or
132:04 - this endpoint and lastly we need to set
132:08 - if name
132:09 - equals
132:11 - Main
132:14 - we're going to run our server and again
132:17 - we're going to set our host to 0.0.0.0
132:22 - and our port in this case is going to be
132:25 - 880.
132:29 - and this is going to be our Gateway
132:32 - service template including both the
132:36 - login endpoint and the upload endpoint
132:38 - and to be continued on the download
132:41 - endpoint so let's go ahead and save this
132:43 - and actually let's go back in here so I
132:46 - don't confuse you
132:47 - so now what we're going to need to
132:49 - create is this upload function here and
132:52 - this util as you can see is coming from
132:56 - this here so from Storage import util so
133:00 - we need to create a storage package and
133:02 - within that package we need to create a
133:04 - util module
133:06 - so we'll make their storage
133:09 - change there into storage and we'll
133:11 - create our init.pi file
133:14 - and now we'll do util.pi
133:17 - and we'll start by importing Pica and
133:21 - Json
133:24 - and we'll Define a function called
133:26 - upload and remember the parameters are
133:28 - the file our grid FS instance our
133:32 - rabbitmq Channel and our access which is
133:36 - the user's access
133:38 - now this function is going to be a
133:40 - little bit complicated so try to keep up
133:43 - and pay attention I'll try to explain
133:45 - things the best that I can so basically
133:47 - with this upload function needs to do is
133:50 - it needs to First upload the file to our
133:54 - mongodb database using gridfs and once
133:57 - the file has been successfully uploaded
133:59 - we need to put a message in our rabbitmq
134:02 - so that a downstream service when they
134:05 - pull that message from the queue can
134:07 - process the upload by pulling it from
134:10 - the mongodb and this queue is allowing
134:13 - us to create an asynchronous
134:15 - communication flow between our Gateway
134:18 - service and the service that actually
134:19 - processes our videos and this
134:22 - asynchronicity is going to allow us to
134:25 - avoid the need for our Gateway service
134:28 - to wait for an internal service to
134:31 - process the video before being able to
134:33 - return a response to the client so the
134:35 - first thing that we want to do is we
134:37 - want to try to put our file into the
134:39 - mongodb so when we put a file in the
134:42 - mongodb we're going to use this FS dot
134:45 - put function and it's going to be the
134:47 - file that we want to put
134:50 - and if this put is successful a file ID
134:53 - is going to be returned a file ID object
134:57 - to be more specific so mongodb is going
134:59 - to return a file ID and that's if it's
135:02 - successful but if it's not successful
135:04 - then we want to catch the error
135:08 - and for now we're just going to return
135:11 - internal server error
135:17 - and if this returns this means our file
135:20 - wasn't uploaded successfully and the
135:22 - function just returned so we don't need
135:24 - to do anything else after that but if
135:26 - the file was successfully uploaded we
135:28 - need to create a message to put onto our
135:30 - queue so we'll set message equal to
135:35 - a dictionary and it's going to contain
135:38 - the video file ID which is going to be
135:41 - the file ID object converted into a
135:44 - string
135:46 - and we also need to create an empty MP3
135:50 - file ID within this same dictionary and
135:54 - for now it's going to be none but
135:56 - Downstream it's going to end up being
135:59 - set to the mp3s file ID in the database
136:02 - but you don't need to think too much
136:03 - about this right now we'll get to it
136:05 - later
136:05 - and we need to also have the username to
136:08 - identify who owns the file and the
136:11 - username is going to come from our
136:13 - access from our auth service and
136:15 - remember there was a claim for username
136:17 - there which contains our user's email
136:19 - and remember in our auth DB the email
136:23 - must be unique so this is a way to
136:26 - uniquely identify our user so that's
136:29 - going to be our message and now we need
136:31 - to put that message on the Queue so
136:32 - we're going to try and put this message
136:34 - on the queue
136:35 - using the channel that's passed to the
136:37 - function and we're going to do basic
136:39 - publish
136:42 - and we're going to set exchange equal to
136:46 - an empty string and this is just going
136:49 - to mean we're going to use the default
136:50 - exchange so I will explain a bit more
136:53 - about how rabbitmq works so for our
136:56 - purposes we're going to use a very basic
136:58 - rapidmq configuration and setup but we
137:01 - need to go over a couple of things so
137:03 - that we have a clear understanding of
137:05 - what's Happening Here let's start with
137:07 - the top level overview of how rabbitmq
137:10 - integrates with our system the first
137:12 - thing that is important to understand is
137:15 - that our producer which is the service
137:17 - that is putting the message on the Queue
137:19 - isn't publishing the message directly to
137:21 - the queue it actually sends messages
137:23 - through an exchange The Exchange is
137:26 - basically a middleman that allocates
137:28 - messages to their correct queue
137:30 - throughout the video I've been referring
137:32 - to rabbitmq as if it were a single cue
137:35 - but under the hood we actually can and
137:38 - do configure multiple cues within one
137:40 - rabbitmq instance for example in our
137:44 - case we'll make use of both a cue that
137:46 - we'll call video and a queue that we'll
137:48 - call MP3 so when our producer publishes
137:51 - a message to The Exchange The Exchange
137:54 - will route the message to the correct
137:56 - queue based on some criteria so how does
137:59 - our Exchange Route messages to the
138:00 - correct queue in our case well since
138:03 - we're going with a simple rabbitmq
138:05 - configuration for the sake of brevity
138:07 - you'll remember that we are using the
138:09 - default exchange by setting the exchange
138:12 - to an empty string and if we take a look
138:14 - at the rabbitmq documentation here the
138:17 - default exchange is a direct exchange
138:20 - with no name EG the empty string
138:23 - pre-declared by the broker and broker
138:26 - just being our rapidm queue instance and
138:29 - this default exchange has one special
138:31 - property that makes it very useful for
138:33 - simple applications and that is that
138:36 - every queue that is created is
138:38 - automatically bound to the default
138:40 - exchange with a routing key which is the
138:42 - same as the Q name so what does that
138:45 - mean exactly simply put that just means
138:47 - that we can set our routing key to the
138:50 - name of the queue that we want our
138:52 - message to be directed to and set the
138:54 - exchange to the default exchange and
138:57 - that will result in our message going to
138:59 - the queue specified by the routing key
139:01 - so with this overview we can see our
139:04 - video queue the exchange our producer
139:07 - and our consumer so let's say that this
139:09 - producer is our Gateway service and this
139:12 - queue is our video queue and this
139:14 - consumer is our video to MP3 converter
139:17 - when the user uploads a video our
139:20 - Gateway stores the video and then
139:21 - publishes a message to The Exchange that
139:24 - is designated for the video queue The
139:27 - Exchange will route that message to the
139:29 - video queue and the downstream service
139:31 - which is the consumer of the video queue
139:34 - will process the message the consumer in
139:36 - this case is our video to MP3 converter
139:39 - service so it will process the message
139:41 - by pulling the video from mongodb
139:44 - converting it to MP3 storing the MP3 on
139:47 - mongodb and then publishing a message to
139:49 - The Exchange that is intended for the
139:52 - MP3 queue but let's not focus on the
139:54 - mp3q just yet we'll get to that later
139:57 - let's just focus on the video queue for
139:59 - now let's say for example our producer
140:02 - is paddling on more messages than our
140:04 - one consumer can process in a timely
140:07 - manner this is where the capability to
140:09 - scale up our video to MP3 consumer comes
140:12 - into the picture but if we're going to
140:14 - scale up our queue actually needs to be
140:17 - able to accommodate multiple instances
140:19 - of our consumer without bottlenecking
140:22 - the entire flow so how do we manage that
140:24 - we manage that by making use of a
140:27 - pattern called the competing consumers
140:29 - pattern this pattern simply enables
140:31 - multiple concurrent consumers to process
140:34 - messages received on the same messaging
140:36 - channel that way if our queue is packed
140:39 - full of messages we can scale up our
140:41 - consumers to process the messages
140:43 - concurrently resulting in more
140:45 - throughput luckily by default our rapidm
140:48 - QQ will dispatch messages to our
140:51 - consuming services using the round robin
140:54 - algorithm which satisfies our needs this
140:57 - basically just means that the messages
140:59 - will be distributed more or less evenly
141:02 - amongst our consumers for example if we
141:05 - have two instances of our consuming
141:07 - service the first message would go to
141:10 - this instance and if another message
141:12 - were to come in it would not go to the
141:14 - same instance it would go to the next
141:16 - one and the same goes for if we already
141:18 - have a bunch of messages on the Queue
141:21 - the distribution of the messages will
141:23 - essentially go in a sequence from one
141:26 - instance of the consumer to the next to
141:28 - the next and then back to the first Etc
141:30 - so basically the messages will be
141:32 - distributed evenly in a round robin
141:34 - fashion so that they can be processed
141:36 - concurrently and that is going to be
141:39 - that so now that that's out of the way
141:41 - let's get back to right adding our code
141:44 - and then we're going to set our routing
141:46 - key equal to video
141:48 - so the routing key is actually going to
141:51 - be the name of our queue and we're going
141:53 - to have a queue called video where we
141:55 - put the video messages on
141:58 - and then the body of the message is
142:01 - going to be Json dot dumps
142:04 - message
142:05 - now similar to
142:07 - json.loads this dumps converts a python
142:10 - object into a Json string
142:13 - so as you can see it serializes the
142:16 - object a python object to a Json
142:18 - formatted string so it's basically doing
142:20 - the opposite of Json loads because the
142:23 - message we need to have a string
142:25 - contained within it not a python object
142:29 - and the python object I'm talking about
142:31 - is this here this is being converted
142:33 - into a Json string
142:35 - which is going to be the body of our
142:37 - message which gives our Downstream
142:40 - service all of the information that it
142:42 - needs to process the video conversion
142:46 - and we need to also set properties equal
142:50 - to Pika dot basic properties
142:56 - and within these properties we need to
142:59 - set the delivery mode
143:01 - and that's going to be set to pica.spec
143:04 - dot persistent
143:07 - delivery
143:09 - mode
143:11 - and this part is very important to make
143:13 - sure that our messages are persisted in
143:17 - our queue in the event of a pod crash or
143:21 - a restart of our pod so since our pod
143:24 - for our rabbitmq is a stateful pod
143:28 - within the kubernetes cluster we need to
143:30 - make sure that when messages are added
143:32 - to the queue they're actually persisted
143:34 - so that if the Pod is reset or the Pod
143:37 - fails when it comes back up or spins
143:39 - back up the messages are still there
143:41 - because if we don't set this if the Pod
143:44 - crashes or is reset then the messages
143:47 - are all going to be gone once the Pod is
143:50 - restored back to its original state so
143:52 - what we're essentially going to need to
143:54 - do is we're going to need to make our
143:56 - cue durable which means that the queue
143:58 - is going to be retained even after a pod
144:02 - restart and we need to make sure the
144:03 - messages within the queue are also
144:05 - durable which means that the messages
144:07 - are going to be retained even in the
144:09 - event of a pod restart or crash so when
144:12 - we actually create the queue we can
144:14 - configure the queue to be durable
144:16 - but that doesn't mean that the messages
144:18 - are going to also be persisted it just
144:21 - means that the queue will be durable so
144:23 - each individual message that we send to
144:25 - the queue we need to set this
144:27 - configuration here to tell rabbitmq that
144:30 - the message should be persisted until
144:32 - the message has been removed from the
144:34 - queue
144:35 - so anyways we're going to try to put our
144:38 - message onto our queue and if that
144:41 - doesn't work out
144:42 - we need to first delete the file from
144:45 - our mongodb because if there's no
144:48 - message on the queue for the file but
144:50 - the file still exists in the DB that
144:53 - file is never going to get processed
144:54 - because the downstream service doesn't
144:57 - know that that file exists if it never
144:58 - receives a message telling it to process
145:01 - that file so we'll just end up with a
145:03 - bunch of stale files in the database if
145:05 - we don't delete them in the event that a
145:07 - message can't be put onto our queue so
145:09 - if the message is unsuccessfully added
145:11 - to the queue we'll do fs.delete and we
145:14 - need to delete using the file ID
145:17 - and the file ID is the file ID object
145:20 - not the string it's defined here
145:23 - and once we've deleted the file we can
145:25 - then return internal server
145:28 - error
145:30 - so in this case in the event of a
145:32 - failure
145:35 - we will neither have a message on the
145:37 - Queue telling the downstream service to
145:39 - process the file and we will also not
145:42 - have a file in our database so it's
145:45 - going to be a complete failure so we can
145:47 - just return internal server error and at
145:49 - that point the user could just upload
145:51 - the file again if they want to try again
145:53 - and this is going to be it for our
145:56 - upload function
146:00 - and we're not using this error variable
146:03 - yet but we might use it later on down
146:06 - the line Depending on time constraints
146:08 - so we'll just leave it for now
146:10 - so let's go ahead and save and quit and
146:12 - we can clear okay so at this point we
146:15 - can go back to our root directory and we
146:18 - can start creating the deployment for
146:21 - this Gateway service so to start we just
146:24 - want to go ahead and freeze our
146:27 - requirements our dependencies into a
146:29 - requirements.txt file and we can start
146:33 - to create our Docker file now our Docker
146:36 - file is going to be quite similar to the
146:38 - one that we used for our auth service so
146:41 - we can actually just go to our auth
146:44 - service
146:45 - and get this Docker file and paste it
146:49 - into this one
146:53 - and everything is pretty much going to
146:55 - remain the same except for we're going
146:57 - to change the exposed port to 8080 and
147:01 - we also don't need this here but other
147:05 - than that the file is going to be
147:07 - identical so we can just go ahead and
147:10 - save this so now let's go ahead and
147:13 - build our image so we'll do Docker build
147:25 - and once we've built the image similar
147:27 - to before we can go ahead and tag our
147:29 - image so we'll do Docker tag and we'll
147:32 - go ahead and take the beginning of this
147:35 - and paste it in and your username for
147:39 - your Docker Hub account and this time
147:41 - instead of auth we're going to say
147:43 - Gateway
147:44 - and latest
147:47 - now at this point on your Docker Hub
147:49 - account you'll only have one repository
147:52 - which is the repository for our auth
147:55 - service and if we want to we can just
147:57 - create the Gateway repository manually
148:00 - but it'll actually create itself once we
148:02 - push to our username with the suffix
148:06 - Gateway
148:07 - so if we just do docker
148:09 - push
148:13 - and username slash forward slash Gateway
148:16 - latest
148:25 - you'll see that if you refresh the page
148:28 - here
148:30 - it automatically created the Gateway
148:33 - repository for us and if we go in here
148:37 - we'll see that we pushed the tag latest
148:40 - a few seconds ago so now we have our
148:43 - Gateway repository in our Docker Hub
148:45 - account
148:46 - so now that we can pull the image
148:48 - containing our source for our Gateway
148:50 - service from the internet we can go
148:52 - ahead and create our kubernetes manifest
148:55 - directory and we can go ahead and change
148:57 - there into there and similar again to
149:00 - our auth service we need to create a
149:03 - Gateway deployment yaml file
149:07 - and this configuration is also going to
149:10 - be very similar to our off service
150:01 - and this time we're going to pull our
150:03 - image from the Gateway Repository
150:07 - so remember your username and then
150:09 - Gateway
150:12 - and similar to the auth service we're
150:14 - going to create a config map
150:21 - called Gateway config map and let's not
150:24 - make the same mistake as last time this
150:26 - should be indented
150:27 - and for our secret ref we're going to
150:31 - create a secret as well
150:37 - and that's going to be it for our
150:40 - deployment
150:42 - and now let's go ahead and make our
150:44 - config map
150:58 - and the data that we need for our
151:01 - environment variables we need our auth
151:03 - service address
151:06 - and in kubernetes the service name will
151:09 - resolve to that service's host IP
151:12 - address so we can just put auth and then
151:14 - the port that the service is available
151:16 - at so we'll do all 5000 and that's going
151:20 - to be the address of our auth service
151:23 - within our kubernetes cluster and we can
151:26 - go ahead and save that and now let's
151:28 - create our secret
151:41 - and as of right now I don't think we
151:43 - have secrets for our Gateway service so
151:46 - let's just put a placeholder here for
151:48 - now and later we can add any secret
151:50 - variables if we need to
151:58 - and let's save that and we need to
152:01 - create service.yaml
152:12 - and the name of our service will just be
152:13 - Gateway
152:24 - and this service will have an internal
152:26 - IP address which will only be available
152:29 - within our cluster but our Gateway API
152:32 - needs to be able to be accessed from
152:35 - outside of our cluster so we're actually
152:36 - going to need to create another
152:38 - configuration called an Ingress to Route
152:41 - traffic to our actual Gateway service
152:43 - which I will get into in a second so we
152:46 - need to set ports equal to
152:49 - 80 80
152:51 - Target port 8080 as well
152:55 - and protocol is of course TCP
153:02 - so now we'll create the ingress.yaml
153:05 - which is going to allow traffic to
153:08 - access our Gateway endpoint so let's
153:10 - take a second to understand what an
153:12 - Ingress is in the context of a
153:15 - kubernetes cluster okay so in order to
153:18 - understand what an Ingress is in the
153:20 - context of kubernetes you first need to
153:22 - understand what a service is in the
153:24 - context of kubernetes so with this
153:26 - configuration file we're creating a
153:29 - service and you can really just think of
153:31 - the service as a group of PODS so in our
153:34 - case we want to create the Gateway
153:36 - service and we want that service to be
153:38 - able to scale up to multiple instances
153:41 - or multiple pods so the service comes in
153:44 - and groups all of the instances of our
153:46 - Gateway service together and it does
153:48 - this by using a selector so in our case
153:51 - we're using this label selector to tell
153:54 - our service what pods are part of its
153:57 - group or under its umbrella so this
154:00 - label selector essentially by lines our
154:03 - pods to the service so any pod with this
154:06 - label will be recognized by the service
154:08 - as being part of its group so now we
154:11 - don't have to think about individual IPS
154:13 - for each individual pod and we don't
154:16 - have to worry about keeping track of the
154:18 - IPS of PODS that go down or are
154:21 - recreated we also don't have to think
154:23 - about how requests to our service are
154:25 - load balanced to individual pods the
154:28 - service abstracts all of this away from
154:30 - us so now we can just send requests to
154:33 - the services cluster IP which remember
154:35 - is its internal IP and we can assume
154:38 - that these requests will be distributed
154:40 - logically amongst our pods based on
154:43 - something like round robin for example
154:45 - which we already went over when
154:47 - explaining webinimq so now that we have
154:50 - a clear picture of a service we can get
154:52 - into what an Ingress is so we have our
154:55 - service with its pods and that service
154:57 - sits in our cluster which is our private
155:00 - network but we need to allow quests from
155:03 - outside of our cluster to hit our
155:05 - Gateway Services endpoints we do this by
155:08 - making use of an Ingress simply put an
155:11 - Ingress consists of a load balancer that
155:13 - is essentially the entry point to our
155:16 - cluster and a set of rules those rules
155:19 - basically say which requests go where
155:21 - for example we have a rule that says
155:23 - that any request that hits our load
155:25 - balancer via the domain name
155:27 - mp3converter.com should be routed to our
155:30 - Gateway service so since this load
155:32 - balancer is the entry point to our
155:34 - cluster it can actually Route traffic to
155:37 - the cluster IPS within the cluster so in
155:40 - this case it would route requests going
155:42 - to the configured MP3 converter.com
155:45 - domain to our Gateway Services internal
155:47 - cluster IP and if we wanted to we could
155:50 - even add a rule to our Ingress that says
155:52 - to Route requests to apples.com to a
155:55 - different service in our cluster for
155:57 - example so that's pretty much everything
156:00 - you need to know about Ingress for the
156:02 - purposes of this video so let's get into
156:04 - writing our Ingress configuration file
156:07 - so for the API version we're going to
156:11 - set
156:13 - networking.kh dot IO V1 and kind is
156:18 - going to be Ingress this time
156:22 - and we're going to name it
156:24 - Gateway Ingress and we're going to use
156:27 - the default Ingress which is basically
156:30 - an nginx Ingress and we can set some
156:33 - configurations for our nginx Ingress
156:36 - using this annotations key and we want
156:39 - to make sure that our Ingress allows the
156:42 - upload of some relatively large files so
156:45 - we're just going to set our Ingress
156:53 - body size to zero which is essentially
156:58 - going to allow any body size and of
157:01 - course you want to fine-tune
157:02 - configurations like these in a
157:03 - production application but again our
157:06 - focus is the overall architecture so
157:08 - we're basically just configuring this in
157:10 - the easy way possible to get things done
157:12 - and this is a typo
157:16 - and we're going to do two more
157:17 - configurations
157:20 - proxy
157:22 - read time out and we're going to set
157:25 - that equal to 600
157:29 - and proxy
157:31 - same time out and we'll also set that
157:34 - one to 600. now for our spec
157:39 - and the rules we're going to Route
157:41 - request to the host MP3 converter
157:45 - .com to our Gateway service
158:03 - so remember our service name is Gateway
158:06 - and our service is available at Port
158:10 - 8080.
158:12 - so you're probably wondering how our
158:15 - kubernetes cluster knows if we're making
158:17 - a request to this host and basically
158:20 - what we're going to do is we're going to
158:22 - make it so that requests to this host on
158:25 - our local machine gets routed to the
158:27 - Local Host so we're just going to map
158:29 - this hostname to localhost on our local
158:32 - machine and we're going to Tunnel
158:34 - requests to our Local Host to minicube
158:36 - which sounds a bit complicated but just
158:39 - bear with me and we'll get to it so
158:41 - we're going to save this file and first
158:44 - we need to make sure that the MP3
158:46 - converter.com gets routed to localhost
158:49 - so you need to open a file called
158:52 - Etc hosts
158:54 - and you're going to have to have pseudo
158:57 - permissions to do that
159:00 - so once in this directory you want to
159:02 - map this address
159:05 - 127.0.0.1 which is the loopback address
159:09 - which localhost also resolves to we want
159:13 - to map it to
159:15 - MP3 converter.com
159:18 - so once we do this whenever we enter
159:20 - This MP3 converter.com into our browser
159:23 - or if we send a request to this host
159:26 - it's going to resolve to localhost
159:29 - so we can go ahead and save that
159:32 - and now we need to configure a mini Cube
159:35 - add-on to allow Ingress so we'll do mini
159:38 - Cube add-ons and let's list the add-ons
159:42 - that are available
159:45 - and as you can see there's an Ingress
159:48 - add-on here that's currently disabled
159:51 - so we can do mini Cube add-ons Ingress
159:55 - enable I believe
159:58 - or we can do
160:01 - enable Ingress
160:04 - and once that's done as you can see here
160:07 - it says that after the add-on is enabled
160:09 - please run minicube tunnel and your
160:12 - Ingress resources would be available at
160:15 - this loopback address that we mapped to
160:18 - mp3converter.com so basically whenever
160:20 - we want to use our microservice
160:23 - architecture or test this overall
160:25 - architecture we're going to run this
160:28 - mini Cube tunnel command
160:33 - and while this is running whenever we
160:35 - send requests to our loopback address
160:38 - they're going to go to our mini Cube
160:41 - cluster via the Ingress and since we
160:44 - mapped mp3converter.com to our loopback
160:47 - address if we send requests to MP3
160:50 - converter.com they're going to go to
160:52 - this mini Cube tunnel so just keep in
160:55 - mind so we control C out of this
160:57 - whenever we
160:59 - cancel this process then we're no longer
161:02 - tunneling the Ingress so as you can see
161:05 - here it says please do not close this
161:07 - terminal as this process must stay alive
161:09 - for the tunnel to be accessible so
161:11 - whenever we test we're going to have to
161:12 - run this mini Cube tunnel basically and
161:15 - we'll get to end-to-end testing a little
161:17 - bit later we're not going to test yet
161:19 - until we configure our q and our
161:22 - consumer service as well as our Gateway
161:25 - service which will be the producer
161:26 - service and that is how we are going to
161:29 - Route requests into our cluster and
161:32 - directly to our API Gateway so if we go
161:35 - and check K9s
161:37 - you may or may not see these two new
161:40 - items here but if you remember we've
161:42 - only deployed our auth service so far so
161:45 - we still need to deploy this Gateway
161:47 - service and similar to the off service
161:49 - we're going to have two replicas of our
161:51 - Gateway deployed so we can quit this and
161:56 - we can attempt to apply our
161:58 - configuration that we have here
162:03 - and it looks like we have an issue here
162:06 - so let's go into our secrets file and it
162:09 - looks like we forgot to capitalize
162:12 - Secret
162:13 - so let's once again try to apply
162:16 - and as you can see all of the resources
162:19 - or the objects were created successfully
162:21 - so let's go into canines
162:25 - and we're having an error with pulling
162:27 - the image here so let's see
162:30 - so it's actually not in error with
162:32 - pulling the image so the Gateway is
162:34 - failing because we basically don't have
162:37 - the rabbitmq queue deployed yet and it's
162:41 - trying to connect to this host here that
162:44 - we haven't yet created so that's fine
162:46 - for now so for now it's not going to be
162:49 - able to deploy so actually so it's not
162:52 - continuously running let's just scale
162:54 - that down for now until we create our
162:56 - rabbit mqq deployment so we can do Cube
163:00 - CTL scale and we can do deployment and
163:04 - we want to change the replicas to zero
163:06 - and we want to do that for the Gateway
163:08 - service so it says that our Gateway
163:11 - service was scaled so now you can see
163:13 - the the Gateway service is gone now it's
163:16 - not trying to be scaled up currently
163:18 - because we need to create that rabbitmq
163:20 - so let's go ahead and quit and from here
163:23 - we can start to get into the rabbitmq
163:25 - stuff so we need to create and deploy a
163:28 - rabbit and Q container to our kubernetes
163:31 - cluster so we're just going to change
163:33 - directory back to our source directory
163:36 - and currently we have our auth directory
163:38 - and our Gateway directory now we need to
163:40 - make a directory for our rabbitmq so
163:43 - we're just going to call it rabbit
163:45 - and we'll CD into rabbit and for rabbit
163:48 - mq instead of making a deployment like
163:50 - we did for the other two Services we're
163:53 - going to need to make a stateful set
163:55 - because we want our cue to remain intact
163:58 - even if the Pod crashes or restarts we
164:01 - want the messages within the queue to
164:03 - stay persistent until they've been
164:05 - pulled from the queue so let me go ahead
164:07 - and explain what a stateful set is okay
164:10 - so a staple set is similar to a
164:13 - deployment in that it manages the
164:15 - deployment and scaling of a set of PODS
164:17 - and these pods are based on an identical
164:19 - container spec but unlike a deployment
164:22 - with a staple set each pod has a
164:24 - persistent identifier that it maintains
164:27 - across any rescheduling this means that
164:30 - if a pod fails the persistent pod
164:32 - identifiers make it easier to match
164:34 - existing volumes to the new pods that
164:37 - replace any that have failed and this is
164:39 - important because if we were to have
164:41 - multiple instances of say for example a
164:44 - MySQL server each individual instance
164:47 - would reference its own physical storage
164:49 - and actually there would be a master pod
164:52 - that is able to actually persist data to
164:54 - its physical storage and the rest of the
164:56 - pods would be slaves and they would only
164:58 - be able to read the data from their
165:01 - physical storage and the physical
165:03 - storage that the slave pods use
165:04 - basically continuously syncs with the
165:07 - master pods physical storage because
165:10 - that's where all of the data persistence
165:12 - happens and most of the details
165:14 - surrounding this are not related to our
165:16 - architecture so I'll spare you the
165:18 - details actually to be honest there's
165:20 - probably a better way to configure our
165:21 - rapidmq broker within our cluster but
165:24 - this configuration will work just fine
165:25 - for our purposes we'll only be making
165:28 - use of one replica to achieve the before
165:30 - mentioned competing consumers pattern
165:32 - anyways the most important configuration
165:34 - that you need to understand for this
165:36 - particular service is how we are going
165:38 - to be persisting the data in our cues
165:40 - remember that if our instance fails we
165:43 - don't want to lose all of the messages
165:44 - that haven't been processed because then
165:47 - the users that uploaded those videos
165:49 - that produce those messages would just
165:51 - never hear back from us so basically
165:53 - what we want to do is we want to mount
165:55 - the physical storage on our local to The
165:57 - Container instance and if the container
165:59 - instance happens to die for whatever
166:01 - reason of course the storage volume that
166:03 - was mounted would remain intact then
166:05 - when a new pod is redeployed it will
166:07 - once again have that same physical
166:09 - storage mounted to it so let me show you
166:11 - what I mean by that to show you this
166:13 - I'll need to show you what the
166:14 - configuration file for our staple set is
166:17 - going to look like although we haven't
166:18 - written the code for this file yet just
166:21 - follow along so that you can understand
166:22 - where we're going with this so as you
166:24 - can see this configuration file follows
166:26 - a similar pattern to what all of the
166:28 - other configuration files did so I'm not
166:30 - going to go into detail about every
166:32 - single line if you need to please refer
166:35 - to the kubernetes API documentation that
166:37 - I introduced to you earlier but let's go
166:39 - ahead and have a look at containers here
166:42 - similar to our deployments this is going
166:44 - to determine the contain painter that
166:46 - gets spun up so the image we're using
166:48 - here is a rapidmq image but the part
166:50 - that we need to pay attention to starts
166:52 - here at this volume mounts Mount path so
166:55 - we want to mount a storage volume to our
166:58 - container right this Mount path is
167:00 - configuring where in our container we
167:02 - want the physical storage volume to
167:04 - mount to so basically anything that is
167:06 - saved in this VAR lib rabbitmq directory
167:09 - within our container we'll actually be
167:11 - getting saved to the physical storage
167:13 - volume that will persist even if the
167:15 - container fails and this particular
167:17 - directory is configured as the mount
167:19 - path for a reason this is actually where
167:21 - rabbitmq is going to store the cues when
167:24 - we create a durable cue and the messages
167:26 - when we configure them to be persistent
167:28 - and I'm going to go into detail about
167:30 - how we make the queue durable and the
167:32 - messages persistent a little bit later
167:33 - for now you just need to understand that
167:36 - we're mounting physical storage to this
167:38 - path and this is the path where rabbitmq
167:40 - will save cues and messages okay so now
167:43 - that that's out of the way the rest of
167:45 - the configure duration here under
167:46 - volumes is basically just the
167:48 - configuration for the physical volume to
167:50 - be mounted to the container for example
167:52 - we need to create an additional resource
167:54 - called a persistent volume claim and
167:57 - this config here basically links this
168:00 - staple set to the persistent volume
168:01 - claim that we're going to create and
168:04 - we're going to call that persistent
168:05 - volume claim rabbitmq PVC so what is a
168:09 - persistent volume claim in simple terms
168:12 - the persistent volume claim or PVC is
168:15 - going to be bound to a persistent volume
168:17 - and within the configuration for the
168:19 - persistent volume claim we'll set how
168:22 - much storage we want to make available
168:24 - to it from the persistent volume and the
168:27 - persistent volume will actually be what
168:29 - interacts with the actual physical
168:31 - storage and I know there are many layers
168:34 - of abstraction here but again for our
168:36 - purposes we really don't need to go into
168:38 - too much detail here all you really need
168:40 - to understand is that this configuration
168:43 - is going to make it so that the dura
168:45 - rabbitmq stores cues and messages will
168:48 - actually be persistent storage so
168:50 - whenever the Pod dies those cues and
168:53 - messages will be retained and the new
168:55 - pod will just reconnect to that
168:56 - persistent volume so let's go ahead and
168:59 - write up this configuration so now that
169:01 - we have an understanding what a staple
169:03 - set is we can go ahead and create a file
169:06 - called stateful set.yaml and actually
169:09 - we're going to want to make this in a
169:12 - manifest directory so we'll just make
169:14 - their manifest
169:17 - and CD into manifests and then we'll
169:20 - create the stateful set.yaml file
169:22 - in the here we'll set API version to
169:26 - apps V1 and this time kind is going to
169:29 - be stateful set
169:32 - and metadata
169:34 - will name
169:35 - rabbitmq and our spec configuration so
169:40 - the service name we're not going to use
169:42 - this
169:44 - so we'll just set it to not applicable
169:48 - and selector match labels as usual app
169:53 - rabbitmq now for our template and this
169:58 - is similar to the deployment it's going
170:00 - to be the template for our pods and
170:02 - we'll do metadata
170:04 - labels app is rabbit mq and our template
170:09 - spec or the Pod spec to be more specific
170:13 - through containers
170:15 - name
170:17 - rabbitmq and image is going to be rabbit
170:21 - mq
170:23 - three management and this is the
170:26 - official rabbitmq image and we're adding
170:29 - this management we're using the one that
170:31 - contains management because we want to
170:33 - have the graphical user interface to
170:35 - manage our cues as well included in the
170:37 - image
170:38 - and then we need to set ports and this
170:42 - container is going to need to include
170:44 - two ports we need the port to access the
170:46 - graphical user interface and we also
170:49 - need the port that handles the actual
170:51 - messages for example the messages that
170:54 - we send to the queue are going to be
170:55 - handled on a separate Port from the port
170:58 - that handles our connection to the
171:00 - graphical user interface and you'll see
171:02 - what I mean by that so we're going to
171:04 - set the name of this port the first port
171:07 - to http
171:09 - because we're going to use HTTP to
171:11 - access the graphical user interface
171:14 - and protocol
171:16 - will be TCP and container Port is going
171:21 - to be one five six seven two
171:24 - and we're also going to need a port for
171:27 - amqp which stands for advanced message
171:31 - queuing protocol and this is just the
171:33 - protocol that we use to send messages to
171:36 - the queue
171:39 - and container Port here is going to just
171:43 - be five six seven two
171:45 - and after ports we'll do environment
171:47 - from and we're still going to use config
171:50 - map
171:54 - and we'll call it rabbitmq config map
171:58 - and secret ref name will be rabbitmq
172:03 - secret
172:05 - and we're also going to need a key
172:07 - called volume mounts
172:10 - and we need to specify the mount path
172:13 - and this is going to be the path within
172:16 - the container that we want mounted
172:19 - so we'll do VAR
172:21 - lib
172:22 - rapid mq and the rabbitmq server is
172:26 - going to store the persisted data like
172:29 - the messages and the cues in this
172:31 - directory or at this path so we want to
172:34 - mount this path to our volume and our
172:37 - volume is essentially going to be
172:39 - storage that we connect to our
172:42 - kubernetes cluster which is where
172:44 - persisted data is going to be stored
172:47 - so name will be rabbitmq volume
172:55 - So within spec at the same level as
172:58 - containers
173:00 - we're going to do volumes
173:03 - name
173:05 - rabbit mq volume
173:08 - and we're going to use persistent
173:12 - volume claim and claim name will be
173:17 - rabbitmq PVC which we need to create and
173:21 - that's going to be it for this stateful
173:24 - set configuration
173:26 - so we can save that so now we need to
173:29 - create our persistent volume claim so
173:31 - we're going to do pvc.yaml
173:34 - and we'll do API version
173:37 - P1 and kind this time is going to be
173:40 - persistent
173:42 - volume claim
173:45 - and metadata
173:47 - we're going to do name rabbit mq PVC
173:51 - which we just referenced in our stateful
173:54 - set file and spec and access modes is
173:59 - going to be read write once
174:03 - resources requests
174:07 - storage
174:09 - one gigabyte and storage class
174:13 - name will be standard
174:17 - and we can save that and as usual we
174:21 - need to create our service.yaml
174:24 - API version V1 kind service
174:30 - metadata name of the service will be
174:34 - rabbitmq
174:35 - and our spec so type again is going to
174:39 - be cluster IP our service is only going
174:42 - to have an internal IP address which is
174:45 - only accessible within our cluster and
174:47 - selector
174:50 - app rabbitmq
174:52 - and ports
174:54 - remember that uh ports we're going to
174:57 - need to have the port for our graphical
174:59 - user interface so basically the rabbitmq
175:02 - like Management console and then we need
175:04 - the port for actual message transmission
175:06 - so we'll do ports name http
175:12 - protocol
175:13 - TCP and we're just going to do port
175:16 - 15672
175:21 - and we'll do Target Port is the same
175:25 - and then we'll do our port for our
175:28 - message transmission which will be amqp
175:31 - again and we'll do protocol
175:34 - TCP and this one's going to be Port 5672
175:40 - and Target board is the same port and we
175:43 - can save that and actually one second we
175:45 - need to go back in here so as you can
175:47 - see we're going to need to allow access
175:50 - to this port from a web browser so we're
175:53 - going to need to allow access from
175:55 - outside of the cluster directly to this
175:58 - port so that we can access rabbitmq's
176:00 - Management console so to do that we need
176:02 - to create an Ingress for this port as
176:05 - well
176:07 - so let's go ahead and quit and do Vim
176:09 - Ingress and we'll do
176:12 - API version
176:14 - networking.kh.iov1
176:19 - and kind is going to be Ingress
176:23 - and a data name rabbit mq Ingress
176:29 - and spec
176:31 - rules and host and we're going to do it
176:35 - at
176:37 - rabbitmqmanager.com which we need to
176:39 - configure in our Etc host file
176:43 - and it's going to be http
176:46 - aths and path
176:50 - type prefix
176:54 - back end
176:56 - service and the back end service is
176:59 - going to be rabbit in queue
177:01 - more specifically rabbitmq's port number
177:05 - 15672 which is the port number for the
177:08 - Management console
177:10 - so we can save that and we need to open
177:13 - this file again
177:17 - and we're basically going to do the same
177:19 - configuration but it's going to be for
177:22 - rabbit
177:24 - mqmanager.com I think that's what we
177:27 - called it
177:29 - yeah
177:30 - rabbitmqmanager.com
177:32 - so we can close that save this and close
177:34 - it
177:35 - and let's make a config map and I don't
177:38 - think we currently need any
177:40 - environment variables but let's just
177:43 - make a template
177:45 - just in case we need to add some
177:59 - and we'll do the same thing for a secret
178:23 - and that should be everything so let's
178:26 - go ahead and try and apply this
178:29 - so we have a couple of Errors the first
178:31 - one here is just a spelling error so
178:34 - config map I misspelled the key metadata
178:37 - so let's go in there and fix that
178:42 - meta data
178:44 - and let's apply again and in the
178:47 - service.yaml it's saying that the API
178:49 - version is not set so let's go and
178:53 - service.yaml and that's because I put
178:55 - ape version
178:57 - so API version and let's apply again
179:01 - and it says that
179:05 - stapleset.spec.templetunknown field
179:06 - volumes and that's because volumes
179:08 - should be at the same level as spec so
179:11 - let's go into stapleset.yaml
179:27 - actually the issue is the opposite of
179:29 - what I said volumes should be at the
179:31 - same level as container
179:33 - so it's under spec
179:36 - so we need to move this in one
179:42 - so let's save that
179:44 - and let's apply again and now it seems
179:47 - all of the objects were created
179:49 - successfully so let's go into canines
179:52 - and we see that our rabbitmq pod is
179:56 - pending and let's go have a look
180:00 - and it seems something's not working as
180:03 - expected so let's do
180:06 - subscribe pod rabbitmq and we have an
180:11 - event here warning failed scheduling one
180:14 - pod has Unbound immediate persistent
180:17 - volume claims
180:19 - okay so it seems there's an issue with
180:22 - our PVC so let's go ahead and do qctl
180:26 - describe
180:28 - PVC
180:30 - and we have another event here warning
180:33 - provisioning failed it says storage
180:36 - class storage uh stranded
180:39 - so it can't find the storage class
180:43 - because the storage class is standard
180:45 - and I have a typo so let's go ahead and
180:49 - Vim into our PVC file and here is the
180:54 - error it should be standard so let's
180:56 - save that
180:58 - and let's apply again
181:00 - and actually for persistent volume
181:03 - claims as said here the spec for this is
181:07 - immutable after creation so we're
181:09 - actually going to have to delete the
181:11 - resources created using these files and
181:14 - we only really need to delete the
181:16 - persistent volumes claim but it doesn't
181:17 - matter we'll just delete all of the
181:20 - resources created with this file or
181:23 - created with these files so basically
181:25 - we're going to use this Cube CTL delete
181:27 - command
181:28 - and we're going to have the flag f for
181:31 - files and we're going to delete all the
181:33 - resources created with the files within
181:35 - this directory so let's just delete them
181:37 - all and now that we're done deleting
181:39 - them we can just go ahead and apply them
181:41 - again
181:42 - and it seems they're created
181:44 - successfully so we can go into canines
181:47 - and it looks like the container is
181:50 - creating
181:54 - and it looks like everything is going as
181:57 - expected so let's leave the logs and
182:01 - leave the container so yeah now we have
182:03 - our rapidmq instance running within our
182:06 - kubernetes cluster
182:08 - let's go ahead and quit since we
182:09 - configured an Ingress for this and we
182:11 - configured this route
182:14 - in our Etc hosts file
182:17 - we should be able to access
182:19 - rabbitmqmanager.com and that should take
182:22 - us to the rabbitmq manager so let's try
182:26 - that
182:27 - so let's go ahead and go to
182:30 - rabbitmqmanager.com
182:33 - it's not working because we forgot to do
182:35 - mini Cube tunnel so let's go ahead and
182:38 - clear this and do mini Cube tunnel
182:41 - and as you can see here it's trying to
182:44 - start a tunnel service for both our
182:47 - Gateway Ingress and our rabbit mq
182:49 - Ingress so let's see if we can access
182:51 - the Management console now
182:54 - so let's just refresh this page and I'm
182:58 - just going to accept the risk and there
183:01 - we go we have access to our Management
183:04 - console and let me just go ahead and
183:06 - zoom in here so the username and the
183:09 - password for this should just be admin
183:13 - and login failed so maybe that's not the
183:17 - correct credentials
183:20 - so let's just go to Google and type in
183:24 - rabbitmq Management
183:26 - console default
183:29 - credentials
183:33 - and here it actually says the username
183:35 - and password are both guest so let's try
183:37 - that
183:38 - so we'll do guest and guest and there we
183:43 - go we are logged in zoomed in a little
183:45 - bit too much
183:46 - and this is what the Management console
183:49 - is going to look like
183:52 - and you don't need to get overwhelmed by
183:54 - all of this we're going to limit our
183:56 - Focus to just this cues section so for
184:00 - example we're going to create our cues
184:02 - here using this add a new queue
184:05 - and yeah so we're going to make use of
184:07 - two cues one of them is going to be
184:09 - called video which is going to be the
184:11 - queue where we put our video messages
184:13 - and let me just show you to give you a
184:16 - bit of a refresher so remember while
184:18 - we're actually using our Ingress we
184:21 - can't exit this so we're going to need
184:23 - to open up a new terminal or a new tab
184:26 - so I'll just open a new tab
184:28 - and I'll zoom in here
184:31 - and I'll just change directory to
184:34 - system design python Source rabbits
184:40 - and actually what I wanted to show you
184:42 - is in the Gateway directory so
184:48 - in our server.pi when we upload
184:55 - we use this util.upload and in here
184:59 - we're putting the message on the Queue
185:01 - called video so routing key is just the
185:04 - queue so in the console here we actually
185:07 - need to create that cue so we're going
185:09 - to create a cue called video and this
185:11 - durability needs to be set to durable
185:13 - because if it's transient then that
185:16 - means that if the container is restarted
185:19 - or shut down or anything the queues no
185:21 - longer going to exist you're going to
185:22 - need to create it again so durable means
185:25 - that the queue will be essentially
185:27 - persisted so if the container restarts
185:29 - or something the queue will still exist
185:31 - afterwards
185:32 - so we'll just add this cue
185:36 - so now we have our video queue here
185:39 - so now let's see if we can start up our
185:41 - Gateway server
185:45 - so we can just quit here
185:48 - and canines
185:50 - quickly
185:52 - so yeah we want to spit up our Gateway
185:55 - service so we'll go ahead and let's just
186:00 - change directory into Gateway manifest
186:04 - and we'll do Cube CTL apply we'll apply
186:07 - all of the files in this directory
186:09 - and let's do canines again
186:12 - and as you can see now our Gateway
186:14 - service is able to start up with no
186:17 - issues
186:19 - so at this point we have our Gateway
186:21 - service and our off service and our
186:24 - rabbitmq queue service up and running
186:26 - within our cluster so that means that
186:28 - right now we can upload files and
186:31 - messages for those uploads will be added
186:33 - to the queue and at this point we have
186:35 - no consumer service to consume the
186:38 - messages from the queue to actually
186:39 - convert the files so we need to create
186:41 - an additional service and this service
186:43 - is going to pull messages off of the
186:45 - queue that the Gateway adds to the queue
186:48 - and it's going to convert them into MP3
186:51 - and then it's going to store the MP3 and
186:53 - mongodb and put a message onto another
186:55 - queue called MP3 which will have another
186:58 - Downstream service pull from but I don't
187:00 - want to confuse you all too much so
187:02 - let's just do this step by step
187:05 - so we can leave this
187:07 - and we need to go back to our source
187:12 - directory because we need to create
187:13 - another service so we're going to make
187:16 - dur converter and this converter service
187:20 - is going to convert videos to MP3 so
187:23 - this is going to be the consumer service
187:25 - that pulls the messages off the queue so
187:28 - it knows which videos it needs to
187:30 - convert and where they're stored Etc so
187:33 - we'll make this directory and we'll just
187:35 - CD into that directory and let's clear
187:37 - and we're going to make a file called
187:40 - consumer.pi
187:42 - and in this file we're going to import
187:45 - Pica of course because we need to pull
187:47 - the messages off the queue sys OS time
187:51 - and from PI we need to import
187:55 -  client
187:59 - and that's not how you spell import
188:03 - and we also need to import grid FS
188:06 - because we need to take the files from
188:08 - mongodb to video files and we also need
188:11 - to upload the MP3 files to mongodb and
188:14 - from convert which is a package that
188:17 - we're going to create ourselves we're
188:18 - going to import to MP3 which is going to
188:21 - be of a module within that package
188:24 - and we're just going to define a
188:26 - function called Main and our client is
188:29 - going to be equal to mongodb client and
188:32 - it's going to be our mongodb host which
188:36 - is on our local machine it's not
188:38 - deployed in our cluster remember so we
188:40 - need to use this host
188:42 - mini Cube internal which basically gives
188:45 - us access to our host systems local
188:48 - environment and the port for mongodb is
188:53 - 27017
188:55 - and we're going to do DB Videos equals
189:00 - client dot videos so this instance of
189:03 -  client is going to give us access
189:05 - to the DBS that we have in our
189:09 - database so we can do DB MP3s
189:13 - equals client Dot
189:15 - MP3s so these databases will exist
189:19 - within our mongodb
189:22 - and then we need our grid FS stuff
189:25 - so we'll do FS videos
189:29 - equals an instance of grid FS
189:34 - which we need to pass our DB Videos to
189:39 - and fsmp3s we need to do the same thing
189:49 - and now we need to configure our rabbit
189:52 - mq connection
189:55 - so connection will equal
189:58 - Pika dot blocking connection just like
190:00 - before
190:10 - and we'll do Pica dot connection
190:13 - [Music]
190:15 - parameters
190:17 - host equals rabbit mq
190:21 - and this is possible because our service
190:24 - name is rabbit and Q and our service
190:26 - name will resolve to the host IP for our
190:30 - rabbitmq service
190:33 - and channel will equal
190:36 - connection.channel
190:38 - so what we need to do is we need to
190:41 - create a configuration to consume the
190:44 - messages from our video queue and to do
190:47 - that we're going to use this Channel and
190:50 - basic consume
190:53 - and let's save this
190:59 - and the arguments that we need to pass
191:01 - to this basic consume is our q and we're
191:04 - going to get the Q name from the
191:06 - environment so the queue that we want to
191:07 - consume from in this case is the video
191:09 - queue but just in case we want to change
191:12 - it in the future we're going to just
191:13 - have an environment variable that
191:15 - contains our Q configuration so we'll do
191:17 - OS Dot
191:19 - environ.git and we'll name the
191:22 - environment variable video queue
191:25 - so that's going to be our q and we're
191:27 - going to need to create a callback
191:29 - function that gets executed whenever a
191:32 - message is pulled off of the queue so
191:34 - we'll say on message
191:36 - callback equals callback and we need to
191:40 - create this callback function so we'll
191:42 - go up here and Define callback
191:45 - so whenever a message is taken off the
191:49 - queue by this consumer service this
191:51 - callback function is going to be
191:52 - executed
191:54 - and it's going to be executed with the
191:56 - parameters Channel method
192:00 - properties and body
192:04 - and what we want to do when we get the
192:06 - message is we want to convert the video
192:08 - to MP3 so we'll do two MP3 dot start
192:13 - so there's going to be a function in our
192:15 - two MP3 module called start and we're
192:19 - going to pass in the body of our message
192:21 - and we're going to pass in FS videos and
192:24 - Fs mp3s and the channel and when we
192:29 - create this function you're going to see
192:30 - why we're doing all of this but for now
192:32 - we're just creating the Callback
192:34 - function that's going to call this
192:36 - function so we're going to set the
192:38 - result to error and if there is an error
192:41 - so if error so if there is an error we
192:43 - want to send a negative acknowledgment
192:46 - to the channel
192:48 - so we'll do basic
192:50 - Knack which stands for negative
192:52 - acknowledgment which basically means
192:54 - that we're not going to acknowledge that
192:57 - we've received and processed the message
192:58 - so the message won't be removed from the
193:01 - queue because we want to keep messages
193:04 - on the Queue if there's a failure to
193:06 - process them so they can be processed
193:08 - later by another process and here we're
193:10 - going to do delivery
193:12 - tag equals method dot delivery
193:18 - tag
193:19 - and this delivery tag uniquely
193:22 - identifies the delivery on a channel so
193:25 - when we send this negative
193:26 - acknowledgment to the Channel with the
193:29 - delivery tag rabbitmq knows which
193:31 - delivery tag or which message hasn't
193:34 - been acknowledged so it'll know not to
193:36 - remove that message from the queue but
193:39 - on the other hand if the error is none
193:41 - then that means there wasn't an issue
193:43 - with the conversion
193:44 - so we'll just go ahead and acknowledge
193:47 - the message so we'll do basic
193:50 - pack for acknowledgment and same thing
193:53 - we're going to do
193:55 - delivery Tech is method.deliverytag and
193:59 - if you see here method is one of the
194:01 - parameters that's passed to the Callback
194:03 - function and that's how we're keeping
194:06 - track of the delivery tag so that's it
194:08 - for our callback function and let's go
194:11 - ahead and format
194:12 - and we'll go ahead and print a message
194:16 - that just says waiting for messages
194:20 - and we can also put to exit press
194:24 - control plus c
194:29 - and basically once we run this uh main
194:32 - function here this is going to be
194:34 - printed and then we're going to do
194:36 - Channel dot start consuming
194:40 - and this start consuming is essentially
194:43 - going to run our consumer so our
194:45 - consumer is going to be listening to the
194:47 - queue or listening on that channel where
194:50 - our video messages are being put so we
194:53 - need to do if name equals
194:57 - Main
194:59 - we're going to try and run our main
195:02 - function
195:03 - and we're going to do accept
195:06 - keyboard interrupt
195:08 - so our main function is going to run
195:10 - until we press Ctrl C and interrupt the
195:14 - process
195:16 - and once that process is interrupted
195:18 - with bias pressing Ctrl C then that
195:21 - event is going to be captured in this
195:23 - try except and we're going to catch the
195:26 - keyboard interrupt which is US pressing
195:28 - Ctrl C to cancel the consumer process
195:31 - and in that case we'll just print
195:34 - interrupted
195:37 - and we'll try sis.exit
195:42 - and
195:44 - we'll accept
195:46 - system exit and then we'll do OS dot
195:50 - exit
195:53 - zero and this is basically us just
195:55 - gracefully shutting down the actual
195:58 - service in the event that we do a
196:00 - keyboard interrupt
196:02 - and that is going to be it for our
196:05 - consumer so we need to go and create
196:08 - this convert package and this two MP3
196:11 - module and we also need to install some
196:14 - things and we also forgot to create a
196:16 - virtual environment so let's go ahead
196:18 - and save this
196:20 - and let's go ahead and do python
196:23 - 3
196:24 - p e and B
196:27 - and now we can activate our virtual
196:30 - environment
196:31 - so
196:34 - Source then activate
196:37 - and we now are using our converter
196:40 - virtual environment and now we can just
196:43 - cat consumer
196:45 - dot pi
196:49 - or maybe it's better to do cat
196:52 - head
196:53 - in 10. so we need to install some of
196:57 - these dependencies so we'll do fifth
197:01 - three install
197:02 - Pica and Pi
197:06 - and actually I don't need the comma
197:08 - there so pip3 install Pika and Pi
197:13 - and let's go in here
197:16 - and it looks like we're good to go so
197:18 - let's clear so now we need to create our
197:21 - convert package so we'll just make dare
197:23 - convert
197:25 - and change directory into convert and we
197:28 - need to create the init.pi file and we
197:31 - need to create a module called to MP3
197:35 - and then we're going to need to import a
197:38 - couple of things so we'll import Pica as
197:40 - usual and we need to import Json as well
197:43 - and we need to import temp file which I
197:46 - will show you what that's going to be
197:47 - used for in a second and Os as well and
197:50 - we need to import this binary Json dot
197:54 - object ID
197:57 - and from there we need to import object
198:01 - ID and I'll show you what this is doing
198:03 - or what this is going to be used for
198:05 - soon as well
198:07 - and lastly we need to import this movie
198:10 - Pi editor
198:12 - which we need to install
198:15 - and we're going to define a function
198:18 - called start and it's going to take in a
198:21 - message a gridfs videos instance a grid
198:26 - FS MP3s instance
198:29 - and a channel
198:31 - and for now let's pass so we can go
198:34 - ahead and recap
198:37 - so just to recap if we go into our
198:40 - consumer.pi file
198:42 - we're importing this module that we just
198:44 - created
198:46 - to MP3 and this two MP3 module contains
198:50 - the start function which is the one that
198:52 - we're creating now
199:02 - so we're going to be using this movie
199:05 - pie
199:06 - to convert our videos to MP3 so we're
199:10 - going to need to install pip install
199:11 - movie pi
199:22 - and from there we can start writing the
199:24 - code for our start function
199:27 - so the first thing that we're going to
199:29 - need to do is we're going to need to
199:31 - load our message
199:37 - which is essentially going to make it
199:40 - into a python object
199:43 - so let me just install something really
199:45 - quick
199:55 - so we're going to deserialize an
199:58 - instance containing a Json document to a
200:01 - python object
200:02 - so at this point our message contains
200:05 - the python object version of our message
200:07 - and the first thing that we want to do
200:09 - before converting the file is we want to
200:12 - create an empty temporary file and we're
200:15 - going to write our video contents to
200:18 - that temporary file so we'll do empty
200:21 - temp file is TF so we'll do TF equals
200:25 - temp file dot named temporary file and
200:29 - as you can see here it says create and
200:32 - return a temporary file and this
200:34 - temporary file is going to be a named
200:36 - temporary file as opposed to a temporary
200:39 - file that does not have a name
200:42 - so if we go to this definition here
200:46 - you can see that this temporary file has
200:49 - a name attribute where you can access
200:51 - the file's name so we're going to create
200:54 - this named temporary file and it's
200:57 - essentially going to create a temp file
200:59 - in attempt directory and we can use that
201:02 - temp file to write our video data to
201:06 - so we'll do video contents
201:08 - and we'll set out equal to FS videos dot
201:13 - get so now we're going to get our video
201:16 - file from grid fs and we're going to
201:19 - have it in this out variable or this out
201:22 - object is going to have a read method so
201:25 - we'll be able to write the data that's
201:27 - stored in this out variable to the file
201:30 - so we need to do object ID
201:34 - and message
201:37 - video fid
201:40 - which if you remember from our Gateway
201:43 - service
201:45 - in the actual storage util function
201:52 - we have video FID set to the file ID
201:55 - that was given to us after we put the
201:58 - video file into mongodb
202:02 - but if you also remember we have to
202:04 - convert that into a string because the
202:07 - FID that comes from the return value of
202:10 - this fs.put is actually an ID object
202:14 - so we needed to convert it into a string
202:16 - to put it into our message
202:19 - so we're taking our string version of
202:22 - our FID and converting it into an object
202:24 - and then we need to use that object
202:26 - version to get the file from our mongodb
202:30 - we can't get the file using the string
202:32 - version of the ID
202:33 - so just really quick let's go ahead and
202:36 - save that actually let's go back in here
202:39 - and for some reason it says no name
202:42 - object ID so
202:45 - let's see
202:56 - and I guess that's just an error so
203:02 - so once we have the video file data we
203:06 - can add video contents to empty file and
203:11 - we're going to do that by taking the
203:13 - empty file which is the TF variable and
203:16 - we're going to write to that file the
203:19 - data that's returned from this read
203:21 - method and this is the read method on
203:25 - the out variable or the out object as
203:29 - this read method which allows us to read
203:31 - the data stored in out
203:35 - so if we go to the definition here we
203:37 - can see that we can read the bytes from
203:39 - the file so the bytes that are returned
203:41 - from this read method here are going to
203:44 - be written to our temporary file here
203:48 - and then what we want to do is we want
203:50 - to convert our video file into audio so
203:54 - our temp file currently has the video
203:56 - file so we'll do create audio from temp
204:00 - video file
204:02 - so audio is going to equal moviepi dot
204:07 - editor Dot video file clip and then
204:12 - we're going to take the TF or the
204:14 - temporary file name
204:15 - which is actually going to resolve to
204:18 - the path of the temporary file and we're
204:21 - going to extract the audio from that
204:23 - file so all of this is going to resolve
204:28 - to our audio file being stored in this
204:30 - audio variable
204:32 - and the last thing we need to do is we
204:34 - need to close our temp file and with
204:37 - this temp file module here the temp file
204:40 - will automatically be deleted after
204:42 - we're done with it so basically after we
204:45 - close the file it will automatically be
204:47 - deleted so we don't need to worry about
204:48 - cleaning that up so now that we've
204:50 - extracted the audio into the audio
204:52 - variable we need to write the audio to
204:56 - the file or to its own file and we're
204:59 - going to do that by first creating a
205:02 - path for our audio file so we'll do temp
205:05 - file path equals temp file dot get
205:09 - tempter and this is going to give us the
205:11 - directory on our OS where the temp files
205:14 - are being stored by this temp file
205:16 - module so we'll take that dur and we'll
205:19 - add it to our desired file name
205:22 - so we're going to name it the video file
205:25 - ID which we'll take from message
205:28 - video file ID
205:31 - .mp3 so what we're doing here is we're
205:34 - first taking the path to our temp
205:37 - directory and we're appending our
205:40 - desired MP3 file name to that path so
205:43 - we're left with the full path to the
205:45 - file and we want to name the MP3 file
205:49 - just the file ID of the video because
205:51 - it's going to be a unique ID so in this
205:53 - case we won't have to worry about
205:55 - collisions with file names because every
205:58 - video file is going to have a unique ID
206:00 - and that's going to be the name of the
206:02 - MP3 file as well so then we want to do
206:05 - audio Dot right audio file and we're
206:09 - going to write the file to the path that
206:12 - we just created so we're taking our
206:14 - audio file that was created using this
206:16 - movie pi and this object is going to
206:19 - have a method called Write audio file
206:21 - and we basically just need to tell this
206:24 - right audiophile method where we want to
206:27 - write the file and what we want to name
206:29 - the file so that's why we're just
206:30 - passing this path that we created here
206:33 - and once the audio file is created the
206:36 - temporary audio file we can save the
206:39 - file to
206:40 - so first we need to open the file
206:43 - so we'll open the file at the path we
206:46 - just created and we just want to read
206:48 - the file and we'll set data equal to
206:51 - that file that we opened dot read and
206:55 - then we'll set file ID equal to
206:58 - fsmp3s dot put and then we're just going
207:02 - to put that data that we extracted from
207:04 - the file so we're storing our MP3 file
207:08 - in our grid fs and at this point we
207:10 - don't need that temp file anymore so
207:12 - when we do F Dot close
207:15 - we also need to go ahead and do OS dot
207:19 - remove
207:21 - TF path because remember in this case
207:24 - this write audio file method created the
207:27 - temporary file and not this temp file
207:29 - module so we have to actually delete
207:31 - this temp file manually ourselves
207:34 - and lastly we need to update our message
207:37 - and remember we have this MP3 FID in our
207:43 - message and we want to set that equal to
207:46 - string version of the FID object that we
207:50 - got from uploading the MP3 to mongodb
207:54 - and lastly we need to put this message
207:57 - on a new cue or a different queue that
207:59 - we need to create called the MP3 queue
208:03 - so we'll do try Channel Dot
208:07 - basic publish
208:15 - and we'll use the default exchange Again
208:17 - by just putting an empty string and our
208:20 - routing key it's going to be mp3 for the
208:23 - MP3 queue but remember we're going to
208:25 - get those from the environment the names
208:28 - of our cues from the environment so
208:30 - we'll just call this environment
208:31 - variable MP3 Q
208:36 - and our body of course is going to be
208:39 - Json dot dumps because we need to
208:42 - convert the python object into Json and
208:46 - our message will be the input
208:49 - and we need to make sure the message is
208:52 - persisted until it's processed so we'll
208:55 - do
208:56 - pica.basic properties
209:00 - and we're going to once again like
209:02 - before set the delivery mode equal to
209:06 - pica.spec dot persistent delivery mode
209:13 - and if we can't put the message on our
209:16 - queue
209:17 - and let's just catch the exception as an
209:20 - error
209:22 - just in case we need that variable and
209:25 - then we'll do FS MP3s dot delete
209:30 - FID so basically if we can't
209:33 - successfully put the message on the
209:35 - Queue saying that there's an MP3
209:37 - available for that message then we want
209:40 - to delete the actual MP3 from mongodb as
209:43 - well because if we don't put the message
209:45 - on the Queue then the file in mongodb
209:48 - the MP3 will never get processed anyway
209:50 - so we need to make sure we remove the
209:52 - MP3 from mongodb if we can't add the
209:56 - message to the queue and in that case
209:58 - we're just going to return
210:00 - failed to publish message
210:04 - and the reason this will work is because
210:08 - let's go ahead and
210:10 - save this
210:11 - if we go back into our consumer.pi file
210:16 - if you remember
210:19 - if this start function here fails then
210:22 - we're going to return an error and that
210:25 - error is going to be stored in this
210:27 - error variable and if there is an error
210:30 - then we're going to send a negative
210:32 - acknowledgment for the actual message
210:35 - that's on our video queue so that means
210:37 - that that message will not be removed
210:40 - from the queue and it can be processed
210:41 - again later so that's why we need the
210:44 - actual start method to fail completely
210:46 - because we're going to basically attempt
210:48 - this whole function again if something
210:51 - goes wrong
210:53 - so that's why we need to delete the file
210:56 - from mongodb if we can't put the message
210:59 - onto the queue
211:05 - and let's just go ahead and format
211:08 - and we can save that and that's going to
211:11 - be it for our consumer
211:13 - so what we need to do now is we need to
211:16 - create our Docker file and our
211:19 - kubernetes configuration to create the
211:22 - service within our cluster so let's pip
211:25 - freeze our requirements into a
211:28 - requirements.txt file as usual and then
211:32 - we'll create a Docker file and this
211:34 - Docker file is once again going to be
211:37 - the same as the previous Docker file
211:39 - so we'll just copy this and paste it
211:43 - here
211:45 - and let's just save that
211:49 - and we're going to need to add something
211:51 - additional here called
211:53 - ffmpeg which is just a dependency for
211:56 - the movie Pi module and we need to
212:00 - change this to consumer.pi
212:03 - and we need to change our exposed to I
212:06 - think Port 3000
212:08 - and let's go ahead and save that
212:10 - actually I made a mistake since this is
212:13 - a consumer we're not going to expose any
212:15 - port
212:16 - because it's not going to be a service
212:19 - that we're making requests to this
212:21 - service is going to consume messages
212:23 - from a queue
212:24 - so it's going to act on its own so we'll
212:27 - save that and let's do Docker build
212:46 - and once that's done we can do Docker
212:48 - tag
212:50 - and we'll just take a piece of this
212:53 - and we're going to use your username for
212:57 - your Docker Hub account and this time
213:00 - we'll call it converter
213:01 - and latest
213:03 - and then we can do Docker push
213:08 - converter latest
213:21 - and now if you go to your Docker Hub
213:24 - account
213:25 - foreign
213:29 - you should see that you now have a
213:31 - converter repository as well
213:36 - and you should have the tag latest here
213:38 - as well
213:39 - so now we can make our manifest
213:41 - directory
213:45 - and for this one we just need to create
213:47 - a yaml file for
213:50 - the deployment
213:52 - and the secret and the config map so
213:55 - we're not going to need to create a
213:57 - service configuration for this one
213:59 - so we'll do API version
214:02 - apps V1 kind deployment
214:06 - metadata
214:08 - name is converter
214:11 - labels app converter
214:15 - spec
214:17 - and we'll do four replicas for this one
214:19 - and selector
214:21 - match labels
214:24 - app converter
214:27 - and strategy is going to be type rolling
214:31 - update
214:33 - and rolling update
214:35 - Max surge we'll just double the number
214:39 - of replicas
214:42 - and now for the template
214:46 - labels app converter
214:55 - and we'll do spec
214:57 - actually spec should go back here
215:03 - and containers
215:06 - name equals converter
215:09 - and image is going to be your username
215:13 - converter
215:14 - and environment from will be
215:18 - config map reference
215:21 - and name is going to be converter config
215:24 - map which we'll create
215:27 - and we'll also do Secret ref
215:30 - which will have the name Secret
215:35 - or converter
215:37 - Secret
215:39 - and that's going to be it for that
215:44 - so let's save and we'll create our
215:47 - config map
215:49 - and we'll set API version to V1
215:54 - kind config map
215:57 - metadata name
215:59 - converter config map
216:02 - and data so we need our mp3q name
216:07 - because remember we're using the
216:10 - environment variable to select the queue
216:12 - in our code and we need our video queue
216:16 - name
216:18 - and actually while we're doing this we
216:20 - need to go create our mp3q
216:23 - so back in our Management console
216:27 - we have our video Cube but we need to
216:30 - create another queue so add new q and
216:33 - we're just going to name it MP3 and it's
216:35 - going to be a durable queue as well and
216:37 - we'll just add Q
216:38 - so now we have both the MP3 queue and
216:41 - the video queue
216:42 - and we can save this now we need to just
216:45 - create our secret.yaml and we don't have
216:48 - any secrets for this service so this is
216:50 - just going to be a template file for now
217:13 - foreign
217:16 - let's save that and let's make some
217:18 - space
217:19 - so now let's go into canines and check
217:23 - to see what we have running so far so we
217:25 - have our auth service our Gateway and
217:28 - our queue
217:30 - so now we're trying to deploy our
217:33 - consumer which is the converter
217:35 - so we'll just apply all the files in the
217:39 - current directory and it seems
217:41 - everything was created so let's see if
217:43 - we run into any issues
217:57 - let's see if we can get a better view of
217:59 - the logs so we'll do Cube CTL
218:03 - logs follow
218:05 - and just select one of these converters
218:09 - and we're getting no module named binary
218:12 - Json dot object ID so I must be pretty
218:16 - sleepy because I don't know why I didn't
218:18 - see this before but object ID here is
218:21 - clearly missing a b
218:23 - so let's go into our
218:27 - convert
218:29 - to MP3 file
218:32 - and go over here and change this to
218:36 - object ID
218:39 - and we'll save that
218:41 - change directory and just to double
218:46 - check let's check to see if there's a
218:49 - linting error
218:51 - and go to definition so we're good to go
218:54 - so we need to rebuild the docker file so
218:56 - we'll do Docker build
219:04 - and we need to update the repository so
219:07 - we need to tag it again first of all
219:13 - foreign
219:18 - Docker push converter latest
219:30 - and now that that's pushed let's once
219:33 - again try to apply our configuration
219:35 - that's in our manifest directory
219:40 - and let's check canines
219:47 - hmm actually let's try to let's first
219:52 - um delete
219:54 - and let's see if we can clear this out
219:57 - really quick
219:58 - and now let's
220:00 - let's check to see if they're closing or
220:03 - shutting down
220:04 - okay we're good so now let's try Cube
220:07 - CTL apply
220:11 - and let's check canines
220:14 - and let's see
220:18 - well we got one two and three running
220:22 - and four running so that's good news all
220:26 - right so at this point we have our auth
220:29 - service deployed we have uh multiple
220:32 - instances of the converter service
220:34 - deployed to pull from our rabbitmq which
220:37 - we have deployed as well and we have our
220:38 - Gateway deployed so at this point we can
220:41 - see if uploading files results in
220:44 - messages getting put on the Queue and we
220:47 - can also see if those messages are being
220:49 - pulled off of the queue by our converter
220:51 - service and that'll probably be the most
220:55 - difficult part to get configured because
220:57 - once we have our uploads resulting in
221:00 - the proper messages being put on the
221:02 - Queue and the converter service
221:04 - consuming those messages and converting
221:06 - the videos and storing them in mongodb
221:08 - that's pretty much the entire
221:10 - functionality of this microservice
221:12 - application so at that point we'll just
221:15 - be creating a service to send
221:17 - notifications when in p3s are finished
221:19 - being converted or when videos are
221:21 - finished being converted to MP3 so let's
221:24 - test the end-to-end functionality of
221:26 - uploading and having those uploads be
221:29 - converted
221:30 - so we can quit this and let's clear and
221:34 - we still want our tunnel to be running
221:36 - so make sure that your mini Cube tunnel
221:39 - command is still running and you have
221:41 - another tab opened to work with whatever
221:44 - it is that we're working with at the
221:46 - moment and we want to test by uploading
221:49 - a video file and when we upload that
221:51 - video file we want to see that the
221:55 - message gets added to our video queue
221:57 - and then gets removed from our video
221:59 - queue and another message gets added to
222:02 - our MP3 queue and we don't have a
222:05 - consuming service to consume the mp3q
222:08 - messages so all the messages should just
222:10 - be piling up at the mp3q if our
222:13 - end-to-end functionality is working as
222:15 - expected and also if it's working as
222:17 - expected we should be able to download a
222:20 - converted video file which would just be
222:23 - an MP3 file from our mongodb so let's go
222:26 - ahead and try to test that
222:29 - and I actually don't have Postman or
222:32 - anything like that installed on this
222:34 - laptop at the moment so I'm just going
222:35 - to use Curl to test this but if you're
222:38 - familiar with Postman you can use that
222:39 - to test as well if not just follow along
222:42 - with the commands that I use and make
222:45 - sure you have curl installed of course
222:48 - and at this point we can just go on
222:50 - YouTube and download a video and we
222:54 - don't want the video to take too long to
222:55 - download so we'll just do something
222:57 - short so this one's 37 seconds
223:00 - Mark Zuckerberg says he's not a lizard
223:04 - so we'll just go ahead and take this and
223:08 - copy the link to the video and I'm going
223:12 - to use this YouTube download tool to
223:15 - actually download the video from YouTube
223:17 - so if you want you can just do Brew
223:20 - install YouTube DL
223:26 - and of course it's already installed for
223:28 - me but that's just in case you want to
223:30 - do the same thing to get a video or you
223:32 - can use whatever video you have on your
223:33 - machine already so I'll just do a
223:36 - YouTube download and then paste in the
223:39 - URL for the video
223:42 - and for some reason that video doesn't
223:45 - work so let's just try a different one
223:47 - so yeah sorry Mark
223:50 - and this video is relatively short so
223:54 - let's go ahead and do this one
223:56 - we'll just take the URL
224:10 - and once that's finished downloading you
224:13 - should have the file in your current
224:15 - directory and just really quick we're
224:18 - going to go into our mySQL database
224:25 - and we're going to show tables
224:31 - actually we need to use database auth
224:35 - and I spelled database wrong
224:38 - and actually it should just pu's auth so
224:42 - now we can just show tables and let's
224:44 - just select all from user
224:52 - select all from user now when you do
224:56 - this you should have credentials here
224:58 - from when we created the database and we
225:01 - used our SQL script to create the user
225:05 - in the beginning of this video so you
225:07 - should have an email and a password and
225:10 - these are the credentials that we're
225:12 - going to use in our basic auth when we
225:15 - send a request to our login endpoint to
225:17 - get a Json web token to upload the video
225:20 - so I'm just going to exit this and I'm
225:23 - going to send a curl request and it's
225:26 - going to be a post request and I'm going
225:28 - to send it to http
225:32 - MP3
225:33 - converter.com because remember our
225:36 - Gateway Ingress resolves this host name
225:38 - and we configured this hostname to
225:41 - resolve to our local host or our
225:44 - loopback address so we're just going to
225:46 - use this when we send requests to our
225:48 - Gateway and it's going to be the login
225:50 - endpoint
225:52 - and in curl you can just do this u-flag
225:55 - to do basic authentication credentials
225:58 - and I'll just do Giorgio
226:00 - email.com
226:02 - and the password is admin123 for me
226:07 - and we're actually getting an internal
226:09 - server error so let's go into canines
226:12 - and check our Gateway logs and that's
226:16 - too small so I'll just do Cube CTL logs
226:20 - f
226:21 - Gateway
226:23 - and it's saying in alt service access
226:26 - dot pylon 16 object has no attribute txt
226:29 - so we can just go into that file so we
226:33 - need to change actually we can just go
226:36 - into the file from here
226:42 - so it's line 16.
226:54 - yeah so I'm just going to change
226:56 - directory
227:08 - and I'll activate this virtual
227:10 - environment
227:17 - foreign
227:33 - and it's because the response is
227:36 - response.txt
227:38 - and not response.txt
227:44 - and let's just see if there's any more
227:49 - and let's check the directory for txt
227:53 - and we should exclude our virtual
227:57 - environment
227:59 - so invalidate.pi
228:09 - so we're going to need to change all of
228:11 - these
228:12 - so
228:14 - I hope there's not too many sorry about
228:16 - that but of course this requirements.txt
228:19 - is supposed to be txt and maybe we
228:24 - should check all of the service
228:26 - directories because I don't remember
228:30 - it should be ignoring the virtual
228:32 - environment there so yeah let's just see
228:35 - how it goes
228:37 - so we need to go back into the gateway
228:40 - and we need to do Docker build again
228:50 - and then we need to do
228:52 - cker tag
229:00 - and this is for the Gateway
229:03 - and then Docker push
229:15 - and we'll just delete all of our Gateway
229:19 - resources
229:21 - and just recreate them
229:31 - and it looks like those are up and
229:33 - running
229:36 - so let's try to get our token again
229:40 - and now we were able to successfully get
229:43 - our JWT
229:45 - so we can just copy this JWT
229:48 - and let's go back to the directory where
229:50 - our video file is so that's in
229:59 - converter
230:00 - and here we need to do curl and this is
230:03 - going to be a post request as well
230:06 - but this time we need to add our file
230:09 - and actually to make this easier let's
230:11 - change the name of this file so we'll
230:13 - move this
230:15 - to test.nkv
230:19 - now we can do curl x post
230:24 - file equals at test.nkv
230:31 - and we can't forget the header which
230:34 - needs to be authorization
230:37 - and remember it has to be a bear token
230:40 - and then we'll paste in the token
230:43 - and that's going to go to our
230:47 - MP3
230:49 - converter.com
230:51 - upload and let's see what happens
230:54 - and we are getting 403 Forbidden so
230:59 - let's go ahead and go into canines
231:03 - and maybe that's coming from our auth
231:05 - service
231:08 - let's just check the Gateway nothing
231:12 - hmm so actually it looks like we're
231:16 - using the wrong uh host name here we
231:19 - have MP3 convert but it should be
231:23 - mp3converter.com let's go ahead and sudo
231:26 - Vim our hosts
231:31 - yeah so it should be mp3converter.com so
231:34 - let's go ahead and try it with that
231:39 - and let's make sure our tunnel is still
231:42 - running
231:44 - and let's see
231:47 - okay so now we're getting an actual
231:50 - error from the actual Services I believe
231:53 - so let's go into canines
231:57 - and we can assume that that error came
232:00 - from the Gateway server all right so
232:03 - we're hitting the server now with our
232:05 - upload and maybe our Gateway server is
232:10 - getting an error from auth
232:15 - but it seems auth is returning a 200 for
232:18 - the validate so if we're getting the 200
232:20 - from the validate endpoint on the auth
232:23 - service
232:25 - then that means that our Gateway
232:30 - is returning a an internal server error
232:34 - when
232:36 - we get a 200 from validate
232:38 - so let's change directory into our
232:42 - Gateway
232:44 - and go into server.pi
232:47 - and let's go to validate so for the
232:50 - upload endpoint
232:52 - we validate and at some point
232:55 - I guess we're returning a 500 but
233:00 - hmm
233:01 - that's a bit strange
233:04 - try and
233:07 - let's get another token really quick
233:16 - and let's try with the new token
233:32 - uh and that's because we aren't in the
233:35 - directory of our file so let's change
233:37 - directory back to converter where our
233:40 - test file is and let's try this again
233:44 - and still an internal server error
234:09 - so yeah it's a debug this so I'm just
234:11 - going to scale all of our services down
234:13 - to just one replica so that we don't
234:15 - have to check multiple replicas for the
234:19 - logs
234:20 - so we can just do Cube CTL scale
234:24 - and we're going to scale the deployment
234:28 - Gateway
234:30 - down to one
234:33 - and we also need to scale our converter
234:38 - converter down to one and we also need
234:42 - to scale our auth service down to one
234:46 - so when we go into canines now
234:50 - you can see that we're terminating all
234:52 - of these extra replicas and we're going
234:55 - to have one auth service running and one
234:58 - converter running in one Gateway running
235:04 - so once that's done terminating
235:07 - we can go ahead and I'm just going to
235:10 - open some extra tabs here and run the
235:14 - logs for our Gateway
235:21 - and
235:24 - same for our auth service
235:30 - and the same for our converter
235:40 - that's strange there's nothing printing
235:43 - for the converter so let me see
235:45 - something
236:14 - foreign
236:50 - [Music]
236:53 - and check the logs so the Gateway is
236:57 - what's returning the 500
237:00 - validates returning a 200 so the
237:02 - validation is going through
237:05 - and we're not getting anything on the
237:07 - converter
237:10 - so if the validation is completing
237:13 - but the Gateway is still returning to
237:15 - 500 then that means that something's
237:17 - happening between validation and
237:21 - actually adding the message and
237:23 - uploading the file so if we go into our
237:27 - Gateway server.pi file
237:31 - we can go to validate and our validation
237:34 - happens here and it's successful
237:36 - and we can let's just assume that we're
237:39 - making it to the upload
237:42 - and here's where we're returning a 500
237:45 - so in this case we're catching the error
237:48 - and we're returning a 500 and we're not
237:51 - doing anything with the error so we
237:53 - can't really see what's happening so
237:54 - let's go ahead and print the error here
237:59 - and here as well we'll print the error
238:04 - so then we can see if we're either
238:06 - getting an error when we're trying to
238:08 - publish the message
238:10 - or if we're getting an error when we try
238:12 - to upload the file
238:15 - so hopefully we can get some more
238:17 - information by printing these errors so
238:20 - we can go ahead and save this and we're
238:22 - going to need to change directory to
238:25 - Gateway
238:28 - and we need to do Docker build and tag
238:31 - and push again
238:32 - so we'll do Docker build
238:35 - because we need to add the code changes
238:37 - with the print statements and then we'll
238:40 - do Docker tag and it's going to be the
238:43 - Gateway repository that we're pushing to
238:47 - so we'll do username
238:50 - Gateway latest
238:55 - and then Docker push
238:57 - Gateway
239:03 - and once we've pushed we can just delete
239:06 - all of our
239:09 - resources for the Gateway using our
239:13 - manifest files and then we can just
239:15 - recreate them
239:23 - on actually
239:25 - we don't want to use we don't want it to
239:29 - scale up more than one right now because
239:31 - we're debugging so let's do Cube CTL
239:34 - scale
239:36 - deployment
239:38 - to one
239:39 - and for the Gateway
239:51 - and let's change directory back to where
239:54 - our file is
239:56 - and our logs since we shut down that
240:00 - container that we were following the
240:02 - logs for we need to do Cube CTL logs
240:04 - again for the new container
240:09 - and let's check our tunnel it's asking
240:12 - for the password again so we need to do
240:14 - that
240:16 - yeah
240:18 - and let's try this again
240:24 - so it was a success that time
240:27 - so I think what was happening was the
240:30 - old Gateway deployment like the old
240:32 - replicas weren't connected to the host
240:34 - because the host variable wasn't
240:38 - resolving to the actual rabbit and Q
240:40 - host in the container and that's
240:42 - something that happens sometimes so
240:44 - basically like huh let's see if we can
240:47 - let's see if I can prove what I'm what I
240:49 - suspect is happening
240:52 - so for example if we have our rapidm QQ
240:57 - here
240:57 - our Gateway is connecting to the
241:00 - rabbitmq using the name rabbitmq which
241:03 - is the name of the stateful set so let
241:07 - me try and clarify
241:09 - so let's change directory to the Gateway
241:12 - and if we go into
241:16 - storage and util.pi
241:24 - actually not storage util it's just in
241:26 - server.pi
241:29 - our connection to rabbitmq we're using
241:33 - this name rabbitmq which is the name of
241:36 - the service
241:38 - because if we
241:40 - check our rapidmq
241:43 - manifestservice.yaml you see the name is
241:45 - rabbitmq
241:47 - so in kubernetes the service name will
241:51 - resolve to that Services host
241:54 - so in server.pi
241:57 - we're depending on this name resolving
242:01 - to the host for abadin Q but it seems
242:04 - that in kubernetes if a container is
242:08 - connecting to a host via this name if
242:11 - that host changes or restarts it seems
242:15 - that the containers that reference that
242:17 - service name still reference in older
242:20 - host I believe
242:22 - so for example if I were to go in here
242:28 - so first let's send that again uh let's
242:31 - change directory
242:33 - to converter and let's send that curl
242:35 - request again
242:38 - hmm and we're getting an internal server
242:41 - error again
242:44 - but this time
242:46 - it's not our internal server error so
242:49 - let's see what's happening
242:54 - so it says we're referencing
242:57 - this
242:59 - error before the assignment hmm
243:03 - so let's go into Gateway where we did
243:07 - that
243:09 - storage util.pi
243:12 - print error
243:15 - print
243:17 - ah here we're not catching the error
243:21 - for example up here we're doing accept
243:23 - exception as error
243:26 - but here we're just doing accept so we
243:28 - need to do exception as error
243:32 - foreign
243:35 - Docker build
243:39 - and Docker tag
243:52 - and Docker push
243:57 - oh actually
244:00 - that's pretty bad so actually I just did
244:03 - that in the converter directory so
244:06 - I basically built the converter image
244:09 - and pushed it to our Gateway image so I
244:12 - pretty much just overwrote the Gateway
244:14 - image with the converter image uh let's
244:17 - see if we can just cancel that
244:19 - and let's change directory to Gateway
244:23 - and now here we can do Docker build but
244:26 - let me just double check to see what I
244:28 - did
244:31 - so yeah I did Docker build in the
244:32 - converter directory so it built the
244:34 - docker file within this directory which
244:37 - is our converter Docker file and then I
244:40 - pushed it to
244:43 - the
244:45 - actual Gateway repository and what did I
244:49 - change
244:53 - yeah I made changes to the Gateway
244:54 - though so Docker build in the Gateway
244:57 - directory to build our Docker file for
245:00 - Gateway
245:02 - Docker tag
245:15 - Docker push
245:20 - and now the latest image for our Gateway
245:23 - repository is going to be this most
245:25 - recent one so that should resolve the
245:28 - issue or accidentally pushed the
245:30 - converter image
245:32 - and we'll do Cube CTL delete again for
245:37 - our Gateway resources
245:40 - and apply
245:43 - and actually once again I forgot to
245:45 - scale so scale it to one
245:49 - foreign
245:52 - so it looks like that time we got to
245:54 - success it worked so
246:09 - so we're still terminating one of the
246:10 - replicas
246:13 - mini cubes asking for a password again
246:15 - because we had to redeploy the Ingress
246:18 - as well
246:27 - so
246:30 - all right so let's go ahead and
246:34 - set up the logs again for our Gateway
246:41 - and let's send the request again for the
246:43 - upload
246:45 - and again we need to go to where the
246:47 - file is
246:51 - and we get a success
246:56 - auth is successful and the converter is
247:00 - writing the audio so that's successful
247:02 - as well
247:05 - so back to my explanation of what I
247:07 - think the issue was before
247:09 - so our Gateway is using the service name
247:12 - for this rabbitmq service to connect to
247:16 - that host but I think that if for
247:17 - example we restart this rabbitmq the
247:20 - Gateway will still be resolving to the
247:23 - old host using that rapidmq service name
247:26 - so the Gateway won't actually be able to
247:28 - connect if we were to restart this
247:30 - unless we restart the Gateway which
247:33 - would refresh its reference to the host
247:35 - I believe
247:36 - so let's just try and test that theory
247:39 - because I don't really like not knowing
247:41 - what happened so if we send again
247:44 - we should get a success and then if we
247:47 - go into canines
247:51 - and we just delete this pod
247:56 - so we're going to delete our rabbitmq
247:57 - pod and it's going to make it restart
248:03 - and the converter breaks because it's
248:05 - it needs to connect to the rapidmq host
248:08 - as well
248:12 - foreign
248:13 - so once the rabbit mq pod restarts let's
248:17 - just restart this one refresh the
248:19 - converter
248:20 - now at this point if my theory is
248:23 - correct the Gateway is still referencing
248:25 - the older rabbitmq host using the
248:29 - rabbitmq service name so it's it should
248:32 - fail if we try to upload right now
248:36 - so if we do this again
248:39 - we get an internal server error so if we
248:43 - reset the Gateway it will reset its
248:46 - variable for rabbitmq or it will reset
248:50 - what it resolves rabbit in Q2 which
248:52 - would be the new host so resetting this
248:55 - should actually make it work
249:01 - so now if we do curl upload
249:05 - we get success so yeah my theory is
249:09 - pretty much correct and it's kind of
249:12 - annoying
249:14 - but yeah just keep that in mind so for
249:18 - example
249:20 - let me just
249:22 - recap what I said
249:28 - in server.pi we're connecting to our
249:31 - rabbitmq using the service name
249:35 - so the service name in kubernetes
249:38 - resolves to the host so yeah the theory
249:41 - is that the cluster IP address that this
249:43 - service name resolves to which is the
249:46 - cluster IP address of the service the
249:49 - rabbitmq service the theory is that IP
249:52 - address changes when we restart the
249:54 - rabbitm coupon but the Gateway pod still
249:57 - references the old IP address using this
250:00 - service name which is why when we
250:03 - restart the Gateway pod this rabbitmq
250:06 - its reference for this rabbitmq service
250:09 - name gets updated which is why it works
250:12 - after doing that
250:14 - but anyways the good news is it seems
250:17 - that everything is working so when we
250:20 - upload a file we're successfully
250:25 - if we were to clear this and do
250:29 - logs for our converter
250:34 - it seems that we're successfully writing
250:36 - the audio so in order to confirm that
250:40 - the end-to-end functionality is working
250:42 - correctly we can check for the existence
250:45 - of a audio file from our video
250:48 - conversion in the DB and that's how
250:51 - we'll check to see if everything is
250:52 - working from end to end and also if we
250:55 - go to our queue let's refresh we can see
250:58 - that our mp3q has four messages ready
251:02 - and remember we don't have a consumer
251:04 - service pulling from the MP3 queue so
251:07 - all of the messages are just going to
251:09 - pile up here so at this point we should
251:11 - have four MP3s in our database and we
251:16 - uploaded the same video every time so
251:18 - all four MP3s should be the same audio
251:20 - file but at this point we should be able
251:22 - to download one of these four MP3s from
251:26 - our mongodb and test it to see if it's
251:28 - working correctly and as you see our
251:30 - video queue is empty because any message
251:33 - that we put on the video queue was
251:35 - processed by our consumer service which
251:38 - converts them to MP3 and then that
251:40 - service then puts them onto the MP3
251:42 - queue so to confirm that everything is
251:44 - working correctly let's just check our
251:47 - mongodb to see if we have four MP3 files
251:51 - which should all be the audio file for
251:54 - the same video that we uploaded four
251:56 - times and we can also go ahead and do it
251:59 - one more time
252:02 - and we're getting an internal server
252:05 - error so
252:06 - let's go ahead and
252:09 - actually don't remember if we restarted
252:12 - the Gateway so let's just restart that
252:24 - just wait till this terminates and we'll
252:27 - do logs again
252:29 - for Gateway
252:33 - and we'll upload that again and we get a
252:36 - success
252:38 - so now there should be five messages in
252:41 - our queue the MP3 queue so as you can
252:44 - see we now have five messages in the
252:46 - queue so now we should have five MP3s in
252:49 - our mongodb database and to test that if
252:53 - you installed mongodb earlier in this
252:56 - course you should have shell
253:00 - which should put you directly into the
253:03 - mongodb that's running on our local host
253:05 - or that's running on our local machine
253:07 - and we should be able to do show
253:09 - databases and it shows that we have this
253:12 - MP3s database and this videos database
253:15 - so this database should have five audio
253:18 - files stored so let's see if we can just
253:21 - use MP3s
253:25 - and now let's try and show collections
253:29 - and as explained before with grid FS the
253:33 - actual file data is stored in these
253:35 - chunks and the files will have a
253:38 - reference or the file is essentially
253:41 - like the metadata for a collection of
253:43 - chunks so if we do DB dot FS dot not
253:47 - chunks fs.files.find
253:52 - this will show all of the objects that
253:55 - we have stored
253:58 - and actually I forgot that when I was
254:01 - testing this prior to making this
254:03 - tutorial I uploaded a bunch of videos
254:05 - that were converted to mp3s so my
254:08 - database is going to have more than five
254:10 - yours should only have the five or it
254:13 - should only have as many MP3s as you
254:16 - sent upload requests that were
254:18 - successful so let's actually do it this
254:20 - way so we can go to the queue the MP3
254:22 - queue
254:23 - and we can actually let's see get a
254:27 - message from the queue so let's just do
254:30 - get message
254:32 - and we see that there's an MP3 file ID
254:36 - that has this ID
254:39 - so let's go ahead and copy this
254:43 - and here we can do DB Dot actually let's
254:48 - uh show collections again
254:51 - and we can do db.fs.files.find
254:58 - and remember we want to use the actual
255:00 - object version of the ID to find it and
255:03 - to do that we'll do
255:07 - underscore ID for the key and then we'll
255:11 - do object
255:12 - ID and inside of there we can put our
255:17 - actual ID and as you can see that actual
255:21 - object ID is stored successfully inside
255:25 - of our MP3s database so now what we want
255:28 - to do is we want to download this and
255:31 - see if it's an actual audio file and to
255:34 - do that we can go ahead and exit this
255:37 - and let's clear and again if you
255:40 - installed mongodb using the instructions
255:42 - earlier in this video you should have
255:44 - this files and you can put for the
255:48 - DB mp3s and we want to get by ID and we
255:52 - want the local file to be called we'll
255:55 - just do test
255:57 - dot MP3 and we need to use this object
256:02 - to get the ID same way that we have to
256:04 - do that within our database so
256:09 - we'll put that in
256:10 - which is the string version of the ID
256:12 - within this object syntax and we should
256:15 - be able to download the file that way
256:19 - so it was able to connect to our mongodb
256:22 - on the local host and it was able to
256:24 - finish writing to test MP3
256:27 - so now this test MP3 file should contain
256:31 - the sound for our video file so it
256:35 - should contain the sound for this file
256:36 - so let me just go into this directory on
256:39 - the user interface okay so now in this
256:42 - directory which is the directory of our
256:44 - converter service we have this video
256:47 - file which is the test MKV file
256:54 - so here's a burger again the double
256:57 - double and I'm just going to start
256:58 - eating away with a bag here and uh
257:01 - and yeah it's cut that a bit short don't
257:03 - know about copyright or whatever so this
257:07 - audio file should be the sound for that
257:10 - video file so let's go ahead and just as
257:13 - you can see it's an audio file then we
257:14 - can just go ahead and play this so
257:16 - here's the burger again the double
257:18 - double and I'm just going to start
257:19 - eating away so yeah it looks like our
257:22 - end-to-end functionality is working up
257:25 - to the point where we put the message on
257:27 - the MP3 queue so at this point we are
257:31 - uploading our video and adding the
257:34 - message to a queue so we're essentially
257:36 - when we upload a video it gets put onto
257:38 - mongodb then we create a message and add
257:41 - it to this video queue and then our
257:43 - consumer converter service is going to
257:46 - pull off of this video queue convert the
257:49 - video into an MP3 and then put a new
257:51 - message on this MP3 queue saying that in
257:54 - mp3 for a specific file ID exists within
257:57 - mongodb so the last thing that we need
257:59 - to create is a service that's going
258:01 - going to consume This MP3 cue and that
258:05 - service is just going to be a
258:07 - notification service that's going to
258:09 - tell our user that a video is done or a
258:13 - video conversion to MP3 process is done
258:15 - so the service is essentially going to
258:18 - pull the message off the queue and it's
258:19 - going to have the ID and the email of
258:22 - the user and it's going to send an email
258:24 - to the user saying hey this ID is
258:28 - available for download as an MP3 and
258:30 - then from there there's going to be a
258:32 - download endpoint that we create on our
258:34 - Gateway where the user can use his or
258:37 - her token to basically request to
258:40 - download a specific MP3 using the file
258:44 - ID that's sent in the notification
258:46 - service email so essentially if you've
258:48 - gotten to this point where you're
258:50 - actually having the messages put on the
258:52 - MP3 queue and the mp3s stored in mongodb
258:56 - then you've essentially completed the
258:58 - most difficult part of this entire
259:00 - tutorial because from there we're just
259:03 - going to pull messages off this queue
259:04 - and send an email and that's pretty much
259:06 - it so if you've gotten this far this is
259:08 - like a major checkpoint so from here
259:10 - onward we're just going to create that
259:13 - additional service and add the code for
259:15 - the download endpoint on our Gateway
259:18 - okay so at this point in the tutorial
259:20 - what we have left is our notification
259:23 - service and we need to update our
259:26 - gateway to have a download endpoint so
259:29 - we can just start by updating our
259:31 - Gateway and then we'll move on to
259:33 - creating our notification service after
259:36 - that so we're just going to change
259:38 - directory into our Gateway directory and
259:41 - let's go ahead and clear this and we can
259:43 - actually close these other tabs that had
259:45 - our logs
259:48 - foreign
259:50 - we can just leave that running for now
259:53 - and we're going to need to update our
259:56 - server.pi file so let's go ahead and
259:59 - open that file and from here we're going
260:01 - to need to import a couple of additions
260:04 - so we need to import send file which is
260:08 - going to be the method that we use to
260:11 - send files back to the user that
260:13 - downloads them
260:14 - and we're going to be pulling that file
260:17 - from mongodb so just like before we're
260:19 - going to need to use the binary Json
260:22 - object ID so we'll import
260:26 - object ID
260:31 - and make sure I spelled it right this
260:33 - time and we're also going to need to
260:35 - change our configuration for mongodb so
260:39 - right now we're setting the
260:40 - configuration for our URI to
260:42 - include the videos database but that
260:45 - kind of limits us to just using the
260:47 - videos database but we need to use both
260:50 - databases the MP3 and the videos one so
260:53 - instead of using this configuration this
260:56 - way we're just going to create two
260:59 - separate instances of Pi for each
261:02 - database and if we have a look at the
261:04 - flask Pi documentation here
261:07 - essentially what we need to do is this
261:09 - here
261:10 - and it says you can create multiple Pi
261:12 -  instances to connect to multiple
261:14 - databases or database servers so we're
261:17 - going to do two of these and one's going
261:20 - to be for our videos database and one's
261:23 - going to be for our MP3s database
261:26 - so we're just going to change this to
261:28 -  video
261:30 - and we can go ahead and
261:34 - include the URI when creating the
261:37 - instance
261:42 - and can't forget the comma here and
261:45 - we're also going to remove this config
261:48 - here so we're not going to use this
261:50 - anymore because we're going to configure
261:52 - the URI for each instance within the
261:56 - instantiation of the pi class
261:58 - so we'll remove that
262:01 - and for MP3 it's going to be
262:04 - similar
262:05 - we'll change this to MP3
262:09 - and we'll change this to mp3s
262:12 - and now for grid FS we need to create
262:15 - two separate FS instances
262:19 - so this one can be videos
262:21 - and this will need to be set to
262:24 - video because we're using the
262:27 - video instance for the videos fs and
262:29 - this one can be MP3s
262:33 - and this will be changed to MP3
262:36 - and since the old grid FS instance was
262:40 - referenced by the variable FS we need to
262:43 - update that so here we're uploading a
262:46 - video and we were uploading it to the
262:49 - videos mongodb instance but now we need
262:53 - to change this to FS videos specifically
262:58 - and that is all that we need to do for
263:01 - that
263:04 - and now we can go to our download
263:06 - template endpoint that we created and we
263:09 - can actually write the code for the
263:11 - download function so we can remove this
263:14 - pass and we're going to do the same
263:17 - validation that we do in the upload
263:19 - endpoint so we can just copy this so
263:22 - we're going to copy the validate token
263:25 - request and we're going to copy loading
263:27 - the access from that variable that
263:29 - validate token resolves to so we can
263:32 - just go ahead and copy that part and go
263:34 - back to our download and paste that in
263:39 - and we need to add the else here
263:44 - and actually I just realized we're not
263:46 - checking the error here in our upload
263:50 - so let's go back to upload and we need
263:53 - to do if error so if there's an error
263:56 - when we try to validate
263:58 - we need to return
264:00 - error and let me just check something
264:02 - really quick
264:08 - actually no we should be good
264:11 - foreign
264:14 - we don't need to have an else
264:17 - so basically if there is no access then
264:20 - we're just going to return
264:23 - not authorized
264:34 - anyway so the download function is going
264:37 - to be pretty straightforward we first
264:39 - need to make sure the file ID exists in
264:42 - the request so basically the
264:44 - notification service is going to send an
264:47 - email to the user informing them that
264:49 - the conversion job is done and in that
264:52 - email it's going to give them a file ID
264:54 - which is the file they should download
264:56 - when they send a request to the download
264:58 - endpoint and that file ID is required so
265:01 - we're going to do
265:04 - FID string equals request Dot args.get
265:13 - and it's going to be fid
265:16 - and basically if this parameter doesn't
265:20 - exist in the request then it's going to
265:22 - return none so if we go here we see that
265:26 - it Returns the default value if the
265:29 - request data doesn't exist and the
265:32 - defaults for the default value is none
265:34 - so we can set this if we want but the
265:37 - none is the behavior that we want so if
265:41 - FID doesn't exist in the request then
265:43 - this FID string will equal none so that
265:47 - means that we can just do if not FID
265:51 - string then we can just return FID is
265:54 - required
265:58 - oh and I updated the upload endpoint to
266:02 - handle the error but I didn't update
266:04 - this one so we'll do if error
266:07 - return error
266:12 - and anyways if the FID string does exist
266:15 - then we're going to use it to get our
266:18 - file from mongodb so we'll do try
266:23 - and we'll set out equal to FS MP3s so
266:28 - this is the mongodb instance for our MP3
266:31 - database dot get
266:33 - and remember we need to use the object
266:35 - ID
266:37 - which is here and it's a mongodb object
266:40 - ID and we're importing that here
266:45 - and to that we're going to pass the FID
266:47 - string which is essentially going to
266:50 - convert our FID string to a object ID
266:54 - which is what's needed to get the object
266:57 - from mongodb so the data for our MP3
267:01 - will be referenced by this out variable
267:04 - so then we can just return
267:07 - send file
267:09 - and we'll return out and we'll do the
267:11 - download name which is going to be the
267:14 - name of the file and we're just going to
267:16 - set it to FID string plus MP3
267:22 - and this is also going to be able to
267:25 - determine the mime type for the file so
267:28 - this is all we need to do to return the
267:30 - file to the client except
267:33 - exception as error
267:36 - we'll print the air and we'll return
267:39 - internal server error
267:47 - and that's pretty much going to be it
267:49 - for our download endpoint
267:52 - so we're just going to get the access
267:53 - via the validate endpoint from our alt
267:56 - service
267:57 - and then we're just going to check to
267:59 - see if admin is true in that user's
268:01 - access and if so we're just going to
268:03 - return the file for the past file ID and
268:06 - we're not going to check the user's
268:08 - email or anything in this case we're
268:09 - just going to assume if they have the
268:11 - file ID then they have access to the
268:13 - file but of course feel free to expand
268:16 - upon this however you like so let's go
268:18 - ahead and save that and since we changed
268:21 - the code we need to build this Docker
268:24 - image again and push it to our
268:26 - Repository
268:27 - so we'll do Docker build
268:38 - and then we'll do Docker tag
268:48 - and Docker push
268:57 - and now we can redeploy this so I'm just
269:00 - going to delete all of the resources for
269:02 - Gateway just in case so I'll just delete
269:05 - everything in the Manifest directory
269:08 - and then I'll just apply them again
269:19 - and we'll check that later for now we
269:21 - can just go into creating our
269:23 - notification service so let's change
269:26 - directory to our source directory and we
269:29 - need to make their notification
269:32 - and we'll CD into notification
269:34 - so this notification service similar to
269:37 - our converter service is going to be a
269:40 - consumer service so we're going to have
269:43 - some similar code to the converter
269:45 - service so we're going to copy some
269:47 - things from our converter consumer.pi
269:50 - file into our notification consumer.pi
269:53 - file so let's go ahead and create a file
269:56 - called consumer.pi
269:58 - and we're just going to go to our
270:02 - converter consumer.pi file
270:05 - and we'll just copy everything over
270:12 - but we're actually not going to need any
270:14 - of that and we're also now going to need
270:16 - any of this but we're going to create a
270:19 - package called send and a module called
270:23 - email that we're going to need to import
270:26 - and this is where we're going to write
270:27 - the code to send the email
270:30 - and our callback functions basically
270:33 - going to stay the same but we are going
270:35 - to change this to email dot notification
270:39 - we're going to create a notification
270:41 - function
270:43 - and it's only going to take in the body
270:46 - of the message but other than that the
270:49 - Callback function is going to be the
270:51 - same and of course we need to change
270:53 - this to mp3q because this consumer
270:57 - service is listening or consuming from
271:00 - the mp3q
271:04 - and everything else is going to stay the
271:06 - same
271:10 - so now we need to go and create this
271:13 - function here this email.notification
271:15 - function
271:16 - so let's save this file and we'll make a
271:19 - directory called send and change
271:22 - directory into send and we'll touch
271:25 - init.pi
271:27 - and we're going to create a file called
271:30 - email.pi
271:32 - so in this file we're going to write all
271:34 - of the code to send in email
271:35 - notification to the client
271:38 - and we're going to just use this python
271:41 - documentation here which gives an
271:44 - example of how to send simple email
271:46 - messages which is all we really need to
271:48 - send so our code is going to be similar
271:51 - to this here
271:52 - but instead of sending the message via
271:55 - our own SMTP server we're going to use
271:57 - Google's SMTP server so we're just going
272:00 - to send it using the same SMTP server
272:03 - that Gmail uses and I will show you how
272:06 - I'm going to do that now so first we
272:08 - want to import
272:11 - SMTP lib and Os and for this tutorial
272:15 - you don't really need to know what an
272:17 - SMTP server is in simple terms it's just
272:20 - a server to send receive and relay
272:22 - outgoing mail between email senders and
272:24 - receivers so we're essentially just
272:27 - going to be using the same SMTP server
272:29 - that Gmail uses within our application
272:32 - to send emails
272:34 - so we need to import the SMTP lib and Os
272:39 - and we also need to import from
272:40 - email.message
272:43 - email message
272:46 - and this is just going to allow us to
272:48 - create an instance of an email message
272:51 - and you'll see what I mean by that in a
272:53 - second so we'll do Define notification
272:59 - and notification will take in message
273:01 - from our queue
273:03 - and then from there we will try and
273:06 - we'll set message equal to json.loads
273:10 - the message we're going to turn our
273:13 - message from our queue into a python
273:15 - object
273:16 - and then we'll do MP3 FID equals the FID
273:21 - in our message
273:24 - and we need a sender address which is
273:28 - going to be the address that we're using
273:30 - to send the email and I'm going to
273:33 - recommend that you create a dummy Gmail
273:35 - account to send the email because in
273:38 - order for this to work within that Gmail
273:39 - account you're going to need to enable
273:42 - or authorize non-google applications to
273:45 - log into your Gmail account so like
273:48 - basically the default setting when you
273:49 - create a Gmail is like only Google
273:52 - applications can log into that account
273:54 - so for example like the Gmail
273:56 - application on your phone can log into
273:58 - that Gmail account but in order for this
274:01 - application to log into that account
274:02 - you're going to have to authorize
274:04 - non-gmail or non-google applications and
274:07 - that means any non-google application
274:09 - can log into your Gmail account if they
274:11 - have your credentials so it's not
274:13 - recommended that you use your actual
274:15 - Gmail account for this because I
274:17 - wouldn't recommend allowing non-google
274:20 - applications to log into your primary
274:22 - Gmail account if you want to be safe you
274:24 - should create a dummy account so the
274:27 - sender address is going to be the email
274:29 - address of the Gmail account that you
274:32 - want to use to send the email so I'm
274:34 - going to store it in an environment
274:36 - variable
274:38 - and the environment variable is going to
274:40 - be called Gmail address
274:42 - and then we're going to also do sender
274:44 - password
274:46 - and it's going to be in an environment
274:49 - variable called Gmail password
274:52 - and I too am going to create a dummy
274:55 - account for these credentials and I'll
274:58 - get into that in a little bit and then
275:00 - the receiver address
275:03 - which is who we're sending the email to
275:05 - is going to be the user that's
275:10 - using
275:16 - is going to be the user who's associated
275:19 - with the JWT
275:23 - because that's the user who uploaded the
275:26 - original file and then we're going to
275:28 - create the message that's going to be in
275:30 - the email it's going to be an instance
275:32 - of email message and we'll do message
275:35 - dot set content
275:38 - and it's just going to be a simple
275:40 - message it's just going to say
275:42 - MP3 file ID
275:47 - and then MP3 fid
275:51 - which is coming from here
275:55 - and then is now ready
275:59 - so then the receiver of this email can
276:02 - just take the file ID that we give them
276:04 - and they can send a request to our
276:05 - download endpoint using this file ID to
276:08 - download that file and we can just say
276:10 - the message subject
276:13 - which is just the same subject that you
276:16 - would put when you're writing an email
276:17 - in Gmail and that's just going to be MP3
276:20 - download
276:21 - and message from is going to be
276:25 - sender address
276:28 - and message two is going to be receiver
276:31 - address
276:33 - and lastly we need to create an SMTP
276:37 - session for sending the mail so
276:39 - essentially we need to connect to
276:41 - Google's SMTP server and then log into
276:44 - our Gmail account and then send the
276:46 - email so we'll set session equal to SMTP
276:50 - lib dot SMTP
276:54 - and we'll put in the Gmail SMTP server
276:56 - which is smtp.gmail.com
277:00 - and then we'll do session dot start TLS
277:06 - and what this does is it puts the
277:09 - connection to the SMTP server into TLS
277:12 - mode and TLS is transport layer security
277:15 - so it's essentially going to make sure
277:17 - our communication between the SMTP
277:20 - server is encrypted and this is
277:22 - essentially to secure our packets in
277:25 - transit so that they can't be
277:27 - intercepted and the data within them
277:29 - read so in simple terms it's just to
277:32 - secure our communication between our
277:35 - application and the service you don't
277:36 - really need to know details about how
277:38 - this is working either just know that
277:40 - it's necessary and then after that we're
277:43 - going to do session.login
277:45 - and we need to log in using our Cinder
277:48 - address and sender password
277:51 - and once we've done that we can do
277:53 - session dot send message
277:57 - and in there we're going to put the
277:59 - message which is the message that we're
278:02 - instantiating here and we're customizing
278:05 - that object here and then we're sending
278:08 - it here
278:09 - and we're going to put the sender
278:11 - address and the receiver address and
278:14 - then once we've sent the message we want
278:15 - to close the session so we can do
278:17 - session dot quit and once that's sent we
278:20 - can just print
278:21 - male scent
278:24 - just so that we can see that it's done
278:25 - and if any of this fails we want to just
278:28 - print the error so we're in this try
278:30 - block here
278:33 - so we'll just do accept exception as
278:38 - error and then we'll print the error
278:41 - and then we'll return the error
278:44 - and the reason we're catching the error
278:46 - here and returning it is because
278:48 - the call to this function is expecting
278:51 - an error so the error should either be
278:53 - none or it should contain an error
278:56 - for example so if we go back up One
278:59 - Directory into our consumer file
279:03 - we see that notification here
279:06 - let me close that notification here is
279:09 - expecting an error and if there is an
279:12 - error then we're going to send a
279:14 - negative acknowledgment so the message
279:17 - can stay on the Queue and be processed
279:19 - by another process but if there's no
279:21 - error we're going to send a basic
279:24 - acknowledgment which means that the
279:26 - message can be removed from the queue
279:29 - so that's going to be it for that
279:33 - and there's a typo here this should just
279:35 - be session
279:39 - and there's also no import for Json so
279:44 - let's do that
279:48 - and we haven't installed these
279:50 - dependencies yet either so that's going
279:53 - to be a problem so we'll just go ahead
279:54 - and save this
279:56 - and we can just cat the head of our file
280:00 - and this file is called email dot by and
280:04 - we'll do
280:05 - actually we didn't start a virtual
280:08 - environment yet so let's change
280:09 - directory and do Python 3 Dash MV and V
280:15 - and start our virtual environment
280:19 - and now we can do fip3 install
280:22 - actually one second I don't think we
280:24 - need to install that I think that's
280:25 - already part of
280:29 - yeah we don't need to install that
280:34 - uh let me install this for my Vim
280:44 - so yeah this is part of Python's
280:46 - standard Library so we don't need to
280:47 - install that and let's see if we need
280:50 - anything installed from consumer.pi we
280:53 - need to install Pica and that's it so
280:56 - we'll do pip3 install Pico and that's
280:59 - going to be it for that so now we can go
281:01 - ahead and we can actually just copy the
281:05 - docker file from our Gateway
281:08 - into this directory
281:10 - and then we have Docker file and we
281:13 - don't need to expose anything and this
281:16 - needs to be changed to consumer.pi and
281:18 - everything else is going to be exactly
281:20 - the same so we can just save that
281:22 - and let's make some space here and we
281:25 - can also copy the Manifest directory
281:28 - from our converter.pi
281:33 - so now we have the Manifest directory
281:35 - from our converter dot pi and we'll
281:38 - change the name of converter deploy to
281:42 - notification deploy.yaml
281:47 - and we can go into that
281:49 - notificationdeploy.yaml file and we'll
281:52 - change all occurrences of converter to
281:57 - notification
282:00 - so I just did it using Vim said command
282:02 - but you can do it however you want just
282:04 - make sure you change all occurrences of
282:06 - converter to notification so we want our
282:09 - deployment name to be notification and
282:12 - then we want the app label to be
282:13 - notification and we'll leave replicas
282:16 - the same and we need to match label
282:18 - notification as well
282:20 - and Max surge is going to be the same
282:22 - and the templates app label is also
282:24 - going to be notification
282:27 - and container name also notification and
282:30 - we're also going to create a Docker
282:32 - repository called notification as well
282:36 - and the same for the config map and the
282:38 - secret
282:40 - so we can just save that and then we
282:42 - need to go into our config map and we
282:44 - need to change this to notification as
282:47 - well
282:49 - and the mp3q is MP3 which is correct
282:54 - and then we need to go into our secret
282:56 - and this changes to notification and we
283:00 - don't have a secret here
283:02 - so let's save that
283:04 - and let's clear change directory back to
283:08 - our root now let's go ahead and dock our
283:10 - build
283:17 - oh and we forgot to do pip freeze
283:21 - now Docker build
283:29 - and Docker tag
283:34 - foreign
283:36 - and then notification latest
283:42 - and now Docker push and notification
283:45 - latest
283:46 - and this is going to create the
283:48 - notification repository in your Docker
283:51 - Hub account
283:56 - and now let's check the docker Hub
283:58 - account
284:11 - and you should have a repository that
284:15 - was last pushed a few seconds ago
284:18 - and if you go to it you should have a
284:20 - tag latest so the next thing that we
284:22 - need to do is we need to configure our
284:25 - Gmail account to allow non-google
284:28 - accounts to log in okay so I've already
284:31 - created a dummy Google account or a
284:34 - Gmail account here so once you've done
284:36 - the same you want to click on this and
284:39 - go to manage your Google account
284:42 - and from here you want to go to security
284:47 - and you can just go down to the bottom
284:50 - here and there's this box here that says
284:53 - less secure app access and I've already
284:55 - turned this on but you're going to want
284:57 - to click this and then change it from
285:00 - off to on
285:03 - foreign
285:09 - I'm allowing apps and devices that use
285:11 - less secure sign-in technology to access
285:13 - my account
285:14 - so make sure that this is set to on
285:17 - before continuing with the tutorial and
285:20 - another thing that we need to do is well
285:22 - me specifically if I go to
285:26 - mySQL U root
285:28 - and I use auth
285:31 - and show tables
285:34 - select all from user
285:39 - you can see that I'm actually using a
285:41 - fake email here so the notification
285:43 - service will try to send an email to
285:45 - this fake email so if you used a fake
285:47 - email for your credentials here you
285:49 - should update it to an actual existing
285:52 - email I'm just going to send it to my
285:55 - dummy email so I'll essentially be
285:57 - sending the notification to myself but
286:00 - you can send the email to whatever email
286:02 - you want to use because at that point
286:04 - that email is just receiving a message
286:06 - so it's not insecure to use an actual
286:09 - legitimate email for the actual sent
286:12 - message
286:13 - so I'm just going to update mine
286:31 - so if I go ahead and select all from
286:34 - user again
286:38 - you can see that my email has now
286:40 - changed
286:42 - and that's actually the email that I'm
286:44 - using here
286:46 - so I can just exit this and that means
286:49 - that I'm going to have to get another
286:51 - token when we test this later on and you
286:54 - will have to do the same if you've
286:55 - changed the email
286:56 - and let's just go ahead and clear so at
286:59 - this point we can go ahead and deploy
287:01 - the notification service so we can just
287:03 - do Cube CTL apply everything in the
287:06 - Manifest directory
287:08 - and we can check this
287:16 - and it looks like it's working as
287:18 - expected
287:20 - and we could just change directory to
287:23 - our source directory and actually let's
287:27 - scale up our Gateway and our auth
287:30 - service as well
287:31 - so I'll just apply all
287:35 - manifests
287:39 - and that's going to scale it up to the
287:42 - two replicas that we have configured in
287:44 - our manifest directory for the all
287:46 - service
287:47 - and we can do the same thing for Gateway
287:53 - and Gateway has two instances deployed
287:57 - as well
287:58 - and the same thing for converter
288:04 - so that's scaling our converter up to
288:07 - four instances
288:09 - so at this point we have everything we
288:11 - have two instances of our auth service
288:14 - four instances of our converter two
288:16 - instances of our Gateway four instances
288:19 - of our notification service and one
288:23 - instance of our rapid mq service so now
288:26 - let's just check to see if our tunnel is
288:28 - running it's asking for the password
288:30 - again because I redeployed the Gateway
288:33 - Ingress so I'll put the password in and
288:36 - our tunnel is running and now I'm just
288:38 - going to go ahead and log in again
288:43 - but this time I'm going to change the
288:46 - email so gmail.com
288:52 - 63 and let's see if we can get the token
288:56 - okay so now we have a token which I will
289:00 - just copy
289:02 - and now I'm going to send and upload
289:05 - request so let me see if I have I need
289:08 - to go to converter
289:10 - and I'll delete the test MP3 that I had
289:13 - from last time
289:15 - and I'll upload the
289:19 - test MKV file
289:22 - but I'm going to change this token
289:26 - so this is going to be the curl request
289:28 - to upload the video file
289:32 - so what we're doing right now is we're
289:33 - testing the end-to-end functionality
289:36 - including the notification service so
289:38 - we're first uploading a video so this is
289:41 - the curl request to upload this video
289:44 - and the header authorization bear is
289:47 - going to be the token that I just got
289:48 - from the login into point
289:51 - and then our upload URL
289:56 - and I'm assuming this is because we need
289:58 - to restart our Gateway service
290:02 - so I'll just delete both of these
290:06 - so they can restart
290:10 - and let's try again
290:12 - and we have a success
290:15 - so if everything's working as expected
290:20 - the message should be being consumed
290:22 - from our MP3 queue as well
290:25 - so let's just restart this and it says
290:28 - we have six unacknowledged now let me
290:30 - check this email
290:43 - Ah that's because I forgot a very
290:46 - important part
290:48 - so let me change directory
290:53 - to the notification service
291:00 - actually I didn't forget so what is the
291:03 - issue
291:06 - okay so for some reason we're
291:07 - experiencing an issue so
291:10 - let's just check the logs for our
291:14 - notification service
291:37 - just send it again
291:39 - okay
291:45 - I'm going to scale everything down
291:49 - Gateway converter
291:52 - all
291:53 - notification
292:02 - actually I'm just going to scale
292:04 - everything down to zero and restart it
292:12 - so the only thing I have up right now is
292:15 - the rabbit in QQ
292:20 - and now I'll scale everything up to one
292:37 - okay so let's try and send the request
292:41 - and now
292:43 - so it seems that somehow the messages
292:45 - aren't being acknowledged
292:48 - actually there's one more thing that I
292:50 - forgot so
292:52 - I need to go into our notifications
292:57 - manifest secret file and we need to set
293:01 - up our Gmail address environment
293:03 - variable and Gmail password
293:06 - so we can just use this placeholder and
293:08 - change it to Gmail address
293:13 - and Gmail
293:16 - password
293:18 - and that's because if you remember
293:22 - in our notifications send email dot Pi
293:26 - file
293:29 - here we set our sender address to an
293:34 - environment variable and we also set the
293:36 - sender password here to an environment
293:39 - variable
293:40 - so we need to provide those credentials
293:43 - here so I'm going to do my credentials
293:46 - and you need to do the ones for the
293:49 - account that you're using
293:52 - foreign
293:57 - and make space and just to be safe I'm
294:00 - just going to scale everything down
294:07 - so I'm scaling everything aside from the
294:09 - queue down to zero
294:15 - and then I will scale it back up
294:24 - actually
294:25 - for the notification service
294:28 - I need to reapply
294:35 - so I'll just remove the resources first
294:40 - and this is for notification
294:45 - and then I will reapply those
295:04 - and let me check to see if those
295:07 - variables are working as expected
295:13 - and they are working as expected so
295:16 - let's see if that resolved the issue
295:20 - so let's just go ahead and send again
295:23 - but
295:24 - it's in our converter directory
295:52 - foreign
295:57 - so it seems that it's still not getting
295:59 - acknowledged so it seems that the
296:02 - messages are being consumed from the
296:05 - queue so the issue has to be in our
296:07 - notification service
296:12 - within the consumer.pi code
296:15 - there's probably an issue
296:20 - so we're consuming the message but we're
296:23 - never acknowledging the message but
296:27 - we're never acknowledging that we've
296:28 - processed the message
296:30 - so our email that notification
296:35 - we basically have everything in this try
296:37 - block
296:38 - and if there's an issue we're printing
296:40 - the error and returning the error
296:43 - [Music]
296:47 - okay so we need to make a small change
296:51 - to our notification
296:55 - send email dot Pi file
296:58 - so here we need to add in the port 587
297:03 - and this is the port for TLS and start
297:07 - TLS so I'm thinking that this should
297:10 - resolve the issue that we're having so
297:12 - I'll go ahead and save this
297:15 - and then we could just do Docker build
297:17 - and actually we're in the wrong
297:19 - directory so
297:22 - let's do Docker build
297:26 - and Docker tag
297:35 - and Docker push
297:45 - and while that's pushing let's check the
297:47 - queue So currently since we had those
297:49 - unacknowledged messages in our queue and
297:52 - our notification consumer service isn't
297:55 - currently consuming because we scaled it
297:57 - down we had messages that were ready but
298:00 - they just got swallowed up by the
298:01 - service
298:03 - and it seems that they're getting
298:05 - acknowledged at this point so this
298:07 - should go down to zero all right so they
298:09 - were all processed so that means that we
298:11 - should have emails for all of those
298:13 - messages so if I go to the inbox here
298:17 - you can see that we have the emails for
298:20 - the messages
298:22 - and let me check spam
298:27 - and I think some of those messages had
298:29 - the old email the one that wasn't a real
298:32 - email so that's why I only we're not
298:34 - getting 11 messages we're only getting
298:35 - these ones
298:38 - but anyways the email contains the MP3
298:42 - file ID string here so we should be able
298:45 - to take this and use it to download the
298:48 - actual file
298:49 - so we can clear this and to download the
298:52 - file we're going to do curl and we want
298:55 - the output of the file to go to a file
298:58 - so we'll just name that file download
299:01 - MP3 download.mp3
299:04 - and it's going to be a get request and
299:07 - the header is going to contain our
299:10 - authorization and it's going to be the
299:13 - authorization token
299:17 - and the URL should be mp3converter.com
299:22 - and don't need the port
299:26 - and it should be the download endpoint
299:28 - and you should put a URL query parameter
299:31 - here that's called FID and set it equal
299:34 - to the ID from the email
299:38 - so my email ID is
299:43 - this ID here which is the same ID that
299:47 - I'm using here
299:49 - and then if you hit enter on that it
299:52 - should download the file
299:54 - and now you should have a file in the
299:56 - directory called MP3 download.mp3 and we
300:01 - should be able to open this file up or
300:03 - listen to this file and it should be the
300:05 - video that we uploaded to audio so let
300:08 - me just go to that in the user interface
300:10 - so as you can see I have this file here
300:12 - called MP3 download.mp3 and if I play it
300:17 - so here's a burger again the double
300:20 - double it will play the sound from the
300:23 - video that we uploaded so that means
300:25 - that our end-to-end application is
300:28 - working and let's just clear here and we
300:31 - don't need this
300:33 - so let's quickly go into canines and we
300:37 - have everything scaled down so let's
300:39 - just reapply the initial configuration
300:44 - so I'm just going to delete the
300:47 - configuration
300:49 - for all of our services
300:59 - except for the rapidmq of course because
301:02 - no need to scale that up or down there's
301:05 - only one pod for that
301:07 - and then now we can just apply
301:10 - auth manifests
301:14 - converter manifests
301:19 - Gateway
301:21 - and notification
301:27 - and now if we go into here
301:31 - we have all of the instances that we
301:33 - have configured for our services
301:38 - so let's just move that video file from
301:42 - converter
301:45 - let's just move that here and let's
301:47 - check our cue really quick so currently
301:50 - our queue is empty
301:52 - and let's go ahead and upload this file
301:58 - actually let me make sure I have a valid
302:01 - token so
302:03 - I'll just do
302:06 - a request to the login endpoint to get
302:08 - the token
302:19 - and there's the new token so I'll just
302:22 - try to upload
302:23 - and let's just Spam it a couple of times
302:31 - so it seems that our cues are processing
302:33 - both the videos and the MP3 messages and
302:36 - at this point it seems that they're all
302:38 - done and if we go into our email we have
302:41 - a few more downloads here so we can just
302:45 - go ahead and copy the file ID for one of
302:48 - those
302:49 - and now we can just go ahead and attempt
302:51 - to download that
302:54 - we'll do a different name for the file
302:57 - so I'll just put something random just
302:59 - something dot MP3
303:04 - and let's change this to the file ID
303:07 - that we just copied from the email
303:09 - whoops that's the JWT let me go copy
303:13 - that again
303:16 - and let's download
303:19 - and we have this file something.pi so
303:22 - let me go to the user interface and play
303:24 - that so in our source folder we have
303:27 - something.pi
303:29 - so here's a burger and as you can see it
303:33 - is working as expected and that is going
303:36 - to be it for this tutorial I think that
303:40 - if you were able to make it to the end
303:41 - of this tutorial and you were able to
303:43 - get everything working you should
303:45 - definitely be proud of yourself because
303:47 - this one was a difficult one and yeah if
303:50 - you have any questions feel free to post
303:52 - them in the comment section I'll try to
303:54 - help you as best I can if you're having
303:55 - troubles getting everything working and
303:57 - I hope that this has been helpful to you
303:59 - and if you've made it this far in the
304:01 - video and if you haven't already please
304:03 - don't forget to like the video and
304:06 - subscribe to the channel for more
304:07 - content like this and yeah I'll see you
304:10 - in the next one