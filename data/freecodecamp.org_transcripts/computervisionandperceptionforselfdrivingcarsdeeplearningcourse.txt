00:00 - An important use case for computer vision 
and machine learning is self driving cars.  
00:04 - In this course, Akshay will teach you how to 
implement many techniques related to perception  
00:09 - of self driving cars, and engineers am SEC 
sec. Welcome to this course on perception  
00:15 - for self driving cars. In this course, we 
will go over a number of different projects  
00:21 - that are related to the perception module of 
the self driving car. This course is meant  
00:28 - for an intermediate level of audience, you are 
required to know the basic concepts of Python,  
00:34 - as well as some basic concepts regarding 
deep learning. This course is meant for you  
00:40 - if you are looking to get into the field of 
self driving cars, or if you are interested  
00:46 - to do projects that are related to computer 
vision using deep learning. Before starting,  
00:52 - if you like the content of this course, you can 
also check out my channel robotics with Akshay  
00:59 - where I post videos related to robotics and 
artificial intelligence. So now let's get started.  
01:08 - Our first project would be road segmentation. In 
this project, we will be doing segmentation of  
01:14 - road images using a deep learning technique called 
fully convolutional networks. In the next project,  
01:21 - we will be doing 2d object detection on various 
different traffic scenarios using a deep learning  
01:27 - technique called the Yolo algorithm. Next, 
we will be tracking those object detections  
01:33 - using a technique called Deep sought. After we 
are done with the 2d part in these three projects,  
01:40 - we will be moving on to the 3d part, we will 
start our 3d part with 3d data visualization  
01:46 - where we will visualize camera images and 
LIDAR data. Next, we will be discussing  
01:52 - multi task learning based on two tasks depth 
estimation and semantic segmentation. Then  
01:58 - we are going to move on to 3d object detection. 
Then move on to our final project the bird's eye  
02:04 - view visualization using an advanced computer 
vision technique called transformers, all the  
02:10 - data set and the core notebooks are provided, 
so that you can follow along with this course.  
02:17 - Alright, so what is the road segmentation 
problem, you would say, indoor segmentation  
02:22 - given an input image, we are required to identify 
where are the roads present in this image.  
02:29 - Now we as humans can see that road is present 
right over here in somewhat in the middle of the  
02:35 - image. So what we are required is to output 
an image something that looks like this.  
02:41 - Now here, we have imaged the road 
that is present in that image.  
02:48 - Now, we aren't going to exactly output this, but 
what we require is that we somehow mark this road  
02:55 - in a separate image. One way to do that would 
be to output a black and white image where white  
03:02 - represents wherever the old is present, and zero 
or black represents the other background in that  
03:09 - image. Let's have a look at another image as well. 
Now this is another road image that we can see  
03:18 - now. Now as humans, it is easy to see 
where is the road present in this image.  
03:23 - For a computer, what it would see is something 
like this. So the computer has to output a  
03:29 - segmentation mask that looks something like this. 
Now these two images that you're seeing over here,  
03:36 - are part of the GT road data set. Now a data 
set is a collection of such input images  
03:43 - and desired output images, which we use to 
baseline or check how our model is performing.  
03:51 - If our model is able to perform well on these 
images, we can be kind of sure that they are going  
03:57 - to perform well in real world as well. But there 
are some complications that are required as well.  
04:03 - Okay, so by now you would have understood what is 
the problem of road segmentation and the data set  
04:08 - that we're working on, which is the kitty robot 
data set. Now for this task of road segmentation,  
04:14 - there exists a number of computer vision 
techniques that we can apply. The older computer  
04:20 - vision techniques required us the programmers to 
actually hard code the different values in order  
04:27 - to segment the road that is present in the image. 
Now this technique is prone to a lot of errors and  
04:33 - usually doesn't generalize very good. Like this 
technique would work for one image very good but  
04:39 - would completely fail for another image. For 
instance, if we develop a technique to segment  
04:44 - a road on an image, then even a little shadow 
on the road would completely ruin our parameter  
04:52 - tuning and we will require to tune the parameters 
again. Then with the advent of deep learning,  
04:58 - we got rid of such high grafting techniques where 
in the computer would automatically learn these  
05:04 - parameters given a diverse set of data. So, one 
such technique that we are going to discuss today  
05:10 - is fully convolutional networks. Now, fully 
convolutional networks were one of the first  
05:15 - techniques that introduced their method of 
segmentation end to end. What I mean by end to end  
05:22 - is that given an input image, if we feed the input 
image to an FCN model, it will directly output the  
05:29 - segmentation that it is trained for, no other pre 
processing or post processing would be required  
05:35 - on that image as such. Now, we are applying the 
fully convolutional network technique on the road  
05:42 - segmentation task, which is we can say a subset of 
the segmentation task as well. So how the complete  
05:48 - picture looks like is that we have an image input 
image that has a road present in it, we pass that  
05:55 - input image to the FCN model that we have, and 
towards the output, we get a 01 image binary  
06:02 - image where one is present wherever the lane 
was present, or the road was present. Otherwise,  
06:08 - we have zero in the complete image. Now, in 
order to understand fully convolutional networks,  
06:14 - we first need to understand an image operation 
called up sampling. So let's have a look at that.  
06:23 - So in simple words, up sampling is an operation 
where we increase the size of a given input. For  
06:29 - instance, if we are given an image like this, a 
two cross two image, then upsampling to x is going  
06:35 - to double the height and the width, therefore, 
we will be having four cross four output,  
06:42 - the complete problem of of sampling is we need 
to decide what are going to be the values in this  
06:48 - whole cross code metrics when we would be given 
the values in this to cross to given metrics.  
06:54 - So there exists a number of techniques that we 
can employ, let's discuss a few of them first, the  
06:59 - first technique for up sampling we have is called 
bed of nails, what we do in bed of nails, whatever  
07:06 - the values present in this matrix, we are going 
to assign that value to this block over here.  
07:12 - So the three values that are related to this block 
which are 123, these three blocks are assigned  
07:18 - a value of zero. Similarly from this block, we 
can take a value and we can assign it over here,  
07:24 - the rest three related values are assigned zero, 
these, which are these three. Similarly, we do the  
07:31 - same thing for these two cell values as well. Now 
this is a really simplistic technique where we are  
07:36 - going to have a lot of zeros in our output image, 
a better method than this is the nearest neighbor  
07:42 - method. In nearest neighbor method, what we do 
is we take the value that is present over here,  
07:48 - and in all the four related cells to this 
cell, we assign the same value. For instance,  
07:53 - if we have one over here, we are going to assign 
the value of one to all the four blocks over here.  
07:59 - Similarly, if we have a value of two over here, 
then we're going to assign the value of two  
08:03 - to these four blocks here. And we do this 
similarly for the complete metrics over here.  
08:10 - Now, an even better method than nearest neighbor 
is called interpolation. Now, interpolation is  
08:16 - more like an average finding technique where we 
assign values based on a weighted average, how  
08:22 - we do that, in interpolation, we try to work our 
way backwards, if we want to calculate the value  
08:29 - of this cell over here, then we approximately try 
to overlap this matrix with this matrix, what we  
08:35 - are going to see is that this block is going to 
lie somewhere over here. Now this cell over here  
08:43 - is close to four values in this matrix, which are 
all the four values over here. Now based on the  
08:49 - distance from these four values, we are going to 
assign an average value to this cell over here.  
08:56 - Similarly, we are going to do this for this cell 
as well, this is going to lie somewhere over here,  
09:02 - and average value will be assigned to this cell 
based on the distance from the four cells that are  
09:09 - present over here. Likewise, if we want to assign 
the value of this cell over here, then this cell  
09:15 - is going to lie somewhere over here. Now, we are 
going to take the average of these two cells only  
09:20 - because these are the nearest neighbors present to 
this cell. So this is how the interpolation method  
09:26 - works. Now interpolation method is one of the best 
technique that we could apply, but there exists  
09:32 - an even better technique that we can apply 
as well, which is transposed convolutions.  
09:39 - In all the techniques that we discussed before, we 
were assigning the values based on some heuristic  
09:44 - that we hardcourt. Instead in transpose 
convolution, we assign the values based on  
09:50 - a learnable weight filter, we define a filter 
of some size with some random weight values.  
09:57 - We then use those weight values to assign the 
value of All these cells that are present in  
10:01 - this matrix and pass this to the network, when the 
network is going to do a backpropagation, we are  
10:08 - going to back propagate the loss of the weights 
of those filter as well and they are going to be  
10:13 - learned as well. For the purpose of this project, 
I tried two methods which were interpolation  
10:19 - and transposed convolutions, from what I saw 
transpose convolutions did not work very well.  
10:25 - But the interpolation method worked really well 
for our case. Now, since we have understood what  
10:31 - the upsampling operation is all about. So 
let's now discuss the architecture of the  
10:35 - fully convolutional network. So for the fully 
convolutional network, we require a backbone  
10:40 - network, which is an image classification network 
called VGG. Net. Now, what is image classification  
10:46 - and what is VGG net? Now image classification is 
a problem where we are given an input image and  
10:53 - we are required to decide what is present in that 
image. Or we can say the computer is required to  
10:59 - decide what is present in that image. Now, before 
doing anything, we already define the classes  
11:05 - before sending an input image. For instance, if 
in our image, we know that there are going to be  
11:11 - three entities that are present, airplane bicycle 
and a person, then we are going to assign a value  
11:18 - zero to the airplane, one to the bicycle and to 
to the person. Then we pass an image through the  
11:25 - image classification network, and it outputs 01 
Or two, representing what it saw in the image.  
11:31 - Now, image classification is kind of a solved 
problem where we have models which give very  
11:37 - good results on some baseline datasets. One 
such good network is VGG. Net. Now since VGG,  
11:44 - network takes in an image input and outputs a 
single number output, we need to convert it for  
11:49 - our use case, which is the segmentation problem. 
In the segmentation problem, the input is an image  
11:56 - and the output is also an image that is of the 
same dimensions. So let's see the changes that  
12:01 - we require to make to this VGG net in order to 
output such images. Now, VGG network is composed  
12:07 - of a number of convolutional blocks, it's placed 
sequentially to each other. Each convolution block  
12:14 - consists of two convolutional layers, followed by 
a max pooling layer. Now what we do is we extract  
12:21 - the output of last three convolutional blocks, 
which are pool three, pool four and pool five.  
12:26 - So what we do first, we upsample, this pool five 
times two, so we have an bigger image over here,  
12:33 - then we add this output to the output of 
pool four, then what we get over here,  
12:38 - we again upsample, this output two times to 
get an output over here. And finally again,  
12:44 - add this pool three layer, which then we get the 
output over here. Finally, we upsample this output  
12:50 - eight times. And the final output that we get 
is the segmentation mask that we require. Now,  
12:56 - this long sequence is what constitutes the FCN 
architecture, or particularly this is the FCN  
13:03 - eight architecture. There exist to other FCN 
architectures as well FCN 32, and FCN 16. What  
13:13 - we do in FCN 32, we don't go through all these 
operations that we have done over here, instead,  
13:18 - we simply upsample, this pool five layer 32 
times and we get the output in FCN 16, we again,  
13:26 - don't do all these operations, we simply go to 
this step over here where we calculate the sum  
13:31 - till pool four, and then we assemble it 16 times 
to get the output since we are performing all the  
13:38 - operations that are present over here, and finally 
upsample eight times therefore, this architecture  
13:44 - is called FCN eight. In one way, we can say that 
FCN eight is a combination of FCN 32 and FCN 16.  
13:52 - What this means is that FCN 32 identifies coarser 
structures in the image FCN 16 identifies more  
14:02 - finer structures in the image and FCN eight 
even more finer structures in the image  
14:07 - using these addition operations that we have over 
here we are combining the knowledge of these three  
14:12 - networks to get the final output. Now these are 
the complete details of the architecture and how  
14:18 - to architecture is somewhat working on the 
inside. Alright, so now our discussion for this is  
14:23 - complete. Now let's have a look at how this model 
was implemented and what are the different results  
14:29 - that we got. So let's have a look at that the 
data set and the code both are present on Kaggle.  
14:35 - The link for both of these is in the description 
you can have a look at them as well. Okay. So  
14:41 - this is the implementation First we define 
an input layer with a parameter input shape.  
14:46 - Now input shape is a tuple that contains image 
size, image size, common number of channels, image  
14:53 - size we have set over here is 128, cross 128, 
and number of channels is three the RGB channels  
15:01 - That way define the VGG network, but that is VGG 
16. TensorFlow or Kara's already provides us with  
15:08 - the implementation of VGG 16, which is pre trained 
on the image net data set. Then we extract these  
15:15 - three blocks, which are the pool three, pool 
four and pull five layers. For the first step,  
15:20 - we have sample two times that we can see over 
here, the tool five layer, then we add the sub  
15:27 - sample layer to the pool for and we apply a one 
cross one convolution over here, as you can see,  
15:34 - now this convolution is more like a placeholder 
to get the dimensions in the right size,  
15:39 - as such, using them or not using them doesn't 
have a big impact on the performance as such.  
15:46 - Then we apply the second up sampling the second 
two acts of sampling and add that upsampled  
15:52 - layer with the pool three layer, again, we 
apply a convolution to D which is again as  
15:57 - kind of a placeholder overhead. Finally, for the 
output, we calculate the upsampling eight times  
16:05 - this final convolution is a required one. Now 
this n classes is said to be one over here,  
16:12 - this is done because we want to set the output 
of the network to be a single channel image.  
16:19 - Now that single channel is going to contain zero 
and one as we discussed before, one is going to  
16:24 - be the mask where the road is present and zero 
otherwise. We also apply a sigmoid activation  
16:32 - over here in order to get the final values 
between zero and one. And then we return the  
16:38 - model with the inputs and outputs as such set 
and we have named the model FCN eight likewise  
16:46 - now let's have a look at the results that 
we got his images were held out from the  
16:51 - model and the model is seeing them from 
for the first time this is a second image.  
17:55 - Now as we can see the model is working really 
well. Now let's apply some changes to the baseline  
18:01 - network that we have and see how the results 
change. Now this is one change that we do. Instead  
18:07 - of the add function we are using the concatenate 
function over here in adding what we do is we  
18:14 - add the corresponding values that are present in 
the matrix of two images. However in concatenate,  
18:21 - we just add that image to the back of the image 
and increase the number of channels of the input.  
18:31 - So now let's see what are the results after 
we apply this concatenate instead of ad?  
19:28 - So we can see that concatenate is also 
performing good, but add is a little  
19:33 - better than this concatenate function. Now, let's 
change that upsampling today with convolutional  
19:41 - 2d transpose, which we can see over here, 
so instead of upsampling, here we are using  
19:46 - transpose convolution where we are going 
to learn the weights of upsampling as well.  
19:51 - And we are using the add function over here. 
So let's see how the results for this look like  
20:25 - So we can clearly see that transpose 
convolution is not getting that good of results,  
20:31 - although it has learned the task, 
but the output is kind of pixelated,  
20:36 - and doesn't really look good 
on the segmentation as well.  
20:40 - Alright, so these are the results of the 
network and some of its extensions as well.  
20:47 - So, before discussing what is to do object 
detection problem, let us first see,  
20:52 - what is the data set that is provided to us in 
this project. So, Lyft is a self driving cars  
20:59 - company that develops self driving car software. 
In order to further the research in self driving  
21:06 - cars. Lyft announced two competitions that were 
held previously, these two competitions were  
21:13 - 3d object detection and motion prediction, 
these two are separate. The data set that  
21:19 - we're going to use for this project is the one 
from the left 3d object detection challenge.  
21:26 - So what was given in the 
object detection challenge.  
21:31 - So, what Lyft did is it drove a self driving car 
around the streets of Palo Alto, and they recorded  
21:39 - the camera data as well as the LIDAR sensor 
data. Now, Lidar is a sensor that works on the  
21:46 - principle of reflection of laser lights and lasers 
starting from the sensor goes and hits an obstacle  
21:53 - returns back to the sensor and the sensor detects 
the distance of that object. So in this data set,  
21:59 - we are given the camera data as well 
as the LIDAR data or the same trip.  
22:04 - As we can see, these are some sample pictures. 
So this is one back image. As we can see,  
22:09 - we have certain cars over here we have the front 
image, we have certain cars over here as well.  
22:15 - Now we can compare the LIDAR data with the camera 
data. So we saw our car is localized at this  
22:24 - cross and we can see that there is one car that 
is present over here that is trying to enter the  
22:30 - lane besides us as we can see we also have a car 
over here and we can also see that we have two  
22:36 - cars behind our car which are present over here. 
These blue bounding boxes represent pedestrians  
22:45 - so we also have front left image and 
right image back right image and back  
22:50 - left image we can also visualize the data 
in a video format so let's play this video  
23:06 - similarly, we can also visualize the LIDAR 
data in a video format let's play this video  
23:26 - so, the code for this data visualization 
will be present in the description box below.  
23:31 - We are specifically going to work 
on the camera images as such.  
23:35 - So now let us discuss what is this object 
detection problem and what is the difference  
23:39 - between 2d and 3d object detection. So in object 
detection, the complete problem is framed as such.  
23:47 - That in a given image, we are required to find 
the location of an object present in that image  
23:54 - as well as also tell what is that object. For 
instance, in this example, we can see over here,  
24:00 - there is one dog that is present and there are two 
cats. So given this image, we need to output where  
24:06 - is the dog present in the form of bounding box 
and where are the cats present. Now in a 2d object  
24:13 - detection case, we would be given a 2d image and 
we are required to give the bounding boxes in 2d.  
24:21 - However, in 3d object detection, apart from a 
2d image, we would also be given some other data  
24:28 - representation as well. Using that image and 
the additional data, we are required to return  
24:34 - a 3d bounding box of that object along with 
its label. A very good example is in the 3d  
24:43 - object detection data as well. As we can see, 
we are given 3d bounding boxes of the various  
24:51 - cars in this image. And we are also given 
an additional data which is the LIDAR data  
25:00 - And we can also see that we are predicting 
pedestrians as well as well as the cars. So  
25:06 - one cool thing that we can also notice is that we 
are also able to detect objects that are included  
25:13 - in the given image. How we are able to 
do that is because of the additional data  
25:18 - that is provided by the LIDAR. Alright, so 
this is all for 3d and 2d object detection.  
25:25 - In this video, we are only going to focus on 
2d object detection. So we are going to run  
25:31 - an algorithm on these given images and predict 2d 
bounding boxes. So let's discuss that algorithm  
25:39 - in detail. As you may have seen in object 
detection, we have to localize the object  
25:44 - as well as tell what that object is. Now in case 
of self driving cars, as we can see over here,  
25:51 - if we are given two cars, then we also need to 
find the position of those two cars as well as  
25:56 - decide are the cars are the pedestrians are their 
bicycles, and so on. Now, the algorithm that we're  
26:02 - going to discuss for 2d object detection is 
yellow, y O L O. And this stands for view only  
26:10 - local ones. Now giving an overall picture, we can 
divide the Yolo algorithm into three stages, which  
26:16 - are the anchor boxes, the intersection over union 
and the bounding box predictions. So how does this  
26:24 - work? Now YOLO is also a deep learning technique 
that we use for object detection. In this we  
26:30 - are given an input image, we pass this input 
image through a convolutional neural network.  
26:36 - And as an output, we get another metrics. Usually, 
for the Yolo algorithm, this metric size is taken  
26:43 - to be 19 Cross 19. What does this input to output 
mapping represent? If we consider this matrix as a  
26:51 - 19 cross 19 matrix, if we take our input image 
as such, and overlap this 1919 matrix over it,  
26:59 - then the 19 cross 19 boxes that we can see over 
our image, all those individual boxes are going  
27:05 - to tell what is present in that box. It can either 
be a car, a truck, or even a motorcycle. And it  
27:13 - is also possible that there is nothing useful 
present in that particular box. For instance,  
27:18 - in our case, it would be we can say Road, for each 
cell, we are going to have this vector in place,  
27:26 - this thing can also be visualized as this matrix 
is actually a volume. So 19 Cross 19 Are the  
27:32 - width and the height of this matrix. And in the 
depth perspective, we have this complete vector.  
27:39 - Now let's see what this vector describes. 
This vector that we discussed, describes the  
27:44 - information that is present in that particular 
cell, the PC value is going to tell whether  
27:49 - something interesting is present in that cell or 
not, is something interesting is not present in  
27:54 - that cell, then the other values are just going 
to be undefined, or we are not going to consider  
28:00 - them in the calculations as such. If we assume 
that PC is one and there is something interesting  
28:06 - present in that cell, then x and y are going 
to describe the position of the center of that  
28:12 - object with respect to a given cell. So x and y 
are going to tell if we start from this upper left  
28:20 - corner of this cell, how much we need to go right 
and how much we need to go down in order to reach  
28:25 - the midpoint of that object, W and H describe 
the width and height again with respect to that  
28:32 - cell. Now these C one C two c three variables 
describe what object is present in that cell.  
28:40 - For instance, if we have three objects, then one 
of these three is going to be one, the other ones  
28:45 - are going to be zero. If we consider that we have 
four objects in our consideration, then we're  
28:50 - going to have four variables over here c one, 
c two, C three and C four. Now we can see what  
28:56 - is going to be present in the output. Some of the 
output cells, as we can see there, red over here,  
29:02 - are going to describe that something special is 
present in that particular cell. So the output  
29:07 - over here gives us what the object is going to 
be and what is going to be the bounding box of  
29:12 - that particular object. One certain example is we 
can see over here for this particular car, we can  
29:18 - see that there are multiple bounding boxes that 
have been generated by this output. And in order  
29:25 - to differentiate I have also drawn one car in the 
corner over here as well, that also has multiple  
29:30 - bounding boxes describing it. Along with these 
bounding boxes and the labels, we also compute  
29:37 - a probability. This probability also describes how 
much confident our our bounding box predictions.  
29:43 - For instance, this bounding box over here, the 
outer one can have a lower probability. However,  
29:49 - the one that is inside is going to have a higher 
probability. So this is how one part the bounding  
29:56 - box prediction part of Ulo works. In order to 
separate these bounding boxes and get a single  
30:03 - bounding box for each object, we use intersection 
over union. And this technique of getting a single  
30:10 - bounding box and separating different bounding 
boxes is called non maximum suppression.  
30:16 - Okay. So first let us see what is intersection 
over union or we can say I owe you. So,  
30:22 - IOU is used to compute how much are two bounding 
boxes different, you will be able to see that  
30:29 - in its mathematical description. So if we have 
two bounding boxes as we have given over here,  
30:35 - we first calculate their intersection and divide 
that by their union. So, this gives us a measure  
30:43 - of how two bounding boxes are similar to 
each other. So, we can use IOU to separate  
30:49 - the bounding boxes of two different objects. 
For instance, these two, we calculate the value  
30:55 - of the bounding boxes with respect to the maximum 
probability bounding box. So, what this means,  
31:02 - if we consider again that the inside bounding box 
has the highest probability, and we calculate the  
31:09 - IOU score for the other four bounding boxes with 
respect to this maximum probability bounding box,  
31:16 - we can see that these two bounding boxes are going 
to have a very high IOU value. Since they have  
31:22 - a very high IOU value, this means that they are 
actually representing the same object. Therefore,  
31:28 - we discard these other two bounding boxes of these 
two other bounding boxes, they have a very low IOU  
31:34 - value, this depicts that they represent some other 
object, and we are going to apply the non maximum  
31:40 - suppression separately on these bounding boxes. 
And this is how the intersection over union part  
31:46 - of you know works, or there can be a scenario 
in which a single cell over here represents  
31:52 - multiple different objects. One small example 
would be two cars that are parked side by side.  
31:59 - So, in one of these cells, we are going to 
capture both of those guys simultaneously.  
32:05 - In order to deal with that problem, 
we use this concept of anchor boxes,  
32:10 - these anchor boxes represent a rough shape for the 
different objects that are going to be present in  
32:16 - our image. These anchor boxes can be designed 
by us humans, or it can be decided by another  
32:22 - machine learning algorithm as well. So how does 
this anchor box help in detection of multiple  
32:28 - objects, so each anchor box describes what shape 
of the object they are looking for. For instance,  
32:34 - this anchor box is looking for a square object. 
This anchor box is looking for a rectangular  
32:40 - object with a greater height than width. And 
this anchor box is looking for an object which is  
32:46 - greater in width, but less in height, each anchor 
box is going to predict a certain specific object,  
32:53 - and how this is going to change our output. For 
each cell, instead of having this single vector,  
33:00 - we are going to extend this vector in our case 
three times since we have three anchor boxes,  
33:06 - this enables us to predict multiple objects 
that will be present in a single cell. So,  
33:12 - all these techniques combined give us the Yolo 
algorithm YOLO algorithm is a really fast and  
33:19 - accurate object detection technique, which makes 
it extremely useful for perception in self driving  
33:26 - cars, there is a lot of work that has been done 
on this YOLO algorithm too much so that that we  
33:32 - have multiple variants of YOLO algorithm with each 
variant of the Yolo algorithm, there has been an  
33:38 - improvement in the speed as well as the accuracy. 
Okay, so now let's see this YOLO algorithm in  
33:45 - action YOLO is a very complex algorithm that 
would take quite a lot of time to develop. Hence,  
33:52 - it is neither feasible nor an intelligent 
way that we implement your law from scratch.  
33:58 - Hence, we are going to use this Charis YOLO v3 
implementation provided by experience or apologies  
34:05 - if I pronounced that incorrectly. This is an 
open source implementation, I would provide the  
34:10 - link to the repository in the description section 
below. This is the notebook that we have the link  
34:16 - for this notebook would also be present in the 
description section below. Since there is nothing  
34:22 - much to discuss with respect to implementation, we 
will directly get to the results that we have on  
34:27 - this dataset. So I have zoomed into show the image 
is better as we can see the camera was attached on  
34:33 - a car so we are also detecting our portion of the 
car as well. We are detecting other cars as well.  
34:41 - However, we can see that this bus is getting 
detected as a truck. We have predictions on  
34:49 - car as well over here. Cars over here. Ours so we 
can see we also have one truck detection over Here  
35:01 - we have the detection of a bus over here as well. 
And we also have a truck that is much behind  
35:19 - Alright, so these were the results that we got 
on running the Yolo v3 algorithm on the left  
35:25 - 3d object detection data set. Now, 2d object 
detection is pretty much a solved problem.  
35:32 - And we already have very good algorithms 
like YOLO that do our tasks. However,  
35:38 - there is some research that is still required on 
3d object detection, which was also the motivation  
35:44 - behind why Lyft conducted this competition as 
well. Alright, so this is it for this video.  
35:53 - For this particular project, we are going to 
use several videos that were recorded by a  
35:59 - camera that was placed in front of an autonomous 
car. So let's have a look at one of the videos.  
36:06 - In this video, we can see that 
there are two cars that are moving.  
36:15 - And this is another video of 
the same highway scenario.  
36:28 - Saw data set has six videos that contain 
this highway scenario. For object tracking,  
36:35 - we need to keep track of both of these cars that 
are moving in front of us. If we assign an ID to  
36:42 - both of these cars, if we say this is the black 
car, and this is the white car, then in each frame  
36:48 - of the video, we need to keep detecting where 
is the black car and where is the white car.  
36:54 - This has to be done for the complete 
duration of the video. In Object Tracking,  
36:59 - we are not only required to detect where 
the car is present in this particular frame.  
37:06 - But we also need to remember that this is the 
car that has been assigned the ID black car.  
37:13 - And this is the car that has been assigned the 
ID white car, the link to this dataset would  
37:19 - be present in the description box below. Okay, 
so now we have defined object detection problem  
37:25 - and had a look at the dataset. Now let us 
understand how this deep sort algorithm works. As  
37:31 - you may have seen before, in object tracking, not 
only do we need to detect the objects, but also  
37:37 - track where they are going. So we consider this 
collection of images, or we can see a video,  
37:44 - we can see that we have two objects that are 
moving in this video, it is this round object  
37:48 - that is going in the horizontal direction and one 
object that is going in the vertical direction.  
37:53 - In Object Tracking, we are not only detecting 
where are the objects present in this image,  
37:59 - but we are also tracking where the 
objects I did one and two are going.  
38:05 - So this is the whole problem of object tracking. 
In our case, it would be traffic tracking,  
38:11 - we need to keep a track of the different cars 
that are moving around our autonomous vehicle.  
38:19 - So the algorithm that we're going to use to solve 
this object tracking problem is called the deep  
38:24 - sort algorithm. Before discussing the deep sort 
algorithm, we would first start our discussion  
38:29 - with the simple sort algorithm. So R stands for 
Simple online real time tracking. And the simple  
38:36 - sort algorithm uses bounding box prediction, 
Kalman filters and IOU matching techniques.  
38:43 - First let us start with bounding box predictions. 
The first step in this algorithm is to generate  
38:49 - bounding boxes or detect where are the 
objects present in the image. Now, this  
38:54 - can be accomplished using any CNN architecture it 
can be YOLO it can be our CNN or it can even be a  
39:01 - simple computer vision handcrafted model. For our 
project we have used the Yolo v3 algorithm briefly  
39:08 - what you know v3 algorithm does given an input 
image it is going to detect all the objects that  
39:14 - are present in that image, draw a bounding box 
around them, and also tell us what that object is.  
39:21 - After we generated the bounding box predictions 
we get to the next step, which is the Kalman  
39:26 - filter. Now Kalman filter is a very broad topic, 
which would take an entire video to explain, but  
39:32 - still we would be left with a lot of ground 
to cover. So to briefly discuss Kalman filter,  
39:37 - we can say in very simple terms that Kalman 
filter is a linear approximation. So what is  
39:43 - the role of kalman filter over here? Kalman 
filter needs to predict what is going to be  
39:49 - the future location of a given detected 
object. Using these future predictions  
39:54 - we can determine whether the object that we 
were tracking isn't the same object or not.  
40:00 - Along with that, using prediction, we can 
also deal with the problem of occlusion.  
40:07 - So what is occlusion? In this example above, 
we can see between these these two frames,  
40:13 - the two balls are going to overlap each other. And 
whichever object is in front is going to occlude  
40:19 - the object that is present behind it. In order 
to deal with this problem in object tracking,  
40:25 - predictions are useful. So how does Kalman filter 
works? Kalman filter assumes a linear velocity  
40:33 - model and generates the predictions according to 
it. Once we get the real data where the object is,  
40:38 - we input that again to the Kalman filter. The 
Kalman filter improves its predictions based on  
40:44 - the real data that it got, and again generates a 
new set of predictions. In this way Kalman filter  
40:52 - works in an iterative manner and keeps improving 
its predictions. As an output, the Kalman filter  
40:58 - does not output a single number, actually, it 
outputs a probability distribution of where the  
41:04 - object can be in a given set of locations. If we 
take the maximum value of that probability, we can  
41:11 - in some way approximate where the object is going 
to be. This is the work of the Kalman filter,  
41:16 - it generates future predictions for the objects. 
The next step we have is called IO U matching.  
41:25 - Io U stands for intersection over Union. In brief 
terms, we can say IOU gives us a quantitative  
41:32 - score to determine how much two bounding boxes 
are similar to each other based on their location  
41:38 - in the image as well as the size, but is this 
IOU matching in the context of sort algorithm,  
41:45 - let us consider this case, we have n different 
cards that we are detecting in a particular  
41:50 - frame. And we have n IDs that we need to assign 
to these particular cards, like in this case,  
41:56 - IDs are useful to determine which object we are 
tracking. So we need to assign these n detections  
42:03 - to these n IDs. For each of the n IDs, we have 
IOU score corresponding to each detection in a  
42:11 - descriptive way, we have the detected objects 
in the row, and we have the IDs in the columns.  
42:18 - So for each of the ID and the GS detection, we 
have an IOU score. Now we want to assign the IOU  
42:25 - scores to these n detections in such a way that 
the total IOU score is maximized. solving this  
42:32 - problem using brute force approach takes order 
complexity n factorial, which is very large.  
42:38 - In order to solve this, the sort algorithm uses 
the Hungarian algorithm, the Hungarian algorithm  
42:44 - solves this linear assignment problem in order 
complexity n cubed, which is a great improvement  
42:52 - over the n factorial algorithm. Once we have 
assigned these n predictions to N IDs, we have  
42:59 - in a sense solved the tracking problem for the AI 
X frame, then we can run this algorithm again in  
43:05 - a loop and again, keep track of the objects that 
are in the next frame of the video. So this is how  
43:12 - the simple salt algorithm works. Not deep salt 
is an extension of the simple sort algorithm.  
43:17 - Now that deep in the deep salt comes from this 
step called the deep appearance descriptor step.  
43:24 - Along with that, we also have a cascade matching 
step that is added to the simple sort algorithm.  
43:29 - So deep appearance descriptor is a convolutional 
neural network, which is trained to detect a  
43:36 - similar object in different images. What this 
means this, this network can tell given a number  
43:43 - of images of the same person in different views, 
whether it is the same person or not. So as an  
43:49 - input, the deep descriptor receives a cropped 
image of the object detected. And as an output,  
43:56 - we try to receive a vector that encodes the 
information that is present in that crowd image,  
44:03 - these encoded vectors would allow us to compare 
different objects. So now we have two models  
44:10 - that are describing us about similar 
objects in different frames of a video,  
44:15 - we combined the score given by these two models in 
a linear fashion. The score by the deep descriptor  
44:22 - is given using the cosine distance metric, and 
the score by the Kalman filter is given by the  
44:28 - melanomas distance before discussing how these 
metrics are computed. Let us first discuss the  
44:33 - notion of a distance metric. A distance metric 
is a score that we assign to two different  
44:40 - entities A and B to tell how similar they are. 
For instance, if we consider it to deep plane  
44:48 - on that plane, we have two points A and B. 
We compute the distance from the origin.  
44:56 - If the two entities A and B are close to each 
other then distance from the origin is also going  
45:02 - to be similar in value. If the two entities 
A and B are very dissimilar to each other,  
45:08 - then these two entities are going to have a 
Euclidean distance that is very dissimilar,  
45:13 - we can say one is going to have a negative value, 
the other one is going to have a positive value  
45:18 - if they are lying on the two sides of the Y axis. 
In these two cases, we need to use another notion  
45:25 - of distance metric. For the deep descriptors, we 
use the cosine distance metric, how do we compute  
45:30 - this? If we are given two entities A and B. And 
from the origin, we draw a vector joining these  
45:43 - and then we calculate the angle between these two 
vectors, the cosine value of this angle is going  
45:50 - to give us the cosine distance metric. Let us 
consider an example. If we have two vectors that  
45:56 - are overlapping each other, the angle between them 
is going to be zero, and the cosine value of zero  
46:02 - is going to be one. This means that these 
two vectors are very similar to each other.  
46:07 - However, if two vectors are perpendicular to each 
other, the cosine value of 90 is going to be zero.  
46:13 - This would mean that these two vectors are very 
dissimilar to each other. In this way, we would be  
46:18 - able to determine whether the two objects that we 
detected how much similar they are to each other.  
46:24 - For the Kalman filter case, we are not using the 
cosine similarity. A reason for this is the Kalman  
46:30 - filter is going to output not a single point, but 
a probability distribution. In order to compare  
46:36 - the similarity between a point and a probability 
distribution, we use melanomas distance.  
46:44 - If we consider that this is the distribution that 
is output by the Kalman filter, this implies that  
46:50 - the probability of the object being over here 
is very high compared to the values outwards.  
46:58 - And this is the point that we want to 
compare with this probability distribution.  
47:04 - In very simple terms, what melanomas distance 
does, for a 2d version, we compute the two  
47:10 - directions in which the data spread is the 
highest. For this case, we can see the data  
47:15 - spread is the highest in this direction and a 
perpendicular this direction. Then we normalize  
47:21 - and transform the coordinate axes in such a way 
that these two axes become the coordinate axes.  
47:29 - In that transformed space, we calculate 
the distance of this point from the origin,  
47:34 - a value of that distance is the melanomas 
distance. Using this metric, we decide how much  
47:42 - a given prediction by the Kalman filter matches an 
object that we just detected. Combining these two,  
47:49 - we get an overall scalar score value that 
we can use to assign detections to their ids  
47:55 - using the N Galeon algorithm. However, in 
the implementation, there is a slight catch,  
48:01 - the authors of the deep sort algorithm noticed 
that the predictions by the Kalman filter are not  
48:07 - very useful, and the score for these predictions 
can be neglected. However, you will see that  
48:12 - in our project, this doesn't work very well. The 
next step that we have is cascade matching. Gaskin  
48:18 - matching is an extension to the IOU matching 
algorithm for the assignment problem, the cascade  
48:24 - matching takes into account the temporal dimension 
as well, in order to reduce the time complexity  
48:30 - for the Hungarian algorithm cascade matching tries 
and match the latest detections with the latest  
48:36 - IDs and the later or old detections with the 
old IDs. All of this process combined gives us  
48:44 - the deep salt algorithm. And we can again run this 
algorithm in a loop to track objects in a complete  
48:53 - video. Alright, so this is all for the explanation 
part of the deep sort algorithm. Now let's have  
48:59 - a brief look at the code and the results and see 
how it worked for our project. So let's do this.  
49:05 - The link to this code or notebook will be 
provided in the description box below as well.  
49:11 - So, these are the different sub parts 
of the complete deep sort algorithm.  
49:15 - The first part is the Yolo v3 network that is used 
to generate the bounding boxes. The second step  
49:22 - is the detections part where we encode the 
given cropped image. The third part is the  
49:29 - Kalman filter, where we use the Kalman 
filter to generate the predictions.  
49:34 - The next part is the IOU matching in this 
part, we solve the linear assignment problem  
49:40 - and the Cascade matching that we discussed 
before. Next up is the nearest neighbor matching.  
49:48 - This contains the code for the Euclidean 
distance as well as the cosine distance.  
49:55 - The next part is tracking. This combines all the 
different modes To do that we developed before  
50:01 - to create a final object that does the complete 
tracking. Finally, we have the object tracking  
50:08 - where we run the complete algorithm. This file 
over here contains the pre trained network for  
50:14 - deep appears descriptive II have this function 
object tracking that takes in the video path  
50:20 - input, and also takes the output path there 
to save the output video. Now let's have a  
50:26 - look at the results of how the final video 
looks like. So this is the first video,  
50:32 - as we can see this one and two, these are the IDs 
that are assigned to these two cars respectively.  
50:49 - This is the second video.  
50:58 - As we can see, the car over 
here was assigned the ID nine.  
51:01 - And even in some frames, the yellow 
algorithm was not able to detect that car.  
51:09 - Even after those Miss detections, the algorithm 
was able to determine that this was the car  
51:15 - labeled ID nine l now we had a look at the 
videos where the algorithm was performing  
51:22 - really well. Now let's have a look at the video 
where the algorithm didn't perform that well.  
51:27 - In this video, we can see that we have 
this car labeled 12. And we have this new  
51:32 - car that is labeled 35. Now you will see what 
is the problem that this algorithm is facing.  
51:39 - Due to the occlusion that 
was given by this black car.  
51:42 - This car is now the identified as the ID 36 
and not the ID 12 That it was assigned before.  
51:53 - So this is one problem that deep sort algorithm 
faced due to a longer occlusion period,  
51:59 - the algorithm was not able to detect the 
correct ID for the car. Even after checking  
52:05 - different hyper parameter values, I was not 
able to get rid of this ID assignment problem.  
52:11 - This I believe would be due to assigning an 
negligible amount of weight to the Kalman  
52:18 - filter predictions. Possibly if we increase the 
weight for the Kalman filter predictions, this  
52:24 - problem would be solved. But that is an extension 
to the project that we developed in this video,  
52:30 - and will be looked as a future work to this 
project. Alright, so this is it for this video.  
52:39 - Okay, so these are the images that are part of 
the TT 3d object detection data set as we can see.  
52:51 - And these are the LIDAR data 
provided in the data set as well.  
53:01 - For instance, we can see a 
wall in this image over here,  
53:05 - the same wall, it has a correspondence in the 
LIDAR scan as well as we can see over here.  
53:12 - These curvy lines are the projections of 
the LIDAR that fell on the road itself. All  
53:19 - also if we consider another example, we can 
see this car in front of the image. This same  
53:28 - car can be seen over here, which is blocking 
the LIDAR signals. Another cool visualization  
53:34 - that we have is the LIDAR data presented in 
the form of a camera as we can see over here  
53:46 - as we saw in the image earlier, 
we had a car in front of us,  
53:50 - we can see that car similarly in these LIDAR scans 
as well. Another example is this car over here.  
54:00 - This car, this car as well as 
the traffic signal can be seen  
54:05 - in this LIDAR scan as well. And this 
is another visualization of the LIDAR  
54:10 - data as well as the bounding box that are 
given in that 3d object detection data set.  
54:21 - Alright, so these are all the visualizations. Now 
let us understand what is the theory behind how we  
54:28 - generate these visualizations. Okay, now in order 
to understand how are we generating these data  
54:34 - visualizations, first step, we need to understand 
the concept of homogeneous transformations.  
54:41 - In a general self driving car setting. We 
usually have two main sensors which are Lidar  
54:48 - and the camera. Typically, these Lidar and 
camera sensors are not going to be placed  
54:56 - at the same location. They are going to have 
different locations turns on a car. For instance,  
55:02 - in this case, the Lidar is on top of the car, 
and the camera is on the front head of the car.  
55:09 - Camera, as we all know, is a visual sensor, 
we use it to capture a 2d scene of a given  
55:16 - 3d environment. Lidar, on the other hand 
is a perception sensor. It is used to map  
55:22 - a 3d environment using a rotating laser. LIDAR 
generates what are called the point clouds. Point  
55:29 - Clouds provide a very good representation of a 
given 3d environment. Now, this is a very general  
55:36 - use case that we want to see the data that was 
captured by a lidar in the frame of a camera. What  
55:45 - I mean by that, if through our eyes, we are seeing 
how the environment is looking like from here,  
55:52 - we also need to see how the same environment at 
the same time is going to look like when we see it  
55:58 - from over here at the camera location. In order to 
convert between these given reference points, we  
56:05 - use what are called homogeneous transformations. 
If we consider that we are given two coordinate  
56:12 - axes, the blue one over here, and the red one over 
here, and we have this object, this black dot that  
56:19 - we want to see through both of these coordinate 
axes. Mathematically, this problem translates to,  
56:27 - if we are given the vector from the origin of 
this coordinate axis to this point over here,  
56:33 - which we call x, we need to find x dash, which 
is a vector starting from the origin of this  
56:40 - coordinate axis to the same point given this 
coordinate axis is called transformation. Now,  
56:48 - any general transformation can be divided into two 
subproblems, which are translation and rotation.  
56:55 - Translation is simply moving the origin of this 
coordinate axis to some other location in space.  
57:03 - If we have a coordinate axis like this, 
and we simply move this to a new location,  
57:08 - then we have what is called a translation.  
57:11 - In rotation, however, we would like to change 
the direction in which these axes are pointing.  
57:18 - However, the origin is going to remain at the same 
place. In order to understand rotation better, we  
57:24 - can divide the rotation operation into three sub 
operations, which are rotation about the x axis,  
57:30 - rotation about the y axis and rotation about 
the z axis. A combination of all these three  
57:37 - rotations gives us the actual rotation 
that is going to finally take place.  
57:42 - Without going into much depth, let us see how the 
mathematical representations of these look like.  
57:48 - In order to get the translated vector, we simply 
need to add another vector to our original vector.  
57:56 - In this case, we are adding this ABC vector 
to our original x to get x t, which is the  
58:01 - translated vector, the rotation operation can 
be understood as a matrix multiplication. If  
58:08 - we consider this three cross three matrix, and we 
multiply this matrix with our original vector x,  
58:14 - we get a rotated vector XR, however, we need to 
keep this in mind, the values of this translation  
58:21 - vector can be taken anything, but the parameters 
of this matrix are bounded between a range  
58:31 - combining these two sub operations of translation 
and rotation, we get what is called a homogeneous  
58:37 - transformation. What homogeneous transformation 
does, it simply reduces translation and rotation  
58:44 - into a single matrix multiplication, as we can 
see over here. So how we derive this matrix  
58:52 - multiplier, first up, we take the rotation matrix 
and place it in the upper left corner over here,  
58:58 - we take this translation vector, we place 
it in the upper right corner over here,  
59:03 - and these remaining part, this last row is 
filled with three zeros and a one. This will  
59:10 - look something like this, this three cross 
three matrix is the rotation matrix. And  
59:15 - this three dimensional vector is the translation 
vector, rest zeros and ones are placed over here.  
59:22 - Now following the rules of matrix multiplication, 
given this is a full Cross for matrix, it can only  
59:28 - be multiplied with a vector of dimensionality for 
in order to make this three dimensional vector,  
59:36 - a four dimensional vector, what we 
do is place a one over here. Hence,  
59:41 - our complete operation looks like this. We'll 
take this matrix, we multiply this matrix with  
59:46 - this extended vector. And what we get as an 
output is this extended vector over here,  
59:52 - because of the structure of this matrix over here, 
this one over here is a guaranteed number. Using  
59:58 - this operation, we Finally get a vector x dash 
representing the point x in a new frame of access.  
60:07 - Now, in order to convert this given 3d data 
into a 2d representative screen, just like  
60:14 - we have in a camera, we use the operations that 
we will discuss over here. Now mathematically,  
60:19 - how does a camera work? If we consider this 
red coordinate axis as the camera axis,  
60:25 - we have this point in the 3d scene that 
we can see through this camera axis.  
60:31 - Now we define a new point which is called the 
camera Center, which is taken to be the origin  
60:37 - of this coordinate axis over here. Now all the 
points that are situated at a distance of one  
60:44 - unit from this camera center in the direction 
of z axis, is what we call the image plane.  
60:52 - Now this image plane is a plane to the surface, 
which has defined its x&y coordinates in this  
60:58 - direction, the x direction is taken upward and y 
direction is taken along this direction. We can  
61:05 - also understand this image plane as sitting at 
the camera center and watching a cinema screen  
61:12 - that is one unit away from us. Now 
this complete problem boils down to  
61:20 - if we are given this 3d point, we want 
to project this 3d point to this image.  
61:29 - That is, how would this 3d Point look like if 
we were to see it in a 2d projected image plane.  
61:37 - And this is exactly what a camera does converts a 
3d scene into a 2d image. We accomplish this using  
61:45 - the similar homogeneous transformations. If we 
are given a transformation matrix M, we can simply  
61:52 - multiply that with a 3d object vector x dash and 
get a new vector, which is x double dash. Also,  
62:00 - in order to get how these points would look like 
in this image plane, we need to divide the X and  
62:07 - Y coordinates of this newly x double dash vector 
with its Z coordinate, which we can see over here.  
62:17 - Solving both of these operations, we would get the 
transformation of a 3d point on a 2d image plane.  
62:24 - In our case, how that works out is we are given 
the point cloud that is generated by the LIDAR  
62:30 - using these operations. We can see how that 
LiDAR point cloud is going to look like  
62:38 - when we put it on an image that is generated by a 
camera. Now this complete concept of homogeneous  
62:44 - Transformations is very fundamental to 
robotics as well as computer vision.  
62:49 - It is used in self driving cars, mobile robots, 
and even robot manipulators in computer vision.  
62:57 - Apart from this application, it is also used in 
3d scene reconstruction. Now the parameters used  
63:03 - for the operations that we did on this side 
of the board are called extrinsic parameters.  
63:10 - And the parameters that we used for this side 
of the board are called intrinsic parameters.  
63:16 - Extrinsic parameters are external to the camera 
and depend on how the camera is placed. Intrinsic  
63:24 - parameters depend on the internals of a camera, 
for instance, the focal length of the camera lens,  
63:32 - there are a number of different ways we can use to 
determine what are the extrinsic and the intrinsic  
63:38 - parameters of a given camera. One very popular 
way to determine these is the checkerboard method.  
63:45 - In this method, we take a printed 
photo of a chest or a checkerboard.  
63:50 - And we click its images in different positions and 
angles. We apply a number of different operations  
63:57 - to those images to determine the extrinsic and 
intrinsic parameters of the camera. This is all  
64:03 - the theory that we needed to discuss for the data 
visualization. Now let's have a brief look at the  
64:11 - code of how this data visualization is working. 
So let's get to it. All the code that we are  
64:16 - using to generate these 3d visualizations will 
be present in the description box below as well.  
64:22 - Alright, so now first, let us discuss how we 
generate this LIDAR data in the form of a camera.  
64:30 - In order to convert we have this points array 
over here. This array contains all the points  
64:36 - of the point cloud that were captured by the 
LIDAR. We pass this points array to a function  
64:43 - called Velo to Cam. Along with that, we also 
pass the extrinsic camera metrics. Well to Cam  
64:51 - over here stands for velodyne to camera velodyne 
is the LIDAR sensor that was used over here.  
65:00 - All in all what this Vallo two cam 
function does, it takes this cloud  
65:05 - array and it takes this v two C matrix, and it 
simply does a matrix multiplication on them.  
65:12 - Once we get the points in the camera reference, 
we then calculate a matrix multiplication of those  
65:18 - points with the intrinsic camera matrix. And we 
do the division operation over here. And finally,  
65:28 - we fill in different colors using this expression 
over here. And calling this function on  
65:33 - different set of point clouds gives us this the 
following visualization that we see over here.  
65:42 - generating this view of the LIDAR 
also called the bird's eye view,  
65:46 - is relatively simple. We simply 
take the points of the point cloud  
65:53 - and scale them in such a way that they 
occupy the complete frame of a given image.  
66:02 - And then finally, fill the different colors 
that are present in over here. And this is  
66:07 - how we get the bird's eye view of the LIDAR data. 
Getting this image visualization is pretty simple.  
66:16 - And this data is already provided. And we do 
not need to do any processing over here as such.  
66:24 - Getting the boxes to print over here is also 
simple, just like how we did for the LiDAR,  
66:30 - we use the extrinsic and intrinsic 
camera parameters to accomplish the same  
66:35 - and this is just a Combined 
view of the Lidar and the boxes.  
66:42 - Alright, so this is all for this video.  
66:48 - Alright, so the problem of multi task learning is 
such that we want to derive a single network that  
66:54 - can be used for various different computer vision 
tasks. The data set that we are going to use for  
67:02 - this particular project is the cityscapes dataset, 
the link for this data scraped would be present in  
67:08 - the description box below. The cityscape data set 
consists of different Street View images, that is  
67:15 - images taken from a car that is being driven on 
the road. This data set is particularly used for  
67:22 - depth estimation and semantic segmentation. Let's 
have a look at the images present in the dataset  
67:28 - and what these two tasks are. So these are the 
different images that are present in the data set.  
67:48 - Okay, now, let us discuss what is the depth 
estimation task. As the name implies in this task,  
67:53 - we want to calculate the depth of any 
object that is present in the image.  
67:59 - Let us have a look at some of the example 
images to make this point more clear.  
68:04 - In this image, we have this struct that is front 
of our car. And through the color gradient we  
68:10 - can see that the depth is almost constant in 
front of this. We can also observe other such  
68:17 - features from this gradient color. The areas 
close to the car are slightly darker in color  
68:23 - and as we move away from the car, the colors 
get slightly lighter. As we can see over here.  
68:32 - A similar look we can have over here, 
there is a car present near our car,  
68:37 - and we can see a very dark color for 
that car. And as the objects go away,  
68:43 - the color gets lighter and lighter. Let's 
have a look at some more example images.  
69:02 - So the overall work in this task is to calculate  
69:05 - depth maps, and the depth maps without the 
underlying image looks something like this.  
69:24 - For the image segmentation tasks, we need to 
segment different objects that are present in  
69:29 - the image. Let's have a look at some example 
images. In this example all the cars and  
69:34 - vehicles are segmented in a yellow color. And the 
trees and background are segmented in red color.  
69:44 - Something similar can be observed in this image as 
well. Let's have a look at another example images  
70:05 - So, the overall work in this task is to calculate  
70:09 - segmentation labels which we can 
see in the example images over here.  
70:27 - Alright, so, this is the visualization 
of the data that we are going to use.  
70:32 - Now, let us have a look at the technique 
that we are going to use to solve the  
70:36 - multitask learning problem. Alright, so the 
problem of multitask learning is as follows.  
70:43 - We want to derive a network that can 
be used to solve different tasks,  
70:48 - these different tasks can be object detection, 
object classification, and Object segmentation.  
70:55 - The constraint in this problem is that we cannot 
have different networks for these different tasks,  
71:01 - we have to use a single network that is run once 
and trained once to solve all the different tasks.  
71:08 - The advantage it has over distributed networks is 
that we don't have to train each and every network  
71:14 - individually for the different tasks resulting 
in saving some time. So the technique that we're  
71:20 - going to discuss for this multitask learning 
is m 10. M 10 stands for multi task attention  
71:28 - network. And this is the architecture that M tan 
follows. Given an input image, the first step we  
71:36 - do is calculate the shared features of that given 
input image. Then we have task specific modules  
71:44 - that use those shared features to solve their 
given task. For n number of tasks, we are going to  
71:52 - have n specific attention modules. These attention 
modules take some data from the shared features  
71:59 - and augment it in such a way that is useful 
for their particular task. The word attention  
72:05 - over here refers to the attention that 
each module gives do its particular task.  
72:11 - For the intent technique, we can use any 
convolutional network to calculate the shared  
72:17 - features for our particular project, we are going 
to use set net to calculate the shared features.  
72:24 - Now signal is a network that is used to solve the 
image segmentation task in the image segmentation  
72:30 - task, we would be given an input image and the 
task is to segment the different objects that are  
72:36 - present in that image. For this image segmentation 
task, the networks that are used have a general  
72:42 - structure and general structure is such that we 
are given an input image the first step we do  
72:48 - is downsample the image and derive the features 
from that image once we downsample the image and  
72:53 - reach the bottleneck, we have sample the image 
to the same dimension as the given input image.  
72:59 - And with the help of the features 
derived in the downsampling task  
73:03 - output a segmentation mask. The downsampling 
part of the network is also called the encoder  
73:09 - and the upsampling part of the network is also 
called the decoder. The only difference between  
73:15 - Cygnet and fully convolutional neural networks 
is the operation that we use for upsampling.  
73:22 - So the Signet architecture has different 
convolution blocks. As you can see over here,  
73:28 - these convolution blocks comprise of convolution 
layers followed by some pooling layers. Now, the  
73:34 - M 10 architecture is in such a way that for each 
of these convolution blocks, we have attention  
73:40 - some modules, these sub modules are for the blue 
tasks and these sub modules are for the red task.  
73:48 - The arrows over here show the data flow that is 
happening between the overall network. Now there  
73:54 - are some differences in the upsampling attention 
modules and the downsampling attention modules.  
74:01 - Let us discuss them in slight detail. For 
the upsample attention module, we get the  
74:07 - input from the previous attention module which 
we can see over here. And we take another input  
74:12 - that is derived from the first layer of the 
convolutional block as we can see over here,  
74:19 - these two matrices are merged the convolution 
operation on them is computed twice  
74:27 - and then we calculate the element wise 
multiplication of the output generated  
74:32 - over here and the input of one of the filters of 
these convolution blocks as we can see over here,  
74:41 - then this element wise multiplication 
is once again sent through a convolution  
74:45 - block and the output is returned to the next 
attention module. For the downsampling module,  
74:51 - we we take the input from the previous 
attention module, which we can see over here  
75:00 - This input is sent through a convolution 
block which is then merged with the input  
75:06 - of the first layer of the convolution 
block which is present over here.  
75:12 - This merge filter is passed 
twice through convolution blocks,  
75:17 - then an element wise multiplication 
is calculated for this output,  
75:21 - and the input that is taken from one of the layers 
of convolution block as we can see over here.  
75:28 - And this final output is sent 
to the next attention module,  
75:32 - the attention module is only different for the 
up sampling and down sampling phase of the shared  
75:37 - feature network. For the different tasks, the 
attention module architecture is exactly the same.  
75:45 - And this is the complete network architecture of 
the M 10 network. One additional technique that  
75:52 - M 10 proposes is what is called dynamic weighted 
averaging. For any multi task learning network,  
76:00 - we calculate the loss by a weighted average of 
the loss calculated for the different tasks as  
76:06 - you can see over here, the adjustment 
of the weights of these losses is very  
76:12 - important for the network to actually 
converge and learn the different tasks.  
76:18 - Dynamic weight averaging is a technique 
that is used to determine these weights  
76:23 - based on the rate of change of the gradient loss. 
Now, what is the rate of change of gradient loss,  
76:30 - if we consider two training batches of images 
input to the network at time t and time t  
76:35 - plus one the difference between the loss that is 
back propagated at time t plus one and time t is  
76:43 - what is called the rate of change or gradient 
loss, we calculate these weight values by taking  
76:50 - a softmax kind of average using the rate of 
change of loss. And this technique gives us  
76:55 - the optimal value of the weights that we 
should have for these individual losses.  
77:00 - So, these are the two main contributions of 
the multitask attention network technique.  
77:06 - So, this is all the theory that we 
need to cover for the M 10 technique.  
77:10 - Now, let us have a brief look at the code and the 
results generated using this technique. Okay, so,  
77:16 - the code or notebook for this m 10 would be 
present in the description box below as well.  
77:23 - So, the first thing that we have done over here 
is implement the data set loader class which we  
77:29 - can see over here. Now, this data set loader 
class is specific to the cityscapes data set,  
77:34 - the architecture that we discussed can be 
applied to a number of different data sets.  
77:39 - And for those data sets, we would have 
to implement those specific classes.  
77:46 - Then, we have defined the utility functions 
model fit which is used to train the model.  
77:53 - We also have the different metrics, the confusion 
matrix and the depth error. And then we have  
78:01 - this function called multi task trainer, which 
handles the complete training of the network.  
78:08 - We have also defined the network in this 
Cygnet class. Finally, we load the data set  
78:16 - and train it on the dataset. One thing to note 
is that the authors of this technique trained  
78:23 - the network on 200 epochs with a learning rate of 
10 raise to power minus four. However, due to the  
78:31 - constraints imposed by candle on the training time 
doesn't allow us to experiment with these values.  
78:40 - Hence, for this particular experiment, the 
epochs are taken to be 100 and the learning  
78:44 - rate is taken to be 10 raised to one minus three. 
The results of the training are decent and good.  
78:51 - However, it would have been better if we use the 
parameters that were provided by the authors.  
78:58 - Let us have a look at some of the 
results. The left one is the original  
79:02 - segmentation map and the right one 
is the predicted segmentation map.  
79:31 - As we can see, there are slight issues that this 
network faces while segmenting by cycles. But  
79:38 - the overall results don't look good. These are 
the segmentation results overlaid on the image.  
79:58 - Now let's have a look at the depth maps the left  
80:01 - one is the original depth map and the 
right one is the predicted Depth Map.  
80:23 - For the depth map we can also say there 
are slight errors here and there but the  
80:27 - results overall do seem good. Now, these 
are the depth maps overlaid on the image  
80:54 - All right, so this is it for this video.  
80:59 - Alright, so the data set that we're going to use 
for this video is the kit a 3d object detection  
81:04 - data set, we will quickly go over this data 
set in this video as well. This data set has  
81:10 - images that are taken from the front camera of 
a self driving car as we can see them over here.  
81:21 - We are also provided with a LIDAR data 
that was captured by LIDAR present on  
81:27 - the self driving car as we can see them over here.  
81:39 - And these are the LIDAR data 
projected in the view of a camera.  
81:50 - So this is the dataset that we are going to work 
with. Now the cool thing about SFA 3d technique  
81:57 - is that this network only uses the LIDAR data 
to do the 3d object detection task. Hence,  
82:05 - the images that present in this data set are 
not used at all. Another thing to note as well.  
82:12 - This data that we're visualizing over here 
is a processed form of the city 3d object  
82:18 - detection dataset. However, for generating 
the final output of that technique,  
82:24 - we are going to use the raw form of the KT data 
set that is provided in the form of a video feed,  
82:32 - the real time video feed and the LIDAR data 
feed would be used for the final prediction  
82:37 - tasks. Alright, so this is all for the data 
set. Now let us discuss the technical details  
82:43 - of this asset with 3d technique. Okay, so as we 
discussed, we are going to use SFA 3d for the  
82:51 - 3d object detection task SFA. 3d stands for 
super fast and accurate 3d object detection.  
83:00 - So SFA 3d as a complete technique does 3d 
object detection using only the LIDAR data?  
83:08 - As an input we provide SFA 3d as a bird's eye 
view input image to the network. The network  
83:17 - predicts the different objects that are present 
in that image and also classifies them as well.  
83:24 - The data collected using a lidar is a 3d data. 
When we view that 3d data from a top view  
83:31 - that top view is also called the bird's eye view 
of that given scene. Once you provide the network  
83:38 - with the bird's eye view image of the LIDAR seen 
as an output, we get seven outputs. These seven  
83:46 - outputs are heat maps of the different 
classes that are present in the image,  
83:53 - the distance of the center of other objects 
from our ego car or the self driving car,  
84:00 - the angles in which the other objects are facing  
84:04 - the dimensions of the objects that we have 
detected, the length width and the height  
84:09 - and the z coordinate of the center of that object 
as we can see over here. So now we have discussed  
84:17 - the input as well as the list of outputs that 
sma 3d provides us now SFA 3d mainly consists of  
84:25 - three components, which are key point FPN, 
the different losses that we're going to use  
84:32 - for this network and the learning rate scheduling 
technique that we employ for the training.  
84:41 - Let us discuss these techniques one by one. 
The first component that we're going to discuss  
84:46 - is the key point SPN before discussing key 
point FPN let us first discuss what is FPN SBN  
84:55 - stands for feature pyramid network feature pyramid 
network is a feature feature extraction technique  
85:01 - that given a single input image outputs 
different feature maps that vary in size.  
85:09 - It's something as you can see over here, given 
a single input image, the outputs are different  
85:14 - feature maps of different sizes. So what is 
feature extraction and feature maps. All the  
85:21 - important data that is contained inside an image 
is what are called features. They can be points,  
85:28 - edges, designs and patterns or even human faces. 
In a convolutional neural network, we have what  
85:36 - are called convolution blocks. These convolution 
blocks consist of different convolution layers  
85:42 - and pooling layers. We provide an input image to 
these convolution blocks, and we get an output  
85:50 - image or we can say a matrix. This output image 
or metrics are what are called feature maps.  
85:56 - The process of extracting features out of a given 
image is what is called Feature Extraction. Now,  
86:02 - feature pyramid networks is not a network by name, 
it's actually a general technique that we can  
86:09 - employ on a number of different neural networks. 
For instance, we can apply this feature pyramid  
86:16 - network technique to generate feature maps on a 
ResNet architecture as well. By post processing  
86:23 - these output feature maps, we can do a number of 
different tasks like object detection, the feature  
86:29 - pyramid network technique works in this way. Given 
this input image, we pass it through a number of  
86:37 - different convolution blocks, and ensure that 
these images get smaller in size. As we can see,  
86:45 - over here, the input image is getting smaller 
in size as we approach the tip of the pyramid.  
86:52 - Then we employ successive iterations of 
convolutional blocks, ensuring that they up sample  
86:58 - the given image as we can see over here, the image 
gets larger in size. As we approach the base of  
87:06 - the pyramid network, along with the convolutions 
that we apply in this part of the network. We also  
87:16 - provide information to this part of the 
network from the previous part of the network,  
87:21 - as we can see through the 
skip connections over here,  
87:25 - and we generate the output feature maps from 
this part of the network as we can see over here.  
87:31 - Now, feature pyramid networks are a great 
technique to employ if we want to create  
87:36 - networks that are scale invariant. So what is 
scale invariant network? Let us assume that we  
87:45 - want to predict a ball that is present on an 
image using a simple convolutional network.  
87:51 - If the ball that is present in that image 
of large size or that ball is of small size,  
87:57 - the neural network should be able to detect 
the ball that is present in that image. This  
88:03 - property of detecting irrespective of the size of 
that object is what is called scale invariance.  
88:11 - for tasks like object detection, we need to 
predict a bounding box that tells us where that  
88:17 - object is present in that image. For tasks like 
these scaling variants is an important property  
88:24 - that neural networks are supposed to have. 
However, for detecting points in a given image,  
88:31 - scaling variants does not play a major role. If 
we want to detect a single point in a given image,  
88:38 - that point is going to remain at the same 
location, irrespective of the scaling that  
88:44 - we apply on that image. Hence, directly applying 
feature pyramid networks to detect points in an  
88:51 - image is not an efficient technique. Key Point 
feature pyramid networks are an extension to  
88:56 - the given SPM networks that help us detect points 
in a given image. The overall architecture of key  
89:04 - point FPN remains the same, the outputs of the SPN 
can be visualized, as we can see over here. These  
89:12 - Gaussian shaped outputs that we see over here 
indicate that the network is really confident  
89:19 - that a point is present at a given location in 
the image. But as we move away from that point,  
89:25 - the confidence level of the network goes 
down, as we can see from the shape over here.  
89:33 - So once we have these feature output 
maps, we convert these feature maps to  
89:38 - a similar size. In this particular case, 
we have taken the reference size to be the  
89:44 - largest feature map that is present with us. 
We upsample the smaller feature maps to match  
89:50 - the size of the largest feature map that we 
have, as we can see in this column over here.  
89:57 - Then we calculate a weighted average of This 
feature maps and get a final single output as  
90:03 - we can see over here, the weights of the weighted 
average are dependent on the confidence value that  
90:10 - the network has in this different feature maps. 
From this feature map, we can take the point that  
90:17 - has the highest confidence value and output 
that as the point that we wanted to detect  
90:24 - this complete post processing that we have done 
after the simple SPM network is what constitutes  
90:30 - the key point FPN network. Alright, so this is 
the complete network architecture of the SFA 3d  
90:37 - techniques. Now let us discuss the different 
loss functions that are employed to train  
90:42 - this network. First off, we are going to discuss 
the outputs that are generated by this network.  
90:49 - The first output is a heat map of the different 
classes. Heat Map is a data visualization  
90:55 - technique that tells us the magnitude of a given 
phenomena occurring on a given input. Roughly,  
91:02 - we can visualize a heat map using a mountain 
terrain. Usually in a mountainous range,  
91:09 - we have a mountain that has the highest peak, 
and has accompanying hills or mountains that  
91:15 - are off smaller peaks, then this highest peak 
Mountain. As we move away from this highest peak,  
91:21 - the height of the ground also starts decreasing 
in all the directions something like this.  
91:29 - As we move away from that highest peak, we 
approach another neighboring mountain or a hill.  
91:35 - After the height decreases, the height 
again starts increasing until we reach  
91:40 - the top or the peak of the neighboring mountain. 
And once again, as we move away from that peak,  
91:47 - the height again starts decreasing. If we view 
such a mountainous region from the top view,  
91:54 - and visualize the height of the ground in 
that image, we get what is called a heat map.  
92:00 - If we denote a larger height by a dark red 
color, and a lower height by a white color,  
92:08 - we are going to observe that the 
highest peak has a solid red color.  
92:12 - As we move away from that peak, that solid red 
color is going to decay into a white color. Again,  
92:20 - as we approach another peak, that white color is 
going to converge into a solid red color. Again,  
92:28 - a heat map can also be visualized in terms 
of confidence of a network that it has on  
92:34 - its predictions. In our particular case, if we 
want to detect three objects, let's say a car,  
92:42 - a truck and a pedestrian, we are going to 
generate three different heat maps. In this case,  
92:48 - the red color denotes that there is a car that is 
present in that image at this particular location,  
92:55 - and the network is confident about this location. 
Similarly, a pedestrian is present in this  
93:01 - location, and the network is confident about 
its prediction in this particular location.  
93:08 - As we move away from both these locations, 
the confidence of the network goes down.  
93:14 - Using this heat map, we detect the class of 
the object whether it is a car or a pedestrian.  
93:21 - In order to encourage the network to learn the 
right classes, we use what is called a focal loss.  
93:29 - The mathematical expression of the 
focal loss is something like this.  
93:34 - Focal loss is usually used when there 
exists a class imbalance in the data.  
93:40 - class imbalance usually occurs when there is 
a less amount of data for a particular class  
93:45 - and a very high amount of data 
for another particular class.  
93:49 - Here P T is the confidence value of the network. 
This scaling factor over here allows the network  
93:56 - to focus its learning more on the predictions that 
it is less confident about and focus less on the  
94:03 - predictions that it is more confident about. Next 
output we have are the distance from the ego car  
94:12 - and the direction in which the other 
objects are facing. In order to learn this,  
94:16 - we use what is called an L one loss. The 
mathematical expression is something like this.  
94:23 - elven loss simply takes the difference of the 
prediction and the actual value and computes the  
94:30 - absolute value of that difference. The next output 
that we have are the dimensions of the object we  
94:37 - are trying to detect and its Z coordinate 
the distance or the height from the ground.  
94:45 - In order to learn these four parameters, we use 
what is called a balanced Elvan loss. Now given  
94:52 - a training data, we can divide the points in that 
data roughly into two categories, outliers and In  
95:00 - liars white training a network that data points 
that generate a loss value that is less than some  
95:07 - threshold, let us say one are called in liars. And 
the data points that have a loss that is greater  
95:15 - than that threshold are called outliers. Due to a 
larger value of loss, the outliers tend to attract  
95:24 - the neural network weights towards itself. Due to 
a smaller loss value, the inliers are not able to  
95:32 - attract the neural network weights towards itself 
that sufficiently well. In order to efficiently  
95:39 - deal with such an imbalance of loss values, 
we use what is called a balanced elven loss.  
95:46 - The balanced elven loss balances the loss values 
that are generated by both of these inliers and  
95:52 - outliers to get a network that performs well 
on both these types of data. Alright, so,  
95:59 - these are all the losses that are used in this 
network. The next component that we are going  
96:03 - to discuss is the learning rate scheduling. While 
training a neural network the learning rate is an  
96:10 - important hyper parameter that we need to take 
care of a particular technique that we apply to  
96:16 - better train a network is called learning rate 
scheduling. During the training process, we tend  
96:22 - to decrease or increase the learning rate values 
in order to fine tune our optimization process.  
96:29 - The learning rate decay technique that we 
use in this network is the cosine annealing,  
96:36 - the graph of the learning rate in cosine annealing 
looks something like this towards the start  
96:43 - the learning rate decreases slowly towards 
the middle the learning rate starts decreasing  
96:49 - in a linear fashion and towards the end, the 
learning rate decrease again starts to slow down.  
96:56 - The combination of all these three components 
is what constitutes the SFA 3d technique,  
97:02 - alright. So, these are all the technical details 
there are to discuss for the SFA 3d technique.  
97:08 - Now, let us see this algorithm in action. 
Okay, so let us have a brief look at the code.  
97:18 - The link to this code and the notebook will be 
provided in the description box below as well.  
97:23 - The first step that we have 
is the configuration step.  
97:27 - Here we set up the different parameters 
and variables that we are going to use  
97:31 - for our network. Next up, we have the different 
data processing functions that we use. One such  
97:39 - data processing technique is to convert the LIDAR 
data into a bird's eye view of the given data.  
97:46 - Next up, we have different utility functions like 
logging that we use to log the training process  
97:54 - learning rate scheduling that we 
use while training the network  
97:58 - and different visualization utility functions that 
we use to visualize and generate the final output.  
98:04 - Next up in this section, we define 
the different loss functions that  
98:08 - we are using to train the network. 
Finally, we define the FPN ResNet  
98:14 - network over here in this section and in this 
section, we finally train the neural network.  
98:21 - One particular problem that 3d object detection 
network face is training. The training of these  
98:29 - networks is really time consuming and require 
very expensive and good quality hardware.  
98:36 - Since we have a time limit and some constraints 
on hardware on Kaggle I was not able to train  
98:43 - the network by myself. Hence I have used the pre 
trained weights that are provided along with this  
98:50 - network architecture. So now let us have a look 
at the final output. So this is the final output  
98:57 - that we have on the raw data set for the purpose 
of visualization I have slowed down this video.  
99:46 - Okay, so some key observations that we can note 
over here, red boxes describe the cars that are  
99:53 - in the same. Blue bounding boxes are for cyclists 
that are present in the scene and yellow bounding  
99:59 - boxes. for pedestrians on the given scene, one 
interesting observation is that the network is  
100:05 - not able to detect that tram that is present in 
this image, since it was not trained on that data.  
100:13 - Also, these blue lines describe the 
orientation of the various surrounding cars.  
100:20 - Alright, so, this is all that is 
to discuss of the SFA 3d technique.  
100:26 - So, let us start with the problem definition. 
In this problem given a set of images taken  
100:33 - from the different cameras present 
on a self driving car, or a robot,  
100:37 - we are required to process these images and 
output a bird's eye view of the given environment.  
100:45 - Bird's Eye View or top view are 
viewing the 3d scenes from the top  
100:52 - as an input, we can either have a single image or 
we can have multiple different images taken from  
100:58 - the different cameras. In this case, we can see 
this is our self driving car. And in this case,  
101:05 - for our particular problem, we have four cameras 
in the front, left, right and the back. Given  
101:12 - these four images, we are required to process them 
and output the bird's eye view of the environment  
101:19 - around the car. As you can see over here, the red 
rectangle is the ego car or our self driving car  
101:26 - and the other blue rectangles are the other cars. 
Now why do we require to calculate this bird's eye  
101:32 - view of the environment? There are mainly 
two reasons we would like to calculate the  
101:37 - bird's eye view. The first application is object 
detection. Calculating a bird's eye view of the  
101:44 - given environment provides us with a different 
way of doing object detection compared to other  
101:50 - different computer vision methods. The second or 
the more important application is path planning.  
101:58 - This bird's eye view provides us with a concise 
representation of the surrounding environment, the  
102:04 - different cars and objects that are present in the 
scene and the road that we are required to go on.  
102:11 - Using this concise representation we can easily 
implement path planning algorithms on our self  
102:17 - driving car. So, this is the complete introduction 
to the problem statement. Now, let us have a look  
102:23 - at the data set that we are going to work with in 
this project. Alright, so the link to this data  
102:29 - and this data visualization notebook would 
be provided in the description box below.  
102:34 - Just like in the problem description, we 
are given four images, the front, the rear,  
102:40 - left and right. Along with that, we are given 
certain bird's eye view images. Now the images  
102:48 - that we have been given are not exactly real 
time images, but semantic segmented images.  
102:55 - We already have the different classes in these 
images, semantically segmented and we have to  
103:01 - derive a bird's eye view image for the same. The 
cool thing about this dataset is that the data set  
103:08 - is derived from a simulation, the self driving 
car drives around the simulation environment.  
103:14 - And we captured the different images from 
the four different cameras that are present  
103:18 - on the self driving car. A drone 
flies just above a self driving car  
103:23 - in order to capture the ground truth bird's eye 
view images. As we can see, we have these four  
103:29 - images taken from the self driving car cameras, 
the front image, the rear image, the left image  
103:35 - and the right image. And this is the ground truth 
bird's eye view image of the 3d environment.  
103:43 - This next image the bird's eye view occlusion 
image contains occlusion as a separate class.  
103:49 - This means that the scene that is hidden 
or occluded by different objects in the  
103:54 - environment are represented using a separate 
class. As we can see in this gray color over here  
104:06 - this car present behind our self driving car 
is blocking the view of the camera in this gray  
104:12 - space. Additionally, some buildings are also 
occluding the space as we can see over here,  
104:19 - and due to this occlusion, we are not able to 
see this car that is present behind the building  
104:25 - and is not visible through our camera. This 
homography image is similar to the bird's eye  
104:31 - view image that we have over here, but contains a 
lot of noise. So what is the homography operation?  
104:39 - In simple terms using a homography operation, 
we can convert from one image plane to  
104:45 - another image plane. In this particular case, 
we are converting from these four images,  
104:52 - the front rear left and right and using 
these to generate a bird's eye view image.  
104:59 - This Bird's Eye View conversion operation is also 
called inverse perspective mapping. More on this  
105:05 - homography and inverse perspective mapping will 
be discussed in the later part of the video and we  
105:10 - will understand why we do not use this technique 
directly to derive the bird's eye view image.  
105:16 - Now, let us have a look at few 
instances of this data set.  
105:50 - Alright, so, the technique that we are going to 
apply to solve this problem is unit access T.  
105:57 - As the name suggests, this technique is an 
extension of the unit semantic segmentation  
106:04 - architecture. Therefore, the unit access T has two 
components the unit architecture itself and its  
106:10 - extension let us discuss the two components one by 
one. So, the first component is the unit network  
106:18 - unit is a network that is used for semantic 
segmentation. In the problem of semantic  
106:24 - segmentation, we are required to segment a region 
of interest from a given image. For instance,  
106:31 - in an image captured by a self driving car, we are 
required to segment the cars that are present in  
106:37 - the image and the road that is present in the 
image. We can do so, by highlighting them with  
106:43 - different colors. The unit architecture is similar 
to other semantic segmentation architectures where  
106:50 - we have an encoder part of the network and 
we have the decoder part of the network.  
106:56 - The encoder part down samples the image 
deriving different visual features in the  
107:01 - image and then we have the decoder part which 
up samples the image and returns us the output  
107:08 - apart from the information derived 
from the decoder part itself,  
107:12 - the decoder uses information from the encoder part 
using skip connections as we can see over here.  
107:21 - Initially, unit architecture was devised 
for biomedical image segmentation. However,  
107:27 - its use cases applies to other domains as well 
for instance in our case self driving cars  
107:34 - unit forms the basic architecture for this 
technique. Now, let us discuss the Xs D are  
107:40 - the extension part of the unit network. The 
extension part of the technique is that we make  
107:46 - use of what are called spatial transformers. So, 
before discussing what are spatial transformers,  
107:53 - let us discuss the concept of homography. For 
instance, we have an object and we take its image  
108:01 - from a camera that is placed over here, we change 
the camera position as well as its orientation to  
108:07 - something over here. And we take the image again, 
the transformation between these two images is  
108:14 - what is called homography. In mathematical terms, 
homography is a simple matrix multiplication.  
108:22 - A cool application of mammography is that if 
we know the homography matrix that converts  
108:30 - from one image plane to other image plane, then 
from a single image, we can directly compute  
108:37 - how that object would look like in a separate 
image plane without even taking a different  
108:42 - picture. However, in application, we would require 
those image planes to be close to each other.  
108:48 - In general, the transformation need not be that 
complex, they can be as simple as translating  
108:55 - the camera plane or even rotating the 
camera plane at a given fixed point.  
109:00 - For instance, given this image, we are rotating 
the image plane by approximately 15 degrees.  
109:07 - Not theoretically, it would seem that we can 
simply take an image x, multiply it by homography  
109:13 - matrix H and get the final resulting image at x. 
However, it is not that simple. In simple terms,  
109:22 - homography is a simple transformation that map's 
pixels from one image to another image. Hence,  
109:29 - what we would have is this image pixel mapped 
to some image pixel in this image over here.  
109:35 - On doing this matrix multiplication, it is easy 
to observe that the output x x is not going to  
109:43 - output integer values. What this means is that a 
given pixel over here is going to map to a number  
109:50 - of different pixels over here. Hence, in order to 
apply this transformation, we are going to have  
109:56 - a lot of different average calculations made 
Making the final image look not really good.  
110:03 - Hence, a solution to this is to do 
an inverse homography calculation  
110:08 - what that means, instead of computing a 
transformation that goes from input to output,  
110:13 - we instead calculate a transformation that 
goes from the output pixel to the input pixels.  
110:20 - For instance, this output pixel over here is going 
to map to the input image in a rotated fashion  
110:27 - something like this over here, we calculate an 
average of the surrounding pixels in the input  
110:34 - image. For instance, this pixel is going to map 
to surrounding these four pixels over here. Hence,  
110:42 - we are going to calculate their weighted average 
pixel values to derive the value of this pixel.  
110:49 - This weighted average is calculated using 
a technique called by linear interpolation.  
110:55 - The weighted average is taken with respect to the 
distance from the surrounding pixels. In place  
111:02 - of by linear interpolation, we can apply other 
different techniques like linear interpolation or  
111:08 - nearest neighbors as well. However, for spatial 
transformers by linear interpolation is the choice  
111:16 - due to its differentiated by nature. So, having 
understood this complete process of homography,  
111:24 - let us discuss the use of this 
homography. In a spatial transformer.  
111:28 - Spatial transformer is an extension module 
that can be applied to any convolution network.  
111:35 - This enables the convolution network to also learn 
the different spatial properties that are present  
111:42 - in the dataset. The spatial transformer learns 
the different parameters of the homography metrics  
111:50 - in order to aid the convolution network. In the 
same example, if we have an object over here,  
111:57 - and we take its image from this point, and let us 
assume for some reason, the CNN architecture is  
112:04 - not able to classify this object accurately, 
how spatial transformers are going to help  
112:10 - the spatial transformers are going to 
learn the homography matrix parameters  
112:15 - in such a way that they present an image to the 
CNN from a different point, so that the CNN is  
112:23 - able to classify that object more accurately. So 
having understood what is a spatial transformer,  
112:32 - and how it helps, let us see how we apply this 
to a unit xs D technique. For the unit accessory  
112:40 - technique, we are not going to learn 
the parameters of the homography matrix.  
112:45 - Instead, we are going to provide the spatial 
transformers with a fixed homography matrix.  
112:52 - So where do we get these homography parameters. 
Since homography, is a general application,  
112:58 - we can also apply this to our self driving cars 
scenario as well. For instance, given this image  
113:05 - from a right facing camera, we can easily derive 
a homography that maps from this right camera to  
113:12 - a virtual camera that is present at the top. We 
can do this for all the four cameras and derive  
113:20 - a top view of the environment. In this way, 
we will easily get the homography parameters.  
113:26 - This technique of deriving the top view from 
these given images is also called inverse  
113:33 - perspective mapping. But there's a natural 
question to ask why we apply complex network  
113:39 - architectures like unit access T, whereas we can 
solve our problem using simple homography ease.  
113:46 - The answer to this is the top view homography or 
inverse perspective mapping makes an assumption  
113:52 - that the world around these cameras is flat. We 
can easily observe that this assumption does not  
113:59 - hold true in any case whatsoever. The cars the 
trucks, pedestrians and the different monuments,  
114:06 - they have considerable height in order for 
the inverse perspective mapping to work.  
114:12 - Although we can get a top view of the environment 
by applying and inverse perspective mapping,  
114:18 - however, the results that we get from 
that technique are not really accurate  
114:23 - and useful for our task. Hence, we need 
such complex architectures like unit SST  
114:30 - Alright, so the two components are unit access D 
are not discussed. Now let us see how these two  
114:36 - components combine to give the final architecture. 
For this network, we are going to have four inputs  
114:43 - for the four different images that we have 
as you can see over here, we are going to  
114:49 - have a similar encoder network for each of these 
four images as can be seen from the red lines.  
114:58 - For the decoder, we again 
have similar architecture,  
115:01 - however, there is a single pipeline that gives us 
the final output. These skip connections over here  
115:08 - are what makes use of spatial transformers. These 
four feature maps at different levels are combined  
115:15 - using spatial transformers. Each feature is first 
pass through a spatial transformer. And then these  
115:23 - different features are combined. This combination 
is then fed through the skip connection.  
115:29 - And this is done similarly for 
each level of the architecture.  
115:34 - Alright, so this is it for the technical 
discussion of unit SSD. Now, let us have  
115:39 - a brief look at the implementation and the 
results that we get from training this network.  
115:45 - Alright, so the link to this implementation 
would be provided in the description box below.  
115:50 - In the first section, we have implemented the 
different utility functions like load image,  
115:56 - and one hot conversion of the input image. 
In simple terms, what one hot encoding means,  
116:03 - given a semantic segmented input that has various 
different colors, we convert that RGB image into  
116:12 - a 3d matrix that contains a sparse representation 
of the different classes. This representation is  
116:20 - helpful for the machine learning model to learn 
and understand the data. In the next section,  
116:26 - we have the different configuration parameters. 
One thing to note is that there are certain  
116:32 - changes that have been applied to these parameters 
in order to obey the constraints that are provided  
116:38 - by us on Kaggle the image shape has been reduced 
in order to get a faster learning curve. The batch  
116:46 - size has also been increased in order to get the 
same results. Also, the algorithm is only trained  
116:52 - for a total of 40 epochs, whereas you would have 
required to train 400 epochs. In this next section  
116:59 - we have the different data loader functions that 
load the data for our machine learning model.  
117:05 - In this next section, we define the different 
network architecture. First we define the  
117:10 - spatial transformer. Next we implement unit 
along with the spatial transformer extension.  
117:17 - And finally, we train the network. Now let us 
have a look at the final output predictions.  
117:22 - Just like before, we have images from 
the four cameras on the self driving car,  
117:27 - the front rear left and right this next image 
is the output prediction generated by our model.  
117:35 - And the next image is the ground 
truth bird's eye view image.  
117:39 - As we can see the model has learned to some 
extent to represent the bird's eye view image.  
117:47 - However, there is some slight 
noise that is present in the output  
117:51 - let us have a look at other images as well.  
118:23 - Overall, we can say the model has learned 
a representation of bird's eye view image,  
118:28 - but it would require more training 
to converge to more better results.  
118:33 - Alright, so this is all that we had to 
discuss for this unit access the technique.  
118:39 - Congratulations on completing the course 
we together did seven different projects  
118:44 - and got to learn more about the different concepts 
applied to perception in self driving cars.  
118:50 - We did three projects related to 2d and then 
did four projects that were related to 3d.  
118:57 - In order to follow up with 
this course. I recommend  
119:00 - that you go over different datasets apart 
from the ones discussed in this course,  
119:06 - read and implement new research papers related to 
perception for self driving cars. Or you can jump  
119:12 - due and learn more about different modules for 
a self driving car like localization or motion  
119:18 - planning. As far as computer vision is concerned, 
you can also try out different other projects  
119:24 - like similarity learning, image captioning and 
generative modeling. This is all that we had to  
119:30 - discuss for this course. And I will see you in 
another video or another course till then bye