00:02 - okay everybody good to go everybody sit
00:06 - awake good let's see if we say manage to
00:11 - stay that way
00:12 - so I'm Philip I want to talk about some
00:15 - machine learning some hyping some making
00:18 - fun of stuff so let's see what we can
00:20 - take this so I work for elastic the
00:23 - company behind elastic Serge dogfish
00:25 - Cubana like previously called the elk
00:26 - stick now we call it the elastic stick
00:29 - I'm part of our infrastructure team I
00:31 - always say in the middle that is a UNIX
00:33 - pipe a kind of piped it out into
00:35 - developer advocacy so I try to speak a
00:37 - lot about the good stuff that we do and
00:38 - why am i talking about machine learning
00:40 - well every company needs to have some
00:44 - machine learning in their stack you can
00:45 - see we have kind of hitting it on the
00:47 - right-hand side at the bottom there kind
00:49 - of like we have something about cloud we
00:50 - have something about machine learning
00:51 - like all the usual stuff that you need
00:53 - to have we'll get back to that later on
00:56 - but at first I want to talk a bit about
00:58 - machine learning in general and what it
01:01 - is and what it is not and what it could
01:03 - do for you so some months ago there was
01:07 - this like machine learning is going
01:09 - viral and everybody was going oh we need
01:11 - machine learning we need to do more
01:12 - about that and one thing about going
01:15 - viral was also that who remembers that
01:17 - iOS park when suddenly the eyes were
01:19 - exchanged for a different character that
01:22 - was actually machine learning as well
01:24 - because that was kind of some Apple
01:26 - added some machine learning for the
01:27 - keyboard that it would replace specific
01:30 - characters or words and that was kind of
01:32 - as soon as you received that bad eye
01:34 - which was replaced by some other
01:35 - character that was already machine
01:37 - learning at work because then your
01:38 - system would learn that other way to to
01:41 - write the replace the term and only once
01:43 - you receive such a change diet then you
01:46 - would be or would do the same when you
01:48 - device would do the same thing because
01:49 - your device had learned that so that was
01:51 - kind of the power of machine learning
01:52 - which was kind of a bark and like kind
01:55 - of a bit of a virus since it was
01:56 - spreading like that but yeah machine
01:59 - learning is kind of going viral in that
02:00 - regard yeah and then there are always
02:04 - the people who are saying like oh we're
02:05 - doing some very fancy technology and
02:07 - even though we probably would have just
02:08 - needed a few if statements which would
02:11 - have been much easier to write in debug
02:12 - which probably looks something like this
02:15 - like
02:15 - have this big digger and then it's just
02:19 - using that little shovel there and to
02:22 - get going so that is some people we are
02:24 - wielding kind of the powers of machine
02:26 - learning just to do something very small
02:29 - and confined and read like easy to get
02:31 - going with sometimes you might need it
02:34 - sometimes you might not need it I guess
02:38 - some of you are probably familiar with
02:39 - that that is the Gartner hype cycle that
02:41 - is probably the only thing that is true
02:43 - that Gartner is always bringing out and
02:44 - because everything else is kind of like
02:46 - just making stuff up or at least it was
02:48 - always my impression that whatever the
02:50 - Gartner is predicting for the next five
02:52 - years is probably not going to be the
02:54 - way it will be but their hype cycle is
02:56 - kind of like very very true like you
02:58 - have some technology trigger and then
03:01 - you have like this peak where everybody
03:03 - is saying like oh this will be so
03:04 - awesome and this will do all the things
03:05 - I can still remember when everybody said
03:08 - that about no sequel and then people
03:10 - were kind of like oh maybe a relational
03:13 - databases were not all that bad and I
03:15 - have the feeling that kind of machine
03:16 - learning and AI and whatever you want to
03:18 - call it this kind of falling into the
03:20 - same perception every now and then and
03:22 - so you have some peak and then you get
03:24 - kind of like a bit of a disillusionment
03:26 - and at some point you may be reached at
03:28 - 2 or 3 activity will machine learning or
03:31 - AI solve all your problems um
03:33 - unfortunately it probably will not but
03:37 - that's probably no surprise so anybody
03:38 - who has been in the field for some years
03:40 - knows ok all the promises like
03:41 - everything will be solved by this
03:43 - technology unfortunately it's not that
03:46 - easy or fortunately it's not that easy
03:47 - otherwise nobody would need us anymore
03:50 - so I want to talk about like a bit
03:53 - machine learning in general and like
03:55 - putting it in with AI and whatever then
03:58 - I will pick a domain of what we can do
04:00 - since our use case is always about
04:03 - machine learning for ops data I will use
04:06 - some op stater for that and then I'll
04:09 - just play around with some data set for
04:11 - a quick finish let's see where we can go
04:14 - so machine learning there's always this
04:16 - misconception of this discussion like
04:19 - what is artificial intelligence what is
04:20 - machine learning what is deep learning
04:22 - how do they belong together isn't all
04:24 - the same thing because some people
04:25 - always say oh it's basically the same
04:27 - thing it's just like depending on when
04:29 - people
04:29 - started with something they had to come
04:31 - up with a new fancy term and that fancy
04:34 - term could be either AI or if you came a
04:36 - bit later it would be machine learning
04:38 - or if you're very up-to-date and it
04:39 - would be deep learning I kind of find
04:43 - this very nice this graphics so you can
04:45 - see it all kind of started in the 50s
04:47 - already back in the days I don't assume
04:51 - that most of you were around an IT back
04:53 - then but maybe somewhere or maybe a
04:56 - little later so in the beginning there
04:58 - was machine learning and there were high
04:59 - hopes and after some initial
05:01 - breakthroughs people assumed well
05:03 - general artificial intelligence will be
05:06 - sooner things so general artificial
05:08 - intelligence will be like machines will
05:11 - be as good as humans at almost
05:13 - everything and they will just learn one
05:15 - field by the other and and initially it
05:18 - was just it started as a very narrow
05:19 - field like you would do something where
05:21 - machines would be good in one specific
05:23 - field and then the idea was well we will
05:25 - just generalize that and this will be
05:27 - applicable to any other thing afterwards
05:29 - and it will just keep generalizing that
05:31 - until you're kind of on human level
05:33 - unfortunately that didn't play out that
05:35 - well and then in the 80s we had like
05:39 - with machine learning we had another
05:40 - wave of trying debt and then like the
05:42 - 2010s we had another wave of that with
05:46 - deep learning yeah
05:49 - so the general purpose of generally AI
05:51 - is always let's get something that is
05:54 - human level this is something we have
05:56 - not reached yet and we don't even have a
05:59 - proper timeline like in the 50s 60s and
06:01 - 70s it was assumed like oh we will get
06:03 - there but it didn't turn out that way
06:06 - because in the 70s there was the
06:07 - so-called AI winter and this was also
06:10 - one of the reasons why you had to find a
06:12 - new fancy name because a lot of people
06:14 - got burnt by that they assumed Oh
06:15 - artificial intelligence will solve
06:17 - everything and a lot of money was pumped
06:19 - into that and people wrote papers and
06:21 - plans and everything and at some point
06:23 - it didn't really plan will work out the
06:26 - way people planned so there was kind of
06:28 - like the first wave of ai ai
06:30 - then came the air I winter and
06:32 - afterwards people came back with better
06:34 - algorithms more computing power it's
06:37 - kind of like coming back in waves and
06:39 - you're getting closer to your target but
06:41 - with generally AI that's something we
06:43 - have not
06:43 - really achieved yet and we don't really
06:45 - know when that will happen what we are
06:47 - very good at right now is something like
06:49 - narrow AI where you have one specific
06:52 - task and the goal is to have something
06:55 - where you're as good or nearly as good
06:57 - as a human for example to identify an
07:00 - image so I was two weeks ago I was in
07:03 - San Francisco and I uploaded this
07:05 - picture to Facebook and then Facebook
07:07 - does the classification automatically
07:09 - for you so for example if you look in
07:11 - the meta tags what Facebook does is it
07:13 - adds the alt tag and it says like image
07:15 - may contain and then it's just guessing
07:17 - what could be in that image and it's
07:19 - actually pretty good
07:20 - because yeah they have a lot of data
07:23 - they have trained on a lot of data and
07:25 - they're pretty good at just detecting
07:27 - what might be in your image even though
07:30 - I haven't told anything like it didn't
07:31 - tag and say okay this is the Golden Gate
07:33 - Bridge
07:34 - I've just uploaded the image without
07:35 - other any other meta information and the
07:39 - narrow eye in that case like the image
07:40 - classification that just kind of found
07:43 - or detected what might be in that image
07:46 - and it's actually pretty good another
07:49 - thing you might have seen which was
07:50 - pretty recent is that cylinder is
07:52 - cutting some marketing jobs where
07:54 - they're automating the marketing now
07:56 - because previously they had a lot of
07:58 - handcrafted content and now they kind of
08:01 - said well we can automate a lot of that
08:03 - with machine learning we can hyper
08:05 - localize the content where we know okay
08:07 - you're interested in these products and
08:09 - you're living in that specific city and
08:10 - previously somebody would probably hand
08:13 - curate the set of data that or the
08:15 - products that might be interesting for
08:16 - you or would write some content that is
08:18 - tailored for someone in a specific
08:20 - region but they came to the conclusion
08:22 - with machine learning this is much
08:24 - easier to do or will scale much better
08:26 - and they don't need all those humans
08:28 - anymore so they now start to kick out
08:31 - some of their marketing people's some
08:33 - will move to a different department but
08:35 - a lot of them will be just kicked out
08:38 - which sucks and people are very afraid
08:40 - that all machine learning will take over
08:42 - all my jobs probably some but probably
08:45 - also not all of them because you might
08:48 - have seen this example as well where
08:50 - some people used Google Translate to
08:52 - translate sorry for the German if you
08:56 - don't speak German
08:57 - some economic topic and this sentence
08:59 - here is actually the very first sentence
09:02 - of Wikipedia if you look up folks with
09:05 - shirts really which is ya know some
09:08 - German sentence and this is what people
09:12 - got when they use Google Translate
09:13 - actually I tried it yesterday evening by
09:15 - now Google got kind of clever I don't
09:18 - know if somebody went in there to fix
09:20 - that because the people tweeted about
09:21 - that and it made the rounds but by now
09:24 - Google Translate got a bit smarter again
09:27 - and the same thing is still not great
09:29 - what you're getting now but it's still
09:30 - got a bit better and it's not just
09:32 - economics of economics and yeah the same
09:34 - word over and over again but yeah having
09:38 - the fear that professional translators
09:40 - will go out of business like next week
09:42 - it's probably a bit premature we're not
09:44 - there yet
09:45 - like it's kind of general idea yes you
09:47 - can do nice stuff and if you don't
09:49 - understand some language you can use
09:51 - Google Translate to get the idea
09:53 - but it's probably not going to replace
09:56 - proper writing so for example our
09:58 - support team when we do a support in
10:00 - some localized language we always say
10:02 - for example people in Japan don't want
10:03 - to speak English and for them it's super
10:05 - important to get support in Japanese and
10:07 - we always say yes you will get support
10:09 - in Japanese during Japanese business
10:11 - hours because then our people are online
10:12 - but if you want to get help after hours
10:15 - either you can write you text in English
10:17 - in English and somebody will help you or
10:19 - we will use Google Translate and we're
10:22 - actually handling support cases with
10:24 - Google Translate and whatever comes out
10:27 - of Google Translate we will try to help
10:28 - you with and if it's no good
10:29 - unfortunately we will need to wait until
10:31 - somebody wakes up who speaks Japanese
10:34 - yeah and then there's always this chat
10:37 - pod thing I know that Vienna has this
10:39 - kind of I always said the chat pod
10:41 - bubble because a lot of people are
10:42 - working in in chat pods I hope not too
10:45 - many are here and come with the
10:47 - pitchforks afterwards but I have no or I
10:51 - had this thing once or twice where
10:54 - somebody came to me and I was or I was
10:56 - talking to somebody and they were saying
10:57 - I'm doing AI and I Wow that sounds super
11:00 - advanced what are you doing and then
11:02 - they people say well we're using this
11:04 - Facebook API and we're sending some data
11:06 - they are in it senses something back and
11:08 - so we're doing AI and I'm always like
11:11 - you doing api's it's just like the pee
11:14 - is probably silent in your AI yeah and I
11:21 - like to say yeah
11:22 - AI API is kind of it's very close some
11:25 - people don't make the distinction we
11:27 - can't have a discussion about that but
11:29 - at least for me when you say I'm doing
11:31 - AI is not just calling some Facebook or
11:33 - whatever API and get something back and
11:35 - use that this is one of my favorite
11:39 - examples I think this is the Pancho one
11:42 - of the chat BOTS which you can ask like
11:44 - what is the word I like and then you can
11:46 - see yeah it's working well or not that
11:49 - well and I get the idea that is trying
11:52 - to be cute but like cute is not the main
11:55 - thing I want if I want some information
11:56 - like just because it's trying to be to
11:59 - speak French or saying it dozed off for
12:01 - a few seconds and that doesn't make
12:03 - something very smart or intelligent it
12:06 - might be cute at first but if I want to
12:08 - get some information this is not going
12:10 - to make me very happy with something
12:12 - else I recently had I'm using a one
12:15 - Telecom and I recently called their
12:17 - hotline and normally you know you dial
12:20 - in and then you need to press five for
12:21 - that service and press three for
12:23 - whatever and they didn't have that
12:25 - anymore they had the whole thing voice
12:27 - automated so you would need to speak to
12:30 - the system and it didn't work at all
12:33 - like my internet was not working and I
12:35 - was trying to in various ways to
12:38 - describe like what my problem was and
12:40 - even when I said internet it would say
12:41 - oh you want to know something about your
12:43 - bill and I like no that's not where I
12:45 - went and then you do like five circles
12:48 - or so and in the end you could get back
12:49 - to that dial like five to get support
12:51 - for your internet or whatever so yeah
12:54 - the future might be very bright about
12:55 - all these automated systems with voice
12:57 - control and chat pots and whatever um
12:59 - there was sometimes there is super
13:01 - annoying or maybe I'm just too old and
13:03 - grumpy by now anyway um and this is
13:07 - other thing maybe you've once seen a
13:09 - stateless jet pod which makes for great
13:11 - conversations because it just knows the
13:13 - current sentence and it doesn't have any
13:15 - context context about what which is
13:21 - another kind of common problem this is
13:22 - solved you can totally do that
13:23 - but like simple chat pod
13:25 - will not be able to do that because yeah
13:28 - life is hard so what is machine learning
13:31 - after all kind of together so you
13:34 - basically have an algorithm than cat
13:36 - that can work with some data you can
13:39 - kind of learn from that and then you
13:41 - make some determination or prediction
13:43 - out of that data so it's basically a
13:46 - trained machine
13:47 - you haven't hand coded the rules so for
13:49 - example one common example could be like
13:51 - classification of spam emails initially
13:55 - people would write rules like if there's
13:57 - something like I'm from Nigeria and I
14:00 - will send you 100 million if you detect
14:03 - something like that in an email it
14:04 - there's a very good chance that this
14:06 - might be spam but of course people are
14:08 - clever and they will reword stuff and
14:10 - then it's kind of like it's something
14:13 - point five billion it's not Nigeria but
14:16 - some other country but so people will
14:17 - found various ways around but with
14:19 - machine learning you can actually not
14:21 - kind of write these rules explicitly but
14:23 - can just learn like what does the spam
14:26 - email look like and probably can find
14:27 - out these are the different kinds of
14:28 - spam emails like these are the ones who
14:30 - want to sell you viagra did this one
14:32 - wants to get your money and this one is
14:34 - just some plain annoying thing yeah and
14:39 - this is the proper definition what
14:42 - machine learning is but you will not get
14:44 - any venture capital with if you just
14:46 - write this because it sounds very dry
14:48 - you learn from some experience with some
14:51 - some class of tasks and then you perform
14:53 - some measure and then the performance at
14:56 - that task
14:56 - improves with the experience the more
14:58 - experience you have the better the
15:01 - machine learning should get this is kind
15:03 - of like what happens this is kind of the
15:04 - algorithmic view but again this is not
15:07 - getting you any venture capital or you
15:09 - cannot build a start-up just based on
15:11 - this one sentence what machine learning
15:14 - is generally doing is something like
15:16 - it's one indie of these four categories
15:18 - commonly so you could have some
15:21 - classification so you could have an
15:23 - either like binary classification guess
15:26 - no or you could have a classification
15:30 - which has kind of like more outputs for
15:32 - example you could have a picture of
15:33 - somebody and then you could try to do a
15:35 - sentiment analysis like is this person
15:37 - happy
15:39 - said angry whatever and then your
15:41 - algorithm tries to classify what kind of
15:43 - facial expression does this person have
15:46 - so that could be a classification then
15:48 - you have regressions where you basically
15:50 - try to know or calculate what a specific
15:54 - value will be in the future which could
15:56 - be like a temperature or a stock price
15:58 - where you do the prediction of all the
16:00 - stock prices will go up or go down you
16:03 - can do ranking ranking is pretty much
16:05 - like you search for something and then
16:08 - you don't just have the hits but the
16:10 - system learns over time like what is it
16:12 - good hit and you kind of like feedback
16:14 - information of what is a good hit into
16:16 - that search system so that your ranking
16:18 - will improve over time and then
16:20 - clustering is just putting stuff that
16:22 - belongs together or isn't kind of some
16:25 - groups that make sense together you just
16:27 - try to classify similar things and then
16:30 - say okay these are all the things that
16:32 - are in that classification or clustering
16:37 - yeah this is what machine learning might
16:40 - look like that one machine is teaching
16:43 - the other little machines of what it
16:45 - knows or at least that's how we humans
16:48 - do it pretty much would be nice like if
16:52 - it was like that so the most common
16:56 - thing is when people say oh we were
16:57 - leveraging machine learning is that they
16:59 - really doing linear regression so
17:01 - basically you have some some information
17:04 - these would be like the little dots and
17:06 - then you want to find the linear
17:08 - regression which is basically this red
17:10 - line and you try to optimize that line
17:12 - and try to keep the distance of all
17:15 - these dots as close as possible to the
17:17 - red line and the more data you have the
17:20 - better that line can be and the more you
17:22 - kind of get to the value of what makes
17:24 - most sense for your data set so if you
17:26 - have like very sparse data and you only
17:28 - have three or five points you will have
17:31 - a very coarse linear regression if you
17:33 - get very or have lots of data the linear
17:36 - regression can kind of optimize much
17:38 - better for all the data points you have
17:39 - and if people are just saying oh we're
17:42 - doing a machine learning oftentimes it
17:45 - just means yes we have this linear
17:47 - regression which is not all that fancy
17:50 - but it can still do meaningful stuff
17:53 - for you yeah whenever people say that
17:56 - assume if there is no other information
17:59 - it's probably a linear regression not
18:01 - something more fancy and then you can
18:04 - still make the distinction between
18:05 - supervised and unsupervised machine
18:07 - learning supervised machine learning is
18:09 - and you have labeled data so you have
18:10 - some input data and then you have some
18:12 - output and you know this is for example
18:15 - with spam emails if enough people have
18:17 - said like this is spam this is not spam
18:19 - you have a huge data set which Google
18:21 - probably has and then oh this is a spam
18:23 - email and this is not a spam email and
18:25 - they can turn on that data because they
18:27 - have some supervised data await and no
18:30 - these are the inputs these are the
18:31 - outputs and then you can take a lot of
18:33 - that data to for your training data and
18:37 - then you can keep some of the remaining
18:39 - data for your test data you could do the
18:42 - same for unsupervised machine learning
18:44 - where you don't have any annotated data
18:46 - you just try to find some hidden
18:49 - relationship in your data so you don't
18:52 - know what is actually this good this is
18:54 - bad and it's dissonant normally you just
18:56 - have data and you try to find some
18:58 - relationship out of that the finding
19:00 - relationship in data can be kind of
19:02 - tricky there's this very nice xkcd comic
19:06 - which I've broken up for better
19:07 - readability here where somebody says
19:10 - like Oh jelly beans cause acne and then
19:12 - the scientists have to investigate even
19:15 - though they just want to play Minecraft
19:16 - so you can see that was kind of like one
19:19 - year ago whenever minecraft was very
19:21 - popular so that first to say like know
19:24 - what like with the 95 percent confidence
19:27 - we can say like no acne is not caused by
19:30 - jelly beans and then somebody says well
19:32 - but it I have heard it it's just a
19:34 - specific color and then they do the test
19:36 - for all the different colors and it's a
19:40 - bit hard to see but all of them like no
19:42 - we found no link and then there is the
19:45 - one where it says whoa and there it says
19:47 - oh there is like a link the problem is
19:50 - if you have like a 95% confidence
19:53 - interval and you make and you run the
19:56 - trial on 20 different data sets just by
19:59 - the mathematics one of them will then be
20:02 - kind of true and then you can write the
20:05 - big news article
20:06 - the green jellybeans obviously daling a
20:09 - workhorse or we're causing acne with a
20:11 - 95 percent confidence and only 5% chance
20:15 - is in there but that's kind of like if
20:19 - you run the same thing 20 times there is
20:21 - a chance that well with 95% confidence
20:26 - one of them will fire if you run it just
20:28 - 20 times and that's pretty much what is
20:30 - happening here then you can do
20:33 - reinforcement learning where you
20:34 - basically have a feedback loop so
20:36 - whatever your outputs are you're kind of
20:38 - continuously optimizing that on the
20:41 - feedback you're getting so for example
20:44 - if people are saying this is helpful
20:45 - this is not helpful and you can then
20:46 - keep optimizing whatever your machine
20:49 - learning algorithm is doing and then
20:52 - there is deep learning which is you have
20:55 - the neural network which is also very
20:56 - fancy nowadays because now finally we
20:59 - have enough computing power or mainly
21:01 - good graphics cards to calculate those
21:04 - to basically calculate a probability
21:07 - vector and you have lots of training and
21:10 - then parallel sted that is if you get
21:13 - enough graphics card to actually do that
21:15 - and not and they haven't been sold out
21:17 - to mine bitcoins before which is kind of
21:21 - sad that people are using all that
21:23 - computing power for mining some
21:24 - cryptocurrency but yeah that's a totally
21:27 - different topic so what basically
21:29 - happens here this is a very simple
21:31 - example and we were having you have
21:34 - slept X hours the night before a test
21:36 - and you have studied Y hours before a
21:39 - test and then you try to find like what
21:41 - does this mean for your test score like
21:43 - the green thing on the right hand side
21:45 - that is basically the outcome of the
21:46 - test score and then you have with these
21:48 - two inputs like they're weighted like
21:50 - how important is sleep how important is
21:52 - how much time you have spent studying
21:54 - and then you can have one or more layers
21:59 - the more layers you have the deeper it
22:02 - gets or as soon as it's multiple once
22:05 - then you will have deep learning and
22:06 - they're kind of a hidden input so you
22:08 - you can see they say it's as mysterious
22:10 - you don't really know how the system
22:12 - behaves in there you just have some
22:15 - weighted inputs and then at the end some
22:17 - value plops out and it says okay if you
22:20 - yep X hours and you studied Y hours your
22:23 - test score will probably be I don't know
22:25 - you will pass you will not pass so you
22:26 - will probably get that great so that is
22:29 - what you can do with neural networks
22:32 - which is also very common for example
22:34 - for image classification or the standard
22:36 - example that a lot of people have is
22:38 - like you have this detection of
22:39 - handwriting so people write the numbers
22:42 - from 0 to 10 to 9 and then you try to
22:45 - detect each image or each number and try
22:50 - to defer like ok which number could that
22:52 - be and then you can extract some
22:54 - characteristics of each number and that
22:56 - is what makes up your neural network and
22:58 - then we will say ok this was the number
23:00 - 5 for example whatever is what is really
23:05 - most important there is to have some big
23:07 - data set because you will always need
23:09 - something to Train there so companies
23:11 - like Google Facebook Amazon they have a
23:14 - huge advantage because they just have a
23:16 - shitload of data to work with so for
23:18 - example when people are now surprised oh
23:21 - how can Facebook be so good at detect
23:23 - stuff um well you have helped with their
23:25 - training data for years and years so
23:27 - this is if you have been on Facebook for
23:31 - a long time or a long time ago probably
23:33 - this was what it looked like 10 years
23:35 - ago so it didn't do any machine learning
23:38 - back then but what you could do is you
23:40 - could upload a photo and you could tag
23:42 - people and you could just say ok this is
23:44 - that person and that gives Facebook
23:47 - actually a huge amount of data because
23:48 - it knows now ok this there is the face
23:51 - of a person and that's probably somebody
23:53 - who is male and from the profile of that
23:56 - person they know like the age the gender
23:58 - the ethnicity whatever so it knows a lot
24:05 - of information and can use all of that
24:07 - information later on to then train its
24:09 - machine learning and that's an advantage
24:11 - that you have given kind of to Facebook
24:13 - and all these other big vendors who can
24:15 - build cool stuff and it's very hard to
24:17 - replicate that for you because you don't
24:19 - have this huge amount of data to work
24:20 - with so yes you can make data great
24:26 - again but only if you have huge data
24:29 - that's the one thing you really need
24:33 - yeah and that's what big companies are
24:35 - doing and then people are like Oh a IML
24:41 - will it just use them interchangeably
24:42 - and it really depends like for example
24:44 - when you're raising money then you want
24:46 - to call it AI when you're hiring then
24:47 - normally people say machine learning
24:49 - that's also some distinction I have seen
24:52 - or some people have framed differently
24:54 - they say when you're fundraising then
24:55 - it's AI when you hire in its machine
24:57 - learning when you're implementing it
24:59 - then it's just a linear regression and
25:01 - as soon as you debugging it it's back to
25:03 - the good old printf so like all the
25:05 - fanciness you have kind of at the start
25:07 - is going kind of like down down down
25:10 - until you back it's basics so while it
25:12 - sounds very fancy and it can do some
25:14 - cool stuff some basics are not going
25:16 - away or there are these cattle contests
25:21 - where you have some problem and you try
25:23 - to solve the problem and at the end at
25:25 - the end you might win I don't know a
25:26 - million dollars depending on the carrot
25:28 - contest so what people are doing is
25:30 - we're just starting a competition we are
25:32 - using some machine learning on it and at
25:34 - the end we will profit but normally what
25:36 - you won't really get the profit you will
25:38 - mainly get some disappointment because
25:40 - what is still totally relevant is that
25:43 - you will need to know the domain and
25:45 - need to know what it is about to
25:46 - optimize that it's not that you can just
25:48 - throw some generic machine learning on
25:50 - it and then it will magically find out
25:52 - what it's just the right thing to do and
25:54 - will make you rich unfortunately it's
25:56 - not that easy okay so enough making fun
26:02 - of machine learning let's do something
26:03 - more productive so since I have an
26:08 - office background mm-hmm and our product
26:11 - does work pretty well with that I've
26:13 - picked something so you have some trend
26:16 - over time and we try to determine some
26:18 - pattern in our data so for example this
26:20 - could be people visiting your website
26:22 - this could be traffic this could be
26:25 - error messages you have or return HTTP
26:28 - return codes you have all of these you
26:30 - can't try to find some pattern in data
26:32 - like that so the first one is you have
26:35 - some trend which is just like it's
26:37 - growing linearly or it's kind of like
26:39 - falling linearly or you have exponential
26:42 - growth or whatever anything that is not
26:45 - stationary so if you just have
26:47 - some stable value you're not going to
26:49 - find out any trend out of that because
26:50 - well no change then oftentimes you have
26:54 - some cycles in the data cyclists would
26:56 - be the days of the week so Monday to
26:58 - Friday you have kind of the same traffic
27:00 - patterns for example I said they Sunday
27:02 - will have totally different traffic
27:04 - patterns at least on the average website
27:06 - and you have some seasonality so for
27:09 - example you know if you have some
27:10 - ecommerce shop like Christmas will be
27:13 - much higher I think like Amazon is
27:15 - making or amazon.com for shipping stuff
27:18 - and it's I don't know making fifty
27:21 - percent or so of its revenue around
27:22 - Christmas or at least some crazy number
27:24 - so they just have a seasonal trend and
27:27 - they know ok November and December will
27:29 - be different than all the other months
27:30 - because everybody is doing their online
27:32 - shopping there well the one thing that
27:35 - is not great if you have machine
27:37 - learning is you if you have like some
27:38 - irregular pattern where there is no real
27:41 - pattern
27:41 - it's just irregular because then you
27:43 - will not be able to properly find
27:44 - anything unfortunately we cannot help
27:47 - with that
27:48 - and then you often try to find anomalies
27:50 - in that data so you have for example a
27:53 - point anomaly a point anomaly would be
27:55 - assumed your your bank knows how much
27:59 - money you get out of the ATM machine
28:01 - every time so whenever you go to the ATM
28:03 - machine you get I don't know let's say
28:05 - 50 euro and you normally do that and at
28:08 - some point you get a thousand-year out
28:10 - of the ATM machine let's assume that's
28:12 - possible then that would be a point
28:14 - anomaly because normally it's always
28:16 - like you get money you get money you get
28:18 - money always the same amount but that
28:19 - one time is a totally different amount
28:21 - so that would be a point normally and
28:24 - you could have a contextual anomaly
28:27 - where kind of the context doesn't fit in
28:30 - with the rest the contextual anomaly
28:32 - could be like and you have more requests
28:34 - and you have more CPU and memory usage
28:36 - in your system but the number of
28:39 - requests is actually lower than before
28:41 - all the traffic is lower than before so
28:44 - you have some kind of metrics are
28:46 - pointing in one direction but another
28:48 - one is kind of like in a different one
28:49 - then you have this contextual anomaly
28:51 - we're kind of in the context of all of
28:53 - them together it's kind of going the
28:55 - wrong way and then you have collective
28:58 - anomalies where you have
29:01 - assume that everything is going in one
29:04 - direction but something is kind of like
29:07 - an outlier out of how the collective
29:09 - kind of direction is going then you can
29:13 - have breakouts these are kind of like
29:15 - changes in your data which are not
29:18 - really anomalies they're just changes
29:20 - like you sometimes have ramp up for
29:21 - example you make an ad campaign and more
29:24 - and more people come to your website
29:25 - then you probably see some shift we're
29:29 - just more and more people are coming
29:31 - this is probably not an anomaly but the
29:33 - result of your ad campaign or you can
29:35 - have a mean shift which basically you're
29:37 - deploying a new version of your software
29:39 - and suddenly you only need half of the
29:41 - CPU anymore because you fix some bugs or
29:43 - optimized something there would be a
29:45 - mean shift where you have like just like
29:48 - you deploy the software it's just
29:49 - dropping to some other value and that's
29:51 - kind of like the new baseline which will
29:53 - always follow whereas the ramp up is
29:55 - just something that develops over time
29:59 - yeah if you want to detect anomalies
30:02 - with machine learning you basically have
30:04 - two options you have a supervised or
30:06 - unsupervised machine learning the good
30:10 - thing is in supervised machine learning
30:11 - is you know okay these are the inputs
30:13 - and these are than the outputs the bad
30:15 - thing about supervised machine learning
30:17 - is you will need to have that annotated
30:19 - data so somebody will have to kind of
30:21 - have gone through all of the data and
30:23 - say like okay with this input like this
30:25 - is an anomaly now and this is not an
30:27 - anomaly and oftentimes you don't have
30:29 - that data so very commonly you don't
30:32 - have supervise but instead unsupervised
30:34 - machine learning where you say okay this
30:36 - is the input this is the output and we
30:38 - assume this isn't normally because this
30:40 - is kind of different of what we had
30:41 - before um yeah examples out of the ITER
30:46 - you get suddenly a spike of five
30:48 - hundreds because somebody made a bad
30:50 - deployment and you know okay something
30:52 - is wrong
30:53 - more 500 something changed in our system
30:56 - we need to fix it again and you could
30:58 - have something like security events for
31:02 - example you have some unusual DNS
31:03 - activity where you could do DNS
31:05 - exfiltration for example if an attacker
31:08 - wants to get out information out of your
31:09 - network in a very sneaky way they could
31:11 - just call subdomains under there
31:14 - control with that specific information
31:16 - they want to get out in as a subdomain
31:18 - and most systems don't detect that but
31:21 - if you kind of like look at the DNS
31:23 - traffic you can spot that even though it
31:25 - can be pretty well hidden or you could
31:28 - have some business analytics where you
31:30 - know okay Jenny we have these log
31:31 - entries or log events suddenly we have
31:34 - something totally different
31:35 - and then your machine learning will
31:37 - probably or hopefully picked it up and
31:38 - say okay here we have something that
31:40 - doesn't follow the regular patterns or
31:42 - events that we had before so something
31:45 - must be off here you could totally do
31:48 - visual inspections and we always say
31:51 - they are not enough interns in the world
31:53 - to monitor all the dashboards that you
31:54 - could build with sorry for the interns
31:59 - but the Sproles are not what you want to
32:01 - spend your summer on especially if you
32:03 - have like some complex or fast-moving
32:05 - data yes you can stare at the graphs the
32:08 - entire time and after one or two weeks
32:10 - you probably know okay these are the
32:11 - patterns that I'm expecting but hey you
32:13 - can easily miss something and B that's
32:15 - probably not where you want to spend
32:17 - your time at what this time is kind of
32:19 - best used for so for example if you have
32:22 - this data here this would be unique IP
32:25 - addresses which have connected to my
32:27 - system worse the anomaly we don't really
32:30 - see one we'll get back to the data that
32:33 - is these are actually locks from our own
32:36 - website where we have counted or it's
32:40 - basically it's an engine X proxy and
32:42 - we're just using the engine X X s log to
32:45 - see what is going on on our site and
32:47 - then to find anomalies of what is going
32:49 - on and what is not really going well on
32:52 - our site so you could totally define
32:57 - some static rules then the problem is to
32:59 - define rules somebody needs to know the
33:01 - system well enough to define the rules
33:03 - you could have false positives and
33:06 - negatives depending on how kind of like
33:08 - narrow the band of alerts would be and
33:11 - as soon as your system starts changing
33:13 - over time you will need to adjust it so
33:15 - if you have more visitors to your site
33:16 - you probably will need to change your
33:18 - alerts on a kind of a frequent basis or
33:20 - adjusting to whatever your system looks
33:22 - like today so if you want to set a
33:25 - threshold here like what is the number
33:27 - of unique
33:28 - Pietrus is connecting to your system
33:30 - which values would you pick like yes you
33:32 - could say maybe here at the bottom to
33:35 - below 2,000 might be a problem you could
33:38 - say that and then you could say oh maybe
33:40 - more than 14,000 would also be a good
33:43 - threshold but there's a huge gap in
33:45 - between and you probably don't really
33:46 - see like is there anything going on in
33:49 - my system that is okay or not okay
33:52 - where's the anomaly like yes you can't
33:54 - set the thresholds but there will be
33:56 - super coarse-grained yeah so we need
34:00 - some kind of machine learning let's see
34:04 - what we can do there and yeah normally
34:08 - machine learning is pretty CPU intensive
34:11 - or if it can run on a GPU you can also
34:14 - run it on a GPU since all my demos are
34:16 - on my laptop it's like running slack or
34:18 - zoom or whatever which will also eat up
34:21 - all my CPU so you can totally build it
34:24 - yourself on some very common frameworks
34:26 - for that are tensorflow is especially
34:28 - well known from Google Kara's psych it
34:31 - and loads of others are where you can
34:33 - just build your own machine learning
34:35 - algorithms or you have some data and
34:37 - basically you want to extract some value
34:40 - out of that and find your own am√©lie's
34:41 - so these are just very widely used tools
34:44 - you can use how do you build that
34:47 - pipeline kind of like the machine
34:49 - learning component that is the fun part
34:50 - and people who are data scientists will
34:53 - always say like yeah we're doing data
34:54 - science the entire day but what they
34:56 - basically do is they probably spent five
34:59 - or ten percent of their time on data
35:00 - science and the rest is like 70% is
35:03 - probably of the rest of the time is like
35:05 - getting the data and cleaning up the
35:07 - data and 30% is complaining about not
35:10 - having having clean data yeah because
35:15 - it's kind of a very common thing you
35:16 - need to find the right data then you
35:18 - need to prepare it in the right way so
35:19 - you can load it and then you need to
35:21 - clean it up you need a proper data
35:23 - storage system where you can keep all of
35:25 - the data that you want to keep around
35:26 - and then you probably want to have some
35:28 - optimization algorithm to make stuff
35:30 - scale and make it faster yeah and who
35:34 - have takes the expectation that your
35:35 - data will be clean and will work the
35:37 - right way it's always kind of cute so
35:40 - yeah finally data scientists ask them
35:42 - and they wait we'll probably make an
35:44 - very unhappy face about data cleaning
35:46 - but it happens and then people always
35:49 - ask like which one is the fastest
35:50 - machine learning algorithm and for all
35:54 - these performance things this is my
35:56 - favorite comic well you have yeah and
35:58 - the similar conditions you're testing to
36:01 - systems we're testing the squid and the
36:03 - house cat and we want to know which one
36:04 - of them is more intelligent or faster or
36:06 - more performant or whatever and you just
36:10 - need to find the right scenario because
36:12 - then your system will always win and
36:14 - this is really the bond number one thing
36:16 - whenever some company benchmarks their
36:18 - tool against some competitors they will
36:20 - probably find one use case that is very
36:22 - good for them and very bad for their
36:24 - competitors that's why I would not trust
36:26 - any vendor based benchmarks I would not
36:29 - normally call them bench marketing and
36:32 - not proper benchmarks because well yeah
36:35 - people do something like this so if you
36:37 - ask me like which is the best machine
36:38 - learning algorithm um this is the answer
36:42 - so what you generally want to do is when
36:45 - you want to find some anomalies you want
36:47 - to find out like what is normal like
36:49 - what is the baseline of stuff that
36:51 - happens here for example you could see
36:53 - we have the black line is some I don't
36:56 - know I think it's that the data transfer
36:57 - the visitors on your website or
36:59 - something like that and you can see
37:00 - there is this pattern where probably the
37:03 - first two a week days then the next two
37:05 - are probably Saturday Sunday and then
37:07 - you have five more days which are
37:09 - probably a weekday again and then you
37:12 - try to get that baseline of kind of what
37:14 - is normal in your system so you normally
37:16 - calculate within some confidence
37:18 - interval you calculate what is the upper
37:21 - into lower bound so the upper bound is
37:23 - here at the lower bound is blue and
37:24 - you're just trying to find the system
37:26 - that is getting closer and closer to the
37:28 - actual kind of X curve or the actual
37:31 - value that you have to get kind of like
37:33 - this dynamic baseline of how close are
37:36 - you getting to what you're expecting and
37:39 - yeah most of the data is normally
37:41 - distributed
37:42 - sometimes you're unlucky and it's kind
37:44 - of like paranormally distributed and
37:45 - that will totally throw off any machine
37:48 - learning so if you don't have like a
37:49 - good regular distribution of your data
37:51 - and no machine learning for that will
37:53 - save you yeah so this would be another
37:56 - example so you can see here we had the
37:59 - first three directions we didn't really
38:01 - know what the system was up to so and
38:03 - here the upper and lower bound is just
38:04 - like this light blue background and that
38:08 - the darker blue lines kind of like this
38:10 - is the actual gate that is coming in but
38:12 - then after about three durations this
38:14 - system here has learned like what is the
38:16 - baseline and what is kind of expected of
38:18 - the values and then you can see the band
38:21 - that the light blue band around the
38:23 - actual data is getting narrower and
38:25 - narrower so it's learning what is normal
38:27 - in that system and at some point you
38:29 - have these colored dots the more it's in
38:31 - the red the more of an anomaly it is so
38:33 - you can for example see pretty much at
38:37 - the right hand side there is this where
38:38 - it's falling down there is kind of like
38:40 - this is an anomaly which would probably
38:42 - be pretty hard to spot otherwise in in
38:45 - that system here and the one thing that
38:50 - you need to keep in mind is that
38:51 - whatever your baseline is your baseline
38:54 - will probably evolve unless you have a
38:56 - system that is super stable which is not
38:57 - that common is that depending on how
39:00 - much more visitors you have or whatever
39:03 - you change in your system the baseline
39:06 - will change and you should have a system
39:07 - that keeps learning the new stuff and
39:09 - ages out the other information so that
39:12 - is not stuck on any other information so
39:14 - here yeah I think it's even the same
39:16 - data you can see three iterations and
39:18 - then it kind of knows what what is
39:20 - happening and then you can see we have
39:23 - since I shouldn't move out of the way
39:25 - I'm not trying to head over all the way
39:30 - so here you can see here we have this
39:33 - anomaly and you can see here the system
39:36 - has also learned the anomaly so it's
39:38 - kind of like expecting there might be
39:40 - something going on here so it doesn't
39:42 - really since it's not a supervised
39:44 - machine learning it's just learning what
39:46 - kind of is happening in the system and
39:47 - it's then continuing that trend and kind
39:50 - of feeding that normally into the model
39:53 - for the future and will keep up to date
39:55 - with whatever happen in your system yeah
39:59 - that's another example where you can see
40:02 - stuff is happening and at some point
40:05 - there you have an anomaly and then it
40:07 - keeps learning like it's aging
40:10 - normally again because here you have the
40:11 - anomaly here it's expecting Myanmar
40:13 - normally since it doesn't happen here
40:15 - the anomaly is getting smaller smaller
40:17 - and smaller so the anomalies are kind of
40:19 - being aged out over time is getting
40:21 - again as well yeah you can have a single
40:25 - time series for example unusual traffic
40:27 - where you can see okay this is a perfect
40:29 - pattern I'm expecting this is what might
40:31 - happen you could also break that up into
40:33 - multiple time series so you could have
40:35 - not one metric but multiple metrics you
40:37 - would be interested in and then each of
40:39 - those would be modeled as an independent
40:42 - baseline so you have a machine learning
40:45 - model for each one of them so for
40:48 - example instead of having two global
40:49 - traffic you could say I want to have
40:50 - traffic by country and then you can see
40:53 - you can break it down into different
40:55 - country countries and then you can see
40:57 - sometimes you just don't have traffic
40:58 - for example here in France probably they
41:00 - had either a strike or a national
41:02 - holiday you never know with the French
41:04 - but but something happened and nobody
41:07 - was online anymore there and so it can
41:10 - make sense to break it up into different
41:12 - intervals to see how that they tourism
41:15 - morning okay so as I said I'm just using
41:19 - for my example I'm using from our own
41:21 - website some some visitors so it's just
41:24 - I think one month so or no it's two
41:27 - weeks of data or something like that
41:28 - it's pretty much the energy engine
41:30 - x-axis lock that you're expecting so you
41:32 - can see you have some bytes that you
41:34 - have transferred with you the geoip look
41:36 - up so we know okay this visitor here
41:38 - probably came from India this is the
41:41 - approximate longitude and latitude where
41:42 - they have come from and then you get
41:45 - like the other information that you want
41:46 - so you see then was the remote IP
41:48 - address they did a get and got a 200
41:51 - back though we couldn't extract any
41:54 - proper browser information from that
41:57 - okay and since I always try to do at
42:02 - least something like this what this move
42:04 - might look like so for example here you
42:07 - can see I have just taken January to
42:09 - okay one and a half month march website
42:12 - visitors which were like around 15
42:15 - million website hits and this is the
42:18 - general distribution so you can already
42:20 - from this data you get a rough idea what
42:21 - is going on in the system so you can see
42:23 - here
42:23 - this was probably weekend these were
42:25 - weekdays weekend again so you get kind
42:28 - of the general pattern then you could
42:31 - see this is just some some default
42:35 - dashboard that we have you can see where
42:37 - are people coming from like where from
42:39 - where is your website being accessed so
42:41 - you can see for example Germany and
42:44 - France are doing pretty well if we zoom
42:46 - in a bit more here we could probably see
42:48 - Austria as well but since I'm not online
42:51 - my map data would need to load online so
42:54 - here somewhere would be Austria and then
42:57 - you can see ok these are the return
42:58 - codes of my data like this is pretty
43:00 - much all the data you can extract from
43:02 - an engine x-axis log so this would be
43:04 - just the return codes you have you could
43:05 - extract like these where the top URLs
43:07 - you hit you see since you have to buy it
43:10 - sent you see how much traffic your
43:12 - website was sending and then you could
43:15 - also do like a breakdown of okay
43:16 - this was the these were the operating
43:18 - systems that were accessing our website
43:20 - and these were I don't know the browsers
43:22 - and their specific versions so that is
43:24 - what you get in the engine x-axis lock
43:25 - there is no machine learning necessary
43:27 - for that that's just block parsing so no
43:29 - no major magic here then you could write
43:33 - your own visualization so for example
43:35 - this was the number of unique remote IP
43:38 - addresses I've shown you before and this
43:41 - was the the bytes that we were
43:43 - transferring from our website which they
43:45 - kind of correlate slightly but not that
43:48 - wrongly for especially here kind of
43:50 - there is no strong correlation to that
43:51 - one and does anybody see the anomaly in
43:54 - the top chart here or should I let you
43:59 - stare at that dashboard for a week and
44:01 - maybe then you can see it I think if I'm
44:04 - not mistaken the anomaly is this one
44:05 - here so this like this downward thing
44:09 - though it's very hard to see we can
44:13 - actually leverage the machine learning
44:14 - stuff now our to see what is going on
44:16 - here
44:16 - so I've quickly built two visualizations
44:20 - for that the first one is about having a
44:23 - single metric that is the same
44:24 - information that I've just shown you
44:26 - that is the unique IP address that we
44:29 - had in our system and you can see our
44:33 - system needs approximately three
44:34 - directions to learn so you can see here
44:37 - these are the first three days this was
44:40 - Wednesday Thursday Friday and then the
44:43 - algorithm thought well I know what to
44:44 - expect the problem was then we have
44:46 - Saturday Sunday and the traffic pattern
44:48 - is totally different so you're kind of
44:49 - throwing off the machine learning
44:50 - algorithm here a bit because that is
44:53 - here you're having your five days of the
44:54 - week days again and then you have your
44:56 - weekend again and you can see the
44:58 - pattern continues that you need about
45:00 - three durations to learn a pattern
45:02 - because we can one weekend to week and
45:04 - three about here it got tweaked days
45:08 - pretty well already and here it also got
45:10 - the week end then and from then onwards
45:12 - it knows okay this is kind of the
45:15 - weekday pattern that I'm expecting this
45:17 - is the weekend is the weekday pattern so
45:19 - system kind of learned where how stuff
45:21 - works and this is here where we have an
45:23 - anomaly so let's quickly see normally
45:34 - and you can see here something was
45:38 - happening like generally we would expect
45:40 - traffic in this area here but somehow it
45:43 - was falling down and each of these
45:45 - points is actually an anomaly and it
45:47 - actually has an anomaly score between
45:49 - zero and hundred this is how we modeled
45:51 - that but you could totally build
45:52 - something similar the closer it gets to
45:55 - 100 the worse the anomaly is and then
45:57 - you could even have like the list which
45:58 - says like Oh normally I'm expecting that
46:01 - value here 1,400 something I got only 86
46:06 - so this is 17 times lower than our than
46:10 - I was expecting and this was on the 27th
46:16 - of February last year anybody remembers
46:18 - what happened then no okay let's jump
46:25 - back to my slides I think we've seen
46:27 - most of those already and that was when
46:31 - there was the first ever bigger as3
46:33 - outage from aw yes and half of the
46:37 - internet basically relied on that so
46:40 - yeah three went down and that affected a
46:43 - lot of stuff including Amazon stuff as
46:45 - well because there were also themselves
46:47 - relying on s3 to be available like
46:50 - everybody else
46:51 - so unfortunately we had the dependents
46:54 - in our website which basically killed
46:55 - our website and our downloads and lot of
46:58 - other stuff we then thought about should
47:01 - we make some multi cloud strategy or
47:03 - whatever but in the end nobody
47:05 - complained because half of the internet
47:06 - was down anyway and if the internet or
47:08 - if your own site is down and everybody
47:10 - everything is on fire nobody complains
47:11 - that your downloads are not working so
47:13 - we kind of gave up on making stuff
47:15 - unnecessarily complex because there was
47:17 - just no point of that but we you could
47:20 - very nicely see that normally there yeah
47:26 - we are nearly out of time okay
47:29 - then I then I will stick to slides then
47:33 - you can build the same thing with
47:34 - multiple models here I have broken my
47:37 - data down into the response codes that
47:39 - my website is sending out and then you
47:43 - can find further anomalies so for
47:45 - example here I have an over the normal
47:47 - is code that is like the top one is kind
47:49 - of like this is anomalies over all the
47:51 - red things are where the anomalies are
47:52 - and then you can break the entire thing
47:55 - down into the response code so you can
47:57 - see for the 404s we have a band of
48:00 - trouble over there this was for example
48:04 - our on our block and we suddenly had a
48:07 - spike of 404
48:08 - this is when we published a bad link on
48:10 - our own block and suddenly the 404
48:12 - started to spike and this was when we
48:16 - had in our CMS since we're using a CMS
48:18 - and CMS are mostly crap when we kind of
48:20 - broke some links internally
48:22 - unintentionally with the CMS change you
48:25 - could totally see that here that we
48:26 - suddenly had this pipe of anomalies
48:28 - where yeah suddenly stuff is not going
48:30 - well you can have combined multiple
48:35 - models and see what they're doing so
48:36 - here I'm combining this was the remote
48:39 - IP addresses the example which I've
48:41 - shown before where you can see this was
48:43 - when our website went down because of
48:45 - Amazon s3 so this is here then was the
48:48 - a3 outage because nobody could access
48:51 - our data anymore and these are the 404
48:53 - and these two totally look correlated
48:55 - right so something is happening here and
48:58 - then suddenly the 404 spiked
48:59 - unfortunately these two events were
49:01 - totally unrelated because this was at
49:03 - the AWS outage and
49:05 - this was our own change on the CMS and
49:07 - that will screw off or throw off any
49:09 - machine running because it tries to
49:11 - learn and correlate some data but if
49:13 - there is no real correlation it will
49:15 - just be kind of like try to infer one
49:17 - which is not there so yeah correlation
49:19 - doesn't mean it's actually a causation
49:21 - everybody knows that but always keep
49:23 - that in mind
49:25 - and yeah it's kcd and extract the best
49:28 - once you take a statistics class does it
49:31 - help to make the distinction maybe yeah
49:35 - and then they're always like what is the
49:38 - cause and what is the reason or like
49:41 - what is the cause and what is the effect
49:42 - um for example for cancer waste it's
49:45 - very interesting study that in the US
49:46 - and the cancer rate has spiked and cell
49:49 - phone usage spiked afterwards so here
49:52 - the theory could be that cancer is
49:54 - causing cell phones which is also not
49:58 - the right thing but if you just look at
49:59 - the data and misinterpreted that is what
50:02 - you might get um yeah
50:05 - any correlated features would mess up
50:07 - your system there's unfortunately no way
50:09 - around that yeah finally thing here what
50:14 - you can also do is future predictions so
50:16 - the yellow stuff here these are future
50:18 - predictions and you can see the further
50:19 - it goes into the future the less certain
50:22 - they are because the more coarse-grained
50:24 - the entire system gets but this is very
50:26 - helpful if you like I'm expecting this
50:28 - number of visitors or this is the disk
50:29 - space I'm expecting in the future or the
50:31 - resource usage and this can predict for
50:33 - the future like this is what you will
50:35 - probably have a new system given that
50:37 - there are no other anomalies yeah then
50:40 - you could start doing like categorized
50:42 - users but let's keep that for today so
50:45 - to wrap up we've seen machine learning
50:47 - domain where I picked like mainly ops
50:50 - data and then some nginx data set where
50:53 - I've played around with if you want to
50:56 - know more about machine learning one a
50:57 - very nice paper I've seen is this one
51:00 - here best practices for machine learning
51:03 - engineering which has some very nice
51:05 - rules so for example the rule number one
51:07 - is don't be afraid to launch a product
51:09 - without machine learning because you
51:11 - might not need it and then yeah if you
51:13 - can interpret the model properly and
51:16 - debugging will be much easier if it's
51:17 - not just a black box
51:19 - spits out random data and plant launch
51:22 - and iterate on stuff but it has 43 rules
51:25 - all about machine learning which
51:26 - probably makes sense if you build
51:28 - anything in that space and yeah and
51:30 - don't forget maybe at some point machine
51:32 - learning is not the hot new thing
51:33 - anymore
51:35 - but something else comes around so yeah
51:37 - Silicon Valley is probably heading
51:39 - somewhere else already again anyway
51:42 - that's it I think we're pretty much out
51:45 - of time if you have questions come to me
51:47 - find me afterwards talk to me if you
51:49 - want to have stickers I have loads of
51:51 - stickers here so if anybody wants
51:52 - stickers yeah very nice take them with
51:56 - that I think we're done
51:57 - thanks very much wait I'm always taking
52:02 - a picture because my colleagues don't
52:03 - know where I am and this is my way to
52:06 - prove that I've been working today smile
52:10 - everybody
52:12 - yeah you can always wave thank you enjoy
52:16 - the break