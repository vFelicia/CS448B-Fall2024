00:00 - Hello, internet, and welcome to this 
course all about open telemetry. 
00:06 - In this course, I'm going to 
be showing you how you can get  
00:10 - full stack observability on the performance of 
the behavior of your software project by using  
00:16 - open telemetry with your analysis tool of choice.
If you want to be able to tell why your project  
00:22 - or app is running too slow, is broken, or 
you just want to improve its code quality. 
00:28 - This is the video for you.
But first, what exactly do we  
00:33 - mean by over telemetry? Well, let's start off with 
the name itself, we have open so like open source,  
00:42 - and telemetry, which is an institute collection 
of measurements or other data at remote points. 
00:48 - And that automatic transmission to 
receiving equipment for monitoring. 
00:53 - The word telemetry actually comes from 
the Greek root word Teller, or remote,  
00:58 - and metron, for measure.
And that's exactly what we are going to be doing,  
01:03 - we are going to be facilitating a 
way to measure the performance of  
01:07 - everything we use in our app remotely.
With any app, when you want to start  
01:12 - looking at this kind of data, you have 
two parts that need to come together. 
01:16 - The first is figuring out how to 
generate and transmit that data. 
01:20 - And then the second part is deciding 
what you are going to do with that data. 
01:26 - So in other words, how are you going to analyze 
it? Open telemetry deals with that bus part. 
01:32 - Up until now there has been no real standardized 
way of describing what your system is doing. 
01:38 - This is down to the fact that we 
all like to think differently,  
01:41 - use different programming languages, different 
machines, and a combination of different ways. 
01:47 - This was a problem, especially for those 
wanting to build observability tools. 
01:51 - At the heart of open telemetry 
project is exactly that,  
01:55 - a standardization for describing 
what distributed systems are doing,  
01:59 - no matter what programming language 
or computer systems you are using. 
02:03 - Today, the open telemetry project can 
be described as a collection of tools,  
02:08 - API's, and SDKs, use it instrument, 
generate, collect and export telemetry data,  
02:15 - so that we can analyze it later on with whatever 
platform we wish by standardizing our data and  
02:20 - means we are not tied to anything in the long run.
And it makes moving from one analysis tool  
02:26 - to the next super easy, I will 
not affect your historic data. 
02:31 - As I mentioned at the beginning, 
this is an open source project. 
02:34 - It is made up of many, many 
developers and their inputs. 
02:37 - If we have a look at GitHub here for the 
project, you will see it is super transparent. 
02:42 - You can see the Governance Committee, 
you can see the technical committee. 
02:46 - If you are interested in getting involved,  
02:48 - please do join their mailing list 
and attend the community meetings. 
02:52 - You can even see all the meetings 
that you are welcome to join. 
02:56 - Before we get started, though, I just want 
to go into a little bit of what we're going  
03:01 - to learn on this course.
So let's break it down. 
03:05 - Okay, so in this course, first we're 
going to look at what are microservices  
03:10 - followed by what is observability then we're 
going to look at melt, followed by the history  
03:16 - of open telemetry, then we're going to 
actually start by setting up a project. 
03:20 - We are then going to talk about tracing, 
distributed tracing and context and propagation  
03:26 - before adding tracing into our project.
We will then move on to looking at metrics before  
03:33 - finally ending on our distributed projects.
I will end this course with where to go next. 
03:42 - We expect our websites, apps and online 
services to load almost instantaneously  
03:48 - right? Think of the frustration some of us feel 
when websites take more than two seconds to load. 
03:54 - on the backend a feat of engineering is needed 
to keep a global system running to make sure  
04:00 - that you have access to Netflix or Instagram.
Only a tap away Let's have a look  
04:06 - at Netflix for a minute.
Netflix at its peak consumes 37%  
04:10 - of internet bandwidth in the US, there are 
1000s of people clicking play at the same time,  
04:17 - with activity peaking in the evening.
Though, as a global platform,  
04:21 - it's a constant pick.
The challenge is how to run a service with  
04:25 - zero loss while processing over 400 billion events 
daily, and 17 gigabyte per second during peak. 
04:34 - In today's video, we will dive deeper into what 
types of systems power these amazing services and  
04:42 - how we can use data to get deeper visibility 
into the insights of these complex systems. 
04:52 - Historically, developers about applications in 
monoliths with large complex code bases, a single  
04:59 - monolith will contain all the code for all the 
business activities and application performed. 
05:04 - This is all fine and dandy 
if you have a small app. 
05:07 - But what happens if your application 
turns out to be successful,  
05:11 - users will like it and begin to depend 
on it traffic increases dramatically,  
05:16 - and always inevitably use requests 
improvements and additional features. 
05:20 - So more developers are roped into 
work on the growing application. 
05:25 - before too long, your application 
becomes a big ball of mud. 
05:29 - a situation where no single developer 
understands the entirety of the application. 
05:34 - Your once simple application 
has now become larger complex. 
05:39 - multiple independent development 
teams are simulataneously. 
05:42 - Working on the same codebase and simultaneously 
changing the same sections of code,  
05:48 - then it becomes virtually impossible to know 
who is working on what poetry does collide,  
05:53 - code quality suffers, it becomes harder and harder 
for individual development teams to make changes  
05:59 - without having to calculate what the 
impact will be to other teams and  
06:04 - teams can lose sight of how that code might be 
incompatible with others, among other issues. 
06:09 - This generally results in slower, less 
reliable applications, not to mention  
06:15 - longer and longer development schedules.
In come microservices, the main principle  
06:21 - behind a microservice architecture is that the 
applications are simpler to build and maintain  
06:26 - when broken down into smaller pieces.
When using microservices, you isolate software  
06:31 - functionality into multiple independent modules 
that are individually responsible for performing  
06:37 - precisely defined standalone tasks.
These modules communicate with  
06:43 - each other through simple API's.
microservice architectures let you  
06:48 - split applications into distinct independent 
services, each managed by different teams. 
06:54 - It leads to naturally delegating the 
responsibilities for building highly  
06:58 - scaled applications, allowing work to be done 
independently on individual services without  
07:04 - impacting the work of other developers in other 
groups working on the same overall application. 
07:11 - However, trying to get visibility into 
this system can be very difficult. 
07:15 - When you have hundreds of services and 
applications your request traveled through  
07:19 - debugging and troubleshooting can be a nightmare.
This is where open telemetry comes in. 
07:26 - As we already touched on telemetry is defined as 
the science or process of collecting information  
07:32 - about objects that are far away and sending 
the information somewhere electronically. 
07:37 - Now.
observability  
07:39 - means how well you can understand what is going 
on internally in a system based on its outputs. 
07:45 - Especially as systems become 
more distributed and complex. 
07:49 - It's hard to see what's going on inside your 
application and why things may be going wrong. 
07:56 - When talking about observability, we need to 
define the data types necessary to understand  
08:01 - the performance and health of our application 
broadly, metrics, events, logs and traces. 
08:09 - Met, metrics are measurements collected at 
regular intervals must have a timestamp and name,  
08:17 - one or more numeric values and a count 
of how many events are represented. 
08:21 - These include error rate, response time or output.
An event is a discrete action happening  
08:28 - at any moment in time.
Take a vending machine. 
08:31 - For instance, an event could be the moment 
when a user makes a purchase from the machine. 
08:37 - Adding metadata to events 
makes them much more powerful. 
08:41 - With the vending machine example we 
could add additional attributes such as  
08:46 - item category, and payment type.
This allows questions to be asked  
08:50 - such as how much money was 
made from each item category,  
08:54 - or what is the most common payment type use.
logs come directly from your app,  
08:59 - exporting detailed data and 
detailed context around an event. 
09:03 - So engineers can recreate what 
happened millisecond by millisecond. 
09:08 - You have probably logged something when you use 
things like system out print, or console log  
09:14 - traces follow a request from the 
initial request to the returned output. 
09:20 - It requires the casual chain of 
events to determine relationships  
09:24 - between different entities.
traces are very valuable for  
09:27 - highlighting inefficiencies, bottlenecks 
and roadblocks in the user experience as  
09:32 - they can be used to show the end to end latency of 
individual cores and a distributed architecture. 
09:38 - However, getting that data is very difficult.
You would have to manually instrument every  
09:43 - single service one by one layer by layer.
This will take as much time as writing the  
09:49 - code itself, which is annoying.
Luckily there are some awesome  
09:53 - open source projects as well as 
companies that make this a lot easier  
09:59 - in today's 16 open tracing was released as a cn CF 
project focused only around distributed tracing. 
10:06 - Because the libraries were lightweight and 
simple, it could be used to fit any use case. 
10:11 - While it made it easy to instrument data, it made 
it hard to instrument software that was shipped as  
10:16 - binaries without a lot of manual engineering work.
In 2018, a similar project called Open census was  
10:23 - open source out of Google, which supported both 
the capturing retracing permission and metrics. 
10:28 - While it made it easier to get telemetry data 
from software that was shipped as binaries  
10:33 - like Kubernetes, and databases, it made it hard to 
use the API to instrument custom implementations,  
10:40 - not part of the default use case.
Both projects were able to make observability easy  
10:45 - for modern applications and expedite wide adoption 
of distributed tracing by the software industry. 
10:51 - However, developers had to choose 
between two options with pros and cons. 
10:56 - It turns out that the approaches of 
the two projects were complimentary  
11:00 - rather than contradictory.
There was a no reason why we  
11:04 - couldn't have both the abstract vendor neutral 
API and a well supported default implementation. 
11:11 - In late 2019, the two projects 
merged to form opentelemetry. 
11:16 - This brought forward the idea 
of having a single standard for  
11:19 - observability instead of two competing standards. 
11:24 - Okay, so first things first, let's go a bit meta 
and understand exactly what is going to happen. 
11:30 - In order for us to view what 
is happening in our app. 
11:35 - Here we have our project.
Think about project as a project we  
11:40 - have made with our code editors of choice.
For example, if we run it,  
11:45 - it runs locally on our machines.
Let's also say this project is  
11:49 - built to listen out for requests.
So like listening out for a get request,  
11:54 - for example, we have decided that we want 
to measure our app's performance based on  
12:00 - the requests that it makes.
To do this, the first step  
12:05 - as we mentioned at the start, would be to 
implement open telemetry into the project. 
12:10 - We are doing this to help us standardize the data.
Once we have implemented open telemetry  
12:18 - and standardize the data, we need to think 
about what we are going to do with the data,  
12:23 - how we're going to view it, and so on.
For this, we can use an analysis tool. 
12:29 - By analysis tool, I mean, any type 
of tool that gives you observability,  
12:35 - we are going to look at a few of these tools, 
one that focuses specifically on tracing,  
12:41 - one that focuses specifically on metrics, and 
one that looks at everything in one platform. 
12:48 - We are then going to send our data 
to our analysis tool of choice. 
12:53 - Here we have an example of what our data can 
look like in a tracing app such as zipkin,  
12:59 - a metrics app such as permittees, and an 
observability tool such as New Relic that  
13:05 - will give us an overview of everything as well 
as other more bespoke insights in one platform. 
13:11 - We will go into each of these 
in their dedicated sections. 
13:16 - Let's start with implementing 
open telemetry first. 
13:20 - Okay, so before we start, the only prerequisite  
13:23 - I am going to ask of you is that you 
have Docker downloaded onto your machine. 
13:28 - Docker is a container platform for rapid 
up microservices development, and delivery. 
13:33 - And if you don't know much about containers 
and micro services, just stick with me for now. 
13:39 - I have a section dedicated to both of 
these topics coming up after this section. 
13:44 - For those of you that don't have it, 
please navigate to the Docker website  
13:48 - and follow the instructions.
In order to get set up. 
13:52 - I personally would choose to 
download the Docker desktop. 
13:57 - As I am working on a Mac, I choose the Mac option.
Once you are done, make sure your Docker  
14:04 - desktop is running on a Mac, I would 
simply hover over the icon like so. 
14:11 - You will see if you open up the platform 
I currently have no containers running. 
14:17 - Having a container is going to be important 
for when we get to using the analysis tools,  
14:23 - such as a tracing back end like zipkin.
Once we have all got Docker up and  
14:28 - running, let's get going.
Let's get our terminal up. 
14:33 - Now I'm going to navigate to a folder 
where I like to store all my projects. 
14:38 - It's called development.
Please go ahead and choose  
14:42 - any directory that you would like to work in.
Now once here using the Mk dir command,  
14:49 - I'm going to make a folder for the 
new project that I'm about to start. 
14:54 - I'm going to call it open telemetry starting out.
Please be aware that if you are using Different  
15:00 - terminal, other commands may need to be used.
Now, let's go into the project using  
15:07 - the command cd.
The first thing I'm  
15:10 - going to do is start our container.
So it's ready for the next section. 
15:15 - So, as mentioned, the first thing that you 
need before you can start collecting traces  
15:21 - is a tracing back end like zipkin, that 
you can export traces to, to set up zipkin  
15:28 - as quickly as possible, run the latest 
Docker zipkin container, exposing Port 9411. 
15:36 - If you can't, or don't want 
to run a Docker container,  
15:40 - you could also choose to download zipkin directly.
So just so you know, that is an option too. 
15:47 - If you want to explore that option more, I 
would suggest visiting the zipkin website. 
15:53 - So this is the command to run our container.
And there we go, we now have this container ID,  
16:02 - it has worked.
Next up, we need to get a package JSON  
16:06 - file in our project, we can do so by typing NPM.
In it, having a package JSON file will make it  
16:14 - easier for others to manage and install 
all the packages we need for the project. 
16:19 - If you are getting errors, it could be because 
you don't have no js and NPM installed. 
16:24 - If that is the case, please visit node j s.org.
In order to get set up with that by  
16:30 - following the download instructions.
Okay, so we have initialized the utility,  
16:38 - I am just going to press enter for all 
these fields, it's prompting me to answer. 
16:43 - So enter and, and, and, and enter. 
16:51 - OK, we are done creating our 
package JSON file for now. 
16:56 - If we list out all the files on our project, using 
the ls command, you will see the file right there. 
17:05 - Now finally, I need to create an app js file.
So a JavaScript file and put that in the project  
17:12 - to some of you might have a different 
approach to adding files to a project. 
17:16 - So that is totally up to you, however 
you'd like to create that file. 
17:20 - But once we are done, we now 
need to open up our project. 
17:24 - As I am using VS code, I'm going to use the 
command code.in order to open up our project. 
17:31 - And there we go. 
17:33 - There is a folder with an app 
js file and a package JSON file. 
17:40 - You will see the package JSON file 
has all the prompts we were asked,  
17:44 - as I skipped all the prompts is just the 
standard default entries are no entries at all. 
17:51 - If you go to the app js file, you will also see 
that it's currently empty, it has nothing in it. 
17:57 - Okay, so the first thing I need to do is change 
this to point to the app js file we created  
18:04 - not the index j s as there is no index js file.
And just for fun, this is not necessary, I'm going  
18:10 - to fill out the description on my project.
So open telemetry costs. 
18:16 - The next thing I want to do 
is just add a start script. 
18:21 - So for this script, I'm going to run node app j s.
Okay, now for the fun part,  
18:29 - let's get to adding some packages.
To start using open telemetry, we are going  
18:34 - to need to install some of its packages.
Using NPM II or install for short. 
18:41 - I'm going to store the open telemetry core 
package, the open telemetry node package  
18:49 - opentelemetry plugin http opentelemetry.
plugin HTTP s, open telemetry exporters  
18:57 - zipkin to get us ready for the next section on 
open telemetry tracing for the next section to  
19:04 - an express the only non open telemetry one.
Okay, so we need all of those to install. 
19:15 - Okay, and great.
So these are looking good. 
19:19 - So if we look back here, here are 
the packages that we just installed. 
19:24 - They have automatically 
populated our package JSON file. 
19:29 - So this is looking good.
We just missed one. 
19:32 - So let's go back and use NPM II or install for 
short and install Open telemetry plug in Express. 
19:40 - And we are done.
Okay, now let's move on to our app js file. 
19:46 - The first thing I'm going to 
do is define a port for us. 
19:50 - So const port, process and V port or a string of 
Port 8080 Next up, we are going to need Express. 
20:04 - So const Express. 
20:08 - And Express is one of the packages we installed.
So this one right here. 
20:13 - So we need to tell our file that 
this const requires Express. 
20:19 - We are then going to call Express 
and stored as the const app. 
20:24 - The first thing I want to do is just 
get a message in our console log to  
20:27 - let us know all is good, and that we 
are listening out and on which port. 
20:32 - So just as a recap, what I am doing here 
is getting the app to start a server. 
20:38 - And then with this code, I'm getting it to 
listen out to our defined port for any requests. 
20:46 - Okay, next up, I'm going to paste 
this super basic piece of code. 
20:53 - This code is an example of a very basic route.
routing refers to how an applications  
21:00 - end points respond to client requests.
We have defined the route using methods  
21:05 - of the Express app object that 
corresponds to http methods. 
21:10 - For example, app get handles GET requests.
These routing methods specify a callback function  
21:18 - called when the application receives a 
request to the specified route an HTTP method. 
21:24 - In other words, it listens out for requests 
that much specified routes and methods. 
21:29 - In this case, the root is our homepage, we 
essentially want to trace every single time  
21:35 - a get request is made to the homepage.
We will do this in the next section. 
21:46 - Now that we have the basic installation done, 
it is now time for us to talk about tracing. 
21:50 - So as you know, open telemetry allows 
us to essentially standardize our data. 
21:55 - The next part is actually viewing the data in a 
way that we can analyze what is happening behind  
22:00 - the scenes, we will do this with a tracing system.
in software engineering tracing involves  
22:06 - a specialized use of logging to record 
information about a program's execution. 
22:11 - This information is typically used by programmers 
to debug by using the information contained  
22:17 - in a trace log to diagnose any problems that 
might arise with a particular software or app. 
22:23 - Distributed tracing, however, also called 
a distributed request tracing is a method  
22:29 - used to debug and monitor applications 
built using a micro service architecture. 
22:35 - Distributed tracing helps pinpoint where 
failures occur, and what causes poor performance. 
22:42 - So as we can now see, being able to 
get tracing data telemetry is pretty  
22:47 - important to the overall performance of an app.
However, as we discussed in the introduction, due  
22:52 - to systems using all types of different languages, 
frameworks and infrastructures, it's a hard thing  
22:57 - to do without some sort of common approach.
That's why Evans limitary can help  
23:02 - so much with distributed tracing.
By providing a common set of API's,  
23:07 - SDKs, and wire protocols.
It gives organizations a single,  
23:11 - well supported integration service for 
end to end distributed tracing telemetry. 
23:16 - For this course, the tracer we're 
going to be using is called zipkin. 
23:21 - zipkin is a distributed tracing system that helps 
them gather timing data needed to troubleshoot  
23:26 - latency problems and service architectures.
Circuit was originally created by Twitter. 
23:32 - And it's currently run by the open 
zipkin volunteer organization. 
23:36 - I am using zipkin for no other 
reason than I had to pick one. 
23:40 - But please do feel free to choose any 
back end tracing system that you wish,  
23:45 - the choice is completely up to you.
In general, once the traces are implemented  
23:50 - into applications, they essentially record timing 
and metadata about operations that take place. 
23:57 - An example of this is when a web server 
records exactly when it receives a request. 
24:02 - And when it sends a response.
This data is usually represented by a bar, just  
24:07 - like this, and goes by the official name of spam.
So in this example, we have two services  
24:14 - and a bunch of spans.
To explain this, imagine this  
24:17 - represents your favorite food delivery app.
So imagine you make an order right. 
24:22 - Now a few things will happen, 
each represented by a span. 
24:27 - You send information back and forth from 
services in order to make the payment,  
24:31 - find a delivery driver closest to you, 
and notify that driver with your order. 
24:35 - Each of these operations generates a spam showing 
you the work being done to make this happen. 
24:40 - In this case, the spans have implicit 
relationships, so parent and child but  
24:45 - also from individual services and the trace.
As you can see, each of the spans starts at  
24:51 - a different point and takes 
a different amount of time. 
24:54 - We call this latency and network latency.
In a nutshell, latency is delay between  
25:00 - an action and a response to that action.
network latency refers to specific delays  
25:06 - that take place within a network.
latency is generally measured in  
25:10 - milliseconds and is unavoidable due to the 
way networks communicate with each other. 
25:15 - It depends on several aspects of a network 
and can vary if any of them are changed. 
25:21 - errors on most systems are 
usually quite easy to spot. 
25:24 - If your bar ends in red or similar, for 
example, you know that an error has occurred. 
25:35 - Now it's time for us to look 
at context and propagation. 
25:39 - These two concepts will allow us to 
understand the topic of tracing a lot better. 
25:44 - So as we know distributed tracing allows us 
to correlate events across service boundaries. 
25:50 - But how do we find these correlations for 
this components in our distributed system  
25:55 - need to be able to collect, store and transfer 
metadata? We refer to this metadata as context. 
26:03 - Context is divided into two types, 
span context and correlation context. 
26:09 - span context represents the data required for 
moving trace information across boundaries. 
26:14 - It contains the following metadata.
We have a trace ID, a span ID, the trace  
26:21 - flags and the trace state of this bond context.
And then we have the correlation context. 
26:30 - A correlation contacts carries 
user defined properties. 
26:34 - This is usually things such as a 
customer ID, providers, hearse name,  
26:39 - data region and other telemetry that gives 
you application specific performance insights. 
26:44 - Correlation context is not required and components 
may choose to not carry or store this information. 
26:52 - A context will usually have information so 
we can identify the current span and trace  
26:59 - and propagation is the mechanism we use to bundle 
up our context and transfer across services  
27:07 - so that we have it context and propagation.
Together. 
27:12 - These two concepts represent the 
engine behind distributed tracing. 
27:17 - If you would like to learn more about these 
two topics and do a deep dive at please  
27:21 - visit the urban telemetry website.
For the purpose of this tutorial,  
27:25 - however, a basic knowledge above will suffice.
So just as a bit of a heads up in this next  
27:37 - section, we will go through how to first 
initialize a global tracer and after that  
27:42 - initialize and register a trace exporter.
Okay, so here we are where we left off. 
27:49 - Now in the last section, we ran the latest 
Docker zipkin container exposing Port 9411. 
27:57 - If we actually visit localhost 9411, we will see 
the zipkin UI that comes as part of doing that. 
28:06 - So here we are, this is what we are 
going to use to view our traces. 
28:11 - Okay, let's carry on.
Next, let's create a file named tracing  
28:18 - and j s and add the following code.
This is just the sample code provided  
28:23 - to us by opentelemetry.
If you visit their website,  
28:26 - you can see I am just copying this code 
right here and pasting it into my project. 
28:32 - You will also see that this file uses two of 
the packages we installed in the initial setup. 
28:38 - Once we have pasted that we need to 
initialize and register a trace exporter. 
28:43 - We have already done part of this as 
we install two packages necessary for  
28:47 - this part and the initial setup section.
And that is the open telemetry tracing  
28:52 - and open telemetry export to zipkin packages.
So first off, let's make a new zipkin exporter. 
29:01 - So provide add span processor, 
new simple span processor. 
29:07 - This is from the open telemetry tracing package, 
as you can see that has shown up at the top  
29:14 - news of can export out.
And that also comes from another package. 
29:19 - So the open telemetry exporters, it can 
package as you can see appearing at the top,  
29:25 - this might not automatically show up for you.
It's just the code editor I'm using. 
29:29 - So you might have to type those two out, and then 
the right service name and choose a service now. 
29:37 - I'm going to put Getting Started but you 
can replace it with your own service name. 
29:42 - Okay, that is looking good.
And so is the app js file. 
29:48 - Okay, great.
All tracing in a civilization  
29:51 - should happen before your application code runs.
The easiest way to do this is to initialize  
29:57 - tracing and a separate file that required using 
the node r option before your application code  
30:04 - runs, I will show you what I mean by this.
So, now, if you run your application with node,  
30:12 - our tracing j s and app j s, your application 
will create and propagate traces over HTTP. 
30:21 - So let's run that.
And now let's send  
30:24 - requests to application over HTTP.
We can do so simply by refreshing the  
30:30 - localhost 8080 page, you'll see traces exported 
to our tracing back end, they look like this. 
30:38 - So he will see we are making a get request to the 
homepage, which is responding with hello world. 
30:45 - And here we have getting started.
As that is what we called our service name. 
30:51 - We also get a start time and a duration as a span.
Now, as you use this more and more often,  
30:59 - some spans might appear to be 
duplicated, but they are not. 
31:04 - This is because some applications can be both 
the client and the server for these requests. 
31:10 - If this is the case, you will see one span 
which is the client side request timing, and  
31:15 - one span which is the server side request timing, 
anywhere that they don't overlap is network time. 
31:23 - Okay, now I'm going to show you one more example.
So just to differentiate, I'm going  
31:28 - to change my service name to get date.
Now I am going to go into my app js file. 
31:37 - I'm just going to copy this 
piece of code right here. 
31:40 - And paste. 
31:42 - Now I want to essentially listen out to any 
time a get request is made to the park date. 
31:48 - So in other words, if someone right now 
went to localhost 8080 forward slash date,  
31:53 - that is us making a get request, we will also 
be able to see respond with the actual date. 
31:59 - So let's go ahead, let's go back to our 
localhost 8080 and type forward slash date. 
32:06 - Oops, I stopped my app running, 
let's make sure it's running. 
32:09 - So I'm just going to start this up again.
Okay, and refresh our page. 
32:15 - And great, there is our object with today's date.
Amazing. 
32:20 - So now if we visit localhost 9411,  
32:24 - so the port we exposed, and click run a query, 
we will see all the requests that have been made. 
32:32 - So there we go, we can now 
see our get date service. 
32:37 - And at the moment, the only request we have 
that it's listening out for is the get request. 
32:43 - Now, I have actually renamed the service, 
remember so if I visit the homepage,  
32:48 - you will see that request is also being stored 
under the get date service name, you can tell  
32:54 - which one it is by the timestamp.
Okay, let's move on. 
32:59 - Okay, we are now done with a 
basic implementation of tracing. 
33:03 - However, we are literally 
just touching the surface. 
33:07 - In the project portion of our course I 
will show you how to use open telemetry  
33:12 - to instrument a distributed system.
By this I mean I will show you how  
33:17 - to trace multiple services and their 
interactions with each other if those exist. 
33:29 - In this next section, we're going to learn 
how to collect metrics with open telemetry and  
33:33 - remediate Prometheus as a monitoring platform 
that collects metrics from monitor targets by  
33:37 - scraping metrics HTTP endpoints on these targets.
In this section, I will show you how to install,  
33:44 - configure and monitor our fast app 
with Prometheus and open telemetry. 
33:48 - We will download, install and run Prometheus to 
expose time series data on hosts and services. 
33:55 - Unlike tracing which works in spans metrics 
are a numeric representation of data  
34:00 - measured over intervals of time.
metrics can harness the power of  
34:05 - mathematical modeling and prediction to derive 
knowledge of the behavior of a system over  
34:10 - intervals of time in the present and future.
Since numbers are optimized for storage,  
34:15 - processing, compression and 
retrieval, metrics enable longer  
34:19 - retention of data as well as easier querying.
This makes metrics perfectly suited to building  
34:25 - dashboards that reflect historical trends.
metrics also allow for gradual  
34:29 - reduction of data resolution.
After a certain period of time, data can  
34:34 - be aggregated into daily or weekly frequency.
Let's have a look at this in action. 
34:40 - In this section, I'm going to be using 
promethium as my metrics backend. 
34:45 - Now that we have set up end to end traces, 
we can collect and export some basic metrics. 
34:52 - First, I'm going to stop this from running.
Let's go to the Prometheus download page  
34:58 - and download the latest release.
permittees for your operating system. 
35:03 - As I'm using a Mac, I'm going 
to click on this one right here. 
35:08 - Once that has downloaded open a command line and 
use CD or the command cd to go into the directory,  
35:16 - where you downloaded the promethease tarball.
In my case, it will be the Downloads directory. 
35:24 - Now I need to unretire it into the newly created 
directory, make sure that you replace the file  
35:31 - name with your downloaded harbor.
So don't necessarily use this one. 
35:35 - And now let's go into the directory.
If I list out all the files and folders,  
35:42 - you will see a file named permit this yamo this 
is the file used to configure permit Yes, for now,  
35:50 - just make sure permit is start by running the dot 
four slash promethease binary and the folder and  
35:58 - browse to localhost 9090.
Okay, great. 
36:04 - And that is our promethease user interface.
So you will see this as server is ready  
36:11 - to receive web requests.
So that should be all good. 
36:15 - I am just going to go ahead and open up a new 
tab right here so I can keep that running. 
36:21 - And I'm going to open up our 
directory using the VS code shortcut. 
36:27 - Once we have confirmed that 
permit the US has started,  
36:30 - we need to replace the contents of the 
primitives yamo file with the following. 
36:35 - So literally just delete everything and 
put in this much shorter piece of code. 
36:41 - This will set the scrape 
interval to every 15 seconds. 
36:46 - We are now ready to monitor 
our Node JS application. 
36:50 - In this next section, we need to initialize 
the required open telemetry metrics library,  
36:56 - initialize a meter and collect metrics and 
initialize and register a metrics exporter. 
37:05 - To do this, we are going to 
have to install some libraries,  
37:08 - we are going to need the open 
telemetry metrics package. 
37:13 - So let's go ahead and install that.
Let's navigate back to our project first. 
37:18 - So don't do this in the 
directory we just downloaded. 
37:22 - And in here, type NPM II or 
install Open telemetry metrics. 
37:29 - Great, we are now ready to initialize 
a meter and collect metrics. 
37:34 - We first need a meter to create and monitor 
metrics a meter and open telemetry is the  
37:41 - mechanism used to create and manage 
metrics, labels, unmetric exporters,  
37:48 - create a file named monitoring j s.
So a JavaScript file and the root of your folder  
37:54 - and add the following code.
So we're going to need the  
37:58 - open telemetry metrics package for this Kant's.
And we're gonna get the meter provider from it  
38:05 - and buy it I mean, the open telemetry metrics 
package, we are then going to make a new  
38:12 - console constant meter.
And we're going to use the  
38:16 - meter provider we're gonna make a 
new meter provider and use get meter,  
38:21 - as well as I'm just gonna put your meter name 
for now, we can change this whenever we want. 
38:27 - Now we can require this file from your 
application code and use the meter  
38:32 - to create and manage metrics.
The simplest of these metrics is a counter. 
38:39 - In this next part, we're going to create 
an export from our monitoring js file,  
38:44 - a middleware function that express can 
use to count all requests made by route. 
38:52 - So first off, we need to 
modify IO monitoring j s file. 
38:57 - So once again, I'm just going to copy this 
sample code from open telemetry open source  
39:02 - projects in order to help us count the requests 
and paste that into my monitoring j s file. 
39:12 - Next, we need to import and use this 
middleware in our application code. 
39:16 - So our app js file.
So we need to get count  
39:21 - all requests from our monitoring js file.
So the module export, we do so by typing  
39:28 - const count all requests require monitoring j s.
So literally, we are using this right here. 
39:39 - Now let's get to using it.
Type app use count all requests and call it now  
39:46 - when you make requests your service you 
will meter will count all the requests. 
39:51 - Perfect.
Next up, let's  
39:54 - look at initializing and registering a metrics 
exporter counter metrics are only useful if you  
40:01 - can export them somewhere where you can see that.
For this we're going to use Prometheus is creating  
40:08 - and registering a metrics exporter is 
much like the tracing exporter above fast,  
40:13 - you need to install the Prometheus 
exporter by running the following command. 
40:20 - So I'm just going to use npm install 
and open telemetry exporter permit this  
40:27 - next step, we need to add some more 
code to our monitoring js file. 
40:31 - So once again, I'm going to copy the code 
given to us by opentelemetry To get started,  
40:37 - and paste it into my monitoring js file.
Don't worry, I will share this repo with  
40:42 - all of you so that if you do get stuck, 
you can refer to my finished project. 
40:48 - Now in a separate tab, so just leave 
this running and show Prometheus is  
40:53 - running by running the Prometheus binary 
from earlier and start your application. 
40:59 - We do so by using the script we wrote.
So NPM start, you should see promethease  
41:06 - scrape endpoint and the HTTP localhost 
94644 slash metrics, as well as  
41:14 - listening for requests on localhost 8080.
Now, each time you browse to localhost 8080,  
41:22 - you should see Hello, in your browser and 
your metrics, and Prometheus should update,  
41:28 - you can verify the current metrics 
by browsing to localhost 9464,  
41:33 - forward slash metrics, which should look 
like this, you should also be able to see  
41:40 - the gathered metrics in your Prometheus web UI, 
we can also add more routes in our app js file. 
41:47 - Let's go ahead and do that to 
see what that would look like. 
41:51 - So I'm just going to add 
some pre written code here. 
41:54 - And that is a middle tier route, and 
another route that has the pot back end. 
42:00 - So now we have our date, homepage from the 
previous section, our back end route now,  
42:08 - and a middle tier route, as 
well as a new homepage route. 
42:13 - I'm also going to need axios for this.
So another package that will  
42:17 - help me in making these requests.
So let's go ahead and import that into my project. 
42:23 - And nice that is done.
Let's run NPM start. 
42:28 - Okay, now let's check everything 
is working as expected. 
42:34 - The home page now responds with Hello backend. 
42:36 - This is actually because we 
have two homepage routes. 
42:39 - So I'm just going to get rid 
of the other one in a bit. 
42:42 - The backend route responds with Hello back 
end, the date route responds with today's date. 
42:48 - So that looks good.
So I'm just going to  
42:52 - delete the initial homepage route that we had that 
responds with hello world and keep the new one. 
42:59 - Okay, and now let's visit middle tier.
And we get response of Hello backend. 
43:05 - And finally, let's visit matrix where we get a 
request counter of all the routes we have visited. 
43:12 - Okay.
So this  
43:16 - looks like it's all working, we visited all the 
routes, and I counter seems to be working fine. 
43:21 - Now let's go ahead and see 
that in the permittees UI. 
43:25 - So we're going to have to 
pick something to execute. 
43:28 - And there we go.
Now, before we move on to our project,  
43:43 - I thought let's take a little bit of time to 
understand exactly what issues can be detected. 
43:50 - With open telemetry.
Here's just a list  
43:53 - that I'm going to go through with you.
Starting with the backend. 
43:58 - In the backend with open telemetry you can pick 
up bad logic or user input, leading to exceptions  
44:04 - being thrown, poorly implemented downstream calls.
So for example to infrastructures like databases  
44:12 - or downstream API's, leading to 
exceptionally long response times. 
44:17 - Or you can pick up poorly performant code on a 
single API, leading to exceptional response times. 
44:26 - On the front end, with open telemetry, 
you can detect bad logic or user input  
44:32 - leading to JavaScript errors.
You can also use it to find  
44:36 - poorly implemented JavaScript, making your UI 
prohibitively slow, despite performant API's. 
44:44 - And you can even use it to locate geo 
specific slowness requiring geo distribution. 
44:52 - And finally, for infrastructure, you can use it to 
identify noisy neighbors running on a host sapping  
44:59 - resource from other apps, configuration changes, 
leading to performance degeneration version audit. 
45:07 - So zero day vulnerability checks, 
ensuring convict changes went through,  
45:12 - or just miss figuration with your 
DNS making your apps inaccessible. 
45:18 - So that is a list of you thinking of issues 
that you can detect with open telemetry. 
45:24 - Now that we have that covered, 
let's move on to our project. 
45:32 - And this part of the course, I want to show 
you what happens when you are building out  
45:37 - an app with a more complicated back 
end that deals with two services. 
45:43 - It is a hypothetical project that you can adapt 
to anything that you wish, it is an app that gets  
45:50 - movies for your database.
By the end of the project,  
45:54 - you will be able to trace exactly how we 
got the movies, and how long each step took. 
46:01 - Okay, so here is a project I have pre made, 
thanks to the open source community and inspired  
46:08 - by an open telemetry contributor, Alan storm.
It is a project that has two services with one  
46:15 - service relying on the other, 
but not the other way around. 
46:20 - In this project, I have a main dashboard 
service, as well as the movies service,  
46:26 - which will return all the movies for our app.
The layout of this project is similar to what  
46:32 - we did in the tracing setup.
However, instead of having a  
46:37 - separate tracing js file to trace each 
service, the code is directly in each file. 
46:46 - So as you can see, each service 
is in the root of our project. 
46:52 - And just going to minimize this so we 
can see the code a little better now. 
46:58 - So from the beginning of our 
course, this should look familiar. 
47:02 - So as a reminder, open telemetry requires that 
we instantiate a trace provider, configure  
47:10 - that trace provider with an exporter 
and install Open telemetry plugins to  
47:16 - instrument specific node modules.
Let's talk through the code. 
47:21 - So we can see here as a refresher, especially 
as it's organized in a different way than  
47:27 - what we saw in the basic implementation.
So first, we are going to get the node trace  
47:35 - provider from the open telemetry node package.
A trace provider is what will help us  
47:41 - create traces on no Jess.
Next, we will get the console  
47:46 - span exporter and the simple span processor 
from the open telemetry tracing package. 
47:53 - And then we need to get the zipkin exporter 
from the open telemetry exporter zipkin package. 
48:00 - Now that we have what is necessary, let's move on. 
48:05 - So at the moment, we have made a tracing program.
And to actually generate the spans if you  
48:12 - remember, we installed a plugin called 
Open telemetry plugin dash HTTP,  
48:19 - the node a trace provider object is smart enough 
to notice the presence of a plugin and load it. 
48:26 - This code creates a trace provider 
and adds a span of processor to it. 
48:32 - The trace processor requires an exporter.
So we instantiate that as well. 
48:39 - Both are responsible for getting the telemetry 
data out of your service and into another system. 
48:47 - With this code, we create an exporter then 
use that exporter when creating a span  
48:53 - processor, and then added that spam 
processor to our trace provider. 
48:59 - Okay, so that is what that code does.
Let's actually name our service. 
49:03 - Now, I have left this blank.
So let's go ahead and fill that in. 
49:08 - As this service is going to 
deal with the main service,  
49:12 - I'm just going to call it dashboard service.
Here we're instantiating, a zipkin. 
49:18 - exporter, and then adding 
it to the trace provider. 
49:23 - We of course need to get Express 
from the package Express. 
49:27 - And so a server and listen out on 
port 3000 on one four connections. 
49:34 - The app is currently not going to respond with  
49:36 - anything for requests any part as 
we haven't written anything yet. 
49:41 - I wanted to respond with the dashboard 
itself and the movies from the movie service. 
49:48 - But before we do that, we need 
to build out our movies js file. 
49:53 - So this file is exactly the same as the other file 
just perhaps with some code in different places. 
50:00 - uses a different port.
In this file, I want to deal with the movies. 
50:06 - So I'm just going to rename this 
service name to movies service. 
50:12 - If I ran this service, we would 
be listening out to Port 3000. 
50:17 - Now, I'm going to determine how our app responds 
to a get request to the movies endpoint. 
50:24 - So I am just going to write up, get, and 
then I'm simply going to put the movies path. 
50:30 - So I'm making this up.
This is an async function, and I'm going  
50:35 - to pass through a request and response.
Okay, so there we go. 
50:43 - And then I'm simply going to 
type rest type Jason as a string. 
50:51 - And response, send Jason stringify. 
50:55 - And I'm just going to send a movie object 
with, let's say, an array of movies. 
51:02 - So let's put some objects in our array, I'm 
going to make the array have movie objects. 
51:08 - And each movie object is going to have a name.
So a name like Jaws, a genre. 
51:16 - So for example, jaws is a thriller as 
a string, and that is my first object. 
51:23 - And then I'm just going to 
make a quick other object. 
51:26 - This time, let's put a different type of film.
So I'm just going to put the string of Ani,  
51:32 - and once again, let's put a genre.
So I'm just making the same movie  
51:37 - object just with a different name and genre.
I'm going to put family and make another object. 
51:44 - I'm going to stop after this one, because 
this is just for illustration purposes. 
51:48 - And let's put Jurassic Park as the 
name and let's put action as the genre. 
51:59 - Okay, that is it.
That's our array of three movie objects. 
52:03 - Okay, now let's run our app.
So let's actually go to  
52:09 - localhost 3000, and put the movies path.
So what I'm doing is I am requesting the URL. 
52:17 - And of course, our app is going to 
listen out for it and make a trace. 
52:24 - So now let's go over to the zipkin 
UI and search for recent traces,  
52:29 - we will see we recorded a single service trace.
However, if we look through into the trace  
52:37 - details, we'll see these traces do not 
look like the traces we've previously seen. 
52:42 - In our previous examples, 
one span equaled one service. 
52:46 - However, a span is just a span of time 
that's related to other spans of time,  
52:51 - we can use a span to measure any 
individual part of our service as well. 
52:56 - The Express auto instrumentation plugin creates 
spans that measure actions within the Express  
53:02 - framework, we can use it to find out how long 
each middleware took to execute, how long your  
53:08 - Express handler took to execute, and so on.
It gives you insights not just into what's  
53:15 - going on with the service as a whole, but 
also individual parts of your Express system. 
53:21 - This is the role most of the contract 
plugins play in the open telemetry project. 
53:27 - The core plugins are concerned with ensuring 
each request is bound to a single trace. 
53:32 - But the contract plugins will create spans  
53:35 - specific to the behavior 
of a particular framework. 
53:41 - Okay, great.
Let's carry on. 
53:44 - So now that we've done that, I want 
to show you how to use open telemetry  
53:48 - to instrument a distributed system.
This is what our dashboard js file  
53:53 - is for is essentially want my dashboard js 
file to call the movies service as well. 
54:00 - So let's get to writing that code.
The first thing I am going to do is  
54:06 - actually use the node fetch library 
which we haven't installed yet. 
54:11 - So this service, use the node fetch 
library to call our movies service. 
54:16 - Let's go ahead and install that.
So I'm just going to get my terminal  
54:20 - and type NPM I for install and node batch.
Okay, now, once again, I'm going to have to  
54:28 - type apt get, and then use the route of dashboard 
and then write in async function, so a sick  
54:37 - function and pass through a request and response.
Now I am going to write here I need to fetch  
54:47 - data from a second service.
And that's the movie service. 
54:50 - I'm just gonna write some pseudocode to remind us 
of that, fetch data running from second service  
54:59 - and My second service is the movies service.
Okay, now, I am going to write a function that  
55:10 - will essentially help us get all the 
URL contact from the movies service. 
55:17 - So essentially our object with the three movies 
in an array or movie objects in an array. 
55:24 - So let's get to writing this 
function, I'm going to write this  
55:26 - function and pass through two parameters.
So whenever I pass through a URL into this  
55:34 - function and a fetch, so the fetch is 
actually going to use our node fetch library,  
55:42 - then I'm going to use these two parameters 
to essentially get the body of that URL. 
55:49 - So I'm going to use a promise 
to do this new promise,  
55:56 - I'm going to pass through resolve and reject.
And then I'm going to use fetch to fetch the URL. 
56:05 - And then whatever is in the body, I'm going 
to use so fetch URL, resolve reject, then  
56:16 - Russ, Russ text, then body.
So that is my function for  
56:25 - getting the URL content.
Let's get to using that. 
56:30 - Okay, I'm just going to actually change this.
I don't like the way this is written. 
56:36 - And I want to make it consistent with the bottom.
So I'm just going to change this up. 
56:42 - So it looks a bit neater is just a 
different way of writing functions. 
56:47 - So just so it's consistent to that.
Okay. 
56:54 - So once again, let's go down 
to the code I've pre written. 
56:58 - And in here, I'm going to fetch data 
running from the second service. 
57:02 - So the movies service, I am going to actually 
save the contents of the URL as movies. 
57:10 - So const movies, await and use the function 
that we pre wrote to pass through the URL. 
57:18 - So the URL we want to get the content of is HTTP  
57:22 - localhost 3004 slash movies.
So there we go, that is ju L. 
57:28 - It is the same URL that I have written here.
So URL U, r, l, that's what we have done. 
57:36 - And then I'm going to need to require node fetch.
So I'm going to look through right require  
57:44 - and the package node fetch, I'm going to put 
rest type Jason, and rez send Jason stringify. 
57:54 - And now I'm going to write the word dashboard.
Okay, so I'm making an object, and I'm gonna  
58:02 - write dashboard, and then whatever we've 
saved as movies should show up here. 
58:08 - So essentially, the contents 
of the URL will show up here. 
58:14 - Okay, so now I cannot run this file.
So I could run this file,  
58:19 - but we will see an error.
This is because our file relies  
58:24 - on the movies service being up and running.
So I'm just going to show you now I'm going  
58:29 - to type node dashboard, j.
s. 
58:32 - So it's listening at localhost 3001.
However, if I visit localhost 3001 for  
58:40 - slash dashboard, I get an error.
This is because we need our  
58:45 - movie service to be running.
So let's go ahead and make that true. 
58:53 - I'm just going to open up a new 
tab and type node movies j s. 
59:00 - Okay, that is not running and 
listening at Port localhost 3000. 
59:05 - So now, here's our movies.
And let's refresh  
59:08 - or let's rerun the dashboard.
So once again, node dashboard, j s. 
59:17 - And then refresh our page.
Amazing, we will see our  
59:21 - dashboard object with the contents of 
the URL from the Ford slash movies path. 
59:29 - Amazing.
So this is working fine. 
59:32 - Our code is working as it should.
Now, let's see how this looks in our zipkin UI. 
59:40 - So I'm just going to rerun the 
query and look at our latest. 
59:44 - And there we go.
So once again, to reiterate that,  
59:49 - in a nutshell, the dashboard service is 
dependent on a movie service to populate it. 
59:55 - This is true for many apps 
you interact with today,  
59:58 - and which is why this example is the one 
I wanted to show you for our project. 
60:03 - Now, we can see here that each span 
from the services are linked together. 
60:08 - The opentelemetry HTTP plugin took care of 
this for us, the node fetch plugin use the  
60:15 - underlying functionality of node j, 
s, HTTP and HTTPS to make requests. 
60:23 - So that's how to instrument an 
application using open telemetry. 
60:28 - Like this is pretty cool, as obviously, 
you can see our dashboard service,  
60:33 - and then you can see exactly what time it's going 
to the get movies, service, and then coming back. 
60:42 - Okay, that brings us to the end of our project.
I hope you've enjoyed this section. 
60:47 - This is, of course, just the surface 
of what you can do the opentelemetry. 
60:52 - There is a lot more to it.
But until you get the fundamentals,  
60:56 - I sincerely hope you can go through 
this course again and again until you  
61:00 - feel more comfortable building your own projects. 
61:09 - So far, in this project, we have directly gotten 
data from an app and send the data to zipkin. 
61:15 - But what happens if we want to try sending it to 
another back end to process our telemetry data?  
61:20 - Does that mean we would have to 
re instrument our whole app? Well,  
61:24 - the amazing contributors to open telemetry 
have come up with a solution to fix this. 
61:29 - The open telemetry collector is 
a way for developers to receive  
61:33 - process and export telemetry 
data to multiple backends. 
61:37 - It supports open source observability data formats 
like zipkin, Jaeger permittees, or flume bit,  
61:45 - sending it to one or more open 
source or commercial backends. 
61:50 - In this next section, I'm going 
to show you how to use it. 
61:56 - Okay, so for this section, and using the open 
telemetry collector, we're going to be using New  
62:02 - Relic as our observability tool of choice.
All I'm going to do is head over  
62:08 - to New Relic and sign up.
Next, you'll see some questions,  
62:13 - please answer these to the best of your ability.
So for example, where you store your data, and  
62:18 - just click save, your account will then be set up.
Once you get to this page right here, I'm just  
62:25 - going to ask you to not interact with anything 
for now, and head over to one dot new relic.com. 
62:35 - Once here, I'm going to ask you to go on the 
drop down of your profile and click API keys. 
62:41 - Once here, I'm going to ask 
you to gravitate to creating a  
62:44 - new queue and just select ingest license.
I'm going to name this otol example. 
62:51 - And I'm just gonna give it some notes 
just so we can keep track of our API keys. 
62:56 - And great, our API key is now created.
Let's copy it and move on. 
63:05 - The next thing we're going to do is 
actually get our open telemetry collector. 
63:10 - For this, I'm going to head 
over to New Relic GitHub account  
63:13 - in which I can get the open telemetry examples.
So I'm just going to ask you to clone this repo  
63:19 - into your local machine.
I have already done this. 
63:23 - So I'm just going to go ahead 
and head over to that repo now. 
63:28 - And here it is.
Once here, I'm going to ask  
63:31 - you to navigate to the collectors and our exporter 
Docker otol config yamo file, because we're gonna  
63:38 - have to change this up a little bit.
So please do head over here now. 
63:43 - And I'm just going to ask you 
to add a little line of code. 
63:47 - This line of code will add zipkin as 
a receiver, a receiver is aware that  
63:51 - data gets into the open telemetry collector.
So because we already have our app configured  
63:56 - to use zipkin, we will be telling 
the open telemetry collector Hey,  
64:01 - we will be sending you data in the form of zipkin.
So that is what is happening, we are adding  
64:06 - zipkin as a receiver and then giving it an 
endpoint, which in this case is 0.0 dot 09411. 
64:17 - Now because zipkin report tracing data, we 
are going to add zipkin as a tracing receiver  
64:23 - underneath service.
And that's it. 
64:27 - The next thing we need to do is go 
over to the Docker compose yamo file,  
64:32 - and make sure that the Docker container 
that runs this open telemetry collector  
64:36 - is actually able to receive the data through the 
same port that it would have if it was zipkin. 
64:43 - So we're going to add the Port 
9411, just like so and save it. 
64:51 - Now, according to the readme to run this, 
we need to use the API key we just created. 
64:57 - So in my terminal, I'm just 
going To export the New Relic  
65:01 - API key I just created with this command.
Next, we need to spin up the Docker container with  
65:09 - this command, making sure of course 
that we are in the correct directory. 
65:14 - So that is my fault.
Let's go into that directory. 
65:17 - So the nr exporter Docker.
And once again, we run the Docker container. 
65:23 - and wonderful.
Now let's go back to our movies dashboard project  
65:28 - that we have been working for in this course.
So now I just need to modify our app ever  
65:35 - since Thirdly, to work with 
the open telemetry collector. 
65:39 - For this to work, we need to change two things, 
we need to change the URL that is reporting at. 
65:46 - So this one right here, I'm 
simply going to use it like so. 
65:51 - So we need to do is for the dashboard 
and also for the movies service. 
65:57 - And that's it.
Now, I'm just  
66:00 - going to reinstall all the dependencies for 
those who are just joining us here and I've  
66:04 - taken this project from the description below.
And then let's run the two services, like we have  
66:11 - been doing previously in this tutorial.
Wonderful. 
66:16 - Okay, now I'm just going to call the 
services by visiting the dashboard service. 
66:21 - As a reminder, the dashboard service 
relies on the movies service,  
66:25 - I'm going to call it multiple times, so we 
can get lots and lots of data to work with. 
66:30 - So maybe just a few more.
Okay, done. 
66:34 - Let's move on.
Now that we have our app and have successfully  
66:38 - instrumented the open telemetry collector 
that is forwarding our data to New Relic,  
66:43 - we should now be able to visualize our data.
For this, we need to go to the  
66:47 - Explorer tab on New Relic.
And once here, we will see the two services,  
66:53 - the dashboard service and the movies service, just 
like in the previous distributed tracing section. 
67:00 - Let's deep dive further.
As you can see in the  
67:04 - dashboard service, that was a spike.
Let's find out what was happening. 
67:09 - So it looks like there were 18 traces with nine 
spans and two entities for the dashboard service. 
67:16 - That sounds right.
Great. 
67:18 - And if we dig deeper, you will see 
the micro service that our dashboard  
67:22 - service was communicating with to 
actually solve the final result. 
67:27 - Wonderful.
As you can see, we can get a lot of awesome  
67:30 - data to do things such as get to the root cause 
analysis of what could be going wrong in your app,  
67:36 - check how your microservices are 
performing, and so, so much more. 
67:42 - Okay, and there we have it.
That was our open telemetry course,  
67:46 - before we finish, I just want to take a moment 
to recap what we have learned in this course. 
67:52 - So to recap, in this course, we learnt 
how to set up the back end for a project. 
67:58 - Then we learnt how to implement tracing into 
our project, as well as metrics if we need to. 
68:05 - And then we also looked at two 
services and how they communicate. 
68:08 - Thanks to distributed tracing.
I hope you now feel comfortable knowing the  
68:14 - benefits of using open telemetry, as well as have 
a good understanding of how you would go about  
68:19 - implementing it into your Node JS projects.
If you are wondering where to go next,  
68:24 - with your newfound knowledge, I would 
suggest learning about infrastructure  
68:28 - monitoring and digital experience monitoring.
These are two other ways to visualize, analyze  
68:33 - and troubleshoot your entire software stack.
I will leave you with this and a link to  
68:38 - New Relic To find out more and get a 
free account with no expiration date. 
68:44 - Thanks so much again for 
watching and I'll see you soon