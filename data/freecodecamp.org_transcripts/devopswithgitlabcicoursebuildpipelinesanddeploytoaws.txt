00:00 - in this course valentine will teach you
00:01 - how to use gitlab ci to build ci cd
00:04 - pipelines to build and deploy software
00:07 - to aws
00:09 - hello frequent campers and welcome to
00:11 - this course which will introduce you to
00:13 - gitlab ci and devops my name is
00:15 - valentine i'm a software developer and i
00:18 - like to share my passion for technology
00:20 - with others in a way that is easy to
00:22 - understand when i'm not speaking at a
00:24 - conference
00:25 - or traveling the world
00:27 - i like to share what i know by being
00:29 - active in groups and forums and creating
00:31 - tutorials on youtube or online courses
00:34 - in this course we will understand what
00:36 - gitlab ci is and why we need this tool
00:39 - and start building cicd pipelines during
00:42 - the course we'll create a pipeline that
00:44 - takes a simple website builds a
00:46 - container tests it and deploys it to the
00:48 - amazon web services cloud
00:50 - also called aws
00:52 - in other words we'll be focusing on
00:54 - automation i've created this course for
00:57 - people new to devops who want to use
00:59 - gitlab to build test and deploy their
01:01 - software don't worry if none of this
01:03 - makes sense right now if you're a
01:05 - beginner that is totally fine you don't
01:07 - need to install any tools or anything
01:09 - else also no coding knowledge is
01:12 - required but if you have some it is
01:14 - great it will help a bit i will explain
01:17 - to you everything you need to know step
01:19 - by step
01:20 - this course focuses on gitlab ci but the
01:23 - course notes are packed with resources i
01:25 - recommend exploring if unfamiliar with a
01:27 - specific topic go right now to the video
01:30 - description and open the course notes
01:32 - there you will find important resources
01:34 - and troubleshooting tips if something
01:36 - goes wrong
01:38 - i will also be publishing there any
01:40 - corrections additions and modifications
01:42 - yes this is a very dynamic industry and
01:45 - things change all the time so if
01:47 - something is not working
01:48 - first check the course notes i am a big
01:51 - fan of learning by doing and you will
01:53 - get hands-on experience building
01:55 - pipelines and deploying software to aws
01:58 - throughout the course i will give you
01:59 - assignments to practice what you have
02:01 - learned
02:02 - well in this course we'll be focusing on
02:04 - a specific technology and cloud provider
02:07 - what you're actually learning are the
02:09 - concepts around devops with the skills
02:12 - acquired in this course i'm sure you'll
02:14 - be able to start using gitlab ci for
02:16 - whatever you need in no time at all
02:19 - this is an action-packed course which
02:21 - i'm sure will keep you busy at least for
02:23 - a few days as always here on
02:25 - freecodecamp
02:27 - please help us make such courses
02:29 - available to you by liking and
02:30 - subscribing also don't forget to drop a
02:33 - comment below the video if you like this
02:35 - course i invite you to check out and
02:37 - subscribe to my youtube channel link in
02:39 - the video description which is packed
02:41 - with content around devops software
02:43 - development and testing
02:45 - also feel free to connect with me on
02:46 - social media i would really love to hear
02:48 - from you finally i would like to thank
02:51 - those who will support free code camp by
02:53 - clicking that thanks button and making a
02:55 - small donation i hope you're excited to
02:57 - learn more about gitlab ci aws and
03:00 - devops and with that being said let's
03:02 - get started
03:06 - i have designed this course to be as
03:08 - easy to follow along as possible you
03:11 - don't need to install any software on
03:13 - your computer and you should be able to
03:15 - do everything just from a browser i'll
03:18 - be using gitlab.com throughout the
03:20 - course if you don't have a gitlab.com
03:22 - account please go ahead and create one
03:26 - by default you will get a free trial
03:28 - with your account which will be
03:29 - downgraded to a free one after 30 days
03:34 - it just takes a few steps to create an
03:36 - account
03:41 - if you don't want to participate in the
03:43 - free trial that's totally fine there's
03:45 - also the possibility of skipping the
03:47 - trial altogether
03:49 - gitlab is a platform that offers git
03:51 - repositories where we store code
03:54 - and code pipelines which help us build
03:56 - software projects
03:58 - now that the registration process is
04:00 - completed let's begin by creating our
04:02 - first project
04:04 - and we're going to create a blank
04:06 - project
04:08 - and i'm going to call this project my
04:09 - first pipeline
04:11 - i have the option of providing a project
04:13 - subscription which is optional
04:15 - and i can also decide on the project
04:18 - visibility either private which means
04:20 - that only i or people who i explicitly
04:23 - grant access to can view this project or
04:26 - public which means that can be viewed by
04:29 - anyone without any authentication
04:33 - i'm not going to initialize this project
04:35 - with a readme file
04:37 - i'm going to simply go ahead and click
04:39 - create project
04:40 - this gitlab project allows us to store
04:43 - files and to use git to keep track of
04:46 - changes and also to collaborate with
04:48 - others on this project
04:50 - if the concepts around git are not clear
04:52 - check the course notes for a free course
04:55 - on getting started with git for gitlab
04:58 - since for this account i haven't
05:00 - configured git you will also get here
05:02 - this warning in regards to adding the
05:04 - ssh key this is also covered in the
05:07 - material i have mentioned but for the
05:09 - moment we don't need this so we can
05:11 - simply discard it
05:13 - the first thing that i like to do is to
05:15 - change a few settings in regards to how
05:17 - this interface looks like
05:19 - so from the user profile
05:21 - i will go to preferences
05:26 - and
05:28 - here from the syntax highlighting theme
05:31 - what i like to do is to select monokai
05:34 - so this is essentially a dark theme
05:37 - and as you probably know we like to use
05:39 - dark themes because light attracts bugs
05:42 - and we definitely don't want any bugs
05:45 - now leaving the joker side some people
05:47 - like it some people don't like it i
05:49 - prefer to use a dark theme when writing
05:52 - code but totally agree that depends on
05:54 - everyone's preference on how to use this
05:57 - there are also some other settings i
05:59 - want you to do right now in the
06:00 - beginning
06:02 - i'm going to scroll here a bit further
06:04 - down
06:05 - and the first thing that i want you to
06:07 - enable is render white space characters
06:10 - in web ide
06:12 - this will show us any white space
06:14 - characters whenever editing files that's
06:16 - super important okay we have everything
06:19 - that we need to do so i'm gonna go all
06:21 - the way to the bottom
06:23 - click on save changes
06:26 - and go back to gitlab
06:29 - you'll see here your projects so
06:32 - currently you have only one project i'm
06:34 - going to click on it
06:36 - and currently we have absolutely no code
06:38 - inside this project
06:41 - the first thing that i want to do is to
06:42 - begin to create a file so from this new
06:45 - gitlab project we're going to use the
06:47 - web ide to create the pipeline
06:50 - definition file i'm going to click here
06:52 - on new file
06:55 - this will open up the gitlab ide
06:58 - so i'm going to create a new file
07:01 - and we are already provided here with a
07:03 - template
07:04 - and this file name must be called dot
07:07 - gitlab dash ci dot yaml if it's not
07:11 - exactly this name if you're writing it
07:13 - on your own
07:14 - this pipeline will not be recognized so
07:16 - the pipeline find will not be recognized
07:18 - by gitlab this is a very common mistake
07:20 - that beginners make
07:22 - probably this is why if you click
07:23 - directly on this one
07:25 - you'll be pretty sure that you don't
07:27 - name this file like anything else now in
07:30 - this file we're gonna define the
07:32 - pipelines and essentially gonna
07:34 - write here configuration
07:37 - to create pipelines in gitlab say
07:39 - it's totally fine if you don't know what
07:41 - that is right now just want to create a
07:43 - very simple example to make sure that we
07:45 - have everything we need in order to
07:48 - follow along with the rest of the course
07:50 - so what i'm going to do here i'm going
07:51 - to write something like test
07:54 - column
07:55 - and then i'm going to go to the next
07:56 - line
07:58 - and you will see here everything is
08:00 - already indented
08:02 - so with this indentation with this four
08:04 - spaces that you can see right now here
08:06 - i'm gonna write script
08:08 - column
08:10 - space
08:11 - and then i'm going to use the echo
08:12 - command to display a text
08:18 - this is going to be the text that we
08:19 - want to display
08:21 - the echo command is used to display a
08:23 - text and we'll be able to see it later
08:25 - on and this dot gitlab ci that yaml file
08:29 - allows us to describe our pipeline
08:32 - as i said don't worry if this does not
08:34 - make any sense right now this is just a
08:37 - test to ensure we have everything we
08:38 - need to get started
08:40 - so now what we're gonna do is to
08:42 - actually commit these changes
08:45 - which means introducing this file inside
08:48 - the gitlab repository
08:53 - if we click again here on the project
08:55 - name we'll exit this view
08:58 - and you will see here
09:01 - that the pipeline failed so every time
09:04 - we make changes to this project the
09:05 - pipeline will be executed based what we
09:07 - have defined
09:09 - in this yaml file
09:10 - but you will see here right on top
09:14 - there's this indication that something
09:15 - is wrong
09:17 - and even here if you look inside the
09:19 - pipeline
09:20 - you will get this additional information
09:22 - so what is going on
09:25 - in order to run your pipeline using the
09:27 - gitlab.com infrastructure
09:29 - you need to verify your account now this
09:31 - is not the same as verifying your email
09:33 - address you already did that but you
09:35 - need to go an additional step of
09:37 - verification
09:39 - unfortunately some people decided to
09:41 - take advantage of this free service and
09:44 - have abused it for mining cryptocurrency
09:48 - for the time being you will be asked to
09:50 - verify your account using a credit card
09:53 - now your credit card will not be charged
09:56 - or stored by gitlab and it is used
09:59 - solely for verifying your account just
10:01 - to ensure that you are one of the good
10:03 - guys and i know that you're one of the
10:05 - good guys and i know that this is a bit
10:07 - annoying in the beginning
10:09 - but this is how things are right now
10:12 - i also know that credit cards are not
10:14 - that widespread in some countries and
10:17 - this may be inconvenient
10:19 - maybe you can ask a friend to help out
10:22 - i hope that gitlab will introduce
10:24 - alternative verification options
10:27 - nevertheless verifying your gitlab.com
10:30 - account and using the gitlab.com
10:32 - infrastructure is the easiest way to
10:34 - follow along with the course
10:36 - so if you can invest five minutes now
10:38 - and get this done
10:40 - it will save you hours later you can use
10:43 - your own infrastructure to run gitlab
10:46 - but it is more complex and from my
10:48 - experience of training thousands of
10:50 - students is that people new to gitlab
10:53 - who use their own infrastructure have
10:55 - issues running their pipelines and waste
10:58 - a lot of time trying to get them to run
11:01 - properly you have been warned but if you
11:03 - want to go this path i've added some
11:06 - resources to the course notes which you
11:08 - can find in the video description
11:10 - now in order to get started with the
11:12 - verification process you have to click
11:14 - here on validate account
11:16 - you will be asked to enter your credit
11:18 - card information
11:22 - i hope that the validation has been okay
11:24 - and in order to see if everything is
11:26 - working properly i'm gonna go back to
11:28 - the project and open the web ide
11:32 - i'm gonna click on the get lab ci
11:36 - file to make a change to it and i'm
11:38 - gonna change here the message so this is
11:39 - gonna be now hello world 2
11:43 - and going to go here commit
11:46 - so essentially making a new change to
11:48 - this file and commit it directly into
11:50 - the main branch
11:55 - and if i'm looking here at the bottom
11:57 - i should be able to see that something
11:59 - is happening so very soon a pipeline
12:01 - will be started
12:03 - and you will see here now pipeline with
12:06 - the specific number is running i can
12:08 - click on it
12:12 - and if you click here on this execution
12:14 - you'll be able to see the job logs
12:18 - and what we're interested in is firstly
12:21 - seeing this message here hello world
12:26 - and additionally what we're also
12:28 - interested in seeing here
12:30 - is
12:32 - seeing this text here which says pulling
12:34 - docker image now this is very important
12:37 - not for this job itself but what we are
12:40 - going to do throughout the course we
12:41 - want to make sure that whatever we have
12:44 - here in terms of the execution that we
12:46 - are actually using docker
12:49 - if this is working it's fantastic you
12:51 - can jump directly into the next lecture
12:53 - otherwise check the course notes for
12:56 - some troubleshooting ideas
13:03 - so what is a pipeline allow me to make
13:06 - an analogy to an assembly line used to
13:08 - manufacture a physical product
13:11 - any product goes through a series of
13:13 - steps
13:14 - let's take a laptop for example
13:17 - to oversimplify the assembly line would
13:20 - have the following steps
13:22 - we'll take an empty laptop body case
13:25 - we'll add the main board
13:27 - and add a keyboard
13:28 - we would do some quality assurance to
13:30 - ensure it turns on and works properly
13:33 - then we would put it in a box and
13:36 - finally ship it to customers
13:38 - and we are not using gitlab to produce
13:41 - physical products we want to build
13:43 - software
13:44 - but producing software has similarities
13:47 - to what i have described before
13:50 - before we can ship anything we go
13:52 - through a series of steps
13:55 - let's try to build the laptop assembly
13:58 - line in gitlab ci well kind of instead
14:02 - of real components we'll use a file a
14:05 - folder and some text
14:08 - so let's begin with the first task
14:10 - take an empty laptop case
14:13 - we'll put this in a job
14:15 - in gitlab a job is a set of commands we
14:18 - want to execute
14:20 - so now let's go back to our project and
14:22 - make some changes to it again i'm gonna
14:24 - open the web ide to be able to view the
14:26 - pipeline file now essentially we already
14:29 - have a job here but we're gonna expand
14:31 - this and make it
14:34 - build a laptop
14:36 - if you're facing any issues getting the
14:37 - following steps to run make sure to
14:39 - watch the next lesson where i'm going
14:42 - over some of the most common mistakes
14:45 - now we already have a job and this job
14:48 - is called test
14:49 - but probably we should go ahead and
14:51 - rename this to maybe build a laptop
14:55 - or we can also call it simply build
14:56 - laptop now in the script part this is
14:59 - where we can
15:00 - essentially write commands now so far we
15:03 - have used this echo command
15:05 - but the way we written this
15:08 - allows us only to write one command so
15:10 - what i'm going to do here i'm going to
15:11 - go on the next line
15:14 - and i'm going to start here with the
15:16 - dash
15:16 - and this will allow us to
15:19 - essentially write multiple commands one
15:22 - after the other
15:23 - so let's rename this in something like
15:26 - building a laptop just to give us some
15:28 - information in regards to what we're
15:29 - trying to do
15:31 - i want you to notice that after this
15:33 - dash
15:33 - i have added a space and you can see
15:36 - that space represented by a dot common
15:39 - mistake that people just getting started
15:41 - with gitlab
15:42 - and
15:43 - this language that you see here which is
15:45 - called yamo
15:46 - is that they don't add the spaces or
15:49 - they don't properly indent what they see
15:51 - so make sure that what you have inside
15:54 - your editor
15:56 - looks pretty much the same as what i
15:58 - have here and you're doing that pretty
16:01 - sure this example will also work for you
16:04 - now as i said the language that we're
16:05 - using here to describe our pipeline is
16:08 - called yaml
16:10 - and yaml is essentially a way to
16:12 - represent key value pairs
16:14 - it may look a bit weird in the beginning
16:17 - but after a few examples i'm sure you
16:19 - will get used to it now so far our
16:21 - problem doesn't do anything and inside
16:24 - this job we actually said we want to
16:26 - build this laptop
16:27 - and we also said that we're not going to
16:30 - use physical components we're going to
16:31 - use folders we're going to use files
16:33 - we're going to use some text
16:35 - so let's begin with the first command
16:37 - which will create a new folder
16:39 - now we want to put this into a folder
16:42 - which is called build so on the next
16:45 - line
16:45 - i'm gonna use a command that will create
16:48 - a folder this command is called make
16:50 - deer
16:51 - and i'm going to call this folder build
16:55 - so now we have a folder and let's go to
16:57 - the next line and actually create a file
17:00 - so i want my file to be inside this
17:04 - build folder
17:05 - and in order to create this file we're
17:08 - going to use the touch command now the
17:10 - touch command is generally used for
17:13 - modifying the timestamp of a file
17:15 - essentially you're touching it you're
17:17 - modifying it
17:18 - but it also has an interesting behavior
17:20 - which we will use here if the file does
17:23 - not exist it will create an empty file
17:26 - and initially for us that's perfectly
17:28 - fine so we're going to create this file
17:31 - inside the build folder which we created
17:33 - one step before and with the forward
17:36 - slash we go inside this build folder we
17:38 - can specify the file name which in this
17:40 - case will be computer.txt
17:43 - now the next question is how do we get
17:47 - some text inside this file
17:51 - and in order to do that we're going to
17:53 - use a command we have already used
17:55 - before and this is the echo command now
17:57 - echo if we don't do anything else like
18:00 - we did here
18:01 - it will just
18:03 - print this message and we'll see it in
18:05 - our build logs
18:08 - but we can also kind of redirect this
18:11 - and send it directly to a file and for
18:14 - that we're going to use an operator so
18:16 - let me show you what i mean by that i'm
18:18 - going to write here again echo
18:20 - and the first thing that we are going to
18:22 - do
18:23 - is we're going to add here the main
18:25 - board now let's see the file itself it's
18:28 - uh just containing the the laptop and
18:31 - then we're adding components to it so if
18:33 - you would keep this as it is right now
18:36 - it will just display this information
18:38 - just display mainboard but we actually
18:41 - want to get it inside this file that you
18:44 - see here so i'm simply going to go ahead
18:46 - and copy it
18:47 - and in order to get this text inside
18:50 - there we're going to use this operator
18:53 - it's actually greater than greater than
18:56 - and essentially this operator takes the
18:59 - output from one command so echo
19:02 - is one command
19:04 - and it will append it
19:06 - to a specified file so now we have
19:09 - specified here this computer.txt file
19:12 - which is inside the build folder
19:16 - so if you want to take a look and see
19:17 - exactly what is the contents of the file
19:20 - we can do that as well
19:22 - and for that we there are different
19:24 - commands that we can use one option
19:26 - would be to use the cat command
19:29 - and
19:30 - cat stands for concatenate and can be
19:33 - used for creating displaying or
19:35 - modifying the contents of files in this
19:38 - case we're using this hat command to
19:40 - view the contents of a file and again we
19:43 - have to specify the path to that file
19:46 - so just to make sure i don't make any
19:48 - mistakes and always go ahead and copy
19:51 - paste
19:52 - the value of the file the name of the
19:55 - file and of course we can also go ahead
19:58 - and add the other steps so which other
20:00 - steps did we had
20:02 - for example we wanted to add a keyboard
20:06 - i'm gonna add this again with a new
20:08 - command
20:10 - and
20:12 - i think that should be it's a way we
20:14 - have the main board we have the keyboard
20:16 - and of course we can also try this cat
20:19 - command once again at the end and this
20:21 - will give us an idea of how the job
20:23 - itself is working
20:25 - in the previous execution we haven't
20:27 - specified a docker image to use
20:30 - and by default it will download a ruby
20:33 - docker image which for our use case here
20:36 - doesn't really make a lot of sense so
20:39 - we're going to use a very simple docker
20:41 - image it's a linux distribution that
20:44 - will be started with our job
20:46 - and we can do that by specifying here
20:49 - another keyword
20:51 - and you will see here it's under the job
20:53 - itself which is build laptop but at the
20:56 - same level with the script so i'm going
20:58 - to write here image
21:00 - column and the image name will be alpine
21:04 - so alpine linux is a very lightweight
21:08 - linux distribution what's most important
21:10 - for us that this distribution has these
21:12 - commands that we're using so these are
21:15 - pretty standard commands that are
21:16 - available essentially in any linux
21:18 - distribution
21:19 - but having a very small linux
21:21 - distribution will make our job run much
21:23 - faster
21:24 - so let's go ahead commit these changes
21:27 - and see how the pipeline runs
21:29 - i'm going to commit again to the main
21:30 - branch and if i'm in patient i can click
21:33 - directly here on the pipeline
21:36 - or i can also go directly on the project
21:38 - page
21:40 - and you will only see here the pipeline
21:42 - running
21:43 - pipeline is also available if you go
21:45 - here inside cicd pipelines
21:49 - it will display your list of all the
21:51 - pipelines that are running
21:53 - you can click here on this one which is
21:55 - the id of the pipeline you will see now
21:57 - our pipeline contains a job
22:01 - and this job has been assigned to the
22:04 - stage test
22:06 - by default if no stage is defined the
22:09 - job will use the test stage
22:12 - now it doesn't really matter how the
22:13 - name of the stage has been called
22:16 - we just want to make sure that the
22:18 - commands that we have executed are
22:20 - actually working properly we're getting
22:21 - here no errors so the job is running
22:25 - so the job has executed successfully we
22:27 - can click on it take a look at the
22:29 - execution logs
22:31 - see exactly what has happened here
22:33 - and you will see here in the logs the
22:35 - commands that we have executed they're
22:37 - all visible here you will see here echo
22:41 - building laptop you will see the text
22:44 - being displayed
22:45 - after that we are creating a new folder
22:49 - we're putting a file inside that folder
22:51 - we're adding some text to that file
22:54 - then we're checking the contents of the
22:56 - file to make sure everything is fine you
22:58 - see here the word mainboard being
23:01 - displayed
23:02 - and then we add the keyboard
23:05 - and then again you will see here the
23:06 - contents of the file containing both
23:09 - main board and keyboard
23:16 - so what is a pipeline it is a set of
23:18 - jobs organized in stages
23:21 - right now we have a single job that
23:24 - belongs to the test stage however in the
23:26 - upcoming lectures we'll be expanding
23:28 - this pipeline
23:36 - it is quite common that when you are
23:37 - just getting started with defining these
23:39 - pipelines that you make some mistakes
23:41 - let me show you like some common
23:43 - mistakes that will lead to invalid
23:46 - configuration which will lead to git lab
23:49 - not running your pipeline
23:51 - i'm going to make this a bit bigger on
23:52 - the screen so that you can easily check
23:54 - and compare it with what you have
23:57 - it's very important that in some places
24:00 - you have this column so for example here
24:02 - i'm defining the job
24:04 - and in order to add image and scripts as
24:08 - essentially properties of this job it's
24:10 - important to have here this column
24:13 - if i'm removing that you will see here
24:15 - in the editor when you see these lines
24:17 - here it will indicate
24:19 - that something is wrong most of the time
24:22 - these messages are really not so easy to
24:24 - understand so it's more important to
24:27 - double check what you have written here
24:29 - to make sure it is exactly as it should
24:31 - be
24:33 - it's also important that you have some
24:34 - spaces where there are spaces expected
24:38 - so for example
24:40 - here with the commands and whenever
24:42 - you're writing echo or make deer or
24:44 - touch there needs to be a space between
24:47 - the dash and the command this is why in
24:49 - the beginning i've asked you to enable
24:52 - this white spaces so that you can easily
24:54 - see them in your script so if i write
24:56 - something like this
24:57 - this again
24:59 - will make something weird so it will not
25:02 - show here an error because this is
25:04 - actually
25:06 - valid yamo
25:07 - but
25:08 - when gitlab will try to execute this
25:11 - pipeline and we'll look at these
25:13 - commands we'll think that you're trying
25:15 - to run a command that starts with dash
25:17 - echo
25:18 - and we'll say i cannot find this command
25:21 - so for that reason you need a space here
25:23 - to make a difference between
25:26 - this dash here which indicates a list
25:28 - and this command
25:30 - what's also important is the indentation
25:32 - as you can notice here there are four
25:34 - spaces
25:35 - so everything that is under build laptop
25:39 - is indented with the level if i add
25:42 - something like this here
25:43 - it it will no longer belong to build
25:46 - laptop and will come out as something
25:48 - that's really weird and most likely it
25:50 - will be invalid so always make sure that
25:52 - you have the right indentation uh two
25:54 - spaces are also fine by default this web
25:57 - ide will use four spaces
25:59 - just make sure you have that indentation
26:01 - there in place
26:03 - in the course notes you will find even
26:05 - more examples of why your pipeline may
26:08 - be failing including some common error
26:11 - codes that you may encounter
26:13 - so definitely check the course notes if
26:15 - you're still having issues getting this
26:17 - pipeline to run
26:24 - in this lesson we'll try to understand
26:26 - what yaml is if you already know yaml
26:29 - feel free to skip this as i will not be
26:31 - covering any advanced features
26:34 - the most important reason why you need
26:35 - to know some yaml basics is because
26:38 - you'll be facing many errors while
26:41 - writing yaml
26:43 - and as i mentioned in the beginning this
26:45 - is normal trust me i also did the same
26:48 - mistakes that you did
26:50 - and sometimes i'm still making mistakes
26:52 - while writing yaml if i'm not paying
26:54 - attention if you've already been exposed
26:56 - to formats such as json or xml
26:59 - i'm sure you'll be able to understand
27:01 - the basics around yaml very easily
27:04 - actually yaml is a superset of json and
27:07 - you can easily convert json to yaml
27:11 - both xml json and yaml
27:14 - are human readable data interchange
27:17 - formats
27:18 - while json is very easy to
27:20 - generate and parse
27:22 - typically yaml is considered easier to
27:25 - read
27:26 - as you will see not so easy to write but
27:28 - we'll get to that
27:30 - quite often yaml is being used for
27:33 - storing configuration
27:35 - especially if you're learning devops you
27:36 - probably face yaml a lot and this is
27:39 - exactly why we're using it in gitlab as
27:42 - well to store a pipeline configuration
27:44 - at its core yaml follows key value
27:47 - storage principles and let me open up
27:50 - here inside the editor a new file and
27:53 - i'm gonna start writing some yaml basics
27:56 - so i'm gonna create here file i'm gonna
27:58 - call it test.yaml
28:02 - and let's start with a few basics so for
28:03 - example how do we represent a key value
28:07 - pair so for example i have here a name
28:10 - typically would write something like the
28:12 - name is john right
28:15 - so this is something that anyone would
28:17 - understand now in yaml we use a column
28:20 - to separate that
28:22 - and so i'm going to add here name column
28:25 - john
28:26 - and what's also more important that we
28:28 - have a space after this column so it
28:31 - will be like this and you will see here
28:33 - the color here will also change so now
28:35 - we have a key value pair the key is name
28:38 - and the value is john
28:40 - we also call this a mapping we're
28:43 - mapping a key with a value
28:46 - of course on a new line we can start
28:48 - adding an additional key value pair for
28:51 - example let's say here h
28:54 - 23
28:56 - and what's important to know is that the
28:58 - order of the properties does not matter
29:01 - so we can define them in any order so if
29:03 - i write
29:04 - first name john and after that h or the
29:07 - other way around
29:09 - that's actually the same both are
29:12 - defined here in this yamo
29:15 - quite often we need to define lists
29:17 - which in yaml are called sequences and
29:20 - we do that by writing each value on a
29:23 - new line
29:24 - so for example let's
29:26 - write here some hobbies call them sports
29:31 - youtube
29:33 - and hiking
29:35 - right
29:36 - and what you also do is we put them each
29:39 - of them on a new line but each line must
29:42 - start with a dash and a space so we have
29:45 - here a dash and a space
29:48 - a dash and a space
29:50 - dash and a space
29:52 - now we have a list
29:55 - now the way i've written this doesn't
29:57 - really work like that so if we remove
29:59 - everything from here this will be
30:01 - probably a valid list
30:03 - but we cannot simply just combine this
30:06 - properties and now just add this list
30:08 - here in between
30:09 - what's important to know about yaml is
30:11 - that it uses indentation for scope
30:14 - essentially it
30:16 - allows you to know which value belongs
30:18 - to which key
30:20 - because it has the proper indentation so
30:23 - for example now we know this is not
30:24 - valid here
30:26 - but if you write here for example
30:28 - hobbies
30:30 - as a key
30:32 - then this list here
30:35 - will belong to this key
30:37 - so essentially the mapping will contain
30:39 - a key and this list of values
30:42 - well this is valid we also like to
30:45 - indent this i'm going to select
30:46 - everything and click on tab
30:49 - and this will indent all these values
30:51 - here
30:52 - additionally we can have some nested
30:54 - structures so for example if i'm trying
30:56 - to write here an address
31:00 - so this will be the key and then on a
31:03 - new line i can write additional key
31:06 - value pairs which all belong to the
31:10 - address and you will see here i have the
31:11 - indentation so i can write something
31:13 - like street
31:15 - and on a new line i can go ahead and
31:17 - write a city
31:19 - and again on a new line i'm gonna write
31:21 - here the zip code
31:24 - and
31:25 - essentially by using this indentation we
31:27 - show that street city and zip they all
31:30 - belong to address
31:31 - if we didn't have this indentation and
31:33 - it would all be a property of something
31:36 - else
31:37 - this is why in this case we're using
31:39 - here name age hobbies and address these
31:41 - are like
31:42 - properties on the first level
31:45 - and
31:46 - street city and zip they are under
31:49 - address
31:51 - in terms of lists we can also build some
31:54 - more advanced lists so this is a very
31:56 - simple list that we've used for hobbies
31:58 - but let's say for example that we want
32:00 - to add here a new key which is called
32:03 - experience
32:05 - and here uh for the experience we're
32:08 - gonna create a list
32:10 - and
32:11 - we just wanna say like what is the
32:13 - professional experience of this person
32:15 - for example we can have here job title
32:19 - that could be junior developer
32:21 - and
32:22 - instead of creating here a new item in
32:25 - the list
32:26 - we just go to the same level as with
32:28 - title
32:29 - then we can start writing something like
32:31 - period
32:32 - so it will indicate in which period
32:35 - this person has been a junior developer
32:37 - let's say from 2000 to 2005.
32:42 - and then we can go ahead and add a new
32:45 - item to the list
32:50 - again with title
32:52 - let's say now this person is a senior
32:54 - developer and we can also define a
32:57 - different period
32:59 - so this will be
33:01 - since 2005.
33:04 - now additionally what we can do is
33:06 - we can take everything that we have here
33:11 - and
33:12 - how about we define here let's say this
33:15 - is a person
33:16 - right
33:18 - and then we can take every everything
33:20 - that we have here and indent it once
33:22 - and then all these properties that we
33:24 - have here will belong to a person now
33:27 - you can see here that we are still
33:29 - have some mistakes here so we can
33:32 - we need to fix something because this
33:34 - editor will tell us this is a bad
33:36 - indentation and as you learn the
33:38 - indentation is very very important so
33:41 - i'm going to add here to spaces
33:44 - and add here again to spaces
33:46 - and now this indentation will be correct
33:49 - now when we are writing gitlab ci
33:51 - pipelines we don't really get to choose
33:54 - which data structures to use
33:56 - we have the liberty of designing the
33:58 - pipeline but we need to do it in a way
34:01 - that gitlab will understand it
34:05 - so if we're looking here at our pipeline
34:07 - we can just decide to use something else
34:10 - so for example instead of image we
34:12 - cannot just decide we're going to call
34:14 - it docker image or docker or instead of
34:17 - scripts we cannot write here scripts
34:19 - because this is something that gitlab
34:22 - will not understand yes from a yaml
34:24 - perspective this is still valid but from
34:28 - the perspective of gitlab this is
34:30 - something that's not according to
34:32 - what we agreed essentially in advance
34:35 - when we started writing these pipelines
34:38 - so it's just important that you
34:39 - understand like what we're doing here
34:41 - and how this indentation and how this
34:44 - key value pairs are being written
34:46 - but when we're writing jobs we we decide
34:50 - how this job will be named we decide
34:52 - what exactly will happen inside it some
34:54 - keys will be reserved and simply cannot
34:56 - be renamed you just have to pay
34:58 - attention that what you're writing here
35:00 - is really important and it has to be
35:02 - exactly at least exactly as the examples
35:05 - that i'm showing you otherwise gitlab
35:07 - will not be able to understand what
35:09 - you're seeing
35:16 - throughout the course we'll be using
35:17 - linux and linux commands
35:20 - we have already learned some commands
35:22 - such as echo touch make deer cat and so
35:26 - on
35:27 - we typically use these commands through
35:29 - a command line interface or cli what you
35:32 - see here
35:33 - for example i can go ahead and create
35:35 - here a new folder
35:37 - i can switch inside that folder
35:39 - we'll see here for example with ls i
35:42 - will be able to list the contents of
35:44 - that folder
35:45 - let's go ahead and write here touch this
35:48 - touch command and create here file name
35:50 - computer
35:52 - and again with ls i can see which files
35:55 - are inside the current folder so we
35:57 - typically type these commands that you
35:59 - have seen in the pipeline through this
36:00 - command line interface or cli
36:03 - sometimes we call it a console a
36:05 - terminal or a shell
36:07 - while technically speaking they are not
36:09 - the same thing you may notice me and
36:12 - others use them interchangeably
36:15 - a command line interface is the opposite
36:18 - of a graphical interface
36:21 - this is a graphical interface you will
36:23 - be able to see text colors you have
36:26 - buttons that you can click
36:28 - things are happening through your
36:30 - interaction you're moving your mouse
36:32 - and you're clicking or something or
36:34 - you're using your keyboard and something
36:36 - changes on the screen
36:38 - this is a command line interface
36:42 - we will work with the command line
36:44 - interface to write commands
36:46 - computers we interact with have no user
36:50 - interface that we can use
36:52 - anyway automating graphical user
36:55 - interfaces is not as easy and as
36:58 - reliable as simply using commands
37:01 - the command line interface that
37:02 - computers have is called the shell
37:05 - the shell is simply the outer layer of
37:08 - the system
37:09 - the only thing that we can see from
37:11 - outside and interact with
37:13 - so we send these commands to the system
37:15 - and the system will run them this is how
37:17 - we can interact with it
37:19 - when using gitlab ci we essentially
37:22 - automate a set of commands which we
37:25 - execute in a particular order
37:27 - while i will explain every command that
37:30 - we'll be using throughout the course
37:32 - if this is something new there's
37:34 - absolutely no replacement for trying
37:36 - things on your own
37:38 - please see the resources in the course
37:40 - notes for this lesson on setting up a
37:42 - linux environment on your own computer
37:45 - and which linux commands you should know
37:53 - let's talk for a minute about the gitlab
37:55 - architecture
37:56 - at the minimum the gitlab architecture
37:58 - for working with pipelines contains the
38:01 - gitlab server
38:03 - and at least one gitlab runner
38:07 - the gitlab server manages the execution
38:09 - of the pipeline and its jobs and stores
38:12 - the results
38:14 - the gitlab server knows what needs to be
38:16 - done but does not do this itself
38:19 - when a job needs to be executed it will
38:22 - find a runner to run the job
38:25 - a runner is a simple program that
38:27 - executes the job
38:29 - a working gitlab setup must have at
38:32 - least one runner but quite often there
38:35 - are more of them to help distribute the
38:37 - load
38:39 - a runner will retrieve a set of
38:40 - instructions from the gitlab server
38:43 - download and start the docker image
38:46 - specified
38:47 - get the files from the project git
38:49 - repository
38:51 - run all the commands specified in the
38:53 - job and report back the result of the
38:56 - execution of the gitlab server
38:58 - once the job has finished
39:00 - the docker container will be destroyed
39:03 - if docker is something new to you check
39:05 - the course notes for a quick
39:06 - introduction
39:09 - what's important to know is that the git
39:11 - repository of the project will not
39:13 - contain any of the files created during
39:16 - the job execution
39:19 - let's go back to our project you will
39:21 - see here that inside the git repository
39:23 - there's no build folder there's no
39:25 - computer.txt
39:28 - so what exactly is happening let's go
39:30 - inside one of the jobs and quickly go
39:33 - through the log so that you can
39:34 - understand what is going on
39:38 - so right here on top you'll have
39:39 - information about
39:41 - the runner that is executing the job
39:44 - and a runner will also have an executor
39:46 - in this case the executor is docker
39:49 - machine
39:50 - and here on line four you will see here
39:53 - which image is being downloaded
39:56 - and you'll see here pulling docker image
39:59 - alpine because this is what we have
40:00 - specified
40:02 - and then essentially the environment is
40:03 - being prepared
40:05 - and then in the upcoming steps
40:08 - you get the files from the git
40:09 - repository so essentially all the files
40:11 - that you have inside the git repository
40:13 - will also be available here inside the
40:15 - runner
40:16 - so after this point the docker container
40:18 - has been started you have all the
40:19 - project files and then you start
40:21 - executing the commands that you have
40:23 - specified inside the pipeline
40:25 - so in this case we're creating the
40:27 - folder we're creating these files we're
40:29 - putting some text inside there
40:31 - and then at the end we're not doing
40:33 - anything else
40:35 - so the job succeeds because there are no
40:37 - errors
40:39 - and
40:40 - the container is destroyed
40:43 - so this docker container that has been
40:45 - created in the beginning just maybe a
40:47 - few seconds ago
40:48 - is then destroyed you cannot log into it
40:51 - you cannot see it anymore it doesn't
40:53 - exist anymore so it has done its job has
40:56 - executed this command that we have
40:57 - specified and then it has been destroyed
41:00 - since every job runs in a container this
41:03 - allows for isolation and flexibility
41:07 - by having our job configuration stored
41:09 - in the yaml file
41:10 - we describe how the environment where
41:12 - the job is running should look like
41:15 - in practice
41:17 - we don't know or care which machine has
41:20 - actually executed the job
41:22 - also this architecture ensures that we
41:24 - can add or remove runners as needed
41:28 - while the gitlab server is a complex
41:30 - piece of software composed of multiple
41:32 - services the gitlab runner has a
41:34 - relatively simple installation and can
41:36 - run on a dedicated server or even on
41:39 - your laptop
41:41 - now right here on top of the job you
41:43 - have seen some information in regards to
41:45 - the runner so
41:47 - the question is where is this job
41:48 - actually running and to be able to
41:50 - understand this we'll have to go here to
41:52 - the project settings
41:54 - cicd
41:56 - and here we're going to expand runners
42:00 - and you'll see here two categories there
42:03 - are specific runners
42:05 - and they are shared runners
42:08 - now for this job we have used shared
42:10 - runners
42:12 - any project can use shared runners
42:14 - within a gitlab installation
42:16 - these shares runners are offered by
42:18 - gitlab.com and are shared between all
42:22 - users
42:23 - now as you've seen there are multiple
42:24 - runners here
42:26 - and honestly we don't really care which
42:28 - of these runners picks up our job
42:31 - because we have defined exactly what our
42:34 - job should do which docker image we want
42:36 - to use which command should be executed
42:39 - so as long as that runner knows how to
42:42 - deal with a docker image and was able to
42:44 - execute these commands then we're fine
42:46 - with any runner
42:48 - now this is an oversimplified version of
42:50 - the gitlab architecture the main idea i
42:52 - want you to get from this is that we are
42:55 - using docker containers in our jobs and
42:58 - every time for every job
43:00 - the gitlab runner will start a new
43:02 - docker container
43:04 - will execute any commands that we have
43:06 - and then when the job execution is done
43:09 - that docker container will be destroyed
43:18 - so now we have created this laptop and
43:21 - have defined all the steps inside the
43:23 - build laptop job
43:25 - and of course with this cad command we
43:28 - have also visually verified that indeed
43:31 - this computer.txt file
43:34 - contains everything that we expect
43:36 - but we really want to automate this
43:38 - process we don't want to go inside the
43:40 - job itself and to look and see if
43:42 - everything was successful
43:44 - so let's expand our pipeline and add a
43:47 - test job we want to make sure that our
43:50 - laptop contains all components
43:53 - so i'm going to go ahead here and create
43:55 - a new job
43:56 - i'm going to call it test laptop
43:59 - and we're going to use the same image
44:00 - the alpine linux image
44:03 - and we're going to also define the
44:05 - script and this time we need to find a
44:07 - way
44:08 - to actually
44:10 - check for example if this file has
44:12 - indeed been created so this would be
44:14 - like a very basic test
44:17 - and
44:18 - what we can do here is to use the test
44:20 - command so the test command
44:22 - will allow us to check if this file has
44:25 - been created
44:26 - and this command also has a flag
44:29 - which we'll gonna write with dash f
44:33 - and then we'll specify the path to the
44:35 - file so essentially this command will
44:39 - ensure that this file really exists and
44:42 - if it doesn't exist it will fail the job
44:45 - so let's go ahead commit this and see
44:47 - how the pipeline looks like
44:50 - if we're looking at the pipeline
44:52 - we'll notice
44:54 - something unusual
44:56 - and
44:57 - that is we're building the laptop and at
44:59 - the same time we're testing it
45:02 - and essentially what we wanted to do is
45:04 - to have this in two separate stages
45:07 - right we first build and then we test
45:11 - now currently what we did is we assigned
45:14 - by default both of these jobs to the
45:16 - test stage they both are running in
45:19 - parallel
45:20 - but of course these are not the kind of
45:22 - jobs that we can execute in parallel
45:24 - because they depend on one another
45:26 - particularly the test job depends on
45:28 - first the build job completely so i'm
45:31 - not even going to look at why the test
45:33 - build failed because by the way this
45:36 - pipeline looks like doesn't make a lot
45:37 - of sense
45:39 - and we have to go back to the concept of
45:41 - stages so what we want to do is to have
45:44 - two different stages and we're going to
45:47 - change our pipeline configuration and
45:48 - define these stages we want to have a
45:51 - build stage and we want to have a test
45:53 - stage
45:55 - in gitlab we can go ahead and define
45:57 - another configuration which is called
45:59 - stages
46:02 - and here as a list we can define the
46:04 - stages that we want to have so we can
46:06 - see here we want to have the build stage
46:09 - and we want to have the test stage
46:13 - and in order to specify which job
46:15 - belongs to which stage
46:17 - in the job configuration we will also
46:19 - say to which stage this job belongs so
46:23 - for example we want the build laptop job
46:25 - to belong to the build stage so i'm
46:27 - going to write here stage
46:30 - and the stage will be built
46:34 - the same goes for the test laptop now if
46:36 - you don't specify a stage here it will
46:39 - automatically belong to the test stage
46:41 - by default
46:43 - but actually you want to make it as
46:45 - explicit as possible so we're going to
46:46 - write here
46:48 - stage
46:49 - test
46:53 - so we here we have defined the stages we
46:55 - have assigned the build laptop job to
46:58 - the build stage
47:00 - and we have assigned the test laptop job
47:02 - to the test stage
47:05 - so let's commit these changes and take a
47:06 - look again at the
47:08 - pipeline if we're looking at the
47:10 - pipeline we'll be able to see now the
47:13 - two stages that we have build and test
47:16 - so these stages will run one after the
47:18 - other
47:19 - this stage does not start until the
47:22 - build stage is over so first we have to
47:25 - build a laptop and then we can start
47:27 - with a test
47:29 - unfortunately this is still failing so
47:32 - in this case we really have to take a
47:33 - look inside this job and understand what
47:36 - is going on
47:40 - and we'll see here that the last command
47:42 - that we have executed
47:44 - is this test command here so up to this
47:47 - point everything seems to be working
47:49 - fine
47:50 - this has been the last command that we
47:51 - have executed
47:53 - and somehow
47:55 - this job has failed
47:59 - in the next lecture we'll try to
48:00 - understand why do pipelines fail
48:04 - so let's jump into that and continue
48:06 - debugging this problem
48:14 - so let's try to understand why did this
48:16 - job fail
48:18 - looking here inside the job blocks
48:21 - we'll be able to see here this error
48:24 - says job failed
48:26 - exit code 1.
48:30 - exit codes are a way to communicate if
48:32 - the execution of a program has been
48:34 - successful or not
48:37 - an exit code 0 will indicate that a
48:40 - program has executed successfully
48:44 - any other exit code which can be a
48:46 - number from 1 to 255
48:49 - will indicate failure
48:51 - so in this case exit code 1 is not a 0
48:56 - so it means something has failed
48:59 - this exit code is issued by one of the
49:01 - commands that we have in our script
49:04 - as soon as one of these commands will
49:06 - execute an exit code execution of the
49:09 - job will stop
49:11 - in this case we have only one command so
49:12 - it's relatively easy to figure out which
49:14 - command has issued this exit code
49:18 - most likely it is the last command that
49:19 - you see here inside the logs
49:23 - so in this case what test is trying us
49:26 - to tell is that
49:28 - it has tested for the existence of this
49:30 - file
49:31 - and couldn't find it if the file would
49:34 - have been there would have gotten an
49:36 - exit code 0
49:37 - and the execution would have continued
49:40 - in this case the file is for some reason
49:42 - not there and we have retrieved an exit
49:46 - code 1. this tells gitlab that this job
49:50 - has failed and then the entire execution
49:52 - will stop
49:54 - if a job in the pipeline fails
49:57 - by default the entire pipeline will be
50:00 - marked as failed as well
50:02 - let me show you an example
50:05 - let's go back to our pipeline definition
50:10 - and we already know that the build
50:12 - laptop job works just fine
50:15 - so let's see here somewhere in the
50:16 - middle we're going to add a new command
50:21 - and this command will be the exit
50:22 - command
50:24 - and we're going to exit with a code one
50:28 - so what we should observe is that
50:30 - commands echo make tier and touch are
50:33 - still being executed
50:35 - but for example here where we're putting
50:37 - the main board
50:39 - we're using cat and again we're putting
50:41 - the keyboard
50:43 - inside this file this shouldn't be
50:44 - executed anymore
50:48 - so now i've executed a pipeline again
50:50 - you will see here that a build job
50:52 - failed
50:53 - and because this job failed the rest of
50:56 - the pipeline was not executed so it was
50:58 - interrupted as soon as something has
51:00 - failed it actually doesn't really make a
51:02 - lot of sense to continue with the rest
51:03 - of the stages if one of the jobs before
51:06 - has failed so let's go inside it and
51:08 - take a look at the execution logs to see
51:10 - what exactly has happened here
51:14 - and you'll be able to see as i said we
51:17 - have here the this echo command that's
51:19 - being executed we have
51:21 - created this directory we have created
51:23 - this file and then we have exit one
51:27 - and then we says here job failed exit
51:30 - code one now we have forced this failure
51:32 - here but i just wanted to demonstrate
51:35 - like where in the execution you can
51:38 - notice that something went wrong
51:40 - and actually going through these
51:42 - commands that are executed looking at
51:44 - your job configuration trying to figure
51:46 - out what has happened what is the last
51:48 - command that did something
51:50 - sometimes inside the logs you will find
51:52 - hints in regards to what went wrong in
51:55 - this case there are not a lot of hints
51:56 - in terms of what went wrong but these
51:58 - are also very simple commands
52:00 - but it's very important to
52:02 - read the logs from from the beginning
52:05 - as high as possible to understand like
52:07 - which docker image has been used what is
52:09 - happening before are there maybe any any
52:12 - warnings or any hints that something
52:13 - went wrong this case that's not the case
52:17 - and then to locate okay what was the
52:18 - last command that i executed why did
52:21 - this command fail
52:23 - so i'm gonna remove this exit code and
52:25 - we're gonna continue with the rest of
52:26 - the course
52:27 - and try to understand how we can get the
52:30 - simple pipeline to run
52:36 - let's go back to our pipeline
52:38 - configuration
52:40 - and understand what are we doing wrong
52:43 - as you probably remember from the
52:44 - architecture discussion
52:46 - every job runs independently
52:49 - which means here the build laptop job
52:52 - will start the docker container will
52:54 - create this folder and this file will
52:57 - put this text as instructed and then at
53:00 - the end
53:01 - we'll destroy the container meaning the
53:03 - file that we had created inside this
53:06 - docker container will be gone as well
53:08 - and test laptop will start a completely
53:12 - new container
53:13 - that doesn't have this file there and
53:15 - for that reason
53:17 - it cannot pass
53:19 - this test file will always tell us that
53:22 - there is no file there because from
53:24 - where could this file come
53:26 - now does this mean that we can only use
53:28 - a single job meaning that we need to
53:30 - move this command here to test inside
53:32 - the build laptop
53:34 - well that would be very inconvenient
53:36 - because then would have a big job that
53:38 - does everything
53:40 - and would kind of lose the overview of
53:42 - what our pipeline steps really are
53:45 - now there is a way to save the output of
53:48 - the job
53:50 - this case the output of the job what we
53:52 - are actually interested in this job is
53:54 - this file
53:55 - including this folder
53:58 - and gitlab has this concept of artifacts
54:01 - now an artifact is essentially a job
54:04 - output it's something that's coming out
54:06 - of the job that we really want to save
54:09 - it's not something that we want to throw
54:11 - away for example we may have used any
54:13 - other files or any other commands within
54:15 - the job but we're only interested in the
54:18 - final output that we have here
54:21 - so in order to tell gitlab hey
54:24 - i really want to keep this file in this
54:26 - folder
54:27 - we need to define the artifacts
54:30 - so the artifacts are an additional
54:32 - configuration an additional keyword that
54:34 - we add to our pipeline as the name tells
54:36 - its artifacts
54:39 - don't write it artifact because gitlab
54:41 - will not recognize that
54:44 - and as a property of artifacts we're
54:47 - going to use paths
54:48 - so notice it is indented it's not a list
54:52 - and then below paths
54:55 - we can add a folder or a file that we
54:58 - want to save now in this case
55:00 - we're going to tell gitlab save
55:02 - everything that is inside this build
55:05 - folder
55:07 - so now let's give it a run and see how
55:10 - the pipeline performs this time
55:13 - now if we're looking at the pipeline
55:14 - execution
55:16 - we now see that both building the laptop
55:19 - and testing the laptop
55:21 - are successful
55:23 - so what exactly has happened behind the
55:25 - scenes
55:26 - did we reuse the same docker container
55:28 - or what has happened there well to
55:31 - understand exactly what has happened and
55:32 - how these jobs now work we have to go
55:35 - inside the logs
55:36 - and we try to understand what exactly
55:38 - did this build job do differently this
55:41 - time
55:42 - and what i want you to notice here is
55:44 - that
55:45 - towards the end if you compare it to the
55:48 - logs of the previous job
55:50 - we also have this indication that
55:51 - something is happening and it will tell
55:53 - you here uploading artifacts for a
55:55 - successful job
55:57 - and will tell you here which artifacts
55:58 - are being uploaded we'll reference here
56:01 - the build folder and says that two files
56:05 - and directories were found
56:07 - and
56:08 - these are being uploaded to the
56:10 - coordinator not a coordinator to put it
56:12 - very simply essentially the gitlab
56:14 - server so in this case
56:17 - the runner has finished this job has
56:20 - noticed inside the configuration oh i
56:22 - need to do something with these files
56:24 - and
56:25 - we'll
56:26 - essentially archive these files
56:28 - and we'll give them back to the gitlab
56:30 - server and tell them hey i'm finished
56:32 - with this job i'm just gonna destroy
56:34 - this docker image
56:36 - you wanted to keep these files so you
56:38 - know here we go just
56:40 - handle these files i'm i'm done with my
56:42 - job here so essentially the runner is
56:45 - not saving these files they're being
56:46 - saved somewhere else in the storage
56:50 - now when the next job is being executed
56:54 - something pretty similar is happening
56:56 - i'm gonna go here to the test laptop job
56:59 - and what you see here in the beginning
57:03 - there's also a new indication inside the
57:06 - logs that something different is
57:07 - happening and we'll see here downloading
57:10 - artifacts and
57:11 - says again downloading artifacts from
57:13 - coordinator
57:15 - which essentially means we now are
57:17 - downloading this build folder inside the
57:20 - new docker container that we have just
57:22 - created
57:23 - we have managed to copy this files from
57:25 - one job to the next one
57:27 - and this is why now this test command is
57:30 - able to find the build folder and the
57:33 - computer.txt file inside it
57:35 - and the job is passing
57:41 - if this job is still failing for some
57:43 - reason it's always a good idea to take a
57:46 - look at the job that has generated the
57:48 - artifacts
57:49 - so in order to do that again we're gonna
57:51 - go and
57:52 - visit the pipeline
57:56 - and if we go inside the build laptop job
58:00 - here on the right hand side
58:04 - you should see some information in
58:05 - regards to the job artifacts
58:08 - and
58:09 - in order for these job artifacts to
58:11 - exist they are saved even after the job
58:14 - has terminated and you have the
58:16 - possibility of inspecting them so if
58:18 - you're not really sure like what is the
58:19 - contents of the file you don't really
58:21 - need to go inside the build pipeline and
58:24 - make some debugging there you could do
58:26 - that but essentially what has been saved
58:28 - here is the final version of the
58:31 - artifacts that are being used so you can
58:33 - go here inside browse
58:35 - you'll find here the build folder that
58:37 - we have specified
58:39 - and we'll find here the computer.txt
58:42 - and of course you can download this file
58:43 - and you can take a look at it after you
58:45 - download it on your own computer and see
58:47 - if it has the right content
58:49 - make sure that when you're testing this
58:50 - you're actually giving the correct path
58:52 - and not some other path
58:55 - that's a very important way on how you
58:56 - can actually inspect the outputs of the
58:59 - job
59:04 - at this point i wouldn't say that we are
59:06 - really done with testing the build
59:08 - output
59:10 - yes this file exists and we have now
59:12 - tested it so this is a very basic test
59:14 - that we have written here
59:16 - but how about checking the contents of
59:17 - the file to see if it contains the main
59:19 - board and the keyboard as we expect
59:23 - again just using a command like cat and
59:25 - displaying this into logs doesn't really
59:27 - help us automate this we need a tool
59:30 - that when it doesn't find the respective
59:33 - text in the file it will issue an exit
59:36 - code and tell us that that text is not
59:38 - really present there
59:40 - and for that purpose we're going to use
59:42 - the grep command
59:44 - so this is the grep command
59:46 - and grep is a cli tool that allows us to
59:50 - search for a specific string in a file
59:53 - so we're going to specify here for
59:54 - example the string to be main board and
59:57 - i'm going to copy it from above just to
59:59 - make sure i don't make any mistakes
60:00 - there
60:01 - and we also can specify in which file
60:04 - we're looking for this so we know that
60:06 - this file already exists
60:08 - so
60:09 - this is an additional test here that
60:11 - we're adding on top of this
60:13 - and now we actually know that the file
60:15 - exists but now we want to check if this
60:17 - word main board is inside the file
60:21 - and of course we can also duplicate this
60:24 - and write an additional
60:26 - test here gonna check for the keyboard
60:28 - as well we're checking for the main
60:30 - board and the keyboard
60:32 - now grep is really complex command and
60:34 - can support regular expressions and many
60:37 - other advanced features
60:39 - but i won't get into those i'm gonna
60:41 - commit these changes and in a few
60:43 - seconds take a look at the pipeline
60:49 - all right so now if we're taking a look
60:51 - at the pipeline we'll see the build job
60:52 - is still successful the test job is
60:55 - still successful
60:56 - so if we're looking inside here
60:58 - what we'll be able to see we'll be able
61:00 - to see some log outputs
61:02 - and then
61:04 - grab here is looking for the word main
61:06 - board inside this file and is able to
61:08 - find it will be displayed here and it's
61:11 - also looking for the word keyboard
61:13 - inside this file and is able to find it
61:15 - what's important about writing tests is
61:17 - to also ensure that your pipeline will
61:20 - fail if one of these tests doesn't work
61:22 - and sometimes you may think that the
61:24 - command does what it's supposed to do
61:26 - but the best way to make sure that
61:27 - you're really mastering that command to
61:29 - actually introduce an error inside your
61:32 - build job essentially
61:33 - and to check if the test job will fail
61:36 - so let's go ahead and try that out
61:40 - so here inside our build job what can we
61:43 - do well for example we could
61:46 - try and make a change here for example
61:48 - i'm going to remove the m from main
61:50 - board
61:52 - and i'm gonna simply
61:54 - commit these changes and to see if this
61:56 - job is now failing
62:06 - we've been looking now inside the tests
62:10 - we'll be able to see here that the last
62:12 - command that was executed was grep you
62:15 - will see it in comparison to the
62:16 - previous one there's no text being
62:18 - outputted below this is the last command
62:20 - that was executed we're getting an exit
62:23 - code 1 which essentially means grep has
62:26 - looked inside this file
62:28 - couldn't find the word main board
62:31 - so let's go back and fix our pipeline
62:34 - but this has been a very important test
62:37 - because if you're not really checking
62:38 - that our pipeline will fail at one point
62:41 - then whatever we did inside the pipeline
62:43 - in terms of testing is really not very
62:46 - useful
62:48 - i'm gonna go and add here main board
62:51 - back to the configuration
62:53 - and
62:54 - as we expect now the pipeline will also
62:57 - work again
62:59 - tests also play a very particular role
63:01 - when we're working on this pipeline when
63:04 - we're building software
63:06 - there are many various levels of testing
63:08 - but essentially tests allow us to make
63:11 - changes to our software or to our build
63:14 - process
63:15 - and to ensure that everything works the
63:17 - same
63:18 - for example i heard that
63:20 - if we are using this operator here and
63:24 - we're putting text inside a file
63:27 - that this approach doesn't really
63:29 - require to have a file already created
63:32 - with the touch command
63:34 - so how about we try this out and see if
63:37 - we can rely on the tests to get this to
63:39 - work so i'm going to simply remove this
63:41 - touch command
63:43 - and i'm going to commit a configuration
63:45 - we'll see what's going on
63:47 - so let's take a look at the pipeline
63:50 - and what do we see
63:52 - this test job is successful
63:55 - and if we really want to manually check
63:56 - once again we can go to the build laptop
63:59 - job
64:00 - we can take a look at the artifacts
64:02 - we'll see the build folder is there
64:05 - the computer.txt file is there
64:08 - so apparently
64:10 - we didn't need to have this touch
64:11 - command in our configuration
64:13 - and by having the tests we have gained
64:16 - an additional level of confidence that
64:18 - whatever changes we're making we kind of
64:20 - trust the tests and if the pipeline is
64:23 - passing we know that in the end we'll
64:26 - have this computer.txt file and i will
64:29 - have the proper content
64:36 - let's take another look at our pipeline
64:38 - configuration
64:40 - what if we need to change the name of
64:42 - the file so so far we have used this
64:44 - name computer.txt
64:47 - but what if we need to call it for
64:49 - example laptop.txt
64:52 - now quite often we don't really like
64:54 - when we have something for example a
64:57 - file name or something that could change
65:00 - and have that spread also the entire
65:02 - pipeline because if we need to make a
65:03 - change we need to identify all the
65:05 - currencies now this is a very simple
65:07 - example and it is relatively easy to
65:10 - identify this
65:11 - but quite often for example going into a
65:13 - large file and doing something like a
65:15 - replace all can lead to some undesirable
65:18 - errors that could occur
65:20 - so quite often when we have something
65:22 - that we are searching multiple times
65:24 - inside a file or inside the
65:26 - configuration
65:27 - we want to put that into a variable
65:31 - and that way if it's inside a variable
65:34 - if we need to make changes to that
65:36 - particular value we only have to change
65:38 - it once and then it would be replaced
65:40 - all over
65:41 - so how do we define a variable well we
65:44 - can
65:44 - go inside our script block and define a
65:47 - variable here so for example you can
65:50 - just define a variable called build file
65:53 - name
65:54 - and we see i'm writing this in lowercase
65:57 - and we have also some underscores to
65:59 - separate the words
66:02 - and then we can use the equal sign and
66:04 - write something like laptop.txt
66:08 - now in order to reference this variable
66:09 - we're going to copy the name and
66:12 - go whenever we need to
66:14 - have this and we're going to start with
66:16 - the dollar sign and then the variable
66:18 - name so we're referencing the variable
66:20 - by using the dollar sign before the
66:22 - variable name
66:24 - now this is a local variable that is
66:26 - available only inside this script and it
66:29 - will only be available inside this job
66:33 - so for this job we need to do the same
66:35 - thing as well
66:37 - as you've noticed quite often we write
66:39 - local variables in lowercase and using
66:42 - lowercase helps avoid any conflicts with
66:44 - any other existing variables
66:47 - now this is one way how we can do this
66:50 - but there's also the possibility of
66:51 - defining a variable block we can go
66:54 - inside the configuration of the job and
66:57 - write something like variables this will
67:00 - not be a list that's very important so
67:04 - this will be
67:06 - build
67:08 - file name
67:10 - column
67:11 - and the value will be laptop.txt
67:14 - so this is essentially almost the same
67:17 - as writing it like this
67:19 - but we are getting it away from the
67:21 - script and we are letting gitlab do this
67:24 - for us
67:27 - and of course if we need to do this we
67:29 - have to take it from here and also put
67:32 - this in the other job
67:35 - so we'll have some duplication here in
67:37 - terms of the configuration
67:39 - we can also define a global variable
67:42 - which is available for all jobs in order
67:45 - to define something globally all we have
67:47 - to do is take it from here
67:50 - and
67:51 - due to the indentation we're going to
67:52 - move it outside of the job
67:55 - and everything that's happening here is
67:57 - on a global level so i'm gonna
67:59 - move it here at the root of this
68:01 - document essentially
68:03 - here we define variables
68:06 - and this is the name and whenever we
68:07 - have this we have to use this syntax
68:11 - and of course we can go ahead and remove
68:13 - it from the test job because it would
68:14 - also be available there
68:17 - now this is essentially as defining an
68:19 - environment variable
68:21 - which is available for the entire system
68:25 - while there is no hard rule we typically
68:28 - write environment variables in all caps
68:32 - and still use underscores to separate
68:34 - the words
68:35 - because we are inside an ide i'm going
68:38 - to select this text
68:40 - use the f1 command
68:43 - and then i'm going to transform to
68:45 - uppercase
68:46 - so this is how i want it to look like
68:49 - build file name
68:52 - and wherever i want to have this
68:55 - again dollar sign
68:57 - and this name so for example i'm going
68:59 - to have it here for computer as well
69:02 - here this command which we may decide to
69:05 - replace or not really depends on us but
69:08 - i'm going to replace it here as well
69:10 - i'm going to use it here
69:14 - and here
69:15 - and here
69:17 - so again wherever we had computer.txt
69:20 - we have now replaced this with this
69:23 - environment variable and whenever we
69:25 - need to make changes to it we can easily
69:28 - just change it here it's pretty easy to
69:30 - see
69:31 - maybe adding some other variables that
69:33 - we have inside our pipelines
69:36 - it will make the managing of these
69:38 - details much easier
69:41 - well in this example it is not necessary
69:43 - depending on which characters you
69:45 - include in your variable value
69:48 - you may need to put everything between
69:50 - quotes
69:52 - this is just something to keep in mind
69:53 - but this is a very simple text here that
69:56 - we have
69:57 - so
69:58 - that will not cause any conflicts in
70:00 - general with the yaml syntax so let's
70:02 - commit these changes and see if the
70:04 - pipeline is still working as it should
70:08 - the pipeline is running successfully and
70:10 - we can go inside the build
70:12 - job that we had here and we can take a
70:14 - look at the artifacts
70:16 - and indeed we'll see that now we are
70:19 - using this file name laptop.txt and no
70:22 - longer computer.txt
70:30 - so i have mentioned devops quite a few
70:32 - times and by now you have heard of
70:34 - devops as well
70:36 - so what is devops
70:39 - let me tell you first what devops is not
70:42 - devops is not a standard or a
70:44 - specification
70:46 - different organizations may have a
70:48 - different understanding of devops
70:51 - devops is not a tool or a particular
70:53 - software
70:54 - nor is it something you do if you use a
70:57 - particular tool or a set of tools
71:01 - devops is a cultural thing
71:03 - it represents a change in mindset
71:07 - let's take a look at the following
71:08 - example
71:09 - you have a customer who wants a new
71:11 - feature a business person let's call
71:13 - them a project manager would try to
71:16 - understand what the customer wants
71:18 - write some specifications
71:20 - and hand them over to the developers
71:23 - the developers will build this feature
71:25 - pass it on to the testers who would test
71:27 - it
71:28 - once ready the project manager would
71:30 - review the work and if all looks good
71:33 - would ask the developers to pass the
71:35 - software package to the sysadmins to
71:37 - deploy it
71:38 - as you can see there's a lot of passing
71:40 - stuff around and in the end if something
71:43 - goes wrong and often things go wrong in
71:46 - such situations everyone is unhappy and
71:50 - everyone else is to blame
71:52 - so why does it happen
71:54 - because every group has a different
71:56 - perspective on the work
71:58 - there is no real collaboration and
72:00 - understanding between these groups
72:03 - let's zoom in on the relation between
72:05 - the developers and the sys admins the
72:08 - developers are responsible for building
72:10 - software
72:12 - ensuring that all these cool features
72:14 - that the customers want make it into the
72:17 - product
72:18 - the it operations team is responsible
72:21 - for building and maintaining the t
72:23 - infrastructure ensuring the i.t systems
72:26 - run smoothly securely and with as little
72:29 - downtime as possible
72:31 - do these groups have something in common
72:34 - yes of course the software the product
72:37 - the problem is the idea operation team
72:40 - knows very little about the software
72:42 - they need to operate
72:43 - and the developers know very little
72:45 - about the infrastructure where the
72:47 - software is running so devops is a set
72:51 - of practices that tries to address this
72:53 - problem but to say that devops is just a
72:56 - combination of development and operation
72:58 - would be an understatement
73:00 - actually everyone mentioned before works
73:03 - on the software just in a different
73:05 - capacity
73:07 - since the final outcome impacts everyone
73:10 - it makes sense for all these groups to
73:12 - collaborate
73:13 - the cultural shift that devops brings is
73:16 - also tightly connected to the agile
73:18 - movement
73:19 - in an ever more complex environment
73:22 - where business conditions and
73:24 - requirements change all the time
73:26 - and where we need to juggle tons of
73:29 - tools and technologies every day
73:31 - the best culture is not one of blaming
73:34 - and finger-pointing but one of
73:37 - experimentation and learning from past
73:39 - mistakes
73:41 - so we want to have everyone collaborate
73:44 - instead of working in silos and stages
73:47 - instead of finger pointing everyone
73:49 - takes responsibility for the final
73:51 - outcome if the final product works and
73:55 - the customers or users of the product
73:57 - are happy
73:59 - everyone wins
74:01 - the customers the project managers the
74:03 - developers the testers the sys admins
74:06 - and anyone else i did not mention
74:08 - everyone wins
74:10 - however devops is more than just culture
74:13 - to succeed organizations adopting devops
74:16 - also focus on automating their tasks
74:20 - manual and repetitive work is a
74:22 - productivity killer
74:24 - and this is what we are going to address
74:25 - in this course
74:26 - automatically building and deploying
74:29 - software which falls under a practice
74:31 - called cicd
74:33 - we want to automate as much as possible
74:36 - to save time and give us the chance to
74:38 - put that time to good use instead of
74:41 - manually repeating the same tasks over
74:43 - and over again
74:45 - but to automate things we need to get
74:47 - good at using the shell working with cli
74:50 - tools reading documentation writing
74:53 - scripts
74:54 - quite often you may see devops being
74:57 - represented by this image
74:59 - while this does not give a complete
75:01 - picture of what devops really is
75:04 - it does show a series of steps a typical
75:07 - software product goes through from
75:09 - planning
75:10 - all the way to operating and monitoring
75:14 - the most important thing i want you to
75:15 - notice in this representation is that
75:18 - this process never stops it goes on and
75:21 - on in an endless loop
75:23 - this means that we continue going
75:25 - through these steps with each iteration
75:27 - or new version of the software
75:30 - what is not represented here is the
75:32 - feedback that goes back into the product
75:35 - devops goes hand in hand with the agile
75:38 - movement
75:39 - if adrian and scrum are new to you
75:42 - make sure to add this to your to-do list
75:44 - nowadays many organizations go through
75:46 - an agile transformation and value
75:49 - individuals who know what agile and
75:51 - scrum are
75:52 - regardless of their role
75:54 - i've added some resources you may want
75:56 - to look into in the course notes
75:59 - if you have some free time while
76:01 - commuting or doing other things around
76:03 - your house i highly recommend you listen
76:06 - to the phoenix project as an audiobook
76:09 - it is an accurate description of what
76:11 - companies that are not adopting devops
76:14 - go on a day-to-day basis and
76:16 - realistically portrays such a transition
76:19 - this is by no means a technical book and
76:22 - i'm sure it will be a fun listen
76:24 - so devops is a set of practices that
76:27 - helps us build successful products
76:30 - to do that we need a shift in thinking
76:32 - and new tools that support automation
76:35 - however i must warn you that you can use
76:38 - tools that have devops written all over
76:40 - them and still not do devops so devops
76:44 - is so much more than just adopting a
76:46 - particular tool with that being said
76:48 - let's continue diving into gitlab ci
76:54 - in this unit we will start working on a
76:56 - simple project we want to automate any
76:59 - of the manual steps required for
77:01 - integrating the changes of multiple
77:03 - developers and create a pipeline that
77:05 - will build and test the software we are
77:07 - creating in other words we will do
77:10 - continuous integration continuous
77:12 - integration is a practice and the first
77:14 - step when doing devops usually we're not
77:17 - the only ones working on a project and
77:20 - when we're doing continuous integration
77:21 - we're integrating our code with the code
77:24 - other developers created it means that
77:26 - every time we make changes to the code
77:28 - that code is being tested and integrated
77:31 - with the work someone else did
77:34 - it is called continuous integration
77:36 - because we integrate work continuously
77:39 - as it happens we don't wait for anything
77:42 - to do that we don't want to integrate
77:44 - work once per week or once per month as
77:47 - it can already be too late or too costly
77:50 - to resolve some issues the more we wait
77:52 - the higher the chances we will run into
77:54 - integration issues in this unit we will
77:57 - use gitlab to verify any changes and
78:00 - integrate them in the project i'm going
78:02 - to be honest with you as we build more
78:04 - advanced pipelines you will most likely
78:07 - encounter some issues if you haven't
78:09 - done it yet go right now to the video
78:11 - description and open the course notes
78:14 - there you will find important resources
78:16 - and troubleshooting tips
78:17 - finally let's do a quick recap when we
78:20 - have multiple developers working against
78:22 - the same code repository ci is a
78:25 - pipeline that allows us to add and
78:27 - integrate our changes even multiple
78:29 - times per day what comes out is a new
78:32 - version of the product if you're still
78:34 - unsure about continuous integration at
78:36 - this point don't worry we'll implement
78:38 - ci in our development process in the
78:40 - upcoming lessons
78:44 - for the rest of the course we'll be
78:45 - using this project this is a simple
78:48 - website built with react which is a
78:51 - javascript technology developed by
78:53 - facebook now we don't want to get too
78:55 - much into the technical details because
78:56 - they don't really matter so much at this
78:58 - point
78:59 - but the first step in order to be able
79:01 - to make changes to this repository is to
79:04 - make a copy of it so for example if
79:06 - you're trying here to open a web ide in
79:08 - this project i will get this option to
79:11 - fork the project by the way you will
79:13 - find a link to this project in the
79:15 - course notes and the course notes are
79:18 - linked in the video description
79:19 - so
79:20 - we can click here on fork
79:23 - and we'll make a copy of this project
79:25 - under our account
79:27 - now that we made a copy out of this
79:28 - project we can then open the web ide and
79:32 - start making changes to it and
79:35 - particularly what we're trying to do is
79:37 - to create the pipeline so let me give
79:39 - you an overview of the tasks that we are
79:42 - trying to automate
79:44 - essentially here in this project we have
79:46 - a couple of files one of these files is
79:49 - this package.json file
79:52 - and this file essentially documents
79:54 - which requirements this project has and
79:57 - in order to actually run this project we
79:59 - first need to install this requirement
80:01 - so locally i already have a copy of this
80:04 - project and the command to install these
80:06 - requirements is called yarn
80:09 - install
80:11 - so now all the requirements have been
80:13 - installed
80:14 - the next step would be to create a build
80:17 - and that would be done using the command
80:19 - yarn build
80:23 - and during this process what has
80:24 - actually happened is that a build folder
80:27 - has been created and this build folder
80:29 - contains multiple files that are
80:31 - required for the website so let me give
80:33 - you an idea how this website looks like
80:36 - and what we actually did here so i'm
80:38 - going to run the command serv minus s
80:40 - and going to specify the build folder
80:44 - and now essentially we have started a
80:46 - server we started an http server which
80:49 - is serving the files available there so
80:51 - i'm going to open this address in a new
80:53 - tab
80:55 - and this is how the website looks like
80:58 - so essentially what we're trying to do
81:00 - in this section
81:01 - is to automate these steps so we want to
81:04 - install the dependencies we want to
81:05 - create a build we want to test the bill
81:08 - to see if the website is working
81:10 - and i've shown you these tools because
81:13 - it is always a good idea to be familiar
81:16 - with the cli tools that we'll be using
81:18 - in gitlab ci
81:20 - now in gitlab we try to automate any
81:22 - manual steps but before we do that we
81:25 - must know and understand these steps we
81:27 - cannot jump into automation before we
81:29 - understand what the commands that we
81:31 - want to do are actually doing
81:34 - now i'm not really referring in
81:35 - particular to the commands that i've
81:37 - shown you here
81:38 - because they are specific to this
81:40 - project you may be using python or java
81:42 - or anything else
81:44 - so you don't need to be familiar with
81:46 - these tools in particular i will explain
81:48 - to you what they do and how they work
81:51 - however what is important to understand
81:53 - is the concepts the concepts remain the
81:55 - same and this is what we are actually
81:57 - focusing on in this course we are
81:59 - focusing on understanding the concepts
82:01 - around automation
82:08 - so let's begin creating the ci pipeline
82:10 - for this project so i'm going to go
82:12 - ahead and create here a new file
82:15 - and of course the definition file for
82:18 - the pipeline will be
82:19 - dot
82:20 - gitlab.ci dot yaml
82:24 - the first job that we want to add here
82:26 - is build website
82:28 - and
82:29 - what we are trying to do here where
82:31 - we're trying to build this website and
82:33 - why do we need to build the website well
82:36 - essentially or most of their projects do
82:39 - have a build step
82:41 - in this case we're essentially creating
82:43 - some production files production ready
82:46 - files which are smaller optimized for
82:48 - production from some source files
82:50 - so we have here in the source files
82:53 - you will see here an app.js and any
82:56 - other file so essentially
82:57 - the build process will take all these
82:59 - files and will make them smaller and
83:03 - will put them together in a way
83:05 - other programming languages may need to
83:08 - have a compilation step or any other
83:10 - steps so typically something happens in
83:12 - the build process where we actually are
83:14 - putting our project together
83:16 - now of course we don't want to do that
83:18 - manually from our computer we want to
83:21 - let gitlab do this for us
83:23 - so let's go ahead and write here the
83:25 - script for this
83:28 - the first step is
83:30 - to
83:30 - essentially run
83:32 - yarn and yarn is a tool that is helping
83:36 - us build this project this is specific
83:39 - for
83:40 - javascript projects essentially node.js
83:43 - projects and yarn is a tool that can be
83:46 - used for getting dependencies they can
83:48 - also be used for building the software
83:51 - so the command that we are running here
83:52 - is build
83:56 - locally as you remember the first thing
83:58 - that i did was to do a yarn install to
84:01 - install dependencies and this is
84:03 - something that needs to happen before
84:04 - the build so every time
84:07 - when we are building this website we
84:09 - need to get the dependencies to make
84:10 - sure that we have all the dependencies
84:12 - that we need and to ensure that all of
84:14 - them are up to date and because they
84:17 - don't remain anywhere in the container
84:19 - we need to do that all the time and
84:20 - normally locally would do that only when
84:23 - we need to do that
84:24 - so for example when we know that we need
84:26 - a newer dependency of a software package
84:28 - but gitlab doesn't have this information
84:31 - so we kind of like need to run this all
84:33 - the time
84:35 - you also need to specify your docker
84:36 - image so let's try for example
84:39 - alpine which we have used before
84:42 - so essentially what i'm trying to do is
84:44 - to replicate the commands that i've
84:46 - executed on my computer
84:48 - installing dependencies and building the
84:50 - project
84:52 - so let's commit these changes and see
84:54 - what pipeline does i'm going to commit
84:55 - them to the main branch and click on
84:57 - commit here
85:01 - we can take a look at what the job is
85:02 - doing
85:04 - and we'll be able to see here that we
85:07 - get an exit code now remember any exit
85:10 - code that is not zero
85:12 - will lead to a job failure and it says
85:14 - here job failed
85:16 - now why did this job fail
85:19 - well we have to look like what is the
85:21 - command that we try to execute
85:23 - and we'll find here
85:25 - something saying that yarn not found
85:29 - so essentially what this means is that
85:31 - the docker image that we have used does
85:33 - not have yarn
85:35 - so
85:36 - how do we
85:37 - install yarn or how do we normally do
85:40 - this
85:41 - now the thing is we don't have to use
85:43 - this alpine image that you have seen
85:45 - here
85:46 - essentially for most projects and this
85:49 - includes node.js which is actually what
85:52 - we're using here and which i already
85:54 - have installed locally this is why it
85:56 - worked locally
85:57 - there are official docker images that we
86:00 - can use
86:01 - and
86:02 - the central repository for such images
86:05 - is docker hub so this is a public docker
86:08 - repository so for example if i'm typing
86:11 - here node
86:12 - i will be able to find here the official
86:14 - image for node
86:17 - and i can essentially instead of using
86:20 - alpine i can simply use here node
86:24 - so let me go back to the project
86:28 - and i'm going to write here node
86:31 - now the thing is when we're doing here
86:33 - when you're writing alpine or node
86:36 - what is actually happening is that we
86:39 - are always getting the latest version of
86:42 - that docker image
86:44 - sometimes it may work but sometimes the
86:46 - latest version may contain some breaking
86:49 - changes which may lead to things not
86:52 - working anymore in our pipeline if one
86:54 - day we're getting one version and next
86:56 - day we're getting something else without
86:58 - us making any changes
87:00 - things may break
87:02 - so for that reason it is generally not a
87:04 - good idea
87:05 - to just use node for example or just to
87:09 - use alpine as we have done before it is
87:11 - better to specify a version
87:14 - to put it down like which version do we
87:16 - need here and to write it down as
87:18 - essentially as a tag
87:20 - now how do we know which version do we
87:22 - need now for node what we're going to do
87:24 - here we're going to head to nodejs.org
87:28 - and here you'll see here two versions
87:31 - that are currently available for
87:33 - download
87:34 - what we want to do is we want to use the
87:36 - lts version now you will see here that
87:39 - the latest lts version that i'm
87:41 - currently seeing right now is 16.13.2
87:44 - now what's important here is the major
87:46 - version which is 16.
87:48 - when you are watching this it's
87:49 - important that you come to this page and
87:52 - you look at a number here probably it
87:54 - will increase over time most likely it
87:56 - will increase over time
87:58 - you'll get a different version it's
87:59 - important here that you get
88:01 - the latest lts version that you see here
88:06 - all right so let's go inside our
88:07 - pipeline and then write this 16 version
88:10 - here so the way we're going to do here
88:12 - we have node which is the base image and
88:15 - then we can specify a tag by writing
88:17 - column
88:18 - and then i'm going to write 16
88:21 - right
88:21 - and most likely this version is already
88:23 - available on node but you can always
88:26 - make sure and check here go to the tags
88:28 - and see if this specific tag is then
88:30 - available there
88:32 - but this major tags typically they are
88:34 - available and
88:36 - will have no issues downloading this
88:38 - so let's commit the changes and see how
88:40 - it goes and again i'm gonna push to the
88:43 - main branch
88:47 - when this executing you will be able to
88:49 - see here which image is being downloaded
88:52 - and you will see here it's node and the
88:54 - tag is for the version 16.
88:58 - now this job is now requiring much much
89:01 - longer to run you will see here the
89:02 - duration in this case was one minute and
89:05 - 35 seconds
89:06 - and this is because we have to download
89:10 - what is a relatively large docker image
89:13 - this node
89:14 - image
89:15 - and then we are installing the
89:17 - dependencies
89:19 - and you'll see here that these
89:20 - dependencies take 44 seconds to be
89:23 - installed to figure out which
89:25 - dependencies are required to download
89:27 - them from the internet that takes a bit
89:30 - and then
89:31 - we are actually doing the build
89:33 - and this also takes a few seconds to
89:35 - complete
89:37 - but fortunately everything is then
89:39 - successful in the end and this is the
89:41 - most important step at this point
89:44 - we don't want to waste a lot of time
89:46 - when we are actually executing these
89:48 - builds and i know for a fact that this
89:50 - note image is a few hundred megabytes
89:54 - large in size
89:55 - this is because it contains a lot of
89:57 - tools and a lot of dependencies which we
89:59 - may not need
90:00 - so for that reason it's always a good
90:02 - idea for these larger images to go here
90:06 - on the specific image to click here on
90:09 - the text tab
90:10 - and generally to take a look at like
90:12 - what is the size of these images and
90:14 - what's happening with them because some
90:16 - of them are quite big
90:18 - so we could theoretically search here
90:19 - for version 16.
90:23 - and by default
90:24 - we can see here that this is the one tag
90:27 - that's more specific it's not exactly
90:28 - what we're using we will see here
90:31 - version 16 something
90:34 - it has about
90:35 - 332
90:38 - megabytes
90:39 - so if we're selecting to use this image
90:42 - every time we'll have to download 300
90:44 - something megabytes that's a lot of
90:47 - download and a lot of time that we are
90:49 - wasting just to start this image so for
90:52 - that reason what i typically do is i go
90:55 - here in the tags and i search for alpine
90:57 - sometimes slim or other images can also
91:00 - be a good idea and what i'm interested
91:03 - in is something like 16 alpine so let me
91:07 - write directly 16 alpine
91:10 - and
91:11 - this looks absolutely fine here
91:14 - so 16 will be again the node
91:17 - version that we are trying to use and if
91:19 - we're looking here at the size
91:22 - take a look at this it's 38 megabytes
91:26 - so what we're going to do we're going to
91:28 - simply take this
91:29 - and go in our pipeline and replace
91:33 - 16 with this stack
91:38 - so here inside instead of using 16 we're
91:41 - going to use 16-alpine
91:44 - gonna commit these changes we're gonna
91:46 - take a look again at the pipeline to see
91:49 - how long does it take right now
91:52 - to build this
91:56 - so this job still needed quite a bit of
91:58 - time one minute and 26 seconds
92:00 - you may see this duration varying but
92:04 - generally it is a very good practice to
92:06 - use
92:07 - images that are as small as possible
92:09 - because this can save time and you may
92:11 - see that maybe this job will even go
92:13 - below one minute it really depends like
92:15 - how fast the
92:17 - runner will pick up the job and will
92:19 - able to start this image
92:21 - but the main idea is the same we now
92:24 - have managed to automate the first steps
92:26 - in our project we're installing some
92:29 - dependencies
92:30 - and then we are running the build
92:32 - so this seems to be working just fine
92:36 - just to recap we are using this node
92:39 - image before we have tried using alpine
92:42 - image
92:44 - now the alpine image didn't work because
92:46 - it didn't have node js installed now
92:49 - essentially what we're using here is
92:51 - essentially the same image as before the
92:53 - same alpine image alpine linux image
92:56 - but it has node installed so it has the
92:58 - dependency that we need and this node
93:00 - dependency contains yarn this is why
93:03 - yarn didn't work before
93:05 - now it's working just fine and it's
93:06 - building the project
93:12 - the most important thing that you can do
93:14 - when you're learning something new is
93:16 - practicing
93:17 - and i want to give you the opportunity
93:19 - in this course to practice along so not
93:22 - just following along what i'm doing but
93:24 - you also have the opportunity to do
93:26 - something on your own
93:27 - and for the next assignment i already
93:29 - think that you have to know how
93:30 - necessary in order to do that
93:32 - what we're trying to do is to create two
93:35 - new additional jobs in this pipeline
93:37 - this is your job now to write these jobs
93:40 - so what are these jobs all about the
93:42 - first job is trying to test the website
93:45 - and the website is currently inside the
93:47 - build folders if i go inside the build
93:49 - folder
93:51 - you will see here a list of files
93:53 - now your job is to ensure that the
93:57 - index.html file is available inside the
94:00 - build folder so this is the first job
94:02 - that you need to create the second job i
94:04 - want you to create is in regards to
94:06 - running units so in order to run unit
94:09 - tests the command that we are using is
94:11 - yarn
94:12 - test
94:13 - and the only thing you need to do is to
94:15 - create a job and to run this command to
94:17 - take a look at the logs and see that the
94:19 - tests have indeed been executed
94:22 - the upcoming lesson will contain the
94:24 - solution to this
94:25 - but please it's super important that you
94:28 - pause this video and try this on your
94:31 - own try as much as you can because this
94:34 - is the best way to learn and to ensure
94:36 - that what i'm showing you in this course
94:38 - is something that you will be able to
94:39 - use in your projects as well
94:46 - i hope you have tried to solve this
94:48 - assignment on your own
94:50 - but anyway this is how i would approach
94:53 - this problem so i'm here inside the
94:56 - editor for the pipeline and let's begin
94:59 - just with the skeleton so what are we
95:00 - trying to do we have two new jobs we
95:03 - have test website
95:05 - where we're essentially trying to test
95:08 - the output of the build website job
95:11 - and we also have a unit tests job
95:14 - so first of all we have to think about
95:17 - stages
95:19 - what we want to do happens essentially
95:22 - after the build
95:24 - well it really depends a bit so for
95:26 - example the test website this definitely
95:28 - needs to happen after the build the unit
95:31 - tests they don't necessarily need to
95:33 - happen after the build
95:35 - but we're going to put them inside
95:38 - a different stage as well so let's go
95:40 - ahead and define the stages here so
95:42 - we're going to have two stages we're
95:44 - going to have here built
95:47 - and we're going to have another stage
95:49 - which is called test
95:52 - and what we need to do is to assign this
95:54 - jobs to a stage so the build will be
95:57 - assigned to the stage
95:59 - built of course and then the test
96:03 - website will be assigned to the stage
96:06 - test
96:08 - and the same goes with the unit test
96:10 - right in order to test the website what
96:12 - we are trying to do well let's try and
96:14 - write the script we are trying to test
96:17 - if we have an index.html
96:21 - file there so as you probably remember
96:24 - the command test
96:26 - dash
96:27 - f
96:28 - so this we are testing for the existing
96:30 - file
96:31 - this needs to be inside the build folder
96:34 - and the name of the file is index.html
96:38 - now what we haven't done so far is
96:40 - inside this build website job to declare
96:43 - artifacts
96:44 - so as it is right now this command will
96:48 - fail
96:49 - so what we have to do here is of course
96:51 - think about the artifacts so which
96:53 - artifacts do you have we have to define
96:56 - the paths
96:58 - and the only paths that we're interested
97:00 - in is the build path
97:02 - so then this will be able
97:04 - to test for this file
97:06 - the next thing we need to think about is
97:09 - which image do we need for test website
97:12 - so theoretically we could use node 16
97:16 - but actually this command test doesn't
97:18 - require something like that so we could
97:21 - just use alpine
97:24 - and test is a pretty general command so
97:27 - we don't have to worry about specifying
97:29 - a version or anything like that
97:31 - so just going with alpine should be just
97:35 - fine
97:37 - there's of the unit test we essentially
97:39 - need this node image because we'll be
97:42 - using yarn so i'm going to simply go
97:44 - ahead and
97:46 - copy this
97:47 - image that we have used in the build
97:49 - website
97:52 - and of course the script will also be
97:55 - kind of a similar
97:57 - uh we still need to install dependencies
98:00 - so yarn install is necessary here when
98:03 - we're trying to run the unit test and
98:05 - the command that we want to run is yarn
98:08 - test
98:11 - so let's double check to make sure that
98:12 - we have everything in place we have
98:14 - defined two stages build and test we
98:17 - have assigned build website to the stage
98:20 - build and we have assigned
98:22 - the test website and the unit tests we
98:25 - have assigned them both to the stage
98:27 - test so this means these two jobs will
98:29 - run in parallel
98:31 - here we're just using the test command
98:33 - to verify if this file exists
98:36 - and here we're installing the
98:37 - dependencies and then running the tests
98:39 - so i'm gonna go ahead
98:41 - commit these changes and we'll inspect
98:44 - together the pipeline
98:48 - and after a minute or two this buy plan
98:50 - will succeed and you'll notice here the
98:52 - two stages build and test we're building
98:55 - a website and then we're testing what we
98:58 - have
98:59 - the unit tests are not necessarily
99:01 - dependent on the build itself but we put
99:04 - them together in a test
99:06 - so if looking here at test website what
99:08 - do we see well we see here the command
99:10 - test minus f so it's testing if this
99:13 - file actually exists if it's part of the
99:16 - job artifacts of course if you're
99:18 - looking inside the build website
99:21 - you can go here and look at the
99:23 - artifacts
99:25 - see here the build folder
99:26 - we'll see here multiple files so not
99:28 - only the index.html file but we'll see
99:31 - here some images some additional files
99:34 - that we don't really care so much at
99:36 - this point but they are available there
99:39 - so that's the most important aspect that
99:41 - we really care about
99:44 - and here the unit tests
99:46 - the unit tests are part of the project
99:50 - and generally when you're writing code
99:51 - we are also writing unit tests to
99:53 - document that what we're doing there is
99:55 - working properly
99:57 - you will see here the command that has
99:58 - triggered the test has been yarn test
100:02 - there's only one
100:04 - file here that contains only one test
100:06 - all these tests have been executed
100:08 - and because the tests are passing the
100:11 - job is also succeeding any of these
100:14 - tests would not work then the job will
100:17 - be failing and the entire pipeline will
100:20 - also be failing
100:26 - the first tool we need for integrating
100:28 - work is already in place
100:30 - we are using git for versioning our code
100:34 - we also use gitlab as a central git
100:37 - repository
100:38 - we take git for granting nowadays but it
100:41 - solves a critical issue and is one of
100:44 - the first devops tools you need to know
100:47 - while this course does not go into git i
100:50 - highly recommend checking the course
100:51 - notes for a comprehensive git tutorial
100:55 - so in our current model every developer
100:58 - pushes changes to the main branch
101:01 - whatever version of code has been pushed
101:03 - last time in the main branch that is the
101:05 - latest version
101:07 - anyone wanting to add new changes will
101:10 - have to make them compatible with the
101:12 - main branch
101:14 - however this model does have a serious
101:17 - flaw nothing is safeguarding the main
101:20 - branch
101:21 - what if the latest changed has
101:23 - introduced an issue and we can't build a
101:25 - project anymore
101:27 - or what if the tests are failing
101:30 - this is now a massive problem as nobody
101:32 - on the team can continue working and we
101:35 - can't deliver new versions of the
101:37 - software
101:38 - a broken main branch is like window
101:41 - production halts in a factory is not
101:43 - good for
101:44 - business we must ensure that the main
101:47 - branch does not contain any broken code
101:50 - the main branch should always work and
101:52 - allow us to deliver new versions of the
101:55 - software anytime we need to do that
101:58 - so how do we solve this
102:00 - the idea is simple we just don't push
102:03 - untested work in the main branch
102:06 - since we can't trust that developers
102:08 - remember to run all tests locally before
102:10 - pushing a change
102:12 - we want to take advantage of automation
102:14 - and gitlab to automatically run the
102:16 - tests before adding any changes to the
102:19 - main branch
102:21 - the idea is to work with other branches
102:23 - which are later integrated into the main
102:25 - branch
102:26 - this way every developer can work
102:29 - independently of other developers
102:32 - there are various git workflows but we
102:34 - will use one that is simple to
102:36 - understand
102:37 - it's called the git feature branch
102:39 - workflow
102:41 - the idea is simple for each new feature
102:43 - bug idea experiment or change we want to
102:47 - make we create a new branch
102:49 - we push our changes there
102:51 - let the pipeline run on the branch and
102:54 - if everything seems okay we can
102:56 - integrate the changes in the main branch
103:00 - so in other words we simulate running
103:02 - the main branch before we actually run
103:05 - the main branch
103:06 - if our branch pipeline fails no worries
103:09 - all other developers are unaffected and
103:12 - we can focus on fixing it or if we don't
103:15 - like how things turned out we can just
103:18 - abandon the branch and delete it no hard
103:20 - feelings there either
103:22 - as part of working with branches we
103:24 - create a merge request that allows other
103:27 - developers to review our code before it
103:29 - gets merged
103:31 - a much request is essentially one
103:33 - developer asking for the changes to be
103:35 - added to the main branch the changes are
103:38 - reviewed and if nobody has objections we
103:41 - can merge them
103:42 - so this is the plan for the upcoming
103:44 - lessons to start working with git
103:46 - branches and merge requests so let's get
103:49 - to work
103:53 - so how can we
103:55 - create merge requests in gitlab
103:58 - first of all to ensure that the chances
104:00 - of breaking the main branch are as small
104:03 - as possible
104:05 - we need to tweak a few settings
104:07 - so we're going to go here to settings
104:10 - general
104:12 - and right here in the middle you should
104:14 - see merge requests i'm going to expand
104:16 - that
104:18 - and what i like to use when using merge
104:20 - requests is to use this fast forward
104:23 - merge essentially no merge commits are
104:25 - created
104:26 - and generally the history remains much
104:29 - cleaner there's also the possibility of
104:31 - squashing commits directly from gitlab
104:34 - and you can put it here and encourage
104:36 - scratching commits is when you're
104:37 - pushing multiple changes to a branch
104:40 - and instead of pushing all these changes
104:42 - back into the main branch we scratch
104:45 - them all together so we essentially like
104:46 - we have only one commit
104:48 - again makes the history much easier to
104:51 - read going forward here in the merge
104:54 - checks we want to make sure that the
104:56 - pipelines succeed before we are merging
104:59 - something so this is a super important
105:01 - setting here that we have
105:03 - so let's go ahead here go at the bottom
105:05 - and click on save changes
105:07 - and additionally again from settings
105:11 - when go here to repository
105:15 - and from the repository you're going to
105:16 - go here to protected branches i'm going
105:19 - to expand this
105:21 - and what we want to do here is we want
105:24 - to protect the main branch
105:27 - so essentially we don't want to commit
105:30 - changes to the main branch anymore we
105:31 - want to prohibit that so nobody will be
105:34 - allowed to directly commit something to
105:37 - the main branch so in order to do that
105:39 - we have to go here to allow to push
105:43 - and instead of having something selected
105:45 - or some role selected here can i use no
105:48 - one so no one is allowed to push to this
105:51 - protected branch only if it goes through
105:54 - a merge request
105:56 - so these are the initial settings that
105:58 - we need to do
105:59 - and we'll be able to see here now let's
106:01 - try and make some changes and open the
106:03 - web ide
106:07 - open the pipeline
106:08 - file
106:10 - and now let's say i'm trying to add here
106:13 - a new stage where i'm trying to do some
106:15 - checks for example there is linter that
106:19 - i can use
106:20 - and linter is simply a static code
106:23 - analyzes tool that is used to identify
106:25 - potential programming errors stylistic
106:28 - issues
106:29 - and sometimes you know questionable code
106:31 - constructs since it is static it does
106:33 - not actually run the application it just
106:36 - looks at the source code
106:38 - and most projects do tend to have such
106:41 - linter inside and just for the sake of
106:43 - completion i also want to add here
106:45 - linter
106:46 - so i'm gonna go ahead here and i'm gonna
106:49 - write here linter this is the
106:52 - name of the job
106:54 - the image that i'm using is still node
106:56 - here
106:57 - and the reason for that is inside the
106:59 - script
107:00 - the command that we are running is
107:02 - actually yarn
107:03 - lint
107:06 - and of course we also need to install
107:08 - all dependencies
107:11 - and additionally we have the possibility
107:13 - of assigning this job
107:15 - to a stage
107:17 - now by default gitlab comes with some
107:18 - predefined stages prefined stages
107:21 - include a press stage
107:24 - build stage a test stage deploy stage
107:27 - and post stage
107:29 - so these are all actually predefined and
107:31 - to be honest this is just to make it
107:34 - clear like which stages we are defining
107:36 - here and which stages we are using
107:38 - but these are already defined by gitlab
107:40 - so we're just essentially redefining
107:42 - something that exists
107:43 - so for this linter i could
107:46 - go ahead and use a stage and this stage
107:49 - name will be dot pre
107:51 - and this linter has absolutely no
107:53 - dependencies on the build itself
107:56 - the same goes for example here with the
107:59 - unit tests
108:00 - the unit tests are also not dependent on
108:02 - the build so i could go ahead and move
108:05 - them to the same stage
108:08 - just to make this clear so there's a dot
108:10 - there and because this is a predefined
108:12 - stage we don't need to redefine it again
108:16 - so i could go ahead and write it here
108:20 - but it's not really needed all right
108:23 - now going back here
108:25 - the editor that i opened here is from
108:28 - the main branch you will see here main
108:30 - is selected right
108:32 - and let's go ahead and commit
108:36 - and you'll see here
108:38 - i'm no longer allowed
108:40 - to commit to the main branch so this is
108:43 - by default
108:44 - disabled the settings that we did ensure
108:47 - that we are no longer allowed to
108:50 - directly make changes to the main branch
108:52 - so we have to create a new branch quite
108:55 - often when we are creating branches we
108:58 - tend to use some naming conventions now
109:00 - totally up to you what you want to use
109:03 - or how your organization
109:05 - uses that
109:06 - quite often you will have something like
109:08 - feature for example for what slash and
109:10 - then a name of a feature sometimes you
109:12 - may reference a ticket or something like
109:15 - that so for example you have your ticket
109:16 - number one two three four
109:20 - add linter
109:22 - so you will see here i'm adding no
109:23 - spaces or anything like that
109:25 - it's totally up to you how you use these
109:27 - forward slashes hyphens as separators
109:30 - and so on
109:31 - you'll also see here the possibility of
109:34 - starting a new merge request
109:36 - so
109:37 - i'm essentially these changes now are
109:39 - not committed i'm going to create a new
109:40 - branch the branch name will be feature
109:44 - forward slash and some name
109:46 - i'm gonna click here on commit
109:48 - this will open an additional window so
109:51 - the branch has been created
109:53 - but now i'm also opening a new merge
109:56 - request
109:58 - and here in the merge request there are
109:59 - a few things that i should fill out so
110:01 - for example the title is not very
110:04 - suggestive so i'm going to write here
110:06 - something like
110:07 - add
110:08 - linter to the project
110:11 - you can also go ahead and provide a
110:13 - description
110:14 - this is also useful for the people who
110:16 - are looking at this merge request to
110:18 - know why this is important what this
110:22 - feature is bringing or if it is a bug
110:25 - fix which issues is this fixing and so
110:27 - on so you could do that
110:29 - there are also some additional labels
110:31 - and options that you can set here
110:34 - i'm not going to go over them because
110:35 - they are essentially
110:37 - relatively easy to explain
110:39 - and i'm going to go here and
110:42 - click on create merge request
110:46 - and now a new merge request has been
110:48 - created
110:49 - and
110:50 - additionally we also have here a
110:53 - pipeline that is running against this
110:55 - branch
110:57 - so what is happening here is that the
110:59 - changes that we made
111:01 - which you will be able to see here what
111:03 - we do do we added here a new stage we
111:06 - added here a linter uh also made some
111:09 - changes to the do the unit test so we
111:11 - can go ahead and
111:13 - look at these changes we have here a
111:14 - pipeline that's executing it's going
111:17 - through all these stages we have here
111:18 - the pre-stage
111:20 - the build stage and the test stage
111:23 - and now we can simulate running this
111:26 - pipeline
111:27 - to see if it fails if it fails we have
111:29 - the opportunity or of fixing things if
111:31 - it doesn't fail then we have the option
111:34 - of merging these changes
111:42 - the next important step in the life of a
111:44 - merge request is the review process
111:47 - especially the code review where
111:49 - somebody else from the team is taking a
111:52 - look at our changes
111:53 - and is giving us feedback
111:56 - now just in case you don't know where
111:57 - your merge request is you will find here
112:00 - on the left hand side this panel says
112:03 - here merge requests and we'll indicate
112:05 - how many merge requests are open at this
112:07 - time so we only have one merge request
112:10 - you will see here this is the title of
112:13 - the merge request
112:15 - so we're trying to add linter
112:18 - and you'll find here like the most
112:19 - important information so this is a
112:21 - request to merge this feature
112:24 - into the main branch right
112:26 - if you need to make changes to it you
112:28 - can open this branch in the id
112:30 - and continue making changes to it
112:34 - you have your information about the
112:36 - pipeline so we know that the pipeline
112:38 - has passed
112:40 - and someone else looking at this we'll
112:43 - be able to see here which commits have
112:45 - been made
112:47 - and also to have an overview of the
112:49 - changes
112:51 - inside here there are different views
112:52 - that you can have
112:54 - you can compare these files and someone
112:57 - can
112:58 - also leave comments here so for example
113:00 - i could simply click here on the line
113:03 - and ask
113:04 - why did you use
113:07 - the
113:07 - prestage
113:10 - right and you can add a comment
113:14 - now where does this merge request go now
113:16 - typically
113:17 - when someone has reviewed this can go
113:20 - ahead here and approve it so you will
113:23 - see here who has reviewed these changes
113:25 - and you may have internal
113:27 - roles like you need to people to review
113:30 - any changes before they are being merged
113:33 - so the changes can be approved and then
113:36 - they can be merged
113:38 - comments can appear so for example here
113:40 - i've added a question to these changes
113:43 - so maybe there's some discussion that
113:45 - needs to happen there feedback can be
113:47 - gathered from different people involved
113:50 - and this feedback may need to be
113:52 - integrated so for example as i mentioned
113:54 - you can still do changes to this the
113:57 - merge request will always have the
113:58 - latest changes so we can make more
114:00 - commits
114:02 - it's also possible that for some reason
114:05 - these changes are no longer needed they
114:08 - are
114:09 - wrong or the approach that was taken
114:11 - doesn't really make sense so so there's
114:13 - also the possibility of closing a merge
114:15 - request you can go here and see closed
114:17 - merge requests so this will be closed
114:19 - and no changes will be merged so that's
114:22 - also a possibility
114:24 - but
114:25 - typically what happens after the review
114:28 - after some changes have been integrated
114:30 - we're gonna go here on this merge button
114:34 - and what is happening here is we are
114:36 - merging this branch
114:38 - into the main branch
114:41 - and there are two options that are
114:43 - enabled by default
114:45 - the source branch so the branch that we
114:47 - have created will be deleted which makes
114:49 - sense because we don't need it anymore
114:52 - and in case we have multiple commits we
114:55 - can also squash those commits and in
114:58 - this case we have only one commit we can
115:00 - modify the commit message if we want to
115:02 - in this case that's not needed
115:04 - and then we can simply click here on the
115:07 - merge button
115:09 - this merge button is available only if
115:12 - the pipeline succeeded but our pipeline
115:14 - did succeed so this is why we can see it
115:17 - so now gitlab will merge these changes
115:20 - into the main branch
115:23 - and again a new pipeline will be
115:25 - generated
115:27 - and this is the pipeline for the main
115:29 - branch so you will see exactly
115:32 - the changes that we made
115:34 - we have here the pre-stage the build and
115:37 - the test
115:38 - these are all available here
115:41 - and exactly what we have run inside a
115:44 - merge request the pipeline is running
115:46 - again against the main branch
115:49 - and this is needed just to ensure that
115:51 - indeed everything is working fine and
115:52 - that there are no integration issues or
115:54 - no other issues so it's a good practice
115:57 - not something that
115:58 - may seem that it's happening somehow
116:00 - as a duplicate it's actually important
116:02 - to ensure that the main pipeline is
116:05 - working properly this is what we care
116:07 - most about to ensure that this main
116:09 - pipeline
116:10 - is working as it should work
116:12 - that's the reason why we have reviewed
116:14 - the merge request to ensure that
116:17 - whatever we're changing here in the main
116:19 - branch is as good as it gets and that
116:22 - the chances of breaking something
116:25 - are drastically reduced
116:33 - right in the beginning of this section
116:35 - i'll show you locally which are the
116:37 - steps they need to build this project
116:39 - but i've also taken the build folder and
116:41 - i've opened the website and you have
116:43 - seen something that looks like this
116:46 - how can we replicate this in gitlab
116:49 - essentially we want to make sure that we
116:51 - indeed have a website that's running
116:53 - that can be started by a server
116:56 - and we want to make sure that you know
116:58 - for example this text here shows up
117:00 - learn gitlab ci how can we do that
117:03 - inside the pipeline currently we're just
117:06 - checking if we have a file which is
117:08 - called index.html
117:10 - that's not really telling us the entire
117:12 - story so this lecture we're going to
117:14 - take a look at how we can do essentially
117:17 - what's sometimes called an integration
117:18 - test we're really testing that the final
117:21 - product is really working
117:24 - i'm going to open up the web ide
117:27 - and inside the pipeline file kind of
117:29 - focus on this test website job
117:34 - so this current test is not really that
117:37 - helpful we want to do something else so
117:39 - for example
117:41 - you have seen me running the following
117:42 - command which has been serve
117:46 - dash s
117:47 - build
117:48 - right
117:49 - so
117:50 - this takes the build
117:53 - and we'll be able to start a server and
117:56 - then we can take a look at what's going
117:57 - on
117:58 - so we're going to remove this because we
118:00 - don't need that anymore
118:02 - i'm going to start here server
118:04 - and just because i want to make sure
118:05 - that this is running as fast as possible
118:07 - and that we're not wasting a lot of time
118:10 - i'm going to go ahead and disable some
118:11 - jobs so i'm going to disable here the
118:13 - linter job that's not really needed in
118:15 - the beginning
118:16 - i'm going to also disable the unit test
118:18 - so to disable the job you simply add a
118:21 - dot in front of the job name
118:23 - and that job will be disabled
118:25 - let's go ahead and commit these changes
118:28 - and again we'll have to create a new
118:29 - branch
118:31 - call this feature integration test
118:33 - i'm going to start a new merge request
118:35 - as well
118:36 - i'm going to call it add integration
118:38 - tests
118:40 - i'm going to click here on create merge
118:41 - request
118:44 - if you're looking at the pipeline we'll
118:46 - see that the build has succeeded
118:49 - but there seems to be an issue
118:51 - with testing the website so let's take a
118:54 - look at the logs and understand what is
118:56 - going on
118:57 - we'll try to understand here what is the
118:59 - error we're getting an exit code
119:02 - the last command that we executed is
119:05 - serv
119:06 - and we're getting here surf not found so
119:08 - again we're trying to
119:10 - run a command
119:11 - this command was working locally for me
119:14 - but at least the docker image that we
119:16 - are using here does not have this
119:18 - command so which docker image are we
119:19 - using we're using alpine
119:21 - and alpine does not have this command so
119:24 - we need to find a way to install this
119:26 - command as well
119:28 - so for that we're going to go back to
119:29 - the editor we can
119:31 - go here from the merge requests
119:35 - click on the merge request
119:38 - and we'll find here open in web id
119:41 - and then we'll be able to see here this
119:44 - is essentially a list of changes that we
119:46 - have this is the review it shows us what
119:48 - we have changed but if you want to make
119:50 - changes to the code
119:52 - we're going to go here this will allow
119:54 - us again to select the pipeline file and
119:57 - make changes to it
119:59 - so let's see what we need to do in order
120:01 - to get this job to run as well now first
120:03 - of all we're gonna use here this same
120:06 - node image
120:09 - because serv is actually
120:13 - something that runs on node so it's a
120:15 - dependency that runs on node
120:17 - and we're gonna use yarn
120:19 - and we're gonna add this dependency
120:23 - i'm gonna see here yarn add serve
120:27 - and actually you're going to use this as
120:29 - a global dependency so i'm going to
120:30 - write here global
120:32 - so using yarn global add serve
120:35 - right this will ensure
120:38 - that we now have serve
120:40 - we can use yarn
120:41 - because we now have the node image
120:44 - again this time we are already in a
120:46 - merge request so we can go ahead click
120:48 - on commit
120:50 - i'm going to commit directly here to the
120:52 - existing branch that we have
120:56 - and the pipeline will be executed again
121:00 - be sure to watch what i'm explaining
121:02 - next because we have introduced an issue
121:06 - in this pipeline
121:08 - and it's a big issue
121:10 - but can explain in a second what that
121:12 - issue is i'm going to go ahead open this
121:15 - pipeline
121:17 - and i'm going to go directly to this
121:19 - test job and wait a bit here to see
121:23 - what is happening and i wait for this
121:25 - job to start
121:29 - so i want you to notice what is
121:30 - happening with this job
121:32 - so we have started this docker container
121:35 - which contains node
121:38 - then we have added this dependency which
121:41 - is serve
121:42 - and then we have executed
121:44 - serv
121:45 - which folder are reserving or serving
121:47 - the build folder then it says here info
121:50 - accepting connections on localhost port
121:52 - 3000
121:55 - so what is going on
121:57 - well
121:58 - we have started an http server which is
122:01 - serving these files inside this gitlab
122:04 - job and when does a server end
122:08 - well it never ends that's the purpose of
122:10 - the server it will serve these files
122:12 - indefinitely
122:14 - and of course this approach that we
122:16 - started here is not good because
122:18 - we're gonna wait here forever
122:21 - well actually not forever jobs do have
122:24 - a specific timeout you will see here the
122:27 - timeout that is one hour
122:30 - so if we don't stop this job
122:32 - this job will run for one hour
122:36 - so this is an important concept when
122:37 - you're starting a server inside a job
122:41 - that job will never end or will run into
122:45 - this timeout so be very careful when
122:46 - you're doing things like that
122:48 - now i'm going to go ahead
122:50 - and manually stop this job you will see
122:53 - here this cancel button here
122:55 - we cannot wait forever so i'm going to
122:57 - cancel it
122:58 - and then this will stop
123:01 - now there is a way to start a server
123:04 - and there is a way to check that text
123:06 - but we still need to make some changes
123:08 - so i'm gonna go back again
123:11 - to the pipeline
123:14 - let's go inside the code
123:16 - open the pipeline
123:18 - and see what we have here now
123:20 - what do we want to do so we have managed
123:22 - to start this server it will run forever
123:25 - but how do we test something so in order
123:28 - to test something
123:29 - we have to execute the upcoming command
123:32 - and in order to actually get this
123:34 - information we need essentially an http
123:37 - client we need another tool
123:40 - that will go to this address which is
123:42 - you have seen there it's starting on
123:43 - localhost which is the host name on port
123:46 - 3000
123:48 - and the protocol is http
123:51 - so we don't have a browser that we can
123:53 - open here so we need something that is
123:55 - as close as possible to a browser
123:58 - so that tool is curl
124:01 - so curl is a tool that will be able to
124:04 - go to this address
124:06 - download the website
124:09 - and then we can
124:11 - do something i'm gonna use here a pipe
124:14 - so this will send the output of this
124:17 - website to the next tool
124:20 - and as you remember from working with
124:21 - files
124:23 - we have the possibility of using grep so
124:25 - grep will search for a specific string
124:28 - so what is the string that we're looking
124:30 - for i think the string is learn gitlab
124:32 - ci i'm gonna check again against the
124:35 - website to make sure that i have this
124:37 - exact text here
124:41 - because this is what we are trying to
124:42 - find
124:44 - inside a website right
124:46 - so we're starting a server
124:48 - then
124:49 - we have this curl command and in grab
124:52 - we're getting
124:54 - the website and then we can search for
124:56 - this string
124:58 - we still haven't solved the problem with
125:00 - the server running forever we don't want
125:02 - it to run forever so we're going to use
125:04 - a small trick and i use here this and
125:07 - sign
125:08 - and what will this do is to start this
125:11 - process in the background
125:13 - so it will essentially still start the
125:15 - server but it will be in the background
125:18 - and it will not prevent this job from
125:21 - stopping
125:22 - so when the curl command has been
125:24 - executed this job will also stop
125:27 - because this command will be executed
125:30 - right after this one
125:31 - curl will not wait for the server to
125:34 - actually start
125:36 - the server needs a few seconds to start
125:39 - and in order to wait for that
125:42 - we don't know exactly when this will
125:44 - start so we're going to use another
125:45 - command which is called sleep
125:48 - and sleep will take a parameter here so
125:51 - this will be in seconds so we can decide
125:52 - how many seconds we want to wait
125:55 - just to be sure i'm gonna use here 10
125:57 - seconds
125:58 - additionally
125:59 - curl is also another command which we
126:02 - don't have
126:03 - and this is actually a dependency of
126:06 - this alpine image so this has nothing to
126:09 - do with yarn or npm as serve does this
126:14 - is an exception
126:16 - so for that reason we're have to use
126:18 - here the package manager from alpine so
126:22 - a b
126:23 - k this is the alpine package manager
126:26 - and we're gonna add here
126:29 - curl so this is how we are adding curl
126:31 - as a dependency so let's give it another
126:34 - try and see if this works
126:41 - now if we're looking at the logs we'll
126:43 - see that this is still failing
126:46 - maybe this is not our day but let's go
126:48 - through the process again and try to
126:49 - understand what is going on and
126:52 - generally i have to tell you we have to
126:53 - get used with errors and
126:56 - the more we understand about this the
126:57 - more we are able to solve this kind of
126:59 - problem
127:01 - so we have added surf we have added curl
127:03 - and we started the server we can see
127:05 - here if we're still getting this
127:07 - information accepting connections at
127:10 - we waited here for 10 seconds
127:13 - curl
127:14 - has downloaded this
127:17 - and then has sent this to grab but we
127:20 - are getting here an exit code one
127:23 - because we're not seeing any errors from
127:25 - curl which means that curl has
127:27 - downloaded this information
127:28 - we have to wonder like what's going on
127:30 - with grep why isn't this information
127:33 - available there in the response
127:35 - now we could go and debug this a bit
127:37 - more
127:38 - but essentially when carl is downloading
127:41 - a website it's not really downloading
127:43 - what you see right now here it's
127:45 - actually if you click on this
127:48 - go here to view page source on any
127:50 - website
127:52 - what curl is actually doing is
127:54 - downloading this
127:56 - now this is the
127:58 - index.html file and this index.html file
128:01 - will download other things in the
128:03 - background we'll download some scripts
128:04 - so that
128:06 - information there is not really coming
128:09 - from the page from this page that we are
128:11 - getting with curl
128:12 - and curl doesn't have the possibility of
128:14 - actually rendering javascript and images
128:16 - and so on so it's very basic tool
128:20 - what we have here for example
128:22 - is this react app in the title
128:27 - so how about if we change this and we
128:30 - are asserting that we have this title
128:33 - so let's go ahead in the application and
128:34 - figure out where this react app appears
128:38 - now i change it to something with git
128:39 - lab so instead of using learn gitlab
128:42 - we're gonna simply try and use react app
128:45 - because we know this is available here
128:48 - so from the project i'm going to open
128:50 - the web ide
128:51 - if i'm not in the right branch i can
128:53 - simply switch the branch
128:56 - open the file that i want to edit
128:58 - so we're going to use instead of learn
129:00 - gitlab ci
129:02 - i'm going to use here react app
129:05 - and we are also gonna enable the rest of
129:08 - the jobs
129:09 - which are the unit tests which have been
129:11 - disabled
129:13 - the linter has been disabled
129:15 - and then we're gonna put all these
129:17 - changes together
129:19 - commit them to the branch we're not
129:21 - going to start here a new merch request
129:23 - we already have a merge request so we're
129:24 - going to disable that if it's enabled
129:27 - i'm going to commit again
129:30 - and finally the tests are working so
129:32 - let's go again and inspect the job logs
129:35 - so what we find here well
129:38 - we have everything that we are
129:40 - interested in
129:41 - and particularly the last part was the
129:43 - one that was failing
129:45 - and now it's able to locate this inside
129:48 - here so this is why you're getting back
129:50 - the entire text
129:52 - you'll see here react app it's in the
129:55 - response
129:56 - so
129:57 - curl and grab now work together and this
130:00 - job succeeds has been a bit of work but
130:03 - i'm happy that we have this test and it
130:06 - ensures that
130:07 - our pipeline really works a bit better
130:10 - and that we're more confident in what
130:11 - we're building
130:14 - i'm gonna go here to the merge request
130:17 - you will see here we have four commits
130:21 - we have here essentially the changes
130:23 - that we made
130:25 - so we reverted
130:26 - all the other jobs that we have disabled
130:29 - and we just have adapted this test
130:31 - website
130:33 - so from that point of view everything is
130:35 - good we're gonna squash here the commits
130:37 - so we can also modify the commit message
130:40 - commit messages add integration test so
130:42 - that looks good
130:45 - i'm gonna click here on merge and this
130:48 - changes will be merged into the main
130:50 - branch
130:57 - so currently our continuous integration
130:59 - pipeline looks like this it is divided
131:01 - in three separate stages
131:04 - and the way i've structured this is just
131:07 - an example there is no hard rule
131:10 - different programming languages and
131:12 - technologies may require a different
131:14 - order you are smart people and i'm sure
131:17 - you'll be able to apply these ideas to
131:18 - whatever you're doing
131:20 - however i just wanted to mention two
131:22 - principles or guidelines that you should
131:24 - consider
131:25 - one of the most important aspects is
131:28 - failing fast
131:29 - for example we want to ensure that the
131:31 - most common reasons why pipeline would
131:34 - fail are detected early
131:37 - for example here linting and unit test
131:40 - quite often developers forget to
131:43 - check their linting make a mistake or
131:45 - forget to run the unit test so if we
131:47 - notice that this is happening a lot it
131:49 - really makes sense to put this as the
131:52 - first stage of the pipeline
131:55 - they typically don't need a lot of time
131:58 - so the faster they fail
132:00 - the better it is because we'll get fast
132:02 - feedback we'll use less resources
132:05 - and of course we know if we're running
132:07 - here in parallel jobs that are similar
132:09 - in size that's also a relatively good
132:12 - way of grouping jobs together now you
132:15 - also have to be very careful when you're
132:17 - grouping jobs in peril for example if
132:20 - you have a job that finishes in 15
132:22 - seconds and you have a job that finishes
132:24 - in five minutes for example let's say
132:27 - here linter needs 15 seconds unit test
132:30 - needs five minutes if the lender fails
132:32 - after just five seconds the unit test
132:35 - will still run for five minutes there's
132:37 - no no stopping there so it does make
132:39 - sense to put in parallel jobs that are
132:42 - similar in size
132:44 - another aspect that you need to consider
132:47 - are the dependencies between the jobs
132:50 - so what i mean by dependencies well for
132:52 - example here we cannot test the website
132:55 - until we have actually built the website
132:59 - but the linter and the unit tests they
133:01 - don't depend on the build output so for
133:03 - that reason we can run them before the
133:06 - build we can run them after a build we
133:08 - don't have a dependency there
133:10 - but in this case for the website we are
133:13 - dependent on the job artifacts because
133:16 - we're using them for something so from
133:18 - that perspective
133:20 - we always need to have the build job
133:22 - before here we can test the website
133:26 - so if jobs have dependencies between
133:29 - them they need to be in different stages
133:32 - now i have to say that there is no
133:34 - replacement for experimentation and
133:37 - trying things on your own
133:39 - if you are unsure if something works or
133:41 - not just give it a try and see how far
133:44 - you can go and why that pipeline fails
133:47 - as you have noticed so far our pipeline
133:49 - does require a bit of time
133:51 - and actually optimizing these pipelines
133:53 - is a lot of work so what i wanted to
133:56 - show you is just a very simple way on
133:58 - how we're gonna save a bit of time
134:00 - throughout this course so that we don't
134:02 - wait so much for each of these stages
134:05 - so again i'm gonna open here the web ide
134:08 - and what i want to do is to essentially
134:11 - restructure these jobs
134:13 - now as you can notice here the build
134:16 - website the linter and the unit tests
134:20 - they are all pretty similar
134:22 - they
134:23 - all use the same image they all need to
134:26 - install yarn dependencies and
134:29 - they just run a command after this
134:32 - so one idea would be to for example take
134:35 - the linter
134:37 - so yarn lint
134:39 - and to put it right here after we have
134:42 - installed the dependencies
134:44 - and in that case we don't need this job
134:47 - anymore same goes for the unit tests
134:49 - right we're gonna take here the unit
134:51 - tests
134:52 - and put them after yarn lint
134:55 - and again we're not gonna need this
134:57 - anymore
134:58 - also not gonna need a stage
135:01 - we can reduce this
135:03 - to just two jobs so
135:06 - this job indeed has
135:08 - this image
135:10 - which is the same as this one but
135:12 - doesn't have any dependencies so it's
135:15 - just
135:16 - installing surf here it's installing
135:18 - curl
135:19 - so i don't want to really combine this
135:21 - in a single job we could theoretically
135:23 - put everything in a single job
135:25 - but i'm just doing it now for
135:26 - performance purposes and in order to
135:29 - save a bit of time we're just gonna
135:30 - stick to this two different stages the
135:33 - build stage and the test stage and even
135:36 - if you're doing here a bit of testing
135:38 - it's acceptable now to be able to save a
135:41 - bit of time and as i said it's totally
135:43 - up to you how you structure your
135:45 - pipeline but
135:46 - this is why we are removing now this i
135:49 - just wanted to show you how they look
135:50 - like but for the rest of the course we
135:53 - don't want to wait so much time waiting
135:55 - for these stages to complete
135:58 - so for that reason this will be a bit
136:01 - easier i'm going to create a merge
136:03 - request and merge these changes
136:06 - into the main branch
136:09 - if you're binge watching this course
136:10 - make sure to take a break after a few
136:12 - lessons there is a lot to take in but
136:15 - you need to take care of your body as
136:16 - well so get up from time to time drink
136:18 - some water eat or go outside and relax
136:21 - your mind and trust me taking a break is
136:24 - so much more productive than staying all
136:26 - day in front of your computer if your
136:28 - body does not feel well this impacts
136:30 - your productivity and energy levels so
136:33 - i'm going to take a break as well and
136:34 - i'll see you in a bit
136:36 - [Music]
136:41 - in this unit we'll learn about
136:43 - deployments and we'll take our website
136:45 - project and deploy it in the aws cloud
136:48 - along the way we'll learn about other
136:50 - devops practices such as continuous
136:52 - delivery and continuous deployment by
136:55 - the end of this section we'll have a
136:56 - complete ci cd pipeline that will take
136:59 - our website build it test it and let it
137:02 - float in the cloud just a quick reminder
137:05 - in case you get stuck or something does
137:07 - not work check the video description for
137:09 - the course notes which include
137:10 - troubleshooting tips this sounds
137:12 - exciting let's begin
137:16 - amazon web services or simply aws is a
137:20 - cloud platform provider offering over
137:23 - 200 products and services available in
137:26 - data centers all over the world
137:29 - aws offers a pay-as-you-go model for
137:32 - renting cloud infrastructure mostly for
137:35 - computation or data storage
137:38 - so instead of buying physical hardware
137:40 - and managing it in a data center
137:43 - you can use a cloud provider like aws
137:46 - such services include virtual servers
137:49 - managed databases
137:50 - file storage content delivery and many
137:54 - others
137:55 - aws started in the early 2000s and its
137:58 - adoption continued to increase over time
138:02 - maybe you have heard the story when the
138:04 - new york times wanted to make over 1
138:06 - million old articles available to the
138:09 - public a software architect at new york
138:12 - times has converted the original article
138:14 - scans into pdfs in less than 24 hours at
138:18 - a fraction of the cost a more
138:20 - traditional approach would have required
138:23 - and this was back in 2007
138:26 - today many organizations can't even
138:28 - imagine their idea infrastructure
138:30 - without using some cloud components
138:33 - while this course focuses on aws the
138:36 - principles presented here apply to any
138:39 - other providers if you don't already
138:42 - have an aws account don't worry it's
138:44 - quite easy to create one
138:46 - and your personal details and click on
138:49 - continue
138:52 - since we're using aws for learning and
138:54 - experimenting with it i recommend
138:57 - choosing a personal account
138:59 - aws is a paid service and even though in
139:02 - the beginning there is a free tier that
139:05 - has some generous limits and is ideal
139:08 - for learning and experimenting you are
139:10 - still required to provide a credit card
139:12 - or debit card for the eventuality you
139:15 - will go over the free limit
139:18 - i'll go with a free basic support plan
139:20 - which is fine for individuals
139:24 - now that we have an aws account that is
139:26 - verified and has a payment
139:28 - what we need to do next is sign in into
139:31 - the console because from the aws console
139:33 - we can access all services that aws
139:36 - offers
139:41 - if you're seeing the aws management
139:43 - console it means that you have set up
139:45 - everything correctly
139:47 - and you can continue with the rest of
139:48 - the course and essentially start using
139:50 - aws right away
139:52 - let's do a bit of orientation this is
139:54 - the main page from where you navigate to
139:56 - all aws services
139:59 - aws services are distributed in multiple
140:02 - data centers and you have the
140:04 - possibility of selecting the data center
140:07 - you would like to use right here on the
140:09 - right side on the top of the menu
140:12 - i will go with u.s east 1 north virginia
140:16 - you can use whichever region you like
140:18 - just remember the one you have selected
140:20 - as this will be relevant later on
140:23 - typically the data centers in the u.s
140:25 - have a lower cost than others that are
140:27 - spread around the globe
140:30 - and right here you have a list of all
140:33 - services available you can also search
140:35 - for services in this search bar
140:40 - finally i highly recommend that you go
140:42 - to your account profile and enable
140:45 - multi-factor authentication this will
140:47 - significantly raise the security of your
140:50 - account
140:51 - that's about it now you can start using
140:54 - aws
141:00 - the first service that we want to
141:01 - interact with is aws s3 which stands for
141:05 - simple storage service you can find a
141:08 - service by simply searching for it
141:12 - you will see here s3 i'm gonna click on
141:15 - it
141:17 - and there you go this is aws s3
141:20 - s3 is like dropbox but much better
141:24 - suited for devops actually for a long
141:27 - while dropbox was using aws s3 behind
141:30 - the scenes for storing files but let's
141:33 - go back to the course
141:36 - since our website is static and requires
141:38 - no computing power or database we will
141:41 - use aws s3 to store the public files and
141:45 - search them to the word from there over
141:48 - http
141:49 - on aws s3 files which aws calls objects
141:55 - are stored in buckets
141:57 - which are like some super container
141:59 - folder
142:01 - now you may notice that your aws
142:03 - interface looks a bit different than
142:05 - mine
142:06 - aws is constantly improving the ui and
142:10 - things may change in time however the
142:12 - principles that i'm showing here will
142:14 - stay the same so let's go ahead and
142:17 - create our first bucket
142:21 - first of all we have to give our bucket
142:23 - a name and the name of the bucket needs
142:26 - to be unique so i'm going to give this
142:28 - bucket a unique name so that i don't run
142:30 - into conflicts with anyone else who may
142:33 - have created the bucket with the same
142:34 - name so i'm gonna just give here my name
142:37 - and a dash and after that i will give
142:39 - here a date
142:41 - additionally you'll have here the aws
142:43 - region and in my case i'm gonna keep it
142:46 - exactly as it is
142:48 - and there are also a bunch of settings
142:49 - that we'll not look into right now
142:52 - so right here at the end i'm going to
142:54 - click here on create bucket
142:57 - all right so the bucket has been
142:59 - successfully created
143:01 - and we can go inside the bucket and to
143:02 - see if there's anything in there
143:05 - but at this point there are absolutely
143:07 - no files of course we could go ahead and
143:10 - manually upload files or download files
143:12 - or things like that so through the web
143:14 - interface it's possible to add files and
143:17 - folders
143:18 - but this is not what we are trying to
143:20 - accomplish with devops we want to
143:22 - automate this process so for that reason
143:24 - i'm going to leave it as it is and in
143:26 - the upcoming lectures we're going to
143:28 - find a way on how to interact with aws
143:31 - s3 from gitlab
143:39 - so how can we interact with the aws
143:40 - cloud and particularly with aws s3 in
143:44 - order to upload our website
143:47 - if you remember in the beginning we
143:48 - talked about how we typically interact
143:51 - with computers and that is through a cli
143:53 - command line interface
143:55 - so for the aws cloud we need a command
143:59 - line interface to be able to upload
144:01 - files for example and fortunately aws
144:04 - has already thought about that and there
144:06 - is an official aws command line
144:09 - interface that we can use there's also
144:11 - here a very
144:13 - in-depth documentation about the
144:15 - different services that are available
144:17 - and throughout the course we're gonna
144:18 - also navigate through this documentation
144:20 - because i want you to understand the
144:22 - process of interacting with such
144:24 - services and the importance of actually
144:27 - using a documentation and not just
144:29 - replicating what i'm doing on the screen
144:31 - now to be able to use the aws cli inside
144:35 - our gitlab pipeline
144:37 - we need to find the docker image and the
144:39 - best place to search for docker images
144:41 - is in docker hub
144:43 - so here inside docker hub i'm going to
144:45 - go ahead and write aws cli
144:50 - and i'm going to find here we'll see it
144:52 - it's under verified content
144:55 - if possible we always try to use
144:57 - verified official docker images i'm
145:00 - gonna take this one
145:02 - so
145:02 - all i have to do is simply copy this
145:05 - and then we're gonna go inside our
145:07 - pipeline
145:10 - open the web id again
145:13 - and let's go ahead and create a
145:14 - completely new job here
145:17 - so i'm going to call this job deploy to
145:21 - s3 and we're gonna add an additional
145:24 - stage here because we need to do this
145:27 - deployment after the ci pipeline is done
145:31 - so we have tested everything and after
145:33 - that if everything passes then we're
145:35 - gonna move to the next stage which is
145:36 - the deploy states and right here stage
145:39 - deploy and the image that you want to
145:41 - use is the one i've just copied
145:44 - which is amazon aws cli
145:48 - now by default this image will not
145:50 - really work the way we have used for
145:52 - example the node image or the alpine
145:54 - image
145:55 - this image has a thing which is called
145:58 - an entry point essentially when we're
145:59 - starting the image there's already a
146:02 - program that's running with that image
146:03 - and this is something that conflicts a
146:05 - bit with the way we're using images in
146:07 - gitlab so we need to override that entry
146:10 - point so for that reason i'm going to
146:11 - move this to a new line i'm going to
146:13 - write here
146:14 - another property which is called name so
146:17 - this is under image and this is not a
146:19 - list and below it i'm gonna write entry
146:22 - point and here
146:24 - i'm gonna overwrite the entry point with
146:27 - this square brackets and this quotes
146:30 - here so then comes the script part and
146:33 - in the script part we are essentially
146:35 - writing
146:37 - well let's test if the tool is working
146:38 - so typically almost every tool
146:41 - will have the possibility of printing
146:43 - out the version so the name of the tool
146:45 - is aws
146:47 - and then the version will be dash dash
146:48 - version
146:50 - now what we're interested in is using
146:52 - aws cli in version two this is the
146:55 - current version another version is
146:57 - version one but we're definitely
146:59 - interested in using version two
147:02 - so for example here from docker hub you
147:04 - can go to the tags
147:07 - and you can also specify
147:09 - a tag
147:10 - if we don't specify a tag we'll always
147:13 - get the latest version
147:15 - but generally it's recommended that you
147:18 - specify a tag in it you go with a tag so
147:20 - for example if i want to specify this
147:22 - tag i'm going to copy the name of the
147:24 - tag i'm going to add here column
147:27 - and then the value that i want to use
147:30 - it's a best practice to always specify a
147:33 - tag but of course by the time you watch
147:35 - this there will be newer version
147:37 - typically anything that starts with two
147:40 - should be good enough
147:43 - so let's commit these changes in a
147:44 - branch and see how the pipeline looks
147:46 - like
147:47 - now right now i'm getting an error and
147:50 - if i'm looking inside the pipeline to
147:51 - see what's going on
147:53 - there are some errors
147:55 - because
147:56 - the stage i have chosen in this case the
147:59 - deploy stage doesn't exist so the
148:02 - pipeline hasn't been executed so we
148:04 - still need to make some changes to it
148:07 - so here where we have the stages i'm
148:09 - gonna add here deploy
148:11 - so now we have the stage build test and
148:13 - deploy again i'm going to commit and see
148:16 - if the pipeline runs
148:18 - so now the entire pipeline has been
148:20 - successful let's take a look at the
148:22 - deploy job see exactly what has happened
148:24 - there
148:25 - and we take a look at the logs here we
148:28 - have used this aws cli image
148:32 - and of course we have printed here the
148:34 - version so we'll see exactly which
148:36 - version we're using
148:37 - i've used the tag so with that tag i'm
148:40 - gonna always get this version but
148:42 - sometimes if you don't wanna specify a
148:44 - tag it's still a good practice to lock
148:46 - the version in the logs
148:48 - just in case you know from one day to
148:50 - the next one or like one month later or
148:52 - something like that your pipeline
148:53 - doesn't work anymore at least you can
148:55 - compare previous logs with existing ones
148:57 - and see if there's any differences in
148:59 - the version that you have used for the
149:00 - various tools
149:07 - i'll tell you right from the start that
149:09 - getting this upload to work will require
149:12 - a bit of learning and a bit of steps so
149:13 - we're not gonna be able to do this right
149:16 - in this lecture and i get some errors
149:18 - just as a heads up but we'll be making
149:20 - progress toward that goal
149:22 - so how do we upload the file so let's
149:25 - continue editing this pipeline now as i
149:27 - said because you're going to need a few
149:28 - tries and go ahead and disable the build
149:31 - website
149:33 - and the test website jobs and this will
149:36 - ensure that we are only running the
149:38 - deployed 2s3 job for the moment until
149:41 - we're ready and we have tested that
149:43 - upload works
149:45 - i always like to make things as simple
149:48 - as possible
149:50 - and whenever i'm working with a new tool
149:52 - i want to make sure that i don't make
149:53 - any mistakes or that i can easily
149:55 - understand what's going on so for that
149:58 - reason i'm gonna leave aside this
149:59 - complexity with building and testing the
150:01 - website we're gonna only focus on deploy
150:04 - 2s3
150:06 - so we have here the aws version and then
150:09 - what you're going to use here is aws
150:11 - this is the name of the cli tool then
150:13 - i'm going to specify the service name
150:16 - which is s3 and then on the service name
150:19 - we are gonna use copy right so we are
150:23 - copying something to s3 now what are we
150:26 - copying well for example let's create a
150:29 - new file from scratch just to make sure
150:31 - that everything is working properly so
150:34 - how do we create a file i'm gonna use
150:35 - echo i'm gonna write here hello
150:39 - s3
150:40 - i'm gonna put it inside
150:43 - let's call this
150:44 - test.txt
150:46 - right
150:47 - so we are putting hello s3 inside this
150:49 - file so we now we have this file so we
150:52 - know exactly what we are uploading
150:55 - and the question is where are we
150:57 - uploading this where we're uploading
151:00 - this to the bucket that we have created
151:02 - now for that reason we need to have the
151:04 - bucket name and if you still have your
151:06 - bucket open you'll be able to see here
151:08 - the destination now this is my
151:10 - destination you will have a different
151:12 - bucket name of course
151:15 - i'm going to go ahead and simply paste
151:17 - this
151:18 - so it's s3 column forward says forward
151:21 - slash the name of the bucket
151:23 - then we have to specify the name of the
151:26 - file so in this case i'm going to keep
151:28 - the same name
151:31 - it's going to be test.txt
151:35 - so we are taking this file that we have
151:38 - created inside gitlab
151:40 - we are uploading it to this bucket
151:42 - and
151:43 - the name of the file will be the same in
151:45 - this case
151:46 - let's give it a try and see how it works
151:50 - because we have disabled the build and
151:52 - the test stages essentially we now only
151:55 - have the deploy to s3 so this makes the
151:57 - entire pipeline execution much faster
152:00 - but let's take a look why this failed
152:03 - so what we did we're still printing the
152:05 - aws version and then the last command
152:08 - that we have executed is this one
152:12 - and i cannot stress how important it is
152:14 - to go through the locks and try to
152:16 - understand what has happened so it says
152:18 - here upload failed
152:20 - trying to upload this file to this
152:22 - location and it says the error
152:24 - essentially is unable to locate
152:26 - credentials
152:27 - the thing is how is aws cli supposed to
152:30 - know who we are
152:32 - i mean if this would work as it is right
152:35 - now we should be able to upload files to
152:37 - any bucket that belong to other people
152:40 - or to delete objects from buckets that
152:42 - belong to other people
152:44 - so we need to tell aws cli who we are
152:48 - and this is what we're going to do in
152:50 - the upcoming lectures and in order to
152:53 - have the entire context in terms of this
152:55 - we have to go through a few stages of
152:57 - preparation
153:04 - existence right now there's something
153:05 - about this pipeline that i don't like
153:08 - and that is because we have something
153:10 - inside here that may change later on
153:14 - and i would really like to make this
153:16 - configurable and not to have this
153:17 - information inside here
153:19 - at the beginning of the course we looked
153:21 - at the defining variables and i've shown
153:23 - you how you can define some variables
153:24 - within the pipeline
153:26 - there's also another place where you can
153:28 - define variables and for that i'm going
153:30 - to copy this value and i'm going to go
153:33 - inside the settings
153:35 - so from your project you can go to
153:37 - settings here on the right hand side
153:40 - and what you're gonna do here is select
153:43 - ci cd
153:44 - and right here in the middle you will
153:46 - see variables
153:47 - i'm going to expand this so that we are
153:50 - able to add variables now typically here
153:54 - we tend to store passwords or secret
153:57 - keys but i'm going to use this bucket
153:59 - name as an example for some important
154:01 - features you need to know about so let's
154:04 - go ahead here and click on add variable
154:07 - and i'm going to name my variable aws
154:09 - underscore s3 underscore bucket
154:14 - and the value will be exactly what i've
154:17 - copied from the pipeline now there are
154:20 - some additional settings here i want you
154:22 - to pay attention to
154:25 - there are two flags one of them
154:27 - is typically enabled by default and that
154:30 - is protect variable
154:31 - this flag alone is the single cause for
154:34 - a lot of confusion and a lot of wasted
154:37 - time for people who are just getting
154:39 - started with gitlab i just wanted to
154:41 - point this out
154:43 - if this flag is enabled
154:45 - this variable that we have defined here
154:48 - will only be available to protected
154:51 - branches so for example the main branch
154:54 - that is a protected branch that
154:57 - information will be available
154:59 - in our current branch i created here a
155:02 - feature branch for working with aws cli
155:07 - if i leave this flag on and try to use
155:10 - this variable there it will not work
155:14 - typically the idea here is let's say you
155:17 - have two environments you have a test
155:20 - environment and you have a production
155:21 - environment
155:23 - you may have different credentials for
155:25 - each systems for securities reason
155:28 - now you typically want to keep the
155:30 - protected variables for the main master
155:32 - branch which deploys to production
155:35 - in this way you ensure that nobody has
155:38 - the credentials to accidentally deploy
155:41 - to production from a different branch so
155:43 - this is a security measure now we are
155:45 - using branches and we are using these
155:47 - services directly so at least at this
155:49 - stage we're going to go ahead and
155:51 - disable this flag because we don't want
155:54 - to protect this variable because
155:56 - it will not be available in our pipeline
155:59 - execution as we are running this on a
156:02 - branch
156:03 - the second flag is disabled by default
156:06 - this is something that will be masked
156:10 - in order to mask a variable this is
156:13 - particularly useful for password so for
156:16 - example if you accidentally try to print
156:19 - one of these variables in your code
156:22 - it will not appear there it will just
156:25 - appear like a must masked out with like
156:28 - simply stars
156:29 - it's okay to have for example usernames
156:32 - or other things like this packet name
156:35 - we don't need to mask this because it's
156:37 - not really a secret but if we had here a
156:40 - password for a password it would make
156:42 - sense to mask this so for this variable
156:45 - i'm gonna disable the protect variable
156:47 - flag and i'm gonna
156:49 - disable the mask variable flag
156:51 - many people think that if you don't have
156:54 - the protect variable flag the variable
156:56 - is somehow public or unprotected that's
156:58 - not the case it's simply available for
157:01 - all the branches inside a project and at
157:03 - this stage that's totally fine i'm gonna
157:05 - change this a bit later on but at this
157:08 - point that is fine
157:10 - i'm gonna go ahead here and add the
157:11 - variable
157:13 - you will be able to see it here being
157:15 - added and of course if you need to make
157:18 - changes to it you can go ahead and make
157:20 - changes from here
157:23 - going back to the pipeline instead of
157:25 - having this value i'm going to start
157:27 - here with the dollar sign
157:29 - aws
157:30 - s3 bucket
157:33 - so the name has to be exactly as you
157:35 - have defined it
157:36 - and
157:37 - don't forget to put the dollar sign in
157:40 - advance because this is what makes this
157:42 - a variable
157:44 - we can run this pipeline again
157:46 - and see how it looks like see if we
157:48 - notice any changes
157:50 - now if we're looking at the logs we
157:52 - should be able to notice something
157:54 - interesting
157:55 - so what is interesting about this is the
157:57 - following
157:59 - we have
158:00 - the same command
158:02 - but now inside the command you notice
158:03 - this variable so
158:06 - when we're writing the command the
158:07 - command that's being locked here this
158:08 - doesn't change it will still show you
158:11 - with the variable so it may seem that
158:13 - it's not resolved but actually if you're
158:16 - looking here at the error that we're
158:18 - getting you will see here that the
158:20 - variable has been resolved
158:22 - an indication that the variable has not
158:24 - been resolved is for example not seeing
158:27 - this text at all
158:28 - so i invite you to play around with the
158:31 - protected flag and to run the pipeline
158:33 - again to see what's happening
158:36 - and you should be able to see here that
158:38 - after s3 you'll got you will get
158:40 - three forward slashes you will not be
158:43 - able to see your bucket name here
158:52 - so let's go back to what we wanted to
158:54 - achieve to actually upload this file and
158:56 - we're still getting this error unable to
158:58 - locate credentials so what should we do
159:01 - should we put our
159:03 - aws email and password somewhere in
159:06 - these variables so that
159:08 - aws can locate them
159:10 - well
159:11 - you're getting pretty warm with that
159:13 - yes essentially we have to provide some
159:15 - credentials but those credentials won't
159:18 - be our regular email and password
159:20 - because that will be highly insecure
159:23 - whenever we're using a service we try to
159:25 - give limited access to that so in order
159:28 - to give this limited access to services
159:30 - in this case we only need s3
159:33 - there is also a service that manages
159:35 - that
159:36 - so if we go back here to aws
159:40 - and search for services the service that
159:42 - we are interested in is iam or yum and
159:46 - this is essentially an identity manager
159:49 - service that aws offers
159:52 - let's go inside it and see what we can
159:55 - do so first of all if you didn't
159:57 - activate multi-factor authentication
159:59 - you're going to get this warning and i
160:01 - definitely recommend that you do that
160:04 - this is just a testing account for me
160:06 - which will be very short-lived so for
160:08 - that reason i didn't do that at this
160:10 - point
160:11 - now here from this service we can create
160:15 - new users essentially we want to create
160:17 - an user
160:19 - that will be able to connect to s3
160:23 - so from the left hand side here
160:26 - let's go to users
160:28 - and i'm gonna add a new user
160:33 - so we have to give here this user a
160:35 - username
160:36 - can be any user typically i'm using
160:38 - something so that i know why i created
160:40 - it so i'm going to call this gitlab i
160:42 - know exactly that this user is intended
160:44 - for gitlab
160:46 - and we have to select an access type
160:49 - now what we are interested in is also
160:52 - called programmatic access so we're
160:54 - going to click this and you will see
160:56 - here that this enables an access key id
160:59 - and secret for aws api cli
161:03 - and so on so we're interested in having
161:05 - this for aws cli
161:08 - so this is why we are creating this user
161:11 - and we are enabling this programmatic
161:13 - access
161:14 - we don't need a password we don't need
161:16 - this user to have access to the
161:18 - management console like we do so for
161:20 - that reason that's sufficient let's go
161:23 - to the next part which is the
161:24 - permissions so essentially the
161:26 - permissions tell what the user is
161:27 - allowed to do
161:29 - and permissions in aws i'm going to say
161:31 - it's not an easy topic so i'm going to
161:33 - go the most straightforward way i'm
161:36 - going to attach an existing policy so
161:38 - i'm going to go here to attach existing
161:40 - policies
161:41 - in this search bar i'm going to select
161:44 - s3 i'm going to search for s3
161:47 - you're going to get some predefined
161:49 - policies so policy is essentially like a
161:51 - set of rules that we can apply to a user
161:54 - and the set of rules that we're going to
161:55 - use here is
161:57 - amazon s3 full access so essentially
162:00 - we're gonna give this user access
162:03 - to everything that is aws s3 so
162:08 - we should be able to create new buckets
162:09 - should be able to delete files and so on
162:11 - so a bit more than what we actually need
162:13 - for this use case
162:15 - but just to simplify things i'm gonna
162:17 - give this full access to the user but of
162:20 - course it's the topic on its own
162:23 - let's go to the next page we see the
162:25 - tags we don't need to add any tags here
162:29 - and then we'll be able to review what
162:31 - we're trying to do
162:33 - and we can go ahead and create this user
162:38 - so now we have successfully created a
162:40 - new user
162:42 - and aws has created something for us and
162:45 - that is an access key id and a secret
162:48 - access key
162:50 - to put it in plain english this is like
162:51 - a username and a password you will see
162:53 - here that the password is not even
162:55 - displayed
162:56 - so what are we going to do with this
162:58 - information so first of all let's go
163:00 - ahead and copy the access key id
163:03 - go inside gitlab
163:06 - again going to go to settings ci cd
163:11 - and expand here the variables
163:14 - and let's go ahead and add a new
163:16 - variable
163:17 - i'm going to start here writing aws and
163:20 - you will see there already some
163:22 - predefined things that pop up
163:25 - so one of them is aws access key id
163:29 - this has to be written exactly as you
163:31 - see it here
163:32 - there's something that's different about
163:34 - this
163:35 - away cli will not be able to pick up
163:37 - this variable it will look exactly for
163:39 - this variable name and will
163:40 - automatically pick it up without us
163:42 - doing anything else so it has to be
163:44 - exactly as it is i'm going to paste here
163:46 - the value of course this is as per my
163:48 - account so you'll have a different value
163:51 - and i'm gonna disable the protect
163:53 - variable flag because we want to be able
163:55 - to use this in a branch as well
163:57 - i'm gonna go ahead add this variable
164:00 - and going back to aws i'm gonna also
164:03 - show here the secret access key
164:06 - of course i'm gonna delete this right
164:08 - after recording
164:09 - but i'm showing you just to know exactly
164:11 - how it looks like it's also important
164:13 - that you don't add any spaces or
164:15 - anything like that
164:16 - so i'm going to copy this secret access
164:18 - key
164:19 - and go here add a variable
164:23 - and i'm going to start typing aws
164:27 - secret access key and i'll paste the
164:29 - value here
164:31 - i'm gonna disable the protect variable
164:33 - flag
164:35 - and click on add variable
164:39 - finally there's still another thing that
164:40 - we need to configure and that is our
164:42 - region
164:43 - so again i'm gonna add here variable
164:48 - start typing aws and we have here the
164:51 - default region so when we're setting
164:53 - this variable
164:54 - all services that we use will be in this
164:56 - region and we don't need to specify the
164:58 - region all the time aws know exactly
165:01 - where this bucket or any other service
165:03 - is located
165:05 - and we cannot simply write here anything
165:08 - we have to write exactly as aws uses it
165:11 - internally so what do i mean by that
165:13 - well let's go back to the aws console
165:18 - and you will see here the s3 service and
165:21 - recently visited so this will make our
165:23 - life easier in terms of
165:25 - grabbing this information
165:27 - and for example for my bucket this is in
165:30 - us east north virginia
165:33 - what we're actually interested in is
165:34 - this code
165:36 - so i'm going to copy this information
165:39 - and paste it here so this will be the
165:40 - default region
165:42 - and again i'm not going to protect this
165:44 - variable
165:46 - and
165:47 - i'm going to add it here
165:48 - what i forgot to do in terms of the
165:51 - secret
165:52 - access key is i haven't masked it so it
165:56 - would be idea to go back to it click on
165:58 - this edit
166:00 - and you will see here this flag
166:03 - because this is essentially a password
166:05 - and i click here this flag to mask it
166:08 - and update it and then you also have
166:10 - here an overview like which one of them
166:13 - is protected
166:14 - which one of them is masked and of
166:16 - course if you need to inspect some of
166:18 - these variables you can go back and
166:20 - click on them and you will be able to
166:21 - see the value but list on this overview
166:24 - here
166:25 - they are hidden
166:27 - there's also this possibility of copying
166:28 - the value without revealing this can be
166:31 - useful as well or you can click here on
166:33 - reveal values and they will be displayed
166:36 - all right so it seems that we have
166:38 - everything in place now at least we have
166:40 - these credentials
166:42 - and because we named them the way we
166:44 - named them aws cli will automatically
166:47 - pick them up so there's actually nothing
166:49 - that we need to change in our pipeline
166:52 - so for that reason we can simply go
166:53 - ahead and rerun the same pipeline once
166:56 - again
166:57 - i'm going to go here to ci cd pipelines
167:00 - go here to the job that failed
167:04 - and here i can simply click on retry
167:08 - this will start the exact same job once
167:11 - again
167:12 - all right so this already looks so much
167:15 - better
167:16 - the job has succeeded we don't see here
167:19 - any errors aws cli is telling us which
167:22 - files have been uploaded
167:24 - so we could jump into s3
167:29 - and take a look inside our bucket to see
167:31 - if we have any files here you will be
167:34 - able to see here our test.txt
167:37 - file right inside here say when it was
167:40 - modified and so on
167:42 - we can download it if we need to
167:44 - but essentially this has been the first
167:47 - major step in terms of interacting with
167:51 - the aws cloud
167:57 - all right so we have made some progress
167:59 - and we have managed to finally upload
168:01 - this file
168:03 - but if you remember we have an entire
168:05 - website with a lot of files
168:08 - so you know going file by file and
168:11 - uploading that is not really the way to
168:13 - go
168:14 - so i can make sure that whatever we have
168:16 - inside the build folder it's going to be
168:18 - synced to our packet
168:20 - now if you remember in the beginning i
168:22 - mentioned the documentation the
168:24 - reference for the aws cli essentially
168:27 - for any service there is here
168:30 - documentation there is here an
168:31 - additional command so we have used aws
168:34 - s3 as the name of the service
168:36 - so in this list of services here you
168:39 - will find
168:40 - s3 somewhere
168:43 - so here we go this is s3 so i'm going to
168:46 - click on it
168:48 - i'm going to be honest with you in the
168:50 - beginning this documentation will look
168:52 - very scary
168:53 - but if you take
168:55 - a bit of time if you have a bit of
168:57 - patience and you go through the
168:58 - documentation
169:00 - this is the way to really master the cli
169:02 - commands that you will be using and this
169:05 - doesn't only apply to aws cli applied to
169:08 - any tools out there so reading the
169:10 - documentation looking at all the
169:11 - parameters everything that you can do
169:13 - with this is really super helpful so for
169:16 - s3 you will see here a lot of
169:18 - documentation but also at the end you
169:20 - will see here a list of available
169:22 - commands
169:23 - so we have used
169:25 - cp for copy so we've copied this file
169:28 - and it's also definitely possible to
169:30 - copy an entire folder
169:32 - but i wanted to show you also different
169:35 - command which is sync now typically
169:37 - syncing something means that whatever we
169:39 - have on one side we have on the other
169:43 - side as well
169:45 - for example to ensure this with copy we
169:47 - first have to remove the folder just to
169:50 - ensure that we really have everything so
169:52 - for example if we added the file or we
169:54 - have removed the file
169:56 - we don't want to have those files on s3
169:59 - anymore if we don't have them on our
170:00 - website anymore
170:02 - so for that reason using something like
170:04 - sync which ensures that all the files
170:06 - are in sync does make a lot of sense so
170:09 - we can go here on sync
170:11 - and we'll
170:13 - tell right from the description it syncs
170:15 - directories and
170:17 - essentially it ensures that whatever we
170:19 - have there on one side is also available
170:22 - on the other side so how do we use this
170:25 - let me
170:26 - demystify this and take a look at our
170:29 - existing pipeline that we have here so
170:32 - first of all we're going to give up on
170:34 - this test file that we have uploaded
170:36 - here this was just for
170:38 - testing purposes and to make sure that
170:40 - we have everything set up correctly
170:43 - so we're gonna use here aws s3 so the
170:47 - command will be sync
170:49 - so what are we thinking we're syncing
170:52 - the build folder
170:55 - so instead of that file we're going to
170:57 - have an entire folder
170:59 - and the question is where are we syncing
171:01 - this we're syncing this to the bucket so
171:04 - we're going to remove this part here
171:06 - because we are going to put it directly
171:08 - inside the root
171:10 - and additionally we're going to also add
171:12 - a flag
171:14 - so if you're looking here through all
171:16 - the options that are available one of
171:18 - the options should be dash dash delete
171:22 - it will essentially ensure that if we
171:24 - delete some files
171:26 - during our build process which existed
171:29 - previously in previous builds that are
171:31 - also deleted during the sync
171:33 - so if i had a file in the build folder i
171:36 - synced it in the last build then i later
171:38 - removed that file i want that one
171:40 - removed from s3 as well
171:44 - so i'm gonna add here
171:46 - delete
171:48 - so this essentially should be enough to
171:50 - upload
171:51 - our build folder now in order to get
171:53 - this to run of course we have to
171:55 - re-enable our previous jobs
171:59 - so now build and test are enabled gonna
172:03 - commit these changes and see if this
172:05 - command worked
172:09 - and the pipeline is now successful and
172:12 - we're most interested in this last job
172:15 - that we have executed
172:17 - and now if we're looking at the logs
172:18 - we'll see exactly which files we are
172:21 - uploading so we'll see here uploading
172:23 - this uploading that so these are all the
172:25 - files that we have inside the build
172:27 - folder
172:28 - what's also important to notice is you
172:30 - notice here this delete so this file is
172:33 - then deleted because it no longer exists
172:36 - in the build folder it actually never
172:37 - existed in the build folder but sync
172:40 - detects that this file doesn't exist
172:42 - there and says okay if it doesn't exist
172:44 - in the build folder it doesn't make any
172:45 - sense to stay on s3 and goes ahead and
172:48 - removes it
172:50 - so let's take a look at the s3 service
172:52 - i'm going to refresh this page and take
172:54 - a look to see how it looks like
172:58 - so now we are able to see
173:00 - all the files that we are actually
173:03 - interested in there all have been
173:05 - uploaded here
173:07 - and we made a very important step
173:09 - towards hosting our website in the aws
173:12 - cloud
173:18 - now currently the files that we have
173:20 - uploaded here they are not publicly
173:22 - available whenever we create a bucket by
173:25 - default it will be private to us so
173:28 - nobody else external has access to it
173:31 - actually that's kind of like the normal
173:33 - use case you don't want the files that
173:34 - you're putting here to be available for
173:37 - everyone but of course there are use
173:39 - cases when you want to make these files
173:41 - accessible to anyone else and actually
173:44 - quite a lot of companies are using s34
173:47 - storing files that they offer for
173:49 - download because it's so much cheaper
173:51 - than hosting them on their own website
173:54 - now in order to make this bucket
173:56 - publicly accessible we need to change a
173:58 - few things
173:59 - and we're going to start here with
174:01 - properties
174:03 - and actually what we are definitely
174:05 - interested in is enabling
174:08 - static web hosting so again i'm here in
174:11 - my bucket inside properties
174:14 - right at the end there's static website
174:17 - hosting
174:18 - i'm gonna go ahead here click on edit
174:22 - and we're going to enable
174:24 - static website hosting
174:26 - and
174:27 - the hosting type will be host a static
174:29 - website
174:31 - and there are also some settings here
174:33 - that we are gonna input
174:36 - websites typically have an index page
174:39 - which is a start page and also they have
174:41 - an error page
174:43 - for this application that we have the
174:45 - index page and the error page are the
174:47 - same and this is the index.html file if
174:50 - you look inside the bucket you should
174:52 - see the index.html file
174:54 - so this is exactly what we're going to
174:56 - have here for the index and the error
175:00 - so i want to save these changes
175:03 - and we're still under properties
175:06 - and if we go back here at the static
175:09 - hosting part
175:11 - you would see here that now we have an
175:13 - address so now our website is hosted on
175:16 - aws we could also get the domain and
175:19 - point this to this address
175:21 - but for what we're trying to do right
175:23 - now this is board enough so we can go
175:24 - ahead and click on it
175:26 - and in the beginning we're gonna get
175:28 - this error and there's still a few
175:30 - things we need to configure don't worry
175:31 - about it
175:32 - just wanted to point out the address and
175:34 - where you can view it
175:36 - so let's see which other settings do we
175:38 - need to make i'm gonna go here to
175:40 - permissions
175:42 - and you will see here the first thing
175:44 - that appears is access bucket and
175:48 - objects not public
175:50 - whatever we have here in the bucket is
175:53 - not public so this is why even if we
175:55 - have enabled static website hosting we
175:58 - still cannot access this information so
176:00 - let's take a look at how we can
176:02 - enable public access i'm going to click
176:05 - here on edit
176:06 - and i'm going to disable this block all
176:09 - public access
176:11 - and save the changes
176:13 - and again because this is like a major
176:15 - thing abs really wants to make sure that
176:17 - we know what we're doing so i'm going to
176:19 - enter here confirm
176:22 - i've been going to the website we can
176:24 - try a refresh and it will still display
176:27 - the same error page here so there's
176:30 - something that's still not working
176:31 - properly
176:33 - what we need to do in addition to what
176:35 - we did here with in regards to the
176:36 - public access
176:38 - is a bucket policy
176:40 - now essentially we need a policy for the
176:42 - objects that are inside the bucket
176:45 - i want to get too much into the details
176:47 - but essentially we need to write this
176:50 - policy so we can go ahead here and click
176:52 - on edit this is like policy generator
176:56 - now what you see here is json
176:58 - and this is the format in which this
177:01 - policy will be written here
177:03 - there are a few things that we need to
177:05 - change about this policy so essentially
177:09 - we can go ahead here and add an action
177:12 - so let's search for example for s3 i'm
177:14 - going to select hit here
177:16 - and the permission that we are looking
177:19 - for is essentially get object
177:22 - i'm going to click on this get object
177:25 - but additionally there are also a few
177:27 - other things that we need to change here
177:30 - just to make sure that everything is
177:32 - working properly and this changes will
177:34 - be made in text
177:37 - but you also find the template in the
177:38 - course notes just in case something
177:41 - doesn't go well you will have the
177:43 - possibility of using that
177:45 - so essentially i'm gonna give here a
177:46 - name and i'm gonna call this public read
177:50 - this is just a name to identify this
177:52 - policy
177:53 - and the principle here we need to be put
177:57 - between
177:59 - double quotes
178:00 - and i'm going to put here this star and
178:04 - then the action will be
178:06 - get s3 object that's allowed the effect
178:09 - is allow
178:10 - and resource we also need to specify the
178:13 - resource and the resource is our bucket
178:15 - i'm going to copy this name it's
178:18 - essentially the address of the bucket
178:21 - and i'm gonna put this here also in
178:24 - quotes
178:25 - and i'm gonna add here very important a
178:27 - forward slash
178:29 - and this star which essentially means
178:32 - that
178:33 - everything that is in this bucket
178:36 - can be
178:37 - retrieved get object
178:39 - it is allowed
178:41 - so this is essentially our policy for
178:43 - public
178:44 - reading all right i agree it's not so
178:47 - easy for beginners but let's hope we
178:49 - haven't made any errors here and that it
178:52 - will work as expected i'm gonna click
178:55 - here on save changes
178:56 - to apply this policy
178:58 - you will see here warnings all over the
179:01 - place you will see under the bucket in
179:03 - red publicly accessible
179:05 - access public
179:06 - so
179:07 - aws is really trying to tell you hey
179:10 - watch out uh if this is not what you
179:12 - intended or this is not what you want
179:15 - this bucket is publicly accessible so
179:18 - really really make sure that you know
179:19 - what you're doing
179:21 - but for us it's fine this is what we
179:22 - wanted to have
179:24 - and now by refreshing this page we're
179:28 - getting the website that we have created
179:31 - and now congratulations really this is
179:34 - absolutely amazing we created pipeline
179:37 - that now takes every commit that we make
179:41 - and deploys it in the aws cloud we take
179:44 - it through all the stages
179:46 - but we only have to make the change and
179:48 - the entire process is automated so if
179:50 - you manage to follow along up to this
179:52 - point
179:53 - wow
179:54 - i'm sure that you have learned so much
179:57 - and trust me
179:58 - we're just getting started there's still
180:00 - so much more to learn and i'm super
180:01 - excited about the upcoming steps
180:09 - now let's go back to the pipeline that
180:11 - we have designed now
180:13 - and i'm still here in an open merge
180:15 - request
180:16 - and
180:17 - if i'm looking at the pipeline i have
180:19 - here all the three stages so the build
180:23 - the test and the deploy
180:25 - now if you think about it this really
180:27 - doesn't make a lot of sense
180:29 - and what doesn't make sense is to deploy
180:32 - to s3 from a merge request or from a
180:36 - branch now if we think as s3 being our
180:40 - let's say our production server or
180:42 - production environment
180:44 - then it means that anyone who opens a
180:46 - merge request and tries out some small
180:50 - changes
180:51 - if the build and the test pass this will
180:54 - automatically get deployed to production
180:57 - and actually this is not what we want
181:00 - we want
181:02 - inside a merge request or as long as we
181:04 - are in a branch we just want to run the
181:07 - build and test essentially to simulate
181:09 - the execution of the main branch
181:12 - and then
181:13 - only when we merge this we want to run
181:16 - the deploy so we want to deploy to
181:18 - production
181:20 - so in order to achieve this we still
181:21 - need to make a few changes to our
181:23 - pipeline
181:25 - now this is how the configuration looks
181:27 - like at this point
181:29 - and the changes that we need to make are
181:32 - here in the deploy to s3 job
181:36 - now how do we exclude this job
181:40 - from this pipeline which is running on
181:43 - the branch and to ensure that it only
181:45 - runs on the main branch gitlab has this
181:48 - feature which is called rules now rules
181:52 - allow you to create really complex
181:54 - conditions and i won't get into those
181:57 - but we're gonna use a relatively simple
182:00 - condition which will check if we are on
182:03 - the main branch or not
182:05 - now in order to set up rules we're going
182:07 - to go to the deploy to s3 job
182:10 - and what i'm going to write here is
182:12 - rules
182:13 - this is a completely new keyword
182:15 - and here we can define a list of rules
182:18 - now we're going to add only one rule and
182:20 - this will be a list so you notice that
182:23 - i'm starting a list and then we have
182:25 - here an if column
182:27 - and now after the if we can specify a
182:30 - condition
182:31 - so in a condition we're typically
182:34 - checking if something equals or does not
182:37 - equal something else
182:39 - so
182:40 - in this case you want to check if the
182:43 - branch we're currently at
182:45 - so we have here something i don't know
182:47 - where we're at
182:49 - equals so we're gonna make two equal
182:52 - sign
182:53 - the main branch right so if
182:57 - we are on the main branch only then run
183:00 - this right
183:02 - now we don't want to put here something
183:04 - hard-coded so we cannot
183:06 - know in advance like which is this
183:08 - branch name where we are currently
183:10 - evaluating this condition so in my case
183:13 - i mean feature deployed to s3 or
183:16 - something like that so it doesn't make
183:18 - sense to add that there to to check this
183:21 - it needs to be dynamic so in order to
183:23 - have something dynamic we need to use
183:25 - some variables
183:27 - now luckily gitlab comes with some
183:30 - predefined variables
183:32 - and one of these variables is
183:34 - ci commit ref name so i'm going to
183:37 - search here for ref name
183:40 - you will see here
183:42 - so see i commit ref name
183:44 - this will give us dynamically
183:47 - the branch or tag name for which the
183:50 - project is built
183:52 - so we can use this as a variable
183:55 - and i'm going to write here dollar sign
183:58 - this one
183:59 - so this will be
184:01 - evaluated to
184:03 - our current branch now with any of these
184:06 - variables if you're not sure what they
184:08 - do how they look like
184:10 - simply use here echo
184:12 - and just keep something like this for
184:14 - debugging purposes in your pipeline
184:16 - until you get
184:17 - familiar with the values that you're
184:20 - having i'm going to remove it because i
184:21 - don't need it but
184:23 - for you definitely recommend
184:25 - use echo to inspect the different values
184:28 - that you're using
184:30 - if you were on a different branch this
184:31 - would be something else so
184:33 - if this equals main then we're gonna
184:37 - essentially
184:38 - run this job otherwise it will be
184:40 - excluded from the pipeline
184:42 - still having something hard coded here
184:44 - is also not something that we like doing
184:47 - yes we could keep here main
184:50 - but there are also variables that can
184:53 - handle this so another variable that we
184:55 - can use is default branch
185:00 - it should be ci default branch
185:04 - and this variable will give us
185:06 - dynamically give us the name of the
185:08 - default branch so if later on for
185:10 - whatever reason we decide to switch from
185:13 - master to main or from main to
185:16 - something else
185:17 - then we don't need to worry about these
185:18 - pipelines because the default branch
185:20 - will automatically get updated in this
185:23 - variable
185:25 - and then
185:26 - we can use it directly here so again
185:28 - i've added here another variable which
185:32 - is the ci default branch so now
185:34 - everything is dynamic so this rule makes
185:37 - sure that when the current branch
185:40 - equals the default branch this code gets
185:43 - executed in our case right now for maine
185:46 - will have this job
185:48 - in other pipelines this job will get
185:51 - excluded not be part of the pipeline
185:54 - so i'm going to commit this
185:57 - and i'm going to take a look at the
185:58 - pipelines after this
186:01 - so what i invite you to do is to go to
186:03 - cicd pipelines
186:05 - and now our pipeline only contains two
186:08 - stages
186:11 - so we have the build and the test stage
186:13 - the deploy job was completely removed
186:18 - i can also inspect the merge request and
186:20 - take a look at what is going on here and
186:22 - also i see that this pipeline has been
186:25 - updated it contains only build and test
186:29 - so at this point i would say okay i'm
186:31 - pretty confident that this functionality
186:32 - is working properly gonna click here on
186:34 - merge
186:35 - and also let the main
186:38 - pipeline run
186:40 - so again if we're looking here cicd
186:42 - pipelines
186:45 - we will now see that
186:47 - the main pipeline has started so this is
186:50 - on the main branch
186:53 - and now this pipeline contains the three
186:56 - jobs required so we have build test and
186:59 - deploy
187:06 - so now the main pipeline is also working
187:09 - we have deployed again to s3
187:12 - but how do we know if our website is
187:15 - still working
187:17 - again we could go to the address again
187:20 - hit the refresh button check again if
187:21 - it's working
187:23 - but again that's not really the point of
187:24 - automation we want to make sure that the
187:27 - entire process is automated
187:29 - so if this is a worry that we have like
187:32 - is the website still working that we
187:34 - have deployed it's probably time to add
187:37 - another stage and to test after this
187:39 - deployment if everything works correctly
187:42 - on the website or at least as much as
187:44 - possible
187:46 - so let's open up the ide and take a look
187:50 - here at adding another stage
187:54 - so we already have here build test
187:57 - deploy
188:01 - so
188:01 - probably the next stage should be
188:04 - let's call it post deployment
188:07 - so this will be
188:09 - after the deployment
188:11 - and then it's probably time to also add
188:13 - another job
188:15 - i'm going to call this production tests
188:18 - and i want to make sure that the stage
188:21 - is the right one
188:22 - let's post deploy
188:25 - and what i'm proposing is to simply run
188:28 - a curl command so that's pretty similar
188:30 - to what we did here
188:32 - so simply going to go ahead and copy
188:34 - this
188:35 - of course this has to be under the
188:37 - script block
188:39 - and there are a few things that we need
188:41 - first of all we need a docker image that
188:43 - has curl
188:45 - and one option would be to search again
188:47 - docker hub for such an image but
188:50 - essentially any docker image that has
188:52 - curl would be sufficient
188:55 - so i can go ahead here on docker hub and
188:57 - search for curl
188:59 - and what i've used in the past is curl
189:02 - images curl
189:03 - but probably also one of these verified
189:05 - content images is probably just as good
189:10 - so the address to this image is this one
189:13 - so this is what we need to do without
189:15 - docker pull
189:16 - so we have here image
189:19 - and paste remove this docker pull
189:23 - that's the name of the image
189:24 - and because curl is such a generic tool
189:28 - don't need to specify a specific version
189:30 - what we're doing is pretty basic and i'm
189:32 - pretty sure it's not going to change
189:34 - very soon
189:36 - the other thing is the address right so
189:40 - the address where we have deployed this
189:43 - and this is available in s3
189:46 - so we've been going back in s3 to our
189:49 - bucket
189:50 - looking here at properties
189:53 - and right at the end this is the address
189:56 - right so can go ahead and copy my
189:59 - address
190:01 - go back to the editor
190:04 - i could paste it here right
190:06 - but again we had a discussion about like
190:09 - not having
190:10 - you know things that could change later
190:13 - on inside our pipeline or have it all
190:15 - over the place so again let's go ahead
190:18 - and define a variable now this time i'm
190:20 - gonna define a variable within the
190:22 - pipeline itself
190:23 - that's also totally fine and also way on
190:26 - how to do things i'm gonna define here
190:28 - variables block
190:30 - and let's call this variable
190:33 - app base url
190:35 - of course you're free to name it as you
190:36 - wish column and then i'm gonna paste
190:39 - here the address to it
190:42 - so this is now the full address so it's
190:44 - all i need is
190:46 - this this is the variable name
190:51 - and here instead of writing something
190:53 - like this
190:55 - and simply go ahead
190:57 - use curl app base url
191:00 - i'm going to search for react tab
191:01 - because this is what we have actually in
191:04 - the body of the index html that are
191:07 - grabbing
191:08 - this should be enough at least to test
191:10 - that what we have on s3 is still
191:13 - reachable that at least this text is
191:15 - still available there
191:17 - additionally we need to make another
191:19 - configuration because otherwise as it is
191:21 - right now this will also run inside the
191:24 - branch so i'm going to copy this rule
191:26 - that we have here on deploy to s3
191:29 - same rule is valid here we want to run
191:32 - these jobs only in the main
191:34 - pipeline so i'm going to go ahead commit
191:37 - these changes and merge them into the
191:39 - main branch
191:41 - and i'm a pretty confident that this
191:43 - pipeline will work without any issues so
191:46 - there's no additional review that i need
191:48 - at this point i'm going to simply merge
191:51 - when the pipeline succeeds
191:52 - and i'm going to grab a coffee until
191:55 - this is merged
191:58 - so after a few minutes here in the merge
192:00 - request we'll see a few things so first
192:02 - of all we have here the pipeline for our
192:05 - branch with two stages
192:08 - we have to build and the test
192:11 - that's it
192:12 - after we have merged this then the
192:15 - pipeline for the main branch has started
192:17 - and this pipeline then has four stages
192:21 - build test deploy
192:24 - and post deploy so essentially the stage
192:28 - that we have added right now and this
192:30 - contains the production tests let's take
192:33 - a look at them to see what has happened
192:34 - here
192:37 - so we have executed this command
192:40 - carl was able to download the website
192:43 - has passed this information to greb and
192:45 - grab a search for react app in the text
192:48 - and somewhere here was react app so for
192:51 - that reason
192:52 - everything worked out successfully so we
192:55 - know that website is still there it's
192:57 - still working so that's perfect
193:04 - so let's take a minute and take a look
193:06 - at what we did so far so this is our
193:09 - main pipeline right this is the pipeline
193:11 - that now deploys to aws s3
193:15 - and we have the build and the test this
193:18 - is what we essentially call continuous
193:20 - integration right this happens also in
193:23 - the merge request but also here we are
193:24 - rechecking that our build is still
193:26 - working and let those tests are executed
193:29 - here the second part
193:31 - this is the cd part in ci cd
193:35 - now cd can mean continuous deployment or
193:39 - continuous delivery i'm going to explain
193:40 - a second what that means
193:42 - right now we're kind of doing continuous
193:44 - deployment in the sense that every
193:47 - commit that goes into the main branch
193:49 - will actually be deployed to production
193:53 - we have automated the deployment to s3
193:56 - and we said that you know our s3 bucket
194:00 - it's hosting our website and is
194:02 - essentially our production environment
194:04 - that the whole world can see
194:07 - now this is a very simplistic view
194:10 - on
194:11 - cicd
194:13 - now quite often pipelines also have a
194:16 - staging environment or a pre-production
194:18 - environment essentially if we make
194:21 - changes to our deployment to s3 and
194:24 - something doesn't work anymore
194:27 - we're not really testing this before
194:30 - and the main branch may still produce
194:32 - some errors and again we come back to
194:34 - the same problems we had when we did ci
194:37 - when nobody else can work on a project
194:40 - anymore
194:41 - so for that reason it kind of makes
194:43 - sense also to add another environment
194:47 - at least if the environment if there are
194:49 - some issues with the environment before
194:52 - production and the main pipeline breaks
194:54 - at least we haven't affected the
194:55 - production environment so it's still
194:57 - good right now we're just deploying
194:59 - anything and we're testing afterwards
195:01 - what is a staging environment so a
195:03 - staging environment is essentially a
195:05 - non-production usually non-public
195:08 - environment that is actually very close
195:11 - to the actual production environment
195:13 - right so we want to keep things as
195:15 - similar to the production environment as
195:17 - possible
195:18 - quite often we use automation to create
195:20 - these environments and to ensure that
195:22 - they're really identical we haven't done
195:24 - that we have used manual work to
195:27 - actually create the s3 bucket and we did
195:29 - some automation afterwards but ideally
195:31 - we would create this entire
195:32 - infrastructure automatically
195:35 - so the idea is to add a staging
195:37 - environment as a pre-production
195:39 - environment essentially
195:40 - we want to try out our deployment on
195:43 - pre-production before we go to
195:45 - production so in case it fails the
195:48 - production environment is unaffected
195:51 - the main two concepts i want you to be
195:52 - aware of is continuous deployment and
195:55 - continuous delivery now with continuous
195:58 - deployment as i said every commit that
196:01 - goes through the pipeline
196:02 - would also land on the production server
196:06 - in the production environment
196:08 - with continuous delivery we're still
196:10 - using automation
196:12 - and every successful merge request again
196:14 - triggers the main pipeline which leads
196:16 - on automatic deployment to the staging
196:18 - environment
196:20 - however we don't go directly to
196:22 - production there is like a button that
196:25 - you click in order to promote a build
196:29 - from the pre-production environment to
196:31 - the production environment
196:33 - so these are like the differences
196:35 - between continuous delivery and
196:36 - continuous deployment
196:38 - in upcoming lectures we're gonna get a
196:40 - bit more into them and you're gonna
196:42 - better understand like what they really
196:44 - mean
196:50 - now it's time for you to practice a bit
196:53 - more and at this point i have an
196:55 - assignment for you
196:57 - and the assignment is essentially you
196:59 - building a staging environment
197:02 - and i want you to follow essentially the
197:03 - same process as we did when we created
197:06 - the production environment i also want
197:08 - you to make sure that you write some
197:10 - tests that ensure the environment is
197:12 - working as expected
197:15 - please pause the video and take this as
197:17 - an opportunity to practice along more
197:20 - both with aws and with building the
197:23 - pipeline
197:29 - i hope you had a good assignment and
197:31 - that everything is working properly in
197:33 - this video i wanted to show you how i
197:35 - would do something similar
197:37 - so first of all i'm going to start with
197:39 - creating the bucket and i'm just going
197:41 - to copy this name that i already have
197:43 - click here on create bucket
197:45 - and essentially i'm going to call this
197:47 - staging essentially the same name but
197:50 - i'm just adding staging to it
197:53 - and in the beginning i'm going to leave
197:55 - all settings as they are
197:57 - and in the upcoming steps i will
198:00 - essentially ensure that this bucket is
198:03 - public
198:04 - so they are the same steps as before and
198:07 - i'm not going to go again over them
198:09 - so after a few clicks i now also have
198:12 - this staging bucket also as public and
198:16 - i've enabled website hosting which means
198:20 - you can go inside the bucket you can go
198:22 - here to the properties
198:24 - and then
198:25 - right here at the bottom i will see this
198:29 - address which we will need later on
198:32 - first of all let's begin
198:34 - with the name itself so i'm going to
198:35 - copy it
198:36 - and i'll have to go inside gitlab and
198:39 - save it in a variable
198:41 - so here in gitlab i'm going to go to
198:43 - settings ci cd
198:45 - go here to variables
198:47 - what you'll notice here is that we
198:49 - already have an s3 bucket
198:53 - of course that's kind of an inconvenient
198:55 - at this point so we'll need to figure
198:57 - out exactly how we can manage this what
199:00 - i'm going to go ahead
199:02 - and also call it aws
199:06 - s3
199:07 - bucket
199:09 - i'm going to call it staging
199:12 - i'm going to add this thing that will
199:15 - make it different
199:16 - essentially now we can protect this
199:18 - variable because we don't want to deploy
199:21 - from a branch anymore
199:23 - so it definitely makes sense to have
199:25 - this protected
199:26 - masking it doesn't make any sense at
199:29 - this point
199:30 - all right so at least now i have this
199:33 - i'm gonna copy the name so that they
199:36 - don't forget it
199:37 - and let's go inside the project open the
199:40 - web ide and start making some changes to
199:42 - the pipeline
199:44 - now first of all because this is a
199:46 - pre-production environment we need to
199:48 - define another stage
199:51 - before we deploy to production let's
199:53 - call this deploy staging and of course
199:56 - what we need to do here is to
199:59 - define a new job
200:02 - now most of this job is pretty similar
200:04 - to what we already have here in terms of
200:07 - deploy to s3 and also in terms of the
200:10 - production test that we have here
200:12 - so it kind of makes sense to copy
200:15 - everything that we have
200:19 - and paste it here again
200:23 - and let's call this deploy to
200:26 - staging
200:29 - and we can call this deploy to
200:31 - production
200:34 - just to have like
200:35 - more clarity in terms of where we're
200:37 - deploying and what we're doing
200:39 - also we need to have the right stage
200:43 - so the stage name is deploy staging
200:47 - apart from this we are using the same
200:49 - image
200:50 - we still want to run this on the main
200:52 - branch
200:55 - and the only thing that we need to adapt
200:57 - here is the bucket so it's going to be a
200:59 - little s3 bucket staging
201:03 - and again the test that we have here
201:06 - they can be called staging tests
201:09 - we'll see here that
201:11 - you know the editor is complaining about
201:12 - the duplicate key
201:14 - so that's a good thing because you know
201:16 - exactly what we need to fix
201:20 - so then we have the staging tests
201:22 - and also here
201:25 - where we need another base url
201:27 - so that's also something that we need to
201:30 - consider
201:32 - i'm gonna add staging to this one as
201:34 - well
201:36 - and from aws i'm gonna copy the url
201:42 - paste it here you can notice that we
201:43 - have two different urls
201:46 - so let's double check everything
201:49 - we have added a new stage
201:50 - deploy staging which is before deploy
201:53 - or we can even call it deploy production
201:57 - right so we have even more clarity just
202:00 - need to make sure that we are adopting
202:02 - this everywhere
202:04 - so we have here deploy staging
202:08 - and then we have deploy production
202:12 - but then we have post deploy now we also
202:15 - want
202:16 - to have this testing
202:18 - after the staging so we need
202:21 - something like test
202:23 - staging for example and we can call this
202:26 - test production right so we're deploying
202:29 - to staging we're testing the staging
202:32 - if both of them are successful then we
202:35 - go ahead and deploy to production and
202:37 - after that we test again the production
202:42 - so let's make sure that we have the
202:43 - right stages
202:45 - otherwise we're going to get some issues
202:48 - so deploy production
202:50 - yes test
202:52 - production
202:55 - test
202:56 - staging
202:58 - deploy staging
203:00 - we have the right bucket name
203:03 - here we don't have the right url we need
203:05 - to grab that as well
203:07 - for staging
203:09 - all right and all of a sudden our
203:11 - pipeline got a bit bigger but no worries
203:15 - one last check
203:18 - deploy staging test staging deploy
203:20 - production test production
203:23 - deploy staging is the stage
203:26 - we're using the staging bucket
203:29 - paging tests using the staging url test
203:32 - staging deploy production
203:36 - test production all right
203:38 - i'm going to go ahead
203:40 - commit these changes and we'll see how
203:42 - they work in the main branch
203:45 - and a few minutes later after the main
203:47 - pipeline has also completed
203:50 - we'll see now that we have here a bunch
203:52 - of stages so
203:54 - after the tests then we're deploying to
203:57 - staging we're testing staging deploying
203:59 - to production we're also testing
204:02 - production
204:03 - of course we could go ahead and
204:06 - combine the staging so for example
204:07 - because our tests are very simple
204:09 - there's nothing that prohibits us from
204:12 - just putting this simple test inside the
204:15 - deployed job itself so that will save us
204:17 - some stages here but i just want to
204:19 - demonstrate like how how longer pipeline
204:21 - essentially looks like
204:23 - and what's the meaning of the stages so
204:26 - most importantly now if something
204:28 - happens with the deploy stage the
204:29 - pipeline breaks here production
204:32 - environment is unaffected we don't have
204:34 - to worry about it so we have time to fix
204:36 - any problems in our deployment
204:43 - if you're looking again at the pipeline
204:46 - well
204:47 - you know all the staging environment has
204:49 - really created
204:50 - a lot of duplication and all these
204:53 - variables with the bucket now we have
204:56 - ideal ss3 packet with staging
204:58 - we also have the space url and the
205:00 - staging url
205:02 - obviously we have now two environments
205:04 - but we haven't really figured out a way
205:06 - on how to properly manage this
205:07 - environment
205:09 - luckily there is a functionality that
205:10 - gitlab offers for managing these
205:13 - environments
205:14 - going inside the project you will find
205:16 - here
205:18 - on the left hand side
205:20 - deployments and inside deployments
205:23 - there's this optional environment
205:26 - what is an environment
205:27 - staging is an environment production is
205:30 - an environment
205:31 - wherever we're deploying something that
205:34 - is an environment and it really makes
205:37 - sense to have these environments defined
205:39 - somewhere and to work with the concept
205:41 - of environment instead of fiddling
205:43 - around with so many variables that's
205:45 - really not the best way on how to do
205:47 - that
205:48 - so what i'm going to do here i'm going
205:50 - to create a new environment
205:52 - and i'm going to call this environment
205:54 - production
205:55 - i'm also going to get the environment
205:57 - url from the pipeline so let's remember
206:00 - this is
206:01 - the production
206:04 - url so
206:06 - let me get that and i'll save it here
206:11 - and going back to environments i'm going
206:13 - to create a new environment
206:16 - and this will be staging
206:19 - again going back to the pipeline
206:23 - copying the name of the environment
206:25 - and putting it here
206:28 - what gitlab will also do is to keep
206:30 - track of deployments on different
206:32 - environments now currently we don't have
206:34 - any deployments but we can directly open
206:37 - the environments from here so in case we
206:39 - forget where the our environments are we
206:42 - can easily click on this open
206:44 - environment link
206:45 - especially for non-technical people that
206:47 - are also working with gitlab it's easier
206:49 - for them to see exactly where is the
206:51 - staging environment what's the url i
206:53 - don't have to ask someone it can just
206:55 - directly go to their respective
206:56 - environment and see that respective
206:58 - version so very very useful feature in
207:01 - terms of environments
207:02 - but what does it do for our pipeline
207:04 - well
207:05 - i'll tell you what this will do we're
207:07 - going to remove this from here right i'm
207:09 - going to go ahead first of all this is
207:11 - out i'm not using this anymore
207:14 - second of all i'm gonna go back to
207:16 - settings ci cd
207:19 - and expand here the variables
207:21 - and i'm gonna go here to this aws s3
207:24 - bucket
207:25 - i'm gonna click here on edit
207:28 - and this is essentially our production
207:31 - environment this is for production
207:33 - and now we can also give a scope so we
207:36 - can tell gitlab hey
207:38 - this variable is associated with
207:41 - production and not with something else
207:44 - so we can essentially scope it
207:47 - and of course in this case because we're
207:49 - using it in main only we're gonna also
207:51 - protect this variable
207:53 - i'm gonna go ahead and update here
207:56 - and the same goes for this other
207:58 - variable right now we don't need this
208:00 - staging added at the end we can just
208:03 - call it aws s3 bucket
208:06 - and we're going to select here the
208:08 - staging
208:10 - environment scope
208:12 - and update the variable so now we have
208:15 - two variables
208:17 - that share the same name
208:19 - but now they belong to different
208:21 - environments
208:23 - now the idea
208:24 - is to do the following we need somehow
208:28 - to adapt our pipeline
208:31 - and let's begin here by
208:33 - deploying to staging
208:35 - first of all we have this aws s3 bucket
208:39 - so i'm going to remove here staging
208:42 - from the end
208:43 - and additionally we need to tell gitlab
208:45 - hey this deployed to staging this is
208:48 - associated
208:49 - with the staging environment i'm going
208:51 - to say here environment staging
208:55 - there are still a few things i would
208:56 - like to change first of all the entire
208:58 - pipeline right now is too long
209:01 - we still have this staging test and
209:03 - production tests and because this is
209:05 - just one curl command we can just move
209:08 - it away from here
209:09 - i'm gonna remove this stage all together
209:14 - and all i want to do is to add it here
209:17 - on deploy to staging so essentially
209:19 - right after you have deployed
209:21 - we're also using this curl command
209:25 - it's pretty similar to what we're also
209:27 - going to do on production
209:29 - but as you probably noticed already this
209:31 - app base url
209:33 - doesn't exist anymore we have removed
209:35 - those variables
209:37 - and we need to find a way to get our
209:40 - environment url
209:42 - and luckily again
209:44 - gitlab to the rescue there is a variable
209:48 - i'm going to go ahead and search for
209:49 - environment
209:52 - and i'm going to have here the ci
209:53 - environment name environment slug
209:56 - environment url so this is what we're
209:59 - interested in the environment url i'm
210:01 - gonna copy this
210:04 - and i'm gonna put it here in the curl
210:06 - command so i'm gonna send the curl
210:08 - command directly to the ci environment
210:10 - url
210:12 - same goes to the production deploy to
210:14 - production
210:16 - i'm gonna add it here
210:19 - so in this situation the production
210:21 - tests they also don't make any sense so
210:24 - i'm gonna remove them
210:26 - and all these extra stages that we have
210:28 - here test production
210:30 - it's not going to be needed
210:32 - test staging it's not going to be needed
210:34 - so now we have a much more simpler
210:36 - pipeline but we're still achieving the
210:38 - same thing
210:39 - most importantly we are using
210:42 - these environment
210:45 - we still have an error inside here
210:48 - but i just gonna commit these changes
210:50 - let the branch pipeline run merge these
210:53 - changes into the master we're gonna take
210:55 - a look at the main pipeline to see which
210:58 - errors we still have there
211:03 - now if we're looking at the main
211:05 - pipeline you will notice something
211:06 - interesting
211:07 - deploy staging
211:09 - has passed it's working perfectly
211:12 - you will see here curl ci environment
211:15 - url
211:16 - it's fetching the page
211:18 - it's passing however
211:20 - if you're looking here at deploy to
211:22 - production
211:24 - it's all of the sudden complaining it's
211:27 - saying here that aws is three bucket
211:31 - parameter validation failed
211:33 - invalid bucket name
211:35 - so what's going on
211:37 - well the following thing has happened
211:41 - we somehow did not associate this with
211:44 - an environment or the environment is not
211:46 - correct so we need to go back and see
211:49 - why this job does not have access to
211:52 - this environment variable
211:54 - i'm going to go ahead here
211:57 - and
211:57 - take a look again at the configuration
211:59 - to make sure that the configuration
212:01 - itself of the job is correct
212:04 - what do we have here
212:07 - or we have deployed to production
212:09 - but as you can notice
212:11 - i haven't defined an environment
212:14 - right so i have defined an environment
212:15 - for deploy to staging but i haven't
212:17 - defined an environment for deploy to
212:19 - production
212:21 - by looking here at the variables
212:23 - you will see that this aws s3 bucket
212:28 - is now scoped only for production
212:31 - and because this job
212:33 - doesn't say anything about production
212:35 - this environment is not exposed in the
212:38 - production job
212:39 - so to be able to solve this we also have
212:42 - to add here environment production
212:45 - i'm going to go ahead and copy this
212:48 - and the environment will be production
212:50 - and of course unfortunately this time we
212:52 - have to go again
212:53 - through all the stages
212:55 - the merge request
212:57 - and committing this in the main branch
213:00 - and just a few minutes later the entire
213:03 - pipeline will then succeed so we have
213:05 - deployed the staging we have deployed to
213:06 - production
213:07 - everything seems to be working fine
213:10 - we can take a look at the jobs to see
213:12 - what they're doing
213:14 - and
213:14 - we'll see here that everything worked
213:16 - out without any issues
213:18 - and of course we can also go here to
213:20 - deployments
213:22 - environments
213:23 - and here we are able to see the staging
213:26 - environment and the production
213:27 - environment and we can easily open them
213:30 - and we can see like what was deployed
213:33 - when was it deployed so it really keeps
213:35 - track here of the different environments
213:38 - and the deployments that took place so
213:40 - we see here on production there's only
213:42 - one deployment
213:43 - in staging we have two deployments that
213:46 - we have committed so we're essentially
213:47 - keeping track of what is going on on
213:49 - this environment
213:55 - you know what
213:56 - i'm still not super happy with this
213:58 - pipeline i mean generally when i see
214:01 - that a lot of things are repeating
214:04 - but it kind of makes me wonder if
214:06 - there's not a better way so if looking
214:08 - here at the deployed staging and we're
214:10 - looking at deeply production
214:12 - well essentially at this point because
214:14 - we have used all these variables
214:16 - these jobs are almost identical the only
214:19 - thing that is different is the different
214:22 - stage
214:23 - different job name and we're specifying
214:25 - the environment but the rest of the
214:27 - configuration is identical
214:30 - and yes the answer is yes we can
214:33 - simplify this even more
214:36 - so essentially we can reuse some job
214:38 - configurations so i can go ahead here
214:40 - and i'm going to copy this
214:43 - and i'm going to go here and i'm going
214:45 - to define
214:47 - essentially a new job i'm just going to
214:49 - call it deploy
214:51 - this will be a special kind of job it
214:53 - will be a job that has a dot in front of
214:56 - it and as you remember we have used the
214:58 - dot notation to disable jobs
215:02 - by having this we can essentially have a
215:05 - configuration here
215:06 - we don't care about the stage right we
215:10 - don't care about the environment so this
215:12 - is something that is not in common with
215:14 - the rest it doesn't even have to be a
215:16 - valid
215:17 - job configuration we have just put here
215:20 - the parts that are really important for
215:22 - us
215:23 - and then
215:25 - in these other jobs we're going to keep
215:27 - what we need
215:29 - so what we need we need a stage and we
215:30 - need the environment and of course we
215:32 - need the job name
215:34 - and this other part here
215:36 - we can simply write something like
215:38 - extends
215:40 - so this will be the extents keyword
215:43 - and what are we extending we are
215:45 - extending dot deploy
215:48 - that dot in front
215:50 - is very important so don't miss it
215:53 - and the same goes for deploy to
215:55 - production
215:57 - let me remove everything
215:59 - i'm just going to keep here the extents
216:01 - and make sure it's properly indented
216:04 - so deploy to staging deploy to
216:06 - production
216:07 - we now have two simple
216:09 - jobs here
216:11 - essentially the deployment part is the
216:14 - same
216:15 - this also gives us peace of mind because
216:17 - if we're making changes to the
216:19 - deployment part we're only making
216:21 - changes here so if we make a mistake
216:23 - there
216:24 - the chances are we're going to be able
216:26 - to catch it before it goes to production
216:30 - let's commit this give it a try see how
216:32 - it works and it should essentially work
216:34 - the same as before
216:38 - and after a few minutes if we take a
216:40 - look at the main pipeline
216:41 - we'll see that it works as it should
216:44 - work it looks as before and this is
216:46 - exactly what we expected we didn't want
216:49 - to see anything else but the pipeline
216:51 - working but now we have a much simpler
216:54 - pipeline configuration
217:01 - and now feel it is time for another
217:03 - assignment
217:04 - now let me tell you my idea now we have
217:07 - this pipeline and we're testing with the
217:10 - curl command that that specific text
217:13 - is on the website
217:15 - and that's fine it ensures that the
217:17 - website is still working
217:18 - we currently we haven't really made any
217:20 - changes to the website itself we haven't
217:22 - added any text or removed anything
217:25 - how can we make sure that you know with
217:27 - what we actually built here
217:29 - actually lands on the staging and in the
217:31 - production environment and it's not
217:34 - somehow a cache or something old there
217:37 - maybe you know maybe the deployment is
217:39 - not even working and you're getting the
217:41 - older version and we think that
217:43 - everything is fine
217:44 - so here's my idea
217:46 - what if we add another file in our build
217:50 - let's call it
217:52 - version.html for example
217:55 - and inside there
217:56 - we put a build number something that is
217:59 - all the time different with each build
218:01 - so for example it will increment from
218:04 - one two three four and so on so that
218:07 - with every build we can identify to
218:09 - build number
218:10 - and we can check which version has been
218:13 - deployed to staging and production
218:17 - how about this idea
218:20 - and in order to get you started i'm
218:21 - going to show you how to get this
218:24 - information inside there how to get
218:26 - this build information this dynamic part
218:29 - maybe this is something that
218:31 - right now you don't know exactly how to
218:33 - do it so no worries i'm going to get you
218:34 - started but the rest of the assignment
218:36 - will have to figure it out on your own
218:39 - trust me you already know all the
218:41 - concept needed in order to implement
218:42 - something very simple
218:44 - so i'm going to go ahead here and define
218:47 - a variable and i'm going to call this
218:49 - variable app underscore version so
218:51 - essentially let's say that this is our
218:54 - application version so want to have
218:56 - something like 12
218:57 - 13 14 and so on
219:00 - now again when we're thinking about
219:03 - something dynamic we have to think back
219:05 - to the list of predefined variables that
219:08 - gitlab offers right
219:10 - so on this list
219:12 - there are various variables that we can
219:15 - use
219:16 - including something that is related to
219:19 - the pipeline id
219:20 - so if we're looking for pipeline
219:22 - underscore id
219:25 - find here some
219:27 - pipeline ids that we can use so these
219:29 - are variables that of course change all
219:32 - the time they are injected by gitlab so
219:34 - we can use them in our jobs
219:36 - so we can get here like an instance
219:38 - level id this typically includes like
219:40 - very large numbers because there are a
219:42 - lot of pipelines on a gitlab instance
219:44 - but you can also get like a project
219:46 - level id so this will be something we
219:47 - can easily relate to it's and we're
219:49 - going to see it increment all the time
219:51 - so i'm going to get this variable
219:54 - and i'm going to use it here
219:56 - so i'm redefining this because i want to
219:59 - give it a bit of meaning i could have
220:01 - used this directly somewhere this is
220:04 - really your solution
220:06 - but if i'm using here app version
220:08 - i'm very clear that this is the
220:10 - application version that we're using
220:12 - here not just some random id
220:16 - so that's it i'm gonna
220:18 - let you do the rest of the assignment so
220:21 - just to recap
220:23 - here somewhere in the build
220:25 - add a new file
220:26 - which is called version.html
220:29 - add that new file
220:31 - put this app version inside that file
220:36 - and then
220:37 - when we're testing our deployments
220:39 - including staging and production so here
220:43 - create a new curl command that will
220:45 - check that that application version is
220:48 - actually available on that environment
220:56 - okay so i hope that you have managed to
220:58 - solve this on your own i already feel i
221:00 - gave you like quite a lot of hints but
221:03 - just to make sure this is how i would
221:05 - solve it so let's go here to the build
221:08 - website
221:09 - apart from all these things that we're
221:11 - doing here what i want to do
221:12 - additionally is to create this file so
221:15 - again
221:16 - in order to create this file we have to
221:19 - first think about the name so it's going
221:20 - to be inside the build folder right this
221:22 - is
221:23 - this is what we're deploying and the
221:24 - name will be version.html
221:27 - and what we're doing is we're taking
221:28 - this application version
221:30 - so in order to get the application
221:32 - version inside the file gonna use echo
221:37 - i'm gonna print the application version
221:39 - and then eventually again redirect it to
221:41 - the file
221:42 - so this is enough to create this file
221:45 - to put it inside the build folder which
221:47 - already exists build yarn build has
221:49 - created that
221:51 - and because this is dynamic it will be
221:53 - available there
221:55 - the next step
221:56 - is part of the deploy template so i'm
221:59 - working directly in the template here
222:02 - so we can essentially duplicate this
222:05 - command
222:06 - we're going to the environment url and
222:10 - we hope that we don't have a forward
222:12 - slash already gonna check that quickly
222:15 - i'm gonna go here to deployments
222:17 - environments
222:19 - i'm gonna go to staging
222:21 - click on edit
222:22 - i'm noticing here i don't have a forward
222:24 - slash so that's already good and i
222:27 - presume that
222:28 - the production environment is also
222:30 - similar but just to check
222:33 - i did
222:34 - i don't have a forward slash okay so
222:36 - inside our configuration i'm gonna add
222:38 - here forward slash
222:41 - i'm gonna write here version
222:43 - and what are we looking for well we're
222:45 - looking for the application version
222:49 - and because the application version is
222:51 - simple number we don't need these quotes
222:53 - here so we can just write grep
222:55 - app version and that should work
223:01 - so already as part of the merge request
223:05 - as soon as the build has completed i
223:07 - want to go inside this build website and
223:09 - i'm going to take a look here at the
223:11 - artifacts to see if this artifact
223:14 - contains this
223:16 - version.html file
223:18 - so i'm able to see it here version.html
223:21 - it has a size that is not zero that's a
223:24 - good thing
223:25 - and we can also download all the
223:27 - artifacts or just look at a single file
223:30 - so after clicking on this
223:32 - somewhere here on the screen it will
223:34 - open up this web page essentially so
223:37 - this is the build number 35. it's pretty
223:40 - small but you should be able to see it
223:42 - on screen as well so this is part of the
223:45 - file so it's already looking good when
223:47 - this much request is completed i will
223:49 - also take a look in the main branch
223:52 - now the main pipeline is also working
223:55 - makes me very happy because
223:57 - i now have more confidence in what is
224:00 - going on in this pipeline i have
224:02 - confidence that if i'm making a change
224:04 - that change actually gets deployed to
224:06 - staging and production and we have this
224:09 - additional check then in place to ensure
224:12 - that
224:18 - all right so let's recap a bit what we
224:21 - did
224:22 - the first part is the ci part continuous
224:25 - integration the second part is cd
224:28 - continuous deployment now we have a more
224:31 - realistic continuous deployment pipeline
224:34 - we're testing something and then we're
224:36 - first deploying to staging making sure
224:39 - that on staging everything works
224:41 - and after that we deploy to production
224:44 - but what is a continuous delivery
224:46 - pipeline this is exactly what i wanted
224:48 - to show you essentially a continuous
224:50 - delivery pipeline is just a pipeline
224:53 - where we don't automatically deploy to
224:55 - production essentially what we want to
224:57 - do is add here a button and only when we
225:00 - are sure that we really want to make
225:02 - that change to production we can click
225:04 - on that button and make that change so
225:06 - let me show you how to do this
225:09 - i assure you it's super super easy
225:12 - so inside the pipeline configuration
225:16 - the
225:17 - job that is affected by this change
225:19 - is deploy to production
225:22 - so here on deploy to production i'm
225:24 - gonna add a condition and the condition
225:26 - is when
225:29 - and we're gonna write here manually
225:32 - it's actually when manual not manually
225:35 - so this will tell gitlab that we only
225:38 - want to run this job manually so we need
225:41 - some manual intervention
225:43 - gonna commit these changes and we're
225:44 - gonna take a look at the final pipeline
225:47 - now this is the final pipeline and i
225:50 - want you to notice something
225:52 - for deploy production
225:54 - this job now looks a bit different right
225:57 - if you look at deploy staging click here
226:00 - on deploy production we'll say here this
226:02 - is a manual action and we have here an
226:04 - additional button right
226:07 - if we also go to
226:09 - our deployments and we take a look at
226:12 - environments
226:13 - we'll be able to see here like what we
226:16 - have on each environment
226:18 - so you'll see here
226:20 - if we're going here on staging
226:22 - this is the staging environment
226:25 - and of course here inside the staging
226:27 - environment we can go ahead and write
226:29 - here version.html
226:32 - so we're getting here
226:34 - on the staging environment for version
226:37 - 40 right so this is the version on the
226:39 - staging environment
226:42 - and if we're opening up to production
226:43 - environment
226:45 - it looks absolutely the same
226:47 - but if we look at a version.html
226:52 - you'll see that this is a different
226:53 - version right
226:55 - on staging and on production we now have
226:59 - different versions because our pipeline
227:02 - has not deployed to production yet
227:04 - so in order to deploy to production we
227:06 - have to click this button
227:08 - and only then
227:10 - will the deploy to production job start
227:13 - so this is essentially the difference
227:16 - between a continuous deployment and a
227:19 - continuous delivery pipeline
227:21 - what we have here is a continuous
227:24 - delivery pipeline we're continuously
227:26 - building packages of software we're
227:29 - deploying them to staging
227:31 - but we're not automatically deploying to
227:33 - production without that manual
227:36 - intervention
227:37 - for some organizations this is mandatory
227:40 - and this is why i'm also showing you in
227:42 - some legacy systems
227:44 - you cannot always deploy everything
227:46 - without checking a few things in advance
227:50 - now if we're looking here deploy to
227:51 - production has also completed
227:54 - so we can take a look here if i'm gonna
227:56 - refresh this website i'm gonna get here
227:58 - version 40. and here on the staging also
228:01 - version 40. so now
228:03 - both staging and production have the
228:06 - same version
228:07 - now i hope that the difference between
228:09 - continuous delivery what we did right
228:10 - now
228:11 - and continuous deployment where every
228:14 - commit that lands in the master branch
228:17 - gets deployed to production makes more
228:19 - sense now
228:24 - hey how are things going are you like
228:25 - the course so far you're following along
228:27 - let me know by leaving a comment in the
228:29 - section below or by sending me a message
228:31 - on twitter or linkedin or on any social
228:34 - platform you can find me i'd love to
228:36 - hear from you and to know how are you
228:37 - using this course
228:39 - we are at the end of this unit so i'm
228:41 - gonna grab a coffee and i will see you
228:42 - in a bit
228:44 - [Music]
228:49 - so far we have worked with a static
228:50 - website and deployed it to aws it's
228:54 - probably one of the easiest scenarios
228:55 - involving gitlab ci and aws however
228:58 - modern applications tend to be more
229:00 - complex and most of them use docker
229:02 - nowadays so in this section we'll
229:04 - dockerize our website and instead of
229:07 - copying some files to aws we'll be
229:09 - deploying an application that runs in a
229:11 - docker container to do that we'll build
229:13 - a docker image as part of the build
229:15 - process store it in the gitlab container
229:17 - registry and deploy to a service on aws
229:21 - called elastic beanstalk so if you're
229:22 - eager to learn more let's jump right
229:24 - into it
229:27 - when we use a cloud provider like aws we
229:30 - can rent virtual machines that have a
229:32 - dedicated cpu
229:34 - memory and disk storage
229:38 - and we can use any operating system we
229:40 - desire
229:42 - but this also means that we are in
229:44 - charge of managing that machine
229:46 - we need to ensure that it's secure and
229:48 - that all software running is
229:50 - updated this is often too much overhead
229:53 - especially for some types of
229:55 - applications but there is a way to take
229:58 - away this complexity and focus only on
230:01 - the application we want to deploy
230:04 - aws elastic beanstalk is a service that
230:07 - allows us to deploy an application in
230:09 - the aws cloud
230:11 - without us having to worry about the
230:13 - actual virtual server that runs it
230:17 - this is a great way to reduce complexity
230:19 - and it's probably one of the easiest way
230:21 - to deploy an application in the aws
230:24 - cloud
230:25 - by default elastic bin stock can run
230:28 - python java node.js php and many other
230:32 - types of applications but it can also
230:35 - run docker containers which really gives
230:37 - us a lot of flexibility
230:40 - we'll be running a web server
230:42 - application that serves our simple
230:44 - website files
230:46 - so this time instead of just uploading
230:48 - some files to aws
230:50 - we are providing the entire application
230:53 - which is self-contained in a docker
230:55 - container
230:57 - i'm sure you'll be amazed by how easy it
230:59 - is to run an application here in the
231:02 - background we will be using a virtual
231:04 - machine but we don't need to worry about
231:07 - managing it
231:08 - however at this point i need to warn you
231:11 - about potential costs
231:13 - while running these servers for a few
231:15 - hours or days
231:17 - will most likely be free or cost you
231:19 - only cents
231:21 - if you let the service run for a month
231:24 - you may get some unexpected charges on
231:26 - your card
231:28 - even if you are not actively using a
231:30 - service once you have created it it uses
231:33 - resources in the cloud
231:35 - so stop any services if you are no
231:37 - longer using them
231:39 - find a way to set a reminder so that you
231:41 - don't forget about them
231:43 - with that being said let's start using
231:45 - elastic bean stock
231:51 - so let's go ahead and create an elastic
231:54 - beanstalk application
231:56 - so i'm here in the aws console
231:59 - and the first step would be to search
232:01 - for elastic bean stock so i'm going to
232:03 - write here eb
232:05 - and you will see here one of the results
232:07 - is elastic bean stock
232:10 - so since i have no applications here i'm
232:12 - getting this getting started guide so
232:15 - i'm going to click here on create
232:17 - application
232:20 - so let's call this application my
232:22 - website
232:23 - you don't need to include any
232:25 - application tags
232:28 - and here on the platform
232:30 - there are different platforms that are
232:32 - supported
232:33 - but what you're actually interested in
232:35 - is in the docker platform so that we can
232:38 - essentially deploy anything we want
232:40 - i'm going to select here docker i'm
232:42 - going to leave the defaults as they are
232:46 - and the first step would be to start
232:48 - with a sample application
232:50 - so we're not going to upload any code
232:52 - we're going to let elastic beanstalk
232:54 - create this instance this application
232:57 - and after that we're gonna add our own
233:00 - application on top of that
233:02 - so i'm gonna click here on create
233:03 - application
233:05 - and it will typically take a few minutes
233:08 - to get this to run
233:11 - and in the end you should see something
233:12 - that looks like this
233:14 - so what has happened here
233:16 - well we have created an application
233:20 - and we have initialized a sample
233:22 - application that elastic bean stock
233:24 - provides
233:26 - in order to actually run that
233:27 - application this wizard that we have
233:29 - used is set up has also created an
233:32 - environment
233:33 - so we have
233:34 - applications which is like an umbrella
233:37 - for environment
233:39 - so if i'm looking here at environments
233:43 - you'll be able to see here that we have
233:45 - an environment that is called
233:47 - my website dash nf
233:50 - it belongs to the application my website
233:53 - and then you get some information in
233:55 - regards to when this was created
233:57 - under which url is this available which
234:00 - platform is this using and so on
234:04 - in order to see like what this
234:06 - environment is doing we're gonna click
234:07 - here on it you'll be able to see here a
234:09 - link
234:11 - so if you click on this link this is the
234:13 - sample application that was deployed so
234:15 - it's just an idea
234:16 - tells you that everything is working
234:18 - properly that we have nothing to worry
234:20 - about this entire setup has worked
234:23 - without any issues
234:26 - now the question is what has actually
234:28 - happened in the background
234:30 - in order to understand that we're going
234:32 - to go here to the services and we're
234:34 - going to take a look at ec
234:38 - and right here ec
234:41 - and that is ec2
234:44 - the service that we're actually
234:45 - interested in
234:47 - this stands for elastic compute
234:50 - these are essentially virtual servers
234:53 - that we can create here but we haven't
234:56 - actually created one
234:58 - but if you're looking here at instances
235:01 - you'll see that we have one running
235:03 - instance
235:05 - and this instance is called my website
235:09 - dash and
235:10 - you'll see here which kind of an
235:12 - instance this is
235:13 - this is a virtual server that's running
235:15 - here and this is the server that's
235:17 - actually running our application
235:19 - additionally if you're going here to s3
235:25 - we'll be able to see that we now have an
235:27 - additional bucket
235:29 - so we still have our buckets that we
235:31 - have used for
235:32 - hosting the website
235:34 - but now elastic bin stock has also
235:36 - created a bucket
235:38 - so actually what elastic beanstalk has
235:40 - done
235:41 - has created this required infrastructure
235:43 - in order to run the application we
235:45 - didn't have to worry about creating that
235:47 - but this is why it took a few minutes to
235:49 - create this entire thing
235:52 - now let's go ahead and try to understand
235:54 - how we can deploy something on your own
235:56 - like how can we get our own application
235:58 - to work
235:59 - and because we're using docker
236:01 - we need to provide a manifest file
236:04 - essentially
236:06 - a file that describes the application
236:08 - that we're trying to deploy
236:11 - so i'm here inside the project again
236:14 - and if you go into the templates
236:17 - you'll find here a file called
236:20 - docker1.aws.public.json
236:26 - and this is the manifest file that i'm
236:28 - talking about
236:29 - it essentially tells aws which container
236:33 - we want to run because we have selected
236:36 - a docker platform we can only run docker
236:39 - containers there
236:41 - and this is a public container
236:44 - the name of the image is nginx which is
236:46 - a web server
236:48 - but what we want to try here is to
236:50 - actually use this configuration to use
236:52 - this file
236:53 - and to deploy this application to aws to
236:56 - make sure that this deployment process
236:58 - is working and this is again something
237:00 - that we'll do manually at this point
237:02 - just to make sure that everything works
237:04 - properly
237:05 - so go ahead and download this file
237:08 - and after that let's go back to aws
237:10 - and open up elastic bean stock
237:15 - here inside the environment will have
237:16 - the opportunity to upload a new version
237:19 - of the application you will see here
237:20 - right in the middle
237:22 - running version this is the sample
237:24 - application then we have this option
237:26 - upload and deploy and we want to upload
237:29 - that json file
237:31 - so i'm going to go ahead and select the
237:33 - file and i can also write here a version
237:36 - label
237:37 - this will be sample application
237:39 - version one that's totally fine and i'm
237:42 - going to go ahead and click on deploy
237:45 - and what will happen is
237:47 - elastic beam stock will take this file
237:50 - and then we'll start a deployment we'll
237:51 - start here updating the environment
237:54 - of course this will take a few minutes
237:56 - but in the end what we want to see here
237:58 - is health status okay wanna see that
238:01 - everything is working properly
238:02 - and want to see that this application
238:04 - has been deployed and is available at
238:07 - this address
238:09 - and now we're seeing here status
238:12 - is okay so health is okay everything
238:14 - seems to be working properly you can
238:16 - also go ahead and refresh this page just
238:19 - to see here which version is running
238:23 - you will see here that the running
238:24 - version is sample application one so
238:26 - this is exactly what we have deployed
238:28 - we can open up this address
238:31 - and we'll see here welcome to nginx
238:33 - so this is the welcome page from the
238:35 - nginx server which we have deployed by
238:39 - having this json file which describes
238:41 - which container we want to use with
238:43 - elastic bean stock
238:45 - so in order to actually deploy our
238:47 - website
238:49 - we need to create a docker image we need
238:51 - to provide this json file
238:54 - and of course we also want to automate
238:55 - everything so this is what we are gonna
238:57 - do in the upcoming lectures
239:05 - so how do we create a docker image with
239:07 - our website
239:09 - a docker image is created by following a
239:12 - set of instructions something like a
239:14 - recipe
239:15 - we store these instructions in a file
239:18 - called docker file so let's go ahead and
239:20 - create one
239:22 - so here from the web ide i'm gonna go
239:24 - ahead and create a new file
239:27 - and you will see here in the suggestions
239:29 - one of the suggestions is docker file
239:32 - so which are these instructions that
239:34 - will write inside the docker file first
239:36 - of all we have to start with a base
239:39 - image essentially an image that we want
239:42 - to add something on top of it
239:44 - in our case because we want to deploy
239:47 - web server with our files
239:50 - we're going to start with the engine x
239:52 - image it is exactly the same image we
239:55 - have used in order to test our
239:57 - deployment to elastic bean stock so i'm
240:00 - gonna write here from nginx
240:03 - and additionally what i highly recommend
240:05 - is writing a version so writing a
240:07 - specific tag
240:09 - so in order to know which tag to choose
240:11 - i'm gonna go to docker hub
240:13 - i'm gonna search here for nginx
240:17 - i'm gonna take here the verified content
240:20 - so this is the official image for nginx
240:23 - i'm gonna take a look here at the tags
240:25 - and let's search for alpine because
240:28 - alpine generally provides us with a very
240:30 - small docker image
240:33 - so what i'm searching here for is a
240:35 - specific tag
240:37 - so this here
240:38 - would be a very good tag to use just as
240:41 - an example i'm going to go ahead and
240:43 - copy that
240:45 - and here in the editor
240:47 - column
240:49 - and the version
240:50 - so you have to be very careful how
240:52 - you're writing this has to be something
240:54 - like this
240:55 - so we're starting from this base image
240:58 - so we have everything that is in the
240:59 - base image we have there so we already
241:01 - have the application essentially the
241:03 - application is the web server
241:05 - now the next step is to actually add our
241:09 - files which are in the build folder
241:11 - and to put them on this web server
241:15 - and we do that by using the copy command
241:17 - and right here copy
241:19 - so what are we copying we're copying the
241:21 - build folder
241:22 - then we have to specify where we are
241:24 - copying it now by default there is a
241:27 - folder
241:28 - where nginx will store
241:31 - files that we want to serve and that
241:33 - folder has the following path forward
241:35 - slash user usair share
241:38 - nginx
241:40 - and then html
241:42 - so we're moving this folder
241:45 - inside html folder so this is everything
241:48 - that we need to do in order to build our
241:51 - docker image these are the only
241:53 - instructions that we need at this point
241:55 - we're getting
241:57 - this base image nginx in a specific
241:59 - version because we're using this tag
242:02 - and then the next instruction is copy
242:05 - everything that is in the build folder
242:07 - and move them to this html folder
242:15 - just because we have created this docker
242:17 - file doesn't mean that something will
242:19 - automatically happen we still need to
242:21 - make some changes to our pipeline
242:24 - and because we're already making changes
242:26 - and because we want to deploy to elastic
242:28 - bean stock we don't really need this s3
242:32 - deployment anymore so i'm going to
242:33 - remove essentially all the jobs that are
242:36 - related to s3 this includes deploy to
242:38 - production deploy to staging this deploy
242:41 - job which is just a template and also
242:44 - test website we're going to introduce a
242:46 - better way of testing so i'm going to
242:48 - remove all of them and what remains is
242:51 - the build website
242:54 - and of course also the stages here they
242:57 - can be removed so what we're gonna do is
242:59 - we're gonna introduce a new stage let's
243:01 - call it package
243:03 - and we're gonna associate this stage
243:05 - with a new job gonna call it build
243:08 - docker image and now
243:10 - the stage will be packaged
243:14 - so how do we build the docker image well
243:17 - it's relatively simple the command is
243:19 - docker
243:20 - build
243:21 - i'm going to also add a dot here because
243:23 - that's going to make a reference to the
243:25 - current folder and the current folder
243:28 - contains the docker file that we have
243:30 - created
243:31 - in order to be able to run this docker
243:33 - command
243:34 - we also have to define the image that
243:36 - we'll use
243:37 - and the image will be docker
243:40 - just using this docker image will not
243:42 - work we're gonna get an error
243:44 - the reason for that is the docker
243:46 - architecture is composed of a client and
243:49 - a server essentially what we have here
243:51 - docker this is the client and the client
243:54 - sends some instructions to a server
243:57 - actually to the docker demon which is
243:59 - the one who builds the job
244:01 - and in order to get access to a daemon
244:04 - inside gitlab ci we need to use a
244:07 - concept and that concept is of services
244:09 - we want to define here a tag services
244:12 - and this contains a list of services
244:15 - that we can start
244:16 - and what we're starting here is actually
244:19 - service called docker in docker
244:21 - you're going to see me using here this
244:23 - docker in docker tag
244:27 - so this service is another docker image
244:29 - that can build docker images cannot be
244:32 - accessible for us over a network
244:34 - and
244:35 - docker here which is the docker client
244:37 - will be able to talk with the docker
244:39 - daemon which is here inside this service
244:42 - i know that now at the beginning this
244:44 - may
244:45 - seem all bit confusing but this is like
244:48 - the minimum what we need in order to be
244:50 - able to build docker images from gitlab
244:53 - what i also like to do is to set some
244:55 - tags so i'm going to go ahead and write
244:58 - a fixed stack for docker and for docker
245:00 - in docker so i'm here at docker hub and
245:04 - this is the docker image you will find
245:05 - the link in the course notes because
245:07 - this is something that's not so easy to
245:09 - find actually
245:11 - not so many people are actually looking
245:13 - for
245:14 - docker and i'm gonna use this version
245:17 - here
245:19 - i'm gonna go ahead and copy it
245:21 - and i'm gonna add it here to my job
245:24 - image
245:24 - and additionally there's another tag
245:26 - with docker in docker this is the tag
245:29 - that i'm gonna use for the docker daemon
245:32 - i'm gonna
245:33 - remove here dent
245:35 - and this will be the docker in docker
245:38 - image you'll see both of them have the
245:40 - same version
245:43 - additionally when we're building images
245:45 - we also like to specify tags like label
245:49 - this will help us identify the images
245:52 - that we create because we can create
245:53 - multiple images of course
245:56 - in order to tag images we first have to
245:58 - specify an image name and also the tag
246:01 - so we can use here dash d
246:03 - you're gonna keep here this dot at the
246:04 - end that's very important
246:06 - don't forget it
246:07 - and what we'll use here is this is the
246:10 - list of environment variables and one of
246:13 - these environment variables is this ci
246:15 - registry image so we're going to use
246:18 - here ci registry image don't forget to
246:20 - put a dollar in front of it
246:22 - this will build the latest tag so this
246:25 - is a tag that always points to the
246:27 - latest image that we have created
246:30 - and additionally
246:31 - i'm going to create another tag
246:33 - which still contains cr register image
246:36 - i'm going to add here a dollar in front
246:38 - to make it a variable
246:39 - i'm going to add here column
246:41 - and we're going to use the app version
246:43 - that we are still
246:45 - having here in our job
246:47 - so we are creating two tags
246:50 - so don't forget the dot at the end this
246:52 - is super important this is one tag that
246:54 - we are creating
246:55 - and this is the second tag that we are
246:57 - creating
246:59 - and this variables will ensure that we
247:01 - have the right name for our docker image
247:05 - additionally in order to make sure that
247:07 - we have indeed built this docker images
247:10 - we're going to use here docker
247:12 - image and ls actually we are building
247:14 - only one image but we are tagging that
247:17 - image with two different tags
247:19 - the latest and the app version so this
247:23 - will show us all the images that are
247:25 - available with all the tags that are
247:26 - available in this instance
247:29 - i'm not gonna run this job only on the
247:31 - main branch i'm just gonna run it inside
247:33 - the branch i'm still here in the branch
247:35 - playing around
247:36 - so we're gonna see how this looks like
247:39 - after we execute it
247:41 - and once the execution is done we're
247:43 - gonna jump directly into the build
247:46 - docker image and try to understand what
247:48 - has happened here so we'll see where
247:50 - there are a bunch of locks that have
247:52 - been generated
247:53 - but most importantly what we want to see
247:56 - here is which docker image are we using
247:58 - so we'll see here this is the docker
248:01 - image which is darker and then we're
248:04 - also this is new we're starting this
248:07 - service with docker in docker
248:10 - so the service will be available over
248:12 - the network inside this image that we
248:15 - have right now this will give us the
248:17 - docker client
248:18 - and there are a lot of locks here and
248:20 - they're not really so relevant the
248:22 - interesting part comes when we're
248:24 - actually
248:25 - building the image and you'll be able to
248:28 - see here this is our command
248:30 - there are some steps how these docker
248:32 - images are being built docker works with
248:35 - concept of layers
248:36 - but we won't go into that but this is
248:39 - exactly what's happening here is every
248:41 - essentially every command that we're
248:43 - executing will create an additional
248:45 - layer on that image
248:47 - we have successfully built this so this
248:49 - is what you want to see
248:50 - it has been successfully built and then
248:52 - we have also the two tags that we have
248:54 - created you will see here the latest tag
248:57 - and this is also the version tag now
248:59 - they both point to the same image
249:02 - but they are different text and you will
249:05 - better see it here with this command
249:07 - that is listing images so this is here
249:10 - the essentially the name of the image
249:12 - we'll see here the tag and you will see
249:14 - that internally the image id is the same
249:17 - so we have the same image but with two
249:19 - different tags
249:20 - so now we have successfully managed to
249:23 - build and tag our image
249:30 - as you might have guessed
249:32 - the docker image that we have just built
249:35 - has been lost as soon as the job
249:37 - finished
249:39 - the docker image that we have created is
249:40 - not a regular file or an archive that we
249:43 - can just declare as an artifact and
249:46 - upload it to s3
249:49 - when we want to preserve a docker image
249:51 - in this case this docker image that we
249:52 - have created here
249:54 - we need to save it in a registry
249:58 - for example docker hub which we've been
250:00 - using to find out tags and images that
250:02 - we can use is a public registry with
250:05 - docker images
250:06 - however typically our projects are not
250:09 - public so it doesn't really make sense
250:11 - to use docker hub so for that reason
250:14 - we need a private registry
250:16 - both aws and gitlab offer private docker
250:20 - registries and for this example we'll be
250:23 - using the docker registry offered by
250:25 - gitlab and you'll see here on the left
250:28 - hand side
250:29 - packages and registry
250:31 - and we're going to use here the
250:32 - container registry because docker is all
250:35 - about working with containers and of
250:38 - course at this point
250:40 - there are no container images stored for
250:42 - this project so alone just building that
250:44 - ochre image does not automatically add
250:48 - it to the container registry actually
250:50 - this is called pushing so when we want
250:52 - to save a docker image in a registry we
250:55 - need to push it to the registry
250:58 - so let's go back and make some changes
251:00 - to our pipeline
251:02 - so let's take a look
251:03 - how do we push so the command in order
251:05 - to push something is docker push
251:08 - and we actually want to push all tags so
251:10 - we're going to use here parameter
251:12 - dash dash all dash tags
251:16 - and then we're going to specify here the
251:18 - ci registry image so this is in term of
251:21 - pushing
251:22 - but
251:23 - where are we pushing
251:26 - and especially if it's a private
251:27 - registry
251:28 - don't we need to log in somehow
251:31 - yes that's correct so alone pushing
251:34 - this will not work in our case we need
251:36 - to do something before we push and we're
251:38 - going to do it right here in the
251:39 - beginning
251:40 - because in case there are some issues
251:42 - which the login we want to know as soon
251:44 - as possible and not only at the end so
251:48 - the command to login is docker
251:51 - login
251:52 - relatively easy
251:54 - and we're going to specify username and
251:56 - password
251:58 - for this service we're not actually
252:00 - using our username and password that we
252:03 - used to log into gitlab we'll use some
252:06 - variables again that will give us some
252:09 - temporary user and passwords don't
252:11 - really care so much about that but
252:13 - essentially this is what we're to do so
252:16 - if you're looking here
252:18 - at the variables that are available
252:20 - we'll find here ci registry see a
252:22 - registry user say a registry password
252:26 - first of all where do we want to log in
252:28 - this is the cia registry
252:31 - so we need to specify where do we want
252:33 - to log in
252:35 - and additionally we need to specify our
252:37 - username and password
252:39 - so we typically specify the username and
252:41 - password like dash
252:43 - u
252:44 - and then specify the username
252:48 - and we're gonna use another variable ci
252:50 - registry user
252:52 - with the dollar sign in front of it
252:54 - that's very important
252:56 - and additionally we can specify password
252:58 - p
253:00 - and we'll see here ci registry password
253:04 - so these are all the credentials that we
253:06 - need in order to log in
253:08 - in more recent versions of docker
253:11 - just specifying your password like this
253:14 - is not really the best way to log in
253:17 - you may get a warning and it's possible
253:19 - that in future releases this argument
253:21 - this dashb will not be available anymore
253:24 - so what i like to do is the following
253:26 - i'm gonna remove it from here just gonna
253:29 - copy this variable
253:31 - and
253:32 - actually what we can do here is we can
253:34 - say to docker login get the password
253:37 - from the standard input and i'll explain
253:40 - a second what the standard input is you
253:42 - wanna write here dash dash password
253:45 - dash std in this is the standard input
253:48 - so the standard input is essentially
253:50 - something that we are piping from
253:52 - another command
253:54 - and that other command in our case will
253:56 - be echo so we're going to echo this ci
254:00 - registry password but we're not going to
254:02 - display it in the logs we're going to
254:04 - use this pipe
254:05 - so this will be available in the
254:08 - standard input for the docker login and
254:11 - docker login will look at that standard
254:13 - input and we'll see oh okay the password
254:15 - is coming from there so we're going to
254:17 - grab it from there
254:19 - using it in this way will ensure that
254:21 - this password doesn't get exposed
254:24 - but really for ci in our case it will
254:26 - not get exposed anyway
254:29 - but it's any good to know
254:31 - and
254:32 - this is why we have this construct here
254:34 - so once again we are echoing this
254:36 - password and we're sending it to docker
254:40 - and docker login will know that this
254:42 - password is coming from the standard
254:45 - input
254:46 - and apart from that we have the
254:48 - parameter with the user for docker login
254:50 - where are we logging in to the registry
254:53 - make sure that all the variables that
254:54 - you are using here have a dollar sign
254:57 - before them otherwise they will not be
254:59 - resolved as variables
255:02 - so let's give this a try as well and see
255:04 - if our image lands up in the registry
255:07 - the build docker image job has been
255:09 - successful we can take a look at the
255:11 - logs to see exactly which tags we have
255:13 - pushed
255:14 - and you will see here this are the tags
255:18 - that are available so we had tag 46 and
255:21 - latest
255:23 - and this is pushing the tags
255:26 - to the registry
255:28 - so no errors here and what we can do
255:30 - next is to go here inside the registry
255:33 - inside the container registry
255:36 - and we'll see here this root image
255:39 - and if we take a look at it we'll see
255:41 - currently we have two tags we have tag
255:44 - 46 and the latest tag will also tell us
255:47 - the size of our image because we are
255:49 - using alpine as a base image also our
255:52 - images are relatively small so that's
255:55 - definitely a good thing
256:03 - so now we have successfully built this
256:05 - container
256:06 - but how do we know that application is
256:09 - actually working how do we know if nginx
256:11 - is serving our files
256:13 - if we have the right files and if
256:15 - everything is working as we think is
256:17 - working before we actually move on to
256:20 - the next step which is deploying to aws
256:23 - so for that reason it does make sense to
256:25 - do some accepting testing on the docker
256:28 - container itself
256:29 - just to make sure that we have
256:31 - everything as we expect
256:34 - so with that being said let's go ahead
256:36 - and add another stage i'm going to call
256:38 - this stage test and of course we'll also
256:41 - have to add another job
256:43 - let's call this job test docker image
256:48 - i'm going to assign the stage test
256:50 - and let's think about like how we can
256:52 - test this
256:54 - essentially the way we're testing this
256:55 - docker image is not much different than
256:58 - the way we have tested for example a
257:01 - deployment or any other things in the
257:03 - past
257:04 - we can still use curl to do that so for
257:07 - example i'm going to write here curl
257:11 - we need here an address http column
257:13 - forward slash forward slash
257:15 - we don't know the address
257:17 - but then again we can use grep to search
257:20 - for example for the app version right
257:23 - and of course we can also search for
257:25 - anything in the page that a website has
257:29 - but say for example here we're going to
257:31 - check the version
257:33 - that
257:34 - html that we have so just want to make
257:36 - sure that we have the right
257:38 - app version
257:40 - and remove one of the dollar signs here
257:43 - so what do we need okay we need
257:46 - curl so we need a simple image that has
257:48 - curl so we're going to use curl
257:51 - images slash forward slash curl
257:56 - so that should be enough to get this
257:59 - but the question is
258:01 - how do we start this container right
258:04 - this is where the part that we have
258:06 - learned before can be very useful and
258:08 - that part is services so again we're
258:11 - going to take advantage of the services
258:14 - to start the docker container that we
258:17 - have just
258:19 - so we're gonna write here services
258:24 - and this time that we'll do is in this
258:26 - list we're gonna define the name
258:30 - this will be the name of the image and
258:32 - the name of the image is of course
258:35 - dollar sign ci registry image
258:39 - and the tag will be the app version
258:43 - we don't want to use the latest i want
258:45 - to go exactly like what is a current tag
258:49 - that we want to see
258:51 - so this will help us start the image
258:54 - and additionally what we can do is to
258:57 - specify an alias an alias allows us to
259:00 - give a friendly name so that we know
259:03 - where this service is available over the
259:06 - network
259:07 - so i'm just going to give the alias of
259:09 - website
259:10 - and here in http
259:13 - the address that gonna use is simply
259:16 - website
259:17 - so http column forward slash forward
259:20 - slash website
259:22 - gitlab will take care of starting this
259:25 - docker container
259:26 - registering it in the network as website
259:30 - and then we can simply call it in our
259:32 - curl script with website gonna go to
259:35 - version.html and inside we're gonna
259:37 - search for the app version
259:40 - so let's give it a try and see how it
259:41 - works
259:43 - and the confirmation
259:44 - that indeed our docker image is working
259:46 - properly can be obtained from this test
259:49 - docker image job
259:51 - so in this case indeed we have checked
259:54 - that
259:56 - the docker image is starting an http
259:59 - server
260:00 - and that the files that we have uploaded
260:02 - in our case this version.html
260:05 - it is available there
260:07 - and it contains the application version
260:09 - that we expected
260:10 - so with that being said in this case we
260:13 - now
260:14 - know that we have a docker image
260:16 - that works and we can deploy it to aws
260:25 - if you remember the first time that we
260:27 - have deployed an application to elastic
260:29 - bean stock
260:30 - we have used this json file which
260:33 - described which docker image we want to
260:35 - deploy
260:37 - in order to automate this process we
260:39 - kind of have to do the same thing in
260:42 - other words to give elastic bin stock
260:44 - this file
260:46 - now whenever we're interacting with aws
260:48 - services we need to
260:50 - provide some files most of the time we
260:52 - need to do that through s3 so we first
260:55 - need to upload those files to s3 and all
260:59 - other services can read from s3 and have
261:02 - that information
261:03 - so in order to automate this we need to
261:05 - generate essentially this json file
261:08 - upload it to s3 and from there we'll
261:11 - tell elastic bin stock to start a new
261:13 - deployment
261:14 - so going back to the pipeline what we
261:16 - need to do is to re-enable the deploy
261:19 - stage and the deploy to production job
261:22 - this time focusing on deploying to
261:24 - elastic pinstock
261:26 - so let's go ahead and add a new stage
261:29 - deploy
261:30 - and of course also a new job here
261:33 - deployed to production
261:35 - since we'll be using aws cli i'm just
261:38 - gonna add the basic structure that we
261:40 - need to this job in order to be able to
261:42 - use aws cli
261:44 - so using aws cli as a docker image we're
261:47 - overriding the entry point
261:49 - this job is part of the deploy stage we
261:52 - have set the environment to production
261:54 - and we are starting our script
261:56 - now if looking at the files here
261:59 - we'll see here in templates that we have
262:01 - this docker run that aws
262:05 - the name of the file is not that
262:06 - important
262:08 - the format of the file is very important
262:10 - because this file tells
262:13 - aws elastic binstock what we are trying
262:15 - to deploy
262:16 - and it will say here that this is our
262:18 - image
262:19 - and this is our tag
262:21 - so we have them here as variables
262:24 - but additionally because this is a
262:26 - private registry
262:27 - we also need to provide some
262:29 - authentication information and that
262:31 - authentication information is another
262:33 - file
262:34 - so that file is this oauth file
262:37 - and in this auth file we will need to
262:40 - add a token that gives access to the
262:42 - registry
262:44 - and essentially what we need to upload
262:46 - to s3 is this file docker run and this
262:50 - auth file which is actually linked from
262:53 - here it's mentioned here of that json
262:57 - so let's go to the pipeline and quickly
262:59 - add these files
263:02 - i'm going to simply paste here the
263:04 - configuration required
263:06 - so now i've added this copy
263:09 - configuration to the script so we're
263:11 - still using aws s3 copy it's pretty
263:14 - similar to what we did before
263:16 - in terms of copying files to aws s3 so
263:20 - i'm going to skip a bit this part
263:23 - now as you may have noticed
263:25 - these files contain some variables
263:28 - and those variables won't get
263:30 - automatically replaced so if we upload
263:32 - the files as they are
263:34 - they will not get replaced second of all
263:36 - we don't even have the right paths here
263:38 - they are in templates and not in the
263:41 - current folder where we're executing the
263:43 - script just as a second thing
263:45 - but i wanted to show you something very
263:47 - important and that is how to do
263:49 - environment variable substitution in
263:52 - files and for that we're going to use a
263:54 - very cool command
263:56 - it is env so from environment subs
264:00 - so relatively easy to tell what this
264:03 - does it replaces environment variables
264:06 - then we need to specify like what is the
264:08 - input file and we do it with this
264:11 - smaller than sign essentially
264:13 - and then we're gonna go here and say
264:15 - something like templates
264:18 - and give here the name of the file
264:20 - and then we will have to specify the
264:23 - output file that will be generated
264:26 - and the output file will be this one so
264:28 - the name of the file will stay the same
264:30 - the location will change it will be put
264:33 - in the current folder and this is
264:34 - perfect for the next command that we'll
264:36 - be using
264:37 - we're doing this environment
264:38 - substitution replacing all the variables
264:41 - here
264:42 - for the auth file just as well
264:45 - so i'm just gonna replace that
264:48 - there and there
264:50 - so any environment variables that are in
264:52 - these files will get replaced
264:55 - in order to use this command we need to
264:57 - install an utility
264:59 - which is called get text now this
265:02 - doesn't come directly in this amazon
265:04 - image so this is something that we need
265:05 - to install additionally i'm going to use
265:07 - this package manager
265:09 - to install this dependency which is
265:12 - called get text
265:15 - and get text has this tool for
265:17 - environment substitution
265:19 - gonna also
265:21 - add this flag here this will essentially
265:23 - answer with yes any questions that this
265:26 - installer may ask because we are in a
265:28 - way that we cannot interact with an
265:30 - installer
265:31 - we're gonna specify this and say okay
265:33 - just install it if there are any
265:34 - questions answer with yes just to ensure
265:36 - that the installation process works
265:37 - without any issues
265:39 - now we have this so we can do
265:41 - environment substitution of course it's
265:43 - going to be very nice to actually take a
265:45 - look at these files so we're going to
265:46 - use here cat
265:48 - just to make sure that these
265:50 - environments were substituted as we
265:52 - expect them
265:54 - and as a final check we should also go
265:56 - in these files and make sure that we
265:58 - have all variables here
266:00 - so
266:01 - see a registry image
266:03 - app version this is something that we
266:05 - already defined
266:06 - so we have this information here
266:09 - aws s3 bucket
266:11 - this is relevant
266:13 - for where we are uploading these files
266:16 - now because we are using elastic bean
266:18 - stock an elastic beanstalk has already
266:20 - created an s3 bucket
266:24 - well maybe we should use that one so i'm
266:26 - gonna switch here to the s3 service
266:30 - and what i'm going to do is i'm going to
266:31 - copy this name
266:33 - could have used this other
266:35 - package that we have created here but i
266:37 - don't want to put anything any
266:39 - credentials in a public folder so
266:42 - with that being said i prefer to use
266:44 - this bucket that is not public
266:46 - and here in the environment variables of
266:49 - the job we still have this aws s3 bucket
266:52 - for production
266:54 - so i'm gonna click here to edit it i'm
266:57 - to change here the value
266:59 - i'm not going to protect this variable
267:00 - because i'm still in a branch and i'm
267:02 - going to update this
267:05 - so
267:06 - at least at this point we have
267:07 - everything let's see this other file the
267:09 - off file
267:10 - you'll see here in the auth file there
267:12 - is this deploy token
267:15 - now what is the thing well we have
267:17 - pushed our docker image in the gitlab
267:20 - repository but aws
267:23 - has no credentials to connect to it and
267:26 - again we're not providing username and
267:27 - passwords not our account username and
267:30 - password
267:31 - we will generate a token and gitlab
267:33 - allows us to generate a deploy token
267:35 - that can be used by aws to log into our
267:39 - private docker container repository
267:41 - and pull that image
267:44 - so now to do that from settings we're
267:45 - going to go here to repository
267:48 - and we'll see here deploy tokens as one
267:52 - of the options
267:53 - and here we can create a deployed token
267:56 - so we're gonna name it aws so that we
268:00 - know why we have created it
268:01 - and i give the username aws
268:05 - and the permissions that we need is read
268:07 - repository and read registry
268:11 - i'm going to go ahead create this deploy
268:13 - token
268:14 - this will be created here
268:17 - this is essentially like a password but
268:19 - it will only be displayed once so i'm
268:21 - going to go ahead and copy this and i'm
268:23 - going to create a new variable to store
268:26 - it
268:29 - i'm going to call this variable
268:31 - gitlab
268:32 - deploy token
268:34 - and format how we'll store this
268:36 - information is i'm going to write here
268:37 - the username aws
268:40 - column and then the password that i've
268:42 - just copied
268:43 - i'm not going to protect this variable
268:45 - but i can just as well mask it just to
268:48 - be sure
268:50 - now here inside the pipeline we still
268:52 - need to do a small change to the
268:55 - pipeline itself to the script itself
268:58 - and
268:58 - the thing is we need to convert this
269:01 - username and password to base64 because
269:04 - this is what we need inside this
269:06 - configuration json this is what aws
269:09 - expects
269:10 - and in order to convert something to
269:12 - base64 it's relatively easy
269:15 - so for example say you have here a
269:17 - string right
269:18 - hello
269:20 - you can convert it to base 64 by simply
269:23 - piping this and writing here base
269:26 - 64. this will output the string encoded
269:29 - as base 64.
269:31 - now of course we don't want to have
269:33 - anything hard coded here this is why we
269:35 - have defined this variable so we are
269:38 - going to use here
269:39 - gitlab deploy token right
269:42 - and additionally we want to make sure
269:44 - that we don't have any new lines or
269:46 - anything like that in our token
269:48 - so we're going to pipe them again to
269:50 - another command
269:51 - and this command will remove any new
269:54 - lines that exist here so
269:57 - so this n here with the backslash
269:59 - this will be a space essentially so if
270:02 - there are any spaces they will be
270:03 - removed
270:05 - additionally we need to set this into a
270:07 - new variable that we can replace
270:09 - so i'm gonna put here
270:11 - a dollar sign at the beginning
270:15 - i'm going to put this between
270:16 - parentheses
270:18 - and what i'm going to do here i'm going
270:20 - to export
270:21 - so this is
270:23 - a way to create an environment variable
270:25 - but from a script and the name will be
270:27 - deploy
270:29 - token
270:30 - right
270:31 - and equals this expression so what we
270:33 - have here
270:35 - this part is an expression
270:37 - this will be evaluated
270:39 - and whatever output comes from this it
270:42 - will be stored in this deploy token and
270:45 - this deploy token
270:47 - is exactly what we have here in our
270:50 - pipeline
270:51 - so before we commit these changes we
270:53 - only are interested in making sure that
270:55 - this job works well so what i'm gonna do
270:58 - is i'm gonna simply disable all the
271:00 - previous jobs i'm gonna put a dot before
271:02 - them
271:03 - so that they don't run
271:06 - and after that i'm gonna commit this
271:07 - pipeline and let's see it in action
271:10 - and it seems that i'm in luck no errors
271:13 - while running this job let's take a look
271:15 - at the logs to see if we have everything
271:18 - and indeed it seems here that something
271:21 - is missing right some information has
271:25 - been replaced
271:27 - the image
271:28 - is replaced here i have the version
271:31 - i have here the bucket that's definitely
271:33 - working fine
271:34 - here in the odds the registry is correct
271:37 - but if i'm looking at all that's
271:38 - information it is missing
271:40 - so let's try and debug and to understand
271:43 - why is this information missing there
271:45 - so here's the pipeline let's double
271:47 - check if we have everything correct if
271:50 - we have used the right variables so
271:52 - gitlab deploy token that should be the
271:55 - name of the variable that we have
271:57 - already find in gitlab as a variable so
271:59 - that seems to be fine
272:01 - deploy token is exactly that what we
272:04 - have used in the file we can double
272:06 - check again the file
272:07 - you see here it's exactly deploy token
272:11 - so that seems to be working well
272:14 - so what is going on
272:16 - let's take a look again at the pipeline
272:18 - to see if there are any clues why this
272:20 - may have failed
272:24 - you know again
272:26 - here we don't see it
272:28 - but let's take a look at the commands
272:30 - that we have executed to see if there's
272:31 - anything suspicious in there so what did
272:34 - we do
272:35 - well first of all we have just started
272:37 - the script here we have aws cli great we
272:41 - are installing here get text so that
272:44 - seems to be working
272:46 - don't see any errors up to this point
272:50 - and then we're exporting this variable
272:52 - right we're running this command this
272:54 - expression
272:56 - and then if we're looking here at line
272:58 - 77 we're seeing here td command not
273:02 - found
273:03 - so apparently this expression that we
273:06 - have placed here gets evaluated into
273:10 - this variable
273:12 - but when it fails
273:14 - it doesn't feel the job
273:16 - i'm going to open up here the pipeline
273:19 - and let's take a look at the deploy to
273:22 - production
273:23 - so this is the td command that we have
273:25 - used it should actually be tr
273:29 - which
273:30 - stands for translate
273:33 - now this was just a silly mistake but it
273:35 - just goes to show how important it is to
273:38 - actually read the logs to understand
273:40 - what's going on now we have both files
273:44 - all the variables have been replaced
273:46 - including this time in the auth file
273:49 - and we can also jump into s3
273:52 - and take a look at this bucket here
273:55 - we'll be able to see here the docker run
273:57 - file and the auth file
274:00 - they are now inside s3
274:02 - docker run tells
274:04 - essentially contains information about
274:06 - what are we trying to deploy
274:08 - links and mentions this auth file which
274:10 - contains authentication information
274:12 - about how to connect to our private
274:15 - registry
274:21 - now let's continue with the actual
274:22 - deployment so we have copied here the
274:25 - docker1.aws.json
274:27 - and the auth.json file and they are in
274:30 - this s3 bucket
274:31 - and now we can initialize the deployment
274:34 - and the deployment happens with the aws
274:37 - cli but this time we'll use a different
274:40 - service so the service to which we're
274:42 - trying to deploy is elastic beanstalk so
274:44 - i'm going to use aws
274:46 - elastic bean stock
274:48 - and this is something that is done in
274:49 - two steps first of all we have to create
274:52 - an application version so the command
274:54 - that i'm going to use here is create
274:56 - application version with dashes
274:58 - the next command that we need to run is
275:01 - update environment so i'm going to copy
275:03 - this
275:04 - and i'm going to write here update
275:06 - environment so why are these two steps
275:08 - necessary in the first step
275:10 - we are taking this docker run file
275:14 - and we are creating a new application
275:16 - version
275:17 - and then once this application version
275:19 - has been created
275:20 - we actually tell elastic bean stock
275:23 - which environment we want to update on
275:26 - elastic bean stock we have the
275:27 - application but there can be multiple
275:29 - environments in our case we have a
275:31 - single environment but theoretically
275:33 - it's possible to take one version to
275:36 - create it once and to take it through
275:38 - different environments but in this case
275:41 - we only have one environment so maybe
275:44 - this is why it may look a bit weird in
275:45 - the beginning but these are the steps
275:47 - that are required in order to get this
275:48 - to run now just saying create
275:51 - application version and update
275:52 - environment is not actually sufficient
275:54 - in order to get this to run we need to
275:56 - specify some additional parameters
275:59 - and the first parameter is we need to
276:00 - identify the application right
276:03 - so this is done by writing here dash
276:05 - dash application dash name
276:08 - and if we're looking here
276:10 - inside elastic bean stock you will see
276:12 - here this is the application name i'm
276:14 - going to copy this as it is here
276:17 - and of course i have the possibility of
276:19 - putting it here
276:20 - directly like application name like this
276:24 - but i don't want to do that i want to
276:26 - take advantage of variables
276:28 - there are multiple places where we can
276:30 - store this but i'm just gonna put them
276:32 - here inside the job itself so we can
276:35 - add here variables block
276:37 - and for example i'm gonna call this app
276:39 - underscore name
276:41 - column
276:42 - and then this will be the value my
276:44 - website so whenever i need to use this
276:46 - i'm going to put it here
276:48 - so application name
276:51 - dollar sign
276:52 - and i'm going to reference the variable
276:55 - now there's something you need to pay
276:56 - attention here
276:58 - because my website is composed of two
277:01 - strings so there's this space here in
277:03 - between
277:04 - if we put this like this
277:07 - this variable gets replaced it will look
277:08 - like this
277:10 - so this command will think that my
277:12 - application name is my
277:15 - and then that website is some other
277:18 - command that we're executing or some
277:20 - other parameter that we're giving here
277:22 - and this is not something that is
277:23 - recognized
277:24 - so in order to get around this we need
277:26 - to put this between codes
277:29 - so we'll have this string between codes
277:31 - because this is why it will know okay it
277:33 - is my and website it contains also the
277:36 - space and the word after this
277:38 - so again i'm gonna also put the variable
277:41 - here between quotes
277:44 - and this application name is also needed
277:47 - for the update environment command so
277:49 - i'm going to copy that there as well
277:52 - additionally when we're creating the
277:53 - application version
277:55 - we need to specify like a label or which
277:59 - version we are deploying
278:01 - we already have an application version
278:03 - here so it does make sense to use that
278:07 - so i'm going to go ahead and right here
278:09 - dash dash version
278:11 - dash label
278:13 - don't worry if this goes on a new line
278:15 - it's just the way it's edited but
278:17 - actually the command is on a single line
278:20 - and i'm gonna use here app version
278:24 - and this here for update environment yes
278:26 - it does also make sense to specify the
278:29 - application version
278:31 - and i'm gonna add the same command here
278:34 - the same parameter version label to this
278:37 - command
278:38 - now also when we creating this
278:40 - we need to specify somehow this file
278:43 - right
278:44 - so elastic bean stock will not know
278:46 - which bucket is this file how can i read
278:48 - it and everything
278:50 - so for that reason
278:51 - we also have to specify that
278:53 - and we're going to use this additional
278:56 - parameter source dash bundle and this
278:59 - will allow us to specify
279:02 - an s3 bucket
279:05 - and the s3 bucket will
279:06 - equal this time the bucket that we have
279:08 - used
279:10 - so it's this one here
279:13 - and we also have to specify
279:16 - the name of the object that we're
279:18 - referencing and this is s3 key
279:21 - you need to pay attention how you write
279:23 - this it needs to be exactly
279:25 - as i have written it here
279:27 - but i also noticed people in the
279:29 - beginning
279:30 - they don't understand when they are
279:31 - specifying these additional parameters
279:33 - here is not for example it's not equal
279:35 - application name equals app name it's
279:38 - just a space and then comes the value
279:40 - here this is a bit
279:42 - different because this entire thing this
279:44 - entire configuration here
279:46 - is a value that will be read later on
279:50 - so for that reason here it's okay to
279:51 - have this equals but otherwise there are
279:53 - no equals in these commands
279:55 - all right so to create the application
279:58 - version we have specified application
280:00 - name we have specified the version label
280:03 - and additionally we have specified this
280:07 - source bundle so essentially
280:10 - where is the file that tells us what we
280:12 - should deploy
280:13 - this is almost the same as when we have
280:17 - uploaded that file manually but with
280:19 - that we have also triggered the
280:21 - deployment automatically so we have done
280:23 - two steps in one step
280:25 - here we are doing it in two separate
280:27 - steps
280:28 - so the next part is after creating this
280:31 - version is to update the environment
280:33 - so we have here the application name we
280:35 - have here the version
280:37 - for which application are we deploying
280:38 - this which version are we deploying
280:41 - the next step would be to also specify
280:43 - which environment are we updating i'm
280:45 - gonna write here environment dash name
280:48 - and we also need another variable for
280:51 - the environment i'm gonna call it here
280:53 - app
280:55 - and name you're free to name them as you
280:57 - wish
280:58 - and again i'm going to go here to
281:00 - elastic bean stock and i'm going to copy
281:02 - this i just want to make sure that i
281:05 - have everything exactly as it is in
281:07 - elastic bean stock if i specify
281:08 - something else
281:10 - then it's not the same
281:12 - so the environment name will be here
281:16 - and as you can notice here this doesn't
281:18 - have any spaces so we don't need to put
281:20 - this value between codes and also all
281:22 - the other values that we had here there
281:24 - are no spaces so we don't need to put
281:27 - any quotes there but if any of these
281:29 - parameters would have a space
281:31 - that value would need to be between
281:32 - quotes
281:34 - finally let's also go ahead and
281:36 - re-enable these jobs because we'll still
281:39 - need a docker image now
281:40 - so we will run the entire pipeline and
281:43 - check if our deployment is working
281:46 - properly
281:48 - however when we take a look at the
281:49 - pipeline we'll see that the deployed
281:51 - production job has failed so let's jump
281:54 - into the logs
281:55 - try to understand what has happened and
281:57 - why this job failed
281:59 - you can see here the commands that we
282:01 - have executed
282:02 - so copying these files to s3 still works
282:05 - so no problems there the last command
282:07 - that was executed is this one so elastic
282:10 - beanstalk create application version so
282:13 - there's a good chance that
282:15 - this command is responsible for whatever
282:17 - error we're getting here and then we
282:19 - need to look into the logs and see what
282:21 - is the error
282:22 - and we can see here it says something
282:24 - access denied essentially it's telling
282:25 - that
282:26 - our user
282:28 - which is the gitlab user that we have
282:30 - created if you remember we have created
282:32 - this user with programmatic access
282:35 - is not authorized to perform
282:37 - some action so create application
282:39 - version
282:40 - if you remember we have selected that
282:41 - the user is allowed to work with s3
282:44 - resources right so uploading files
282:47 - deleting files things like that with s3
282:50 - that works
282:51 - but we haven't authorized our user to
282:54 - work with elastic bean stock
282:56 - so we need to change that
282:58 - so from the aws console let's go ahead
283:01 - and open the
283:03 - im service for identity management
283:07 - we'll open up our user
283:09 - you see here this is the username gitlab
283:13 - and what we can do here is essentially
283:15 - to attach additional policies so we have
283:18 - here this
283:19 - amazon s3 full access
283:21 - so that works but now we want to add an
283:24 - additional policy so we're gonna click
283:26 - here on add permissions
283:28 - and gonna attach existing policies
283:32 - i'm going to search here for elastic
283:34 - bean stock
283:36 - and i'm going to give here
283:38 - administrator access for aws elastic
283:41 - beanstalk
283:42 - now i'm not going to go into this
283:44 - policies in very detail we're just
283:46 - interested in getting this to run but of
283:49 - course if you would use something like
283:50 - this in production you would need to be
283:52 - very careful about what each user is
283:55 - allowed to do
283:57 - so
283:58 - just keep that in mind just using this
284:00 - very generous policies
284:01 - in production is typically not a good
284:04 - idea
284:05 - so i'm gonna add these permissions then
284:08 - and now we'll see that our user
284:10 - now has this additional permission here
284:13 - so we can now work with elastic
284:15 - beanstalk
284:17 - so going back here to the logs
284:19 - instead of re-running the entire
284:21 - pipeline we can just go to this job and
284:24 - hit this retry button
284:26 - and it will retry the same job we
284:28 - haven't changed any files
284:30 - and dynamically this policy should now
284:32 - be applied to the user
284:33 - and hopefully this job will work
284:35 - so let's take a look to see what the job
284:38 - is doing
284:39 - and now this time it looks much better
284:42 - so we have no errors
284:45 - and let's take a look at the first
284:47 - command create application version now
284:49 - we have executed this and we're getting
284:51 - back a response right
284:54 - so it's going to tell us again like the
284:56 - application name which version we have
284:59 - which bucket we're using
285:02 - which is the name of the file and so on
285:05 - it also starting to process this
285:07 - and the next thing what we're getting
285:10 - is the update environment so that work
285:12 - without any issues it's going to tell us
285:14 - again what is the environment name
285:16 - so we can take a look at that
285:18 - application named version
285:19 - and also some technical details about
285:21 - this
285:24 - we can take a look back into the aws
285:27 - console and then look here into the
285:29 - elastic bin stock service
285:31 - to see what's going on and we'll be able
285:33 - to see here running versions is 60. i
285:36 - can click here on the environment
285:38 - just to make sure that we indeed have
285:40 - everything the health is okay
285:42 - so
285:43 - we can also go ahead and click on this
285:45 - url
285:46 - and this will open up our website so it
285:49 - does seem to work very well
285:57 - so now we have manually tested that the
285:58 - deployment has been successful
286:01 - but how about that we check in our
286:02 - pipeline that the deployment has been
286:03 - successful and we can use the exact same
286:05 - approach that we have used so far i'm
286:08 - gonna jump back into the pipeline
286:11 - and here inside the deployed production
286:14 - job essentially what we want to do is to
286:16 - use this curl command so should be
286:19 - pretty similar to what we have here
286:22 - when we're testing the docker image
286:24 - i'm gonna add it here to a new line
286:26 - of course we also need to get the
286:30 - address itself of the application of the
286:32 - environment and if you remember we had
286:34 - something for that already we just need
286:37 - to update the url for the application
286:40 - so i'm going to simply copy this url
286:42 - here
286:44 - and from gitlab i'm going to go to
286:45 - deployments environments
286:49 - and for the production environment i'm
286:51 - going to go ahead and change some
286:52 - settings
286:55 - and we're no longer using the s3 url
286:58 - we're using this one and remove here the
287:00 - forward slash so this will be my
287:03 - external url for this environment
287:06 - and what we want to use here is of
287:08 - course the variable that will get us
287:10 - this environment
287:12 - and from the predefined variables
287:14 - probably remember we have used this in
287:15 - the past ci environment name slug url we
287:20 - actually want the url so i'm going to
287:22 - copy this
287:24 - get here into the pipeline configuration
287:27 - and instead of website
287:29 - can also replace http because i don't
287:31 - need that anymore
287:32 - so i can use ci environment url forward
287:35 - slash version
287:37 - and then we're getting that version from
287:39 - there
287:40 - now i'm going to tell you if we leave
287:42 - this command as it is right now
287:44 - this will not work
287:46 - and the reason for that is
287:49 - when we are updating this environment
287:51 - aws needs just a few seconds to actually
287:55 - do this deployment to put the new
287:57 - version on the environment this is not
287:59 - instant
288:00 - but the connects command will run
288:02 - immediately after we have essentially
288:05 - triggered this update
288:07 - and that environment may not be ready or
288:09 - may still have the older version so for
288:12 - that reason this curl here will fail so
288:15 - we'll not get this response the
288:17 - deployment cannot be verified at this
288:19 - point that it has been successful or not
288:21 - so what we need to do here is to wait a
288:24 - bit so what we can do is
288:26 - of course to use something like sleep
288:28 - and wait for like 10 seconds or
288:30 - something like that
288:32 - but in aws there's a better way to do
288:34 - that when we're using the aws cli
288:37 - we have a tool which is called weight
288:41 - so
288:41 - weight is an option
288:43 - it is for various services that are at
288:46 - aws including for elastic bean stock so
288:49 - we can right here elastic bean stock
288:51 - weight
288:53 - and then what are we waiting for we are
288:55 - waiting for this environment to be
288:57 - updated so i'm going to write here
288:59 - environment
289:00 - dash updated
289:02 - and then we need to specify the
289:04 - application name the environment name
289:07 - and the version label right so
289:08 - essentially everything that we had here
289:10 - on the previous command
289:12 - i'm gonna simply copy
289:14 - and i'm gonna add it here to the new
289:16 - line
289:18 - and this command will do the following
289:19 - it will essentially in the background
289:21 - it will check with aws hey are you done
289:24 - updating that environment no not yet i
289:27 - need some time okay
289:28 - wait a bit and then we'll try again hey
289:30 - are you done updating that environment
289:32 - okay i'm done good and then we can stop
289:35 - waiting and then the next command the
289:37 - curl command can run and then we can
289:39 - check if indeed the correct version has
289:41 - been deployed and if our environment is
289:44 - working properly
289:45 - so i'm going to go ahead and commit
289:47 - these changes and we'll let the entire
289:49 - pipeline run and see how this goes
289:51 - and the pipeline is still successful so
289:53 - we can take a look into the deploy to
289:55 - production job and see what exactly
289:57 - happened here
289:59 - and you'll be able to see here that
290:01 - this command is executed
290:04 - and
290:05 - after this the next command with curl is
290:08 - also executed and it passes without any
290:11 - issues you will see here version 61 has
290:13 - been deployed this is exactly also the
290:15 - version label that we have here defined
290:18 - so this again confirms that the version
290:21 - that we wanted to deploy has actually
290:23 - landed on the environment where we
290:25 - wanted to have it
290:32 - so that's about it in terms of deploying
290:35 - to elastic beanstalk
290:37 - so just to recap we have gone through
290:39 - all the stages and we have started by
290:44 - initially building our code whatever we
290:46 - have here compiling it
290:48 - building it running some tests
290:51 - and then publishing this as artifacts
290:53 - and then we have created a docker image
290:55 - essentially we have created an
290:56 - application
290:58 - we have tested that application
291:00 - and then of course we have really a
291:02 - simple pipeline here we didn't
291:05 - went through any other stages we
291:07 - deployed directly to production but the
291:10 - principles that i've shown you here can
291:12 - be used for similar projects and of
291:14 - course based on all the other
291:16 - information that i've shown you
291:17 - throughout the course you can build
291:18 - power plants that are more complex than
291:20 - this one but the most important thing is
291:23 - to understand how you can build such
291:25 - pipelines how you can deploy them and
291:27 - how you can end up with something that
291:30 - works and generally to make this process
291:33 - as enjoyable as possible
291:40 - since now we are coming towards the end
291:42 - of the course i thought it would be a
291:45 - good idea for a final assignment
291:48 - and what i have here is a project which
291:51 - will document who has completed this
291:54 - course so what you have to do is to
291:57 - check the course notes and in the course
291:59 - notes you will find the link to this
292:00 - repository and go ahead and click here
292:03 - on request access
292:06 - and
292:07 - in
292:08 - a few hours probably you receive access
292:10 - to this repository and you will be able
292:12 - to make changes to it so you can open
292:15 - the web ide
292:16 - and
292:17 - open a merge request and you will have
292:20 - to add
292:21 - if you wish of course your name to a
292:24 - list of people who have completed this
292:26 - course so let me show you how to do this
292:30 - so once you have access to this you will
292:32 - no longer see this part with requesting
292:35 - access and you can open the web id and
292:38 - you will be not asked to fork this
292:40 - project that's the main difference
292:42 - and then you can go ahead and change the
292:45 - files the files that we want to change
292:47 - are located here in source
292:49 - and the file that contains
292:52 - the code is this app.js and here
292:56 - there will be a table
292:58 - and i invite you to add your name
293:01 - and other information to this table so
293:04 - for example i've added here my name i
293:06 - had it here my username for gitlab but
293:10 - also my country of origin and also
293:12 - message to the entire world
293:14 - and i'm gonna submit this as a merge
293:16 - request
293:17 - and i invite you to do the same so i'm
293:20 - going to go ahead here commit this
293:23 - and i cannot commit directly into the
293:25 - main branch so i have to go through the
293:27 - path of creating a merge request
293:30 - so i creating a feature branch with my
293:34 - name
293:36 - let's give it a meaningful title
293:40 - and then i can just go ahead and create
293:43 - the merge request
293:45 - once you have access to this project you
293:47 - will be able to see any other merge
293:49 - requests
293:50 - and i invite you to take a look at them
293:52 - to see what other people have changed to
293:55 - make sure that everything is working
293:56 - properly and if someone breaks the
293:59 - pipeline in their own branch maybe you
294:01 - can give people some tips in regards to
294:02 - what they did wrong and what didn't work
294:04 - so well
294:05 - essentially be part of this review
294:07 - process try to understand how to
294:09 - collaborate on this project
294:12 - and of course once your merge request
294:14 - gets reviewed it will be merged into the
294:16 - main branch and then you will be able to
294:18 - see
294:19 - your name appearing on a web page
294:23 - so i think that's kind of a nice and an
294:25 - interactive way of essentially
294:27 - concluding this course almost and so
294:30 - yeah i hope you will do this along
294:33 - in terms of editing
294:34 - let me give you an advice
294:36 - once there are a few people that have
294:38 - been added to this list here
294:40 - what i highly recommend is that you
294:42 - don't just add your name at the end
294:45 - because that's the highest chance that
294:46 - you will run into complex so when you're
294:48 - trying to make changes try to put your
294:50 - name somewhere in the middle or
294:52 - something like that between others try
294:54 - to keep the indentation and everything
294:55 - so that it looks nice
294:57 - but yeah that's my advice to you
295:00 - so see you
295:02 - even after this course i'm gonna
295:04 - collaborate more inside the merge
295:06 - request
295:06 - try to play more with gitlab with
295:08 - pipelines see how it works and yeah
295:11 - looking forward to your contributions
295:16 - all right you did it this is the end of
295:18 - the course but don't go away yet i still
295:21 - have some valuable tips for you first of
295:23 - all i want to give you a final reminder
295:25 - to terminate any aws services you have
295:28 - created so that you don't encounter any
295:30 - unexpected costs we have accomplished so
295:33 - many things in a very short amount of
295:34 - time i know this was a lot to take in
295:37 - but i hope it was useful and that this
295:39 - has opened your appetite for learning
295:41 - more about devops gitlab and aws if you
295:44 - enjoy this content you could support me
295:46 - in creating more courses like this one
295:48 - by going to my youtube channel and
295:50 - subscribing link in the video
295:52 - description thank you very very much but
295:55 - there's also so much more to learn if
295:57 - you found it hard to work with cli
295:58 - commands i do recommend learning about
296:01 - unix utility commands and bash there are
296:04 - also other gitlab features worth
296:06 - exploring if you like working when
296:08 - deploying docker containers you may also
296:11 - want to learn about kubernetes for all
296:13 - the topics mentioned above and anything
296:15 - else i forgot to mention you will find
296:17 - links in the course notes if you enjoy
296:20 - my teaching style and you want to take a
296:22 - more advanced gitlab course go to
296:24 - vdespa.com and check out the courses
296:26 - that i'm offering if you are unsure
296:28 - which course is right for you just send
296:30 - me a message on social media i'm more
296:32 - than happy to help i hope you enjoy
296:34 - spending time with me and i will see you
296:36 - next time