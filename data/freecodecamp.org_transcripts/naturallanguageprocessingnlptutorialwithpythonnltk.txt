00:00 - welcome everyone to free code camp i
00:02 - chrisley on behalf of edureka will take
00:04 - this session on natural language
00:06 - processing popularly known as nlp now
00:10 - edureka is a global e-learning company
00:12 - that provides online training courses on
00:14 - the latest trending technologies so
00:17 - without any further delay let's have a
00:18 - look at the agenda for this session
00:21 - so i'll start off by explaining the
00:22 - evolution of the human language then
00:25 - we'll understand what is nlp and how it
00:27 - came into the picture moving forward
00:30 - we'll have a look at the various
00:31 - application of nlp in the industry and
00:34 - next we'll understand the different
00:35 - components of nlp and the difficulties
00:37 - faced while implementing those and
00:40 - finally i'll finish off this video by
00:41 - explaining you guys the various steps or
00:43 - the paths involved in the natural
00:45 - language processing along with the demo
00:47 - for each one of those steps
00:49 - so the success of human race is because
00:52 - of our ability to communicate that is
00:54 - information sharing using this ability
00:58 - we have marched ahead of other animals
00:59 - and have become the most sophisticated
01:01 - creatures and this is what
01:03 - differentiates us among all the other
01:06 - animals we begin to look for ways to
01:08 - preserve our thoughts feelings messages
01:11 - and other information we started with
01:13 - oral communication like other animals
01:15 - but because of its informal nature we
01:18 - began painting on walls and caves where
01:20 - we lived now there was a need to
01:22 - standardize the drawing so that everyone
01:24 - could understand and that's where the
01:26 - concept of developing a language came in
01:28 - however many such standards came up
01:30 - resulting in many languages with each
01:33 - language having its own basic sets of
01:36 - alphabets combination of alphabets which
01:38 - were known as words and the combination
01:40 - of words which were arranged
01:42 - meaningfully which became the sentence
01:45 - now each language has a set of rules
01:47 - based on words and are combined to form
01:50 - these sentences now these set of rules
01:52 - are nothing but what we call it as
01:55 - grammar now i'm not gonna take any more
01:57 - time explaining you guys about grammar
01:59 - and all now coming to today's world
02:01 - according to the industry estimates only
02:03 - 21 of the available data is present in
02:06 - the structured form data is being
02:08 - generated as we speak as we tweet as we
02:11 - send a message on whatsapp instagram
02:13 - imessaging and various other platforms
02:16 - majority of this data exist in the
02:18 - textual form which is highly
02:19 - unstructured in nature now in order to
02:21 - produce significant and actionable
02:23 - insights from this text data it is
02:26 - important to get acquainted with the
02:27 - techniques and the principle of natural
02:29 - language processing so let's understand
02:31 - what exactly is nlp
02:33 - now natural language processing that is
02:36 - nlp refers to the artificial
02:38 - intelligence method communicating with
02:40 - an intelligent system using natural
02:42 - language now it is a part of computer
02:45 - science and artificial intelligence
02:47 - which deals with the human language by
02:49 - utilizing nlp and its components one can
02:52 - organize the massive chunks of text data
02:54 - perform numerous automated tasks and
02:56 - solve a wide range of problems such as
02:59 - automation summarization machine
03:01 - translation named entity recognition
03:03 - relationship extraction sentimental
03:06 - analysis speech recognition and topic
03:08 - segmentations now we'll learn about all
03:11 - of these later in this video now the
03:13 - goal here is to process i'd rather say
03:16 - understand the natural language in order
03:18 - to perform useful tasks some of these
03:20 - tasks include making appointment buying
03:22 - things spell checking generating
03:24 - responses and social media monitoring
03:26 - now if we look at the various
03:28 - application of nlp in the industry
03:30 - firstly we have the spell checking which
03:33 - is usually there in you can find it
03:35 - mostly in words or the document reader
03:37 - even online also you can do the spell
03:39 - checking now next we have keyword search
03:41 - and it is also a field where nlp is
03:43 - heavily used now extracting information
03:46 - from websites or any particular document
03:49 - also requires the knowledge of nlp now
03:51 - one of the coolest application of nlp is
03:53 - the address in matching which is
03:55 - basically a recommendation of ads based
03:58 - on your search what it does is analyzes
04:00 - the text of the data which you are
04:02 - already using or searched and match it
04:04 - with the text data of the advertisement
04:08 - now sentimental analysis is also a very
04:10 - major part of nlp another application is
04:13 - the speaker ignition now here we are
04:15 - also talking about the voice assistants
04:17 - like the siri google assistant the
04:20 - cortana and we need to thank apple for
04:22 - creating the first voice assistant that
04:24 - is siri now next we have the
04:26 - implementation of chatbots now most of
04:28 - you guys might have used the customer
04:31 - chat services provided in various apps
04:33 - now most of these apps use the chatbot
04:35 - which is it uses nlp to process the data
04:38 - which we entered and then it provides
04:40 - response based on an input that is also
04:42 - an application of natural language
04:44 - processing now another application of
04:46 - nlp is the machine translation now the
04:49 - most common example of it is the google
04:52 - translate as you know and now it uses
04:54 - nlp to translate the data or i should
04:57 - say the text from one language to the
04:59 - other and that too in real time now nlp
05:03 - consists of two components
05:05 - which is the natural language
05:06 - understanding referred to as nlu and the
05:10 - natural language generation which is
05:11 - referred to as nlg now understanding the
05:14 - natural language involves
05:16 - mapping the giving input in natural
05:18 - language into useful representation and
05:21 - analyzing different aspects of the
05:23 - language now natural language generation
05:25 - is the process of producing meaningful
05:27 - phrases and sentences in the form of
05:30 - natural language from some internal
05:32 - representation now it involves text
05:34 - planning which includes retrieving the
05:36 - relevant contents from the knowledge
05:38 - base it involves sentence planning which
05:41 - includes choosing require words from
05:44 - meaningful phrases setting tone of the
05:46 - sentences
05:47 - and finally we have text realization it
05:50 - is mapping sentence plan into the
05:52 - sentence structure
05:54 - now we'll learn about this later in this
05:56 - video and usually natural language
05:59 - understanding which is nlu is much much
06:01 - harder than energy now you might be
06:03 - thinking that even a small child can
06:05 - understand a language so how come it is
06:07 - so easy for human beings is so difficult
06:09 - for the computer to process it so let's
06:12 - understand what are the difficulties a
06:14 - machine faces while understanding a
06:16 - language
06:17 - so in natural language understanding
06:19 - there are certain ambiguities which are
06:21 - the lexical ambiguities syntactical
06:23 - ambiguity and the referential ambiguity
06:26 - now understanding a new language is very
06:28 - hard taking our english into
06:30 - consideration there are a lot of
06:32 - ambiguity and that to in different
06:33 - levels now starting with lexical
06:35 - ambiguity lexical ambiguity is the
06:37 - presence of two or more possible
06:39 - meanings within a single word it is also
06:42 - called semantic ambiguity for example
06:44 - let's consider the following sentences
06:46 - and let's focus on the italicized rules
06:49 - now she is looking for a match now what
06:51 - do you infer here from the match word is
06:54 - she looking for a match that is head to
06:56 - end match or is she looking for a match
06:58 - as in a partner the fisherman went to a
07:00 - bank is it the bank where we withdraw
07:03 - our money or is it the bank where he
07:06 - rows his boat or he catches the fishes
07:09 - now syntactical ambiguity in english
07:12 - grammar this ambiguity is the presence
07:14 - of two or more possible meanings within
07:17 - a single sentence or a sequence of words
07:20 - it is also called structure ambiguity or
07:22 - grammatical ambiguity now let's have a
07:24 - look at these sentences the chicken is
07:26 - ready to eat so is the chicken ready to
07:29 - eat something or is the chicken ready
07:31 - for us to eat so this is a kind of
07:33 - syntactical ambiguity which is often
07:36 - very hard to info for a new person or
07:39 - i'd rather say a computer because it
07:42 - means the meaning of the sentence is
07:43 - different for the different tones or in
07:46 - different aspects so for example if i
07:48 - look at the last statement i saw the man
07:51 - with the binoculars so do i have a
07:54 - banner or the man has a binocular
07:57 - it might be possible that you might be
07:59 - thinking that i saw the man with
08:01 - binoculars
08:02 - means that i have the banners but
08:04 - somewhere some people might think that
08:07 - the guy which i am seeing has the
08:09 - binoculars rather than me so that is
08:11 - syntactical ambiguity now coming to the
08:13 - third ambiguity which is the referential
08:15 - ambiguity now this ambiguity arises when
08:17 - we refer to something using pronouns now
08:20 - the boy told his father the theft he was
08:24 - very upset now when we talk about he was
08:27 - very upset if you focus on the
08:29 - italicized word he does this mean that
08:32 - the boy was upset or the thief was upset
08:35 - or the father was upset nobody knows
08:38 - this is referential ambiguity
08:40 - now coming back to nlp
08:42 - for using nlp onto our system or doing
08:45 - any natural language processing we need
08:47 - to install the nltk library that is the
08:50 - natural language toolkit so nltk is a
08:53 - leading platform for building python
08:55 - programs to work with human language
08:58 - data it provides easy to use interfaces
09:00 - to 50 corpora and lexical resources such
09:03 - as wordnet along with the suit of text
09:06 - processing libraries for classification
09:08 - tokenization stemming tagging and much
09:11 - more
09:12 - now let me show you how you can download
09:14 - an ltk
09:16 - now in order to download nltk just go to
09:18 - your python shell and just type in the
09:21 - word nltk dot download using the
09:24 - parenthesis and then you will get this
09:26 - type of a window pop-up which is the
09:28 - nltk downloader you just need to select
09:31 - all option and click on the download
09:33 - button and it will download all the
09:35 - corpora and the
09:36 - text it has all the packages it has into
09:39 - one single place and you can choose the
09:41 - directory where you want to install it
09:43 - it's better if you download it in your
09:45 - python directory only it will be easier
09:47 - for you to access all the files and all
09:50 - the text which it has to offer now when
09:52 - we process text there are a few
09:54 - terminologies which we need to
09:56 - understand
09:57 - so the first terminology here we are
09:58 - talking about is tokenization so it is a
10:01 - process of breaking up strings into
10:03 - tokens which in turn are small
10:05 - structures or units that can be used for
10:07 - tokenization now it involves three steps
10:10 - breaking a complex sentence into words
10:12 - understanding the importance of each of
10:14 - the words with respect to the sentence
10:16 - and produce a structural description on
10:18 - an input sentence so if we take the
10:21 - sentence today we will understand
10:22 - tokenization so as you can see we have
10:25 - five tokens the first one today we will
10:29 - understand tokenization so all of these
10:31 - words in computer terms are known as
10:34 - tokens and this is what is referred to
10:37 - as tokenization so let me show you guys
10:39 - how you can implement tokenization using
10:42 - the nltk library
10:45 - so here i'm using jupiter notebook
10:49 - you are free to use any sort of id also
10:51 - my personal preference is jupyter
10:53 - notebook so first of all let's import
10:55 - the os the nltk library which we have
10:58 - download and the nl ticket corpus
11:02 - now let's have a look at the
11:04 - corpora which is being provided by the
11:05 - nltk
11:07 - that is the whole data so as you can see
11:09 - we have so many files
11:12 - and all of these files have different
11:13 - functionalities some have textual data
11:15 - some have different functions associated
11:18 - with it we have stopwatch as you can see
11:19 - here your state union names we have
11:22 - twitter sample data we have different
11:24 - kind of data and different kind of
11:26 - functions here
11:27 - so let's take the brown into
11:29 - consideration as you can see here we
11:31 - have brown and brown zip so first we all
11:34 - we need to do is import the brown and
11:37 - then let's have a look at the words
11:38 - which are present in the brown you can
11:40 - see we have the fluton country grand
11:43 - jury said and it's going on and on
11:46 - now let's have a look at the different
11:48 - gutenberg fields so as you can see under
11:50 - gutenberg file we have austin mr text we
11:53 - have the bible text we have the blake
11:55 - poems the carol alice text
11:58 - we have the edgeworth parents.txt
12:01 - shakespeares we have we have the
12:03 - shakespeare we have the whitman leaves
12:05 - so nltk is a very big directory and a
12:08 - very big library to download you might
12:10 - need a couple of minutes to download
12:12 - this library
12:14 - so let's take the shakespeare hamlet.txt
12:18 - and if you look at the
12:19 - words which are inside this hamlet file
12:22 - we see it starts with the tragedy of
12:25 - hamlet by and it goes on and on so if we
12:28 - have a look at the first 500 words of
12:31 - this textual paragraph or what we say
12:33 - the textual file
12:35 - so i'm using here for word in hamlet and
12:38 - i'm using the colon and 500 that is the
12:41 - end point so as you can see it starts
12:42 - with the tragedy of hamlet by william
12:44 - shakespeare 1599 actus primus schona
12:48 - prima and it goes on and on
12:51 - so for natural language processing you
12:52 - can use either of the text which is
12:54 - provided here for understanding or you
12:56 - can create your own words
12:58 - so for example here i have defined a
13:01 - paragraph based on artificial
13:03 - intelligence
13:04 - okay so it goes on like according to the
13:06 - father of artificial intelligence john
13:08 - mccartney it is the science of
13:09 - engineering and so on and on so let us
13:12 - first define this string okay ai now why
13:15 - i'm taking a string is because it is
13:17 - easy to show you guys how to work on a
13:19 - string so if we do the type ai you can
13:21 - see it is sdr which is string
13:24 - now under nltk we have the nltk.tokenize
13:27 - and we are going to import the word
13:30 - tokenize
13:31 - right as a function will understand how
13:33 - it works
13:34 - now we will run the word underscore
13:36 - tokenize function over the paragraph and
13:39 - assign it a name
13:41 - let's assign it as ai underscore tokens
13:44 - so now if we'll have a look at the air
13:45 - underscore tokens
13:47 - you can see it has divided the whole ai
13:49 - paragraph which we gave into tokens
13:52 - so as you can see it has taken a comma
13:54 - also into a consideration the hyphen
13:56 - also and it goes on and on
13:59 - now let's have a look at the number of
14:00 - tokens where we have here
14:03 - so for that we are going to use the
14:04 - length function so as you can see we
14:06 - have 273 tokens
14:08 - now from nltk we have a probability
14:11 - function which is the frequency distinct
14:13 - so
14:14 - i'll show you what it does
14:16 - so for a word in ea underscore tokens
14:20 - f-test and we are going to convert it to
14:23 - lower keys so as to avoid the
14:24 - probability of considering a word with
14:26 - uppercase and lowercase as different and
14:28 - then we are going to assign it a number
14:31 - this is basically a word count program
14:33 - and this is being implemented using the
14:35 - frequency distinct function which is
14:36 - already present in the nltk library so
14:39 - let's see what's the output of this one
14:42 - so as you can see
14:43 - comma has appeared 30 times
14:45 - full stop has appeared 9 times question
14:48 - mark 1 and so on and on like you see
14:50 - intelligence has appeared 6 times
14:52 - intelligent sixth time intelligently has
14:55 - appeared one time
14:56 - now suppose if you want to know the
14:59 - frequency of any particular word here so
15:01 - for that we are going to use the
15:03 - function f test
15:05 - that is the distinct frequency so we are
15:07 - going to use that function over the
15:09 - particular word so for example let's say
15:11 - i want to know the frequency of
15:13 - artificial
15:14 - so as you can see it is three times
15:16 - and if we check it in the database you
15:19 - can see it is artificial it is given as
15:21 - three
15:22 - now if you want to have an look at the
15:24 - number of distinct words here
15:26 - all we need to do is pass on the f-test
15:29 - function to the length function so you
15:31 - can see it's 121. so earlier we had 273
15:35 - tokens and now from that we have 121
15:39 - distinct tokens
15:40 - now suppose if we were to select the top
15:42 - 10
15:43 - tokens with the highest frequency for
15:45 - that i am assigning a new f test
15:48 - underscore top 10 and i am using the
15:50 - most underscore common function here and
15:53 - passing 10 that is the number of items i
15:56 - want
15:57 - so
15:57 - let's see the output
15:59 - so as you can see
16:00 - as i mentioned earlier comma appears 30
16:02 - times that is the highest frequency of
16:04 - any world and
16:06 - is appeased five times so these are the
16:09 - top ten words which are the most
16:10 - reoccurring words in the given paragraph
16:13 - or the given sentence
16:15 - now let's use the blank line tokenizer
16:18 - over the same string to tokenize a
16:19 - paragraph with respect to a blank string
16:22 - so as you can see we are importing the
16:24 - blank line underscore tokenize
16:26 - earlier what we did was import the word
16:29 - underscore tokenize and now we are using
16:31 - the blank line under stroke noise and
16:34 - again we are passing the same ai
16:35 - paragraph which i gave earlier and then
16:37 - we are checking the length of the ai
16:39 - underscore blank so as you can see it
16:42 - provides us the output nine now what it
16:45 - tells us is the number of paragraphs
16:48 - which are separated by a new line in our
16:50 - given document
16:51 - so suppose you want to have a look at
16:53 - the first paragraph or supposedly the
16:57 - second paragraph
16:58 - all you do is pass on the number and it
17:00 - will give you the whole paragraph as you
17:03 - want which is separated by a new line of
17:05 - course
17:06 - now coming back to our tokenization part
17:09 - we have
17:10 - diagrams diagrams and engrams now
17:14 - diagrams are tokens of two consecutive
17:16 - written words similarly diagrams are
17:19 - referred to tokens of three consecutive
17:21 - written words and usually n-grams is
17:23 - referred to as tokens of any number of
17:24 - consecutive written words for n numbers
17:27 - so let's see how we can implement the
17:29 - same using
17:31 - nltk libraries for diagrams diagrams and
17:34 - the engrams
17:36 - so first what we need to do is import
17:38 - backgrounds diagrams and engrams from
17:41 - nltk.util
17:42 - so let's take a string the best and the
17:44 - most visual thing in the world cannot be
17:46 - seen or even test
17:48 - they must be filled with the heart what
17:50 - a beautiful code so let us now first
17:52 - create the tokens of the our string
17:54 - using the word underscore tokenize as we
17:56 - did earlier now to create a background
17:58 - what we need to do is use the list
18:01 - function and inside that we are going to
18:03 - use the nltk.diagrams
18:05 - and pass on the tokens
18:07 - so as you can see it has created a
18:10 - diagram of the given document similarly
18:13 - if we create the trigrams and the
18:15 - engrams
18:16 - so all you need to do is change the
18:18 - bigrams to trigrams and it will give you
18:20 - the trigram list now let us now create
18:23 - an n-gram list okay
18:26 - so guys as you can see here we are using
18:27 - the same function lltk.n grams and
18:30 - inside that we are passing the quotes
18:31 - under course token and
18:34 - here in place of n we are going to give
18:36 - our number so let's say suppose i
18:38 - provide five here so as you can see it
18:41 - has given us an n-gram of length five
18:44 - now once we have got all the words or as
18:46 - i say the tokens we need to make some
18:49 - changes to the tokens and for that we
18:51 - have stemming
18:53 - now what stemming does is normalize the
18:55 - words into its base root form so if we
18:58 - have a look at the words here we have
18:59 - affectation effects affections affected
19:02 - affection and affecting now all of these
19:05 - words originate from a single root world
19:07 - that is the effect now stemming
19:09 - algorithm works by cutting off the end
19:12 - or the beginning of the word taking into
19:14 - account a list of common prefixes and
19:17 - suffixes that can be found in an
19:18 - infected world now this indiscriminate
19:21 - cutting can be successful in some
19:23 - occasions but not always
19:26 - and that is why we affirm that this
19:28 - approach presents some limitations now
19:31 - let's see how we can implement stemming
19:33 - and we'll see what are the limitations
19:35 - of stemming and how we can overcome them
19:38 - now there are quite a few different
19:39 - types of stemmer so let's start with the
19:41 - portal stemmer so for that we are going
19:43 - to use from nltk.stem import portastimo
19:47 - and let's see what does this give us the
19:50 - output of stemming the word having so as
19:53 - you can see it has given us the root
19:54 - word have
19:55 - now similarly if we provide a list of
19:58 - words such as give giving given and gave
20:01 - and i have given this name of words to
20:04 - stem
20:05 - so forwards inverse system print words
20:07 - and we are using the portastimer which
20:10 - is the psd.stem method so as you can see
20:12 - it has given us the output give give
20:15 - given and gave so you can see the
20:17 - stemmer remove the only ing and replace
20:20 - it with e now there is another stemmer
20:22 - which is known as the lancaster stemmer
20:24 - so let's try to stem the same thing
20:26 - using the lancast system and see what is
20:28 - the difference
20:30 - so let's stem the same thing using the
20:32 - lancast system and see what are the
20:34 - differences we have
20:36 - so first of all we are going to import
20:38 - the landcast system and we are going to
20:40 - provide lst which is the lancast system
20:43 - of function and in the similar manner
20:45 - that we did for the portastema let us
20:46 - execute the lancast system also so as
20:49 - you can see here the stemmer has stemmed
20:51 - all the words as a result of it you can
20:53 - conclude
20:54 - that the lancaster stemmer is more
20:56 - aggressive than the potter stemmer now
20:58 - the use of each of the stemmers depend
21:00 - on the type of the task you want to
21:02 - perform
21:03 - now we have another stemmer which is
21:05 - known as the snow world stimmer now for
21:07 - snowball stimmer we have to provide the
21:09 - language which we are using
21:11 - so similarly if we use the snowball
21:13 - stemmer on the given words
21:15 - as you can see it has given us a
21:16 - different output from the previous
21:19 - lancaster stemmer so as you can see the
21:21 - output is the same as that of the portal
21:24 - stemmer here but it differs in different
21:27 - terms now the use of each of this
21:29 - stemmer depends on the type of the task
21:30 - you want to perform for example if you
21:32 - want to check how many times the word
21:34 - giv is used above you can use the
21:36 - lancaster's demo and for other purposes
21:39 - you have the snowballs number and the
21:40 - porter stem as well so what happens is
21:43 - that sometimes
21:45 - stemming does not work properly it does
21:48 - not always result to give us the root
21:50 - word so for example if we take the word
21:53 - fish and fishes and fishing all of that
21:56 - stems into fish so lemmetization is an
22:00 - another concept
22:01 - so on one hand where stemming usually
22:04 - works by cutting off the end and the
22:06 - beginning of the wood limitization takes
22:09 - into consideration the morphological
22:12 - analysis of the world so to do so it is
22:15 - necessary to have a detailed dictionary
22:17 - which the algorithm can look into to
22:19 - link the form back to its lemma
22:22 - now what limitization does is groups
22:24 - together different infected forms of the
22:26 - word called lemma it is somehow similar
22:29 - to stemming as it maps several words
22:31 - into one common root now one thing to
22:34 - keep in consideration here is the output
22:36 - of limitization is a proper word so for
22:38 - example if we look back to our output
22:40 - given by the lancaster's demo you can
22:43 - see it has given us the output give
22:44 - which is not a word so the output of
22:46 - limitization is a proper word
22:48 - and
22:49 - for example if you take a limitation of
22:52 - gone going it all goes into the word go
22:56 - now again we can see how it works with
22:58 - the same example of words let's try
23:00 - limitization using the nltk library
23:04 - so first of all we are going to import
23:05 - the word net
23:07 - as mentioned earlier it requires a
23:09 - dictionary which the algorithm can look
23:11 - into to link back the form into its
23:14 - original lemma and since the output is a
23:16 - perfect word so it needs to have
23:18 - additionally so for here we are going to
23:21 - import the wordnet dictionary and we are
23:23 - going to import the word netlemortizer
23:26 - so let us first
23:28 - take the word corpora and see what its
23:30 - lemma is
23:32 - so what do you think guys what will be
23:33 - the output
23:34 - as you can see it has given the right
23:36 - output which is the corpus
23:38 - so let's use limitation on the given
23:40 - words
23:41 - so as you can see here the lemmatizer
23:43 - has kept the words as it is this is
23:45 - because we haven't assigned any pos tags
23:48 - here and hence it has assumed all the
23:51 - words as none now we'll learn about pos
23:54 - later in this video but just to give you
23:56 - a hint of what pos is pos is basically
23:59 - parts of speech
24:01 - so as to define which word is a noun
24:04 - which is a pronoun
24:06 - and which is a subject and much more
24:09 - now do you know there are several words
24:11 - in the english language such as
24:14 - i at for begin gone no various
24:19 - which are thought of as useful in the
24:21 - formation of sentence and without it the
24:23 - sentences won't even make sense
24:26 - but these do not provide any help in nlp
24:30 - so these lists of words are known as
24:32 - stop words
24:33 - so you might be confused as if are they
24:36 - helpful or not
24:37 - it is helpful in the english language
24:39 - but it is not helpful in the language
24:41 - processing so nltk has its own list of
24:44 - stop word you can use the same by
24:46 - importing it from nltk.corpus
24:50 - so once we import the stopwatch we can
24:53 - just have a look at the different
24:54 - stoppers which is provided by nltk
24:56 - so we need to mention which language we
24:58 - are using so let's have a look at the
25:01 - number of stoppers we have in english
25:03 - format so just 179
25:07 - so remember guys when we use the fdist
25:09 - underscore top 10 to check the top 10
25:12 - tokens which have the highest number of
25:14 - frequency
25:15 - so let us take that again
25:17 - so as you can see here apart from the
25:19 - word intelligence and intelligent
25:22 - all of the words are usually stop words
25:25 - or basically i should say that they do
25:27 - not match any particular word they are
25:29 - like special digits and characters which
25:32 - do not add any value to the language
25:34 - processing
25:35 - and hence it can be removed
25:38 - so first of all we will use the compile
25:40 - from the re module to create a string
25:43 - that matches any digit or the special
25:45 - character now we'll create an empty list
25:48 - and append the words without any
25:49 - punctuation into the list
25:51 - so i'm naming this as post punctuation
25:54 - and if you have a look at the output of
25:56 - the post punctuation
25:58 - so as you can see
26:00 - it has removed all the various numbers
26:03 - and digits and the comma and the
26:05 - different elements
26:07 - now when i was talking about pos which
26:09 - is parts of speech
26:11 - now generally speaking the grammatical
26:13 - type of the word
26:15 - the verb the noun adjective adverb and
26:18 - article now it indicates how a word
26:21 - functions in meaning as well as
26:23 - grammatically within the sentence
26:26 - a word can have more than one parts of
26:28 - speech based on the context in which it
26:30 - is used for example if we take the
26:32 - sentence google something on the
26:34 - internet
26:35 - now here google is used as a verb
26:38 - although it is a proper noun now these
26:40 - are some sort of ambiguities and the
26:41 - difficulties which makes the natural
26:44 - language understanding even much more
26:46 - harder as compared to natural language
26:48 - generation because once you understand
26:50 - the language generation is pretty easy
26:52 - so pos tags are usually used to describe
26:57 - whether the word is a noun
26:59 - an adjective a pronoun
27:01 - a proper noun singular plural verb is it
27:05 - a symbol is it an adverb so as you can
27:08 - see here we have so many tags and
27:10 - descriptions
27:11 - for the different kinds of words
27:13 - all the way ranging from coordinating
27:15 - conjunction to what work wrp
27:19 - now let's take an example of these two
27:21 - sentences so the first sentence is the
27:23 - waiter clears the plates from the table
27:25 - so as you can see from the starting if
27:27 - we use the word take into consideration
27:29 - the it is a determiner
27:32 - now waiter is a noun cleared is a verb
27:35 - the is again determiner the plates are
27:37 - noun from is not defined here the is
27:40 - again the determiner and the table is
27:42 - again a noun
27:43 - now again if we take into consideration
27:45 - the sentence
27:46 - the dog ate the cat
27:48 - similarly this is the determiner dog
27:51 - noun eight is the verb and again it's
27:54 - the same determiner and the noun
27:57 - so let's see how we can implement pos
27:59 - tags and do the tagging using the nltk
28:02 - library so let us take a sentence
28:05 - timothy is a natural when it comes to
28:07 - drawing first of all we'll tokenize it
28:09 - using the word underscore tokenize
28:12 - and then we are going to use the pos
28:14 - underscore tag on all of these tokens
28:17 - so as you can see it has defined timothy
28:19 - as a noun is as a verb as a determiner
28:22 - natural as an adjective when as a wrp
28:26 - objective and it as a preposition it
28:29 - comes as a verb to and drawing as a verb
28:32 - as well
28:33 - now let's take another example that john
28:35 - is eating a delicious cake
28:38 - so as you can see here the tiger has
28:40 - tagged both the is
28:42 - and eating as well because it has
28:44 - considered is eating as a single term
28:46 - now this is one of the small
28:48 - shortcomings of the pos taggers when it
28:51 - comes to tying the words
28:54 - now another concept in natural language
28:56 - processing is ner which is known as
28:59 - named entity recognition
29:01 - so the process of detecting the named
29:04 - entities such as a movie a monetary
29:07 - value be it an organization a location
29:10 - quantities and a person from a text is
29:13 - known as named intelligent recognition
29:16 - now there are three phases of ner which
29:19 - is first is the noun phrase
29:20 - identification now this step deals with
29:23 - extracting all the noun phrases from a
29:25 - text using dependency passing and paths
29:28 - of speech tagging which is the pos tag
29:30 - now secondly we have the phrase
29:32 - classification now this is the
29:33 - classification step in which all the
29:36 - extracted nouns and phrase are
29:38 - classified into respective categories
29:40 - such as location names and much more
29:43 - now apart from this one can curate the
29:45 - lookup tables and dictionaries by
29:47 - combining information from the different
29:49 - sources and finally we have the entity
29:52 - disambiguation now sometimes it is
29:54 - possible that the entities are
29:56 - misclassified hence creating a
29:58 - validation layer on top of the result is
30:00 - very useful the use of knowledge graphs
30:03 - can be exploited for this purpose now
30:05 - the popular knowledge graphs are the
30:07 - google knowledge craft the ibm watson
30:09 - and also wikipedia
30:11 - so if we have a look at a certain
30:13 - example suppose this is the sentence the
30:15 - google ceo sundar pichai introduced the
30:18 - new pixel at minnesota roy center event
30:21 - so as you can see
30:23 - google is an organization sundar pichai
30:26 - is a person minnesota is a location
30:29 - henry center event is also an
30:30 - organization this is an additional layer
30:33 - on top of the pos tagging so as to
30:36 - clarify and give us more depth into what
30:38 - the sentence is about and what the
30:40 - sentence is conveying us so for using
30:43 - ner in python you need to import the any
30:46 - underscore chunk from the nltk module in
30:48 - python so once we have imported any
30:51 - underscore chunk
30:52 - now let us take this sentence into
30:54 - consideration which is the us president
30:56 - stays in the white house now again what
30:59 - we need to do is tokenize this sentence
31:02 - and after tokenizing what we need to do
31:04 - is add the pos tags
31:06 - so that it will be easier for us to
31:09 - conclude the ners
31:12 - so we are passing the any underscore
31:14 - tags into any underscore chunks function
31:17 - and let's see what's the output so as
31:19 - you can see it has given d as a
31:21 - determiner it has given us as an
31:24 - organization and white house is clubbed
31:26 - together as a single entity and is
31:29 - considered as a facility so as you can
31:32 - see adding a layer of dictionary and
31:34 - then adding the tags
31:35 - and then creating the name entity
31:37 - recognition makes the understanding of
31:40 - language so much more easier
31:43 - now as you can see in the nei entity
31:45 - list we have geosocial political group
31:48 - geopolitical entity we have facility
31:51 - location
31:52 - organization and person so as you saw
31:55 - just from the previous example
31:57 - as you can see we have given
31:59 - organization
32:00 - we have facility
32:02 - now an important thing to consider in
32:05 - nlp is also
32:06 - the syntax now any linguistics syntax is
32:10 - the set of rules principles and the
32:12 - processes that govern the structure of a
32:14 - sentence in a given language the term
32:16 - syntax is also used to refer to the
32:19 - study of such principles and processes
32:22 - so we have a certain rules
32:24 - as to what part of sentence should come
32:26 - up at what position now with these rules
32:29 - we create a syntax tree whenever there
32:31 - is a sentence as an input so syntax tree
32:35 - in layman terms is basically a tree
32:37 - representation of the syntactic
32:39 - structure of sentences or strings now it
32:42 - is a way of representing the syntax of a
32:44 - programming language as a hierarchical
32:45 - tree-like structure and this structure
32:48 - is used for generating symbol tables for
32:50 - compilers and later code generation the
32:53 - tree represents all the constructs in
32:55 - the language and their subsequent rules
32:57 - now consider the statement the cat sat
33:00 - on a mat
33:01 - so as you can see this is how a syntax
33:04 - trade looks like
33:05 - we have a sentence we have the noun
33:07 - phrase the preposition phrase and again
33:09 - the noun phrase is divided into article
33:11 - and noun then we have the verb which is
33:13 - sat again we have the preposition which
33:15 - is on and again finally we have the noun
33:18 - phrase which consists of article and the
33:20 - noun now in order to render syntax trees
33:23 - in your notebook you need to install
33:25 - ghost strips which is a rendering engine
33:30 - so you can download ghost strip from
33:32 - this downloads page
33:34 - i'll not go much into the details of
33:36 - that
33:37 - now let us discuss
33:39 - an important concept with respect to
33:42 - analyzing the sentence structure which
33:45 - is chunking now chunking basically means
33:48 - picking up individual pieces of
33:49 - information and grouping them into
33:51 - bigger pieces
33:53 - so as we saw earlier that we have a
33:55 - sentence and we divided it into
33:57 - different tokens that was tokenization
33:59 - and this is sort of like the opposite
34:01 - part or we should say the opposite of
34:04 - what we call known as tokenization a
34:06 - little bit of changes to that
34:08 - tokenization part we'll see what are the
34:10 - different changes now the bigger pieces
34:13 - are usually known as chunks here now in
34:15 - the context of nlp chunking means
34:18 - grouping of words or the tokens into
34:21 - chunks now let's have a look at the
34:23 - example of chunking here
34:25 - so let's consider the statement we got
34:28 - the pink panther
34:29 - so as you can see here the pink which is
34:32 - an adjective panther which is a noun and
34:35 - though which is the determiner are
34:36 - chunked together into a noun phrase
34:39 - now this also helps in determining and
34:42 - the processing of the language
34:45 - so suppose if someone is asking whom did
34:47 - we caught this morning so the regular
34:50 - response to this question would be we
34:52 - caught the pink panther now the question
34:54 - is asking who so the pink panther
34:56 - becomes a noun phrase basically so this
34:58 - is something what the chunking does is
35:01 - that it understands the language and
35:03 - when it has understood what it does is
35:06 - basically picks up the individual pieces
35:08 - of information and groups them into
35:10 - chunks
35:11 - so that it will be easier for us to
35:13 - process that data
35:15 - so let's take a new sentence the big cat
35:18 - ate little mouse who was
35:20 - after the fresh cheese
35:22 - so let's tokenize it and add the post
35:25 - tags also so as you can see just under
35:28 - one line or i should say and one command
35:30 - i have tokenized it and i have added the
35:32 - pos tags also
35:34 - so let's define the grammar here
35:37 - so next what we are going to do is
35:39 - create a regular expression parser for
35:42 - the grammar np which is the grammar
35:43 - which we have provided
35:45 - and then we'll create the chunk
35:46 - underscore result and we'll pass this
35:49 - whole sentence or i should say the
35:51 - tokens into the chunks
35:53 - there was a slider as nltk was unable to
35:56 - file the js file but as you can see here
35:58 - we have a tree in which the np is
36:01 - the big cat
36:02 - the big cat is a noun phrase it is a
36:05 - verb tree then again we have the noun
36:08 - phrase which is the little mouse who is
36:11 - a word phrase was is a verb after is in
36:15 - and again we have the noun phrase the
36:16 - fresh cheese so you can see here
36:19 - although we do not have a particular
36:21 - tree like structure we can see that from
36:24 - the outer we can see we have a tree
36:25 - starting from the start is here and the
36:28 - end is here and you can see it starts
36:30 - with the noun phrase the big cat now the
36:33 - big cat as i mentioned earlier in our
36:36 - previous example similarly to the pink
36:39 - panther it has been taken into
36:41 - consideration as a noun phrase which is
36:43 - basically a chunk it has created
36:45 - different chunks in the given sentence i
36:48 - should say
36:49 - so this is it guys i hope you got an
36:52 - idea of what the different parts of nlp
36:54 - is what is nlp how it is used
36:57 - the different elements of nlp such as
37:00 - starting from tokenization stop words
37:03 - stemming limitization again we have the
37:06 - stopwatch part of speech pos tags ner
37:10 - named entity recognition and after ner
37:13 - we had the syntax
37:15 - we created a syntax tree to know how
37:19 - exactly the sentence is arranged and how
37:22 - it uses the dictionary the pos tags and
37:24 - the tokenization to create a particular
37:27 - tree so as to form a meaning out of that
37:29 - sentence
37:30 - and finally we are using chunking as to
37:33 - chunk together the small tokens into
37:35 - meaningful words so guys i hope you
37:37 - liked this session and i'm sure you guys
37:39 - will start implementing these procedures
37:42 - on the given text using the nltk library
37:45 - and let me tell you one thing guys the
37:46 - nltk library is filled with lots and
37:49 - lots of text data and lots and lots of
37:52 - example this is just the beginning of it
37:55 - and you can dig deeper into nlp and go
37:58 - into topics like context free grammar
38:01 - and much more which is already there in
38:03 - the nltk package and is very useful when
38:06 - it comes to natural language processing
38:08 - thank you