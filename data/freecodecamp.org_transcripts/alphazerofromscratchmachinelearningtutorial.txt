00:00 - Alpha zero is a game playing algorithm
00:02 - that uses artificial intelligence and
00:04 - machine learning techniques to learn to
00:06 - play board games at superhuman levels in
00:10 - this machine learning course from Robert
00:11 - Forster you will learn how to create
00:13 - Alpha Zero from scratch
00:16 - thank you for tuning in in this video we
00:19 - are going to rebuild Alpha 0 completely
00:21 - from scratch using Python and The Deep
00:24 - learning framework pytorch Alpha 0 was
00:27 - initially developed by deepminds and it
00:29 - is able to achieve magnificent
00:31 - performance in extremely complex board
00:33 - games such as go where the amount of
00:35 - liquid bot positions is actually
00:37 - significantly higher than the amount of
00:39 - atoms than our universe so not only is
00:43 - this very impressive just from an AI
00:45 - standpoint on but additionally to that
00:47 - the machine Learning System of alpha
00:50 - zero learns all of this information just
00:52 - by playing with itself and moreover the
00:55 - various domains in which this algorithm
00:57 - can be used are almost Limitless so not
01:00 - only can this algorithm also play chess
01:02 - and shogi in a very impressive way but
01:05 - the recent Alpha tensor paper that was
01:07 - also published by deepmind showed that
01:09 - Alpha 0 can even be used to invent novel
01:12 - algorithms inside of mathematics
01:15 - so together we will first of all
01:17 - understand how this algorithm works and
01:19 - then we will build it from ground up and
01:21 - train and evaluated on the games of
01:24 - Tic-tac-toe and Connect 4 so that you
01:27 - will also understand how flexible Alpha
01:29 - 0 really is when it comes to adapting it
01:32 - to various domains so let's get started
01:34 - okay great so let's start with the brief
01:37 - overview of the alpha zero algorithm so
01:40 - first of all it is important that we
01:42 - have two separate components and on one
01:45 - hand we have the save play part right
01:48 - here and yeah during this phase our
01:50 - Alpha 0 model basically plays with
01:53 - itself in order to gather some
01:55 - information about the game and while we
01:58 - play with ourselves we also generate
02:00 - data and this data will then be used in
02:04 - The Next Step during training so in this
02:06 - training phase right here which is say
02:09 - the second component we like to optimize
02:12 - our model based on the information it
02:14 - gained while playing with itself
02:16 - and then next we basically want to
02:19 - fulfill the cycle and then use this
02:22 - optimized model to play with itself
02:25 - again
02:26 - so if the idea is essentially to repeat
02:29 - the cycle n number of times until we
02:33 - have yeah reached an array Network so a
02:36 - model right here that is capable of
02:38 - playing a certain game better than any
02:41 - human could because it just played with
02:44 - itself use the information again to
02:47 - optimize yeah itself and then play with
02:50 - itself again basically and then we just
02:54 - like to repeat that
02:55 - so in between these two components we
02:58 - have this Alpha 0 model right here which
03:00 - is just in Array Network
03:02 - and now let's look at this scenery
03:05 - Network more closely so let's see how
03:07 - this actually is architectured so
03:12 - basically we take the state SCS input
03:15 - and for the case of tic-tac-toe for
03:18 - example the state would just be bot
03:20 - position
03:22 - um so just turn two numbers
03:24 - and then we once we have the state yes
03:27 - input we receive a policy and the value
03:30 - as output so the policy here is this
03:34 - distribution and for each action
03:37 - basically it will tell us how promising
03:40 - this action would be based on this state
03:42 - we have received here as input so we'll
03:45 - basically tell us where to play based on
03:48 - the state right here
03:50 - so this is the policy and then the value
03:54 - is just a float and it will basically be
03:58 - a measure telling us how promising the
04:01 - state itself is for us as a player to be
04:04 - in
04:05 - so maybe a concrete example that make
04:08 - might make all of this easier to
04:10 - understand is something we have down
04:12 - below here
04:13 - so
04:15 - here for the state we just take in this
04:17 - position and remember that we are player
04:19 - X in this case
04:21 - and yeah if you're familiar with the
04:23 - roots of tic-tac-toe you should see that
04:25 - we should play down here because
04:28 - um yeah then we would have three x's on
04:30 - the lowest Road and yeah we would win
04:33 - the game so if our model also
04:36 - understands how to play this game then
04:38 - we would like to receive a policy
04:40 - looking some word like this where
04:43 - basically the highest bar right here
04:46 - is also this position uh down below that
04:51 - would basically help us to win the game
04:54 - so yeah this is how the policies should
04:56 - look like and for the value we can see
04:58 - that this state is quite nice for us as
05:01 - a player because we can win here so we
05:03 - would like to have a very high value so
05:08 - yeah in this case the value is just a
05:10 - float and the range of negative 1 and
05:12 - positive one so we would like to have a
05:15 - value that is close to positive one
05:16 - because this state is quite optimal
05:19 - okay now we uh yeah looked at how the
05:23 - general architecture is built and how
05:25 - the model is built but next we want to
05:29 - understand the self play and the
05:31 - training part more in depth so before we
05:34 - do that you should also understand what
05:37 - a Monte Carlo tree search is because
05:39 - that is vital for self-play and the
05:41 - general algorithm
05:44 - so the Monte Carlo research is this
05:46 - search algorithm right here and it will
05:49 - take a state on our case a block
05:51 - position as input
05:53 - and then we find the action that looked
05:55 - most promising to us
05:58 - and this algorithm gets this information
06:00 - by just as for declaring a root node at
06:03 - the initial state right here
06:05 - and then by building up this tree into
06:08 - the future
06:09 - so just like we can see right here
06:12 - so basically the idea is that we create
06:16 - these nodes into the future
06:19 - right here and we get these notes by
06:22 - just taking actions that lead us to the
06:26 - Future
06:27 - so this note right here for example is
06:30 - just the place we arrive at when we take
06:34 - this action one
06:36 - based on this initial State at our root
06:39 - note
06:40 - so each node first of all the state like
06:45 - S1 in this case
06:46 - and so S3 right here would just be the
06:50 - state we get when we first follow play
06:53 - action one based on this initial State
06:54 - and then when we would play action 3
06:57 - based on this state right here
06:59 - so having these actions after each other
07:03 - basically
07:05 - and then additionally to the state each
07:08 - node also stores this variable W and W
07:13 - is just the total number of wins
07:16 - that we achieved when we played in this
07:18 - direction into the future
07:21 - so basically when we took action one
07:23 - initially and arrived at this node we
07:27 - were able to win three times when we
07:29 - walked into the future even further
07:32 - so
07:34 - next to this variable we also have this
07:37 - n variable right here and N is just the
07:40 - total visit count so basically the total
07:43 - number of times that we went into this
07:46 - direction so here we can say that we
07:49 - took action one four times
07:52 - and out of these four times we were able
07:55 - to win three times right
07:57 - so after we have basically built up a
08:02 - tree like this and we can then find the
08:05 - action that looks most promising to us
08:08 - and the way we do this is by looking at
08:11 - the children of our root node so these
08:14 - two notes right here
08:16 - and then for each node we can calculate
08:19 - the winning ratio so here we can say
08:21 - that this node one three out of four
08:23 - times while this node one one out of two
08:26 - times and then we can say that you know
08:28 - what this node right here has a higher
08:30 - winning ratio and thus it looks more
08:32 - promising to her to us
08:34 - and because of that we can also say that
08:36 - action one looks more promising than
08:38 - action 2 because this action was the one
08:42 - that even led us to this note here in
08:44 - the first place so basically the result
08:46 - of this tree right here that we got at
08:48 - this time would be that action one
08:51 - should be the action we we would like to
08:54 - play
08:55 - and then we could just use this
08:56 - information to uh say you know what in
09:00 - our actual game of tic-tac-toe for
09:01 - example we now want to use this action
09:05 - right here
09:06 - so this is great but now the question
09:09 - becomes how can we even build up a tree
09:12 - like this and also how do we get the
09:14 - information whether we win at this given
09:18 - position or not right
09:20 - so the way we come up with all of uh
09:23 - this data right here is basically by
09:25 - walking down all of these four steps for
09:30 - a give number of iterations
09:33 - so at the beginning we like we are in
09:36 - our selection phase
09:37 - and uh during selection we basically
09:41 - walk down our tree
09:43 - but we walk down our tree until we have
09:46 - reached the so-called Leaf node so a
09:49 - leaf node is a node that could be
09:50 - expanded even further
09:53 - um into the future so one example of a
09:56 - leaf node would be this note right here
09:58 - because you can see that we have only
10:00 - one child as this note but actually we
10:03 - could also expand in this direction if
10:05 - we say that for each node there are
10:07 - always two possible ways in which you
10:10 - could expand in this example right here
10:12 - so this is a leaf node and then also
10:15 - this would be a default right here
10:16 - because this node doesn't have any child
10:19 - right and then also we can say that this
10:23 - node also is the leaf node because it
10:25 - only has one side right here but we
10:27 - could still expand it in this direction
10:28 - so yeah these are Leaf nodes for example
10:31 - where these aren't any lymph nodes
10:33 - because yeah this one already has two
10:36 - children and this one also has two
10:37 - children already
10:39 - so yeah this is basically how selection
10:42 - looks like
10:43 - but now the question also becomes
10:46 - um when we walk down which direction we
10:49 - should pick right so when we just start
10:52 - at the top we now have to decide whether
10:54 - we want to walk down in this direction
10:56 - or whether we want to walk down in this
10:59 - direction right here basically the way
11:01 - of finding out the direction we choose
11:04 - when working down
11:05 - is by taking the direction or the child
11:09 - right here that has the highest UCB
11:12 - formula like this
11:13 - so while this might look a bit complex
11:16 - in reality we just take the child but
11:22 - um tends to win uh more often while also
11:26 - trying to take a child that has been
11:29 - visited at just a few number of times
11:32 - relatively right
11:34 - so on one hand we when we maximize this
11:37 - formula we want to take the check with
11:39 - the highest winning ratio
11:41 - and also
11:43 - um want to take the child that has
11:46 - relatively few visits compared to this
11:49 - large n which is just the total number
11:52 - of visits for the parent
11:54 - so if we would calculate the UCB formula
11:57 - for B both of these nodes right here we
12:00 - can see that the UCD formula here is
12:03 - higher because this node has been
12:06 - visited uh less often right and because
12:09 - of that we would like to work in this
12:11 - direction right here okay so during
12:13 - selection we can now say that we walk
12:16 - down in this direction right here
12:18 - and now we have arrived at this node so
12:20 - first of all we have to check if we
12:22 - worked out walk down further or if we
12:25 - have in fact reached the leaf node and
12:27 - now we can say let's see that we have
12:29 - which Leaf node right here right because
12:31 - we could still expand in this direction
12:33 - so we could still have a child right
12:35 - here and because of that we actually
12:38 - skip to the next phase of the MCTS and
12:42 - that is the expansion phase
12:44 - so right here in expansion we want to
12:47 - create a new node and dependent to our
12:49 - tree
12:50 - so I will just create this node right
12:53 - here and now we call it give it the
12:56 - state as 7. then we can say that we took
12:59 - the action A7
13:01 - to get to this UD created node right
13:04 - here
13:06 - and at the beginning this node should
13:08 - have a winning count of zero and also a
13:12 - visit count of zero so I'm just going to
13:14 - declare W7 right here and also in seven
13:18 - like this
13:22 - and both of them should just be equal
13:26 - to zero at the beginning right because
13:28 - we have just created this node and added
13:32 - it to our tree but we have never been
13:34 - there chilly
13:36 - so now we can say that we further worked
13:39 - down here to the node we have just
13:42 - created
13:43 - and so now we have also finished this
13:47 - expansion phase right here
13:49 - and next we have arrived at this
13:52 - simulation phase
13:54 - and here the idea is basically to play
13:56 - randomly into the future until we have
13:59 - reached a state uh where the game is
14:02 - over right so we want to play into the
14:04 - future until the game is finished
14:06 - and
14:08 - basically here we can just say we play
14:11 - using random actions so we just play
14:14 - randomly and finally we have just
14:17 - reached this node right here and yeah
14:20 - here the game is over
14:23 - so this is the simulation phase and now
14:27 - when we have reached this terminal a
14:30 - node basically we then have to check
14:33 - whether we have won the game or we have
14:36 - lost or Draw has occurred so in this
14:39 - concrete case we actually have one
14:45 - and with that we can also finish the
14:48 - simulation phase
14:50 - so just finally now we have arrived at
14:54 - the back propagation phase
14:56 - and here we like to get this information
14:59 - from our simulation and then we want to
15:02 - give this information all the way up to
15:05 - our parents until we have reached the
15:07 - root note
15:09 - so here because we have won the game
15:11 - actually and during back propagation we
15:14 - can
15:16 - um first of all higher the number of
15:18 - wins for this node right here so we can
15:21 - say that we have one one time actually
15:23 - then we can also hire the total number
15:27 - of visits so here we have visited this
15:30 - node one time and we have one in this
15:33 - case
15:34 - and because we want to back propagate
15:36 - all the way up we can now also say that
15:38 - here for this note and the number of
15:41 - wins now is two
15:43 - so the number of points we got when we
15:45 - walk in this direction okay
15:47 - and then also the total number of visits
15:51 - should now be three
15:53 - because we have yeah visited it when we
15:55 - select it inside in this direction
16:00 - and now we can also back propagate uh
16:03 - all the way to our root mode
16:05 - and here we can just say that the
16:08 - general visit count is seven and the
16:10 - total number of wins would be five right
16:14 - okay
16:16 - so now this would be a whole iteration
16:19 - of this MCTS process
16:21 - and um yeah then if we would again
16:25 - um choose an action and we would just
16:28 - check the children of our route again
16:31 - and again could just choose the action
16:34 - based on the winning
16:36 - amount of wins right here
16:39 - so this is great uh let's also just have
16:43 - an example where we start from at the
16:45 - beginning so that you yeah might find it
16:48 - easier to understand the search
16:49 - algorithm as well
16:51 - so again we want to perform these steps
16:54 - right here so at the beginning we are
16:57 - just in our selection phase so we want
16:59 - to walk down until we have reached the
17:01 - leaf node but here you can see that this
17:04 - root node already is a leaf node right
17:06 - so basically we can now move on to
17:10 - expansion and create a new node right
17:12 - here
17:13 - so let's say that we just create
17:16 - this node right here
17:19 - by taking action one
17:22 - and then we will arrive at State one
17:26 - here's when
17:27 - then we can say that at this node that
17:31 - we have just created the total number of
17:33 - wins
17:35 - right now should just be zero
17:37 - and also the visit count should just be
17:40 - zero
17:41 - and so we have
17:43 - W1 and N1 just created right here
17:47 - and yeah both of them can just equal a
17:51 - zero like this
17:57 - so now we have also finished the
18:00 - expansion part for the first iteration
18:02 - and next comes the simulation part right
18:05 - so we have worked in this way basically
18:10 - right here
18:11 - and now we want to simulate into the
18:13 - future just by taking these random
18:16 - actions so let's write this right here
18:19 - and
18:20 - now we have reached a terminal node
18:23 - and now we have to check whether we have
18:25 - won or lost or a rule is occurred again
18:28 - so in this case we can just say that we
18:31 - have one again
18:32 - so because of that we actually can act
18:36 - propagate now
18:38 - and in back propagation we can say that
18:40 - the number of wins here is one
18:43 - with either this account being one as
18:45 - well
18:46 - and then we can do the same for our root
18:49 - node right here so we can just say that
18:51 - the number of wins for our root node is
18:53 - also one
18:55 - and the this account of our root node is
18:57 - also works well
18:59 - so now that we have finished with this
19:02 - iteration I'm just going to clean
19:05 - everything up here
19:06 - foreign
19:08 - and we can move on to the next iteration
19:11 - right
19:12 - so again we started selection and we
19:14 - walk down until we have reached the
19:16 - refund
19:17 - but again we can see that our root node
19:19 - actually still is the leaf node so let's
19:21 - just expand this direction now
19:26 - and here we will just give us action 2
19:30 - and we will arrive at state 2.
19:33 - and here again we have to set the total
19:37 - number of wins to zero
19:38 - so
19:40 - W2 should be equal to zero sorry this
19:44 - also S2
19:47 - and then the visit count so N2 should
19:51 - all also equal zero right here
19:54 - so let's write N2 equals zero as well
19:58 - so let's do it like this
20:03 - and yeah now basically we walk in this
20:07 - direction right here and next we can do
20:09 - simulation again
20:10 - so let's simulate all the way down until
20:13 - we have reached the terminal node
20:16 - and in this case we can say that we have
20:19 - actually lost the game
20:20 - so when I propagate all the way up I
20:24 - will only increase the visit count right
20:27 - here
20:28 - so the physical should now be one but
20:31 - the total number of wins should still
20:33 - stay the same being zero because we have
20:36 - lost here when we simulate it
20:39 - so because of that when we back
20:41 - propagate to our root mode we should
20:43 - also only increase the visit count while
20:47 - leaving the total number of wins the way
20:51 - it was earlier
20:52 - so let's also finish up with this
20:56 - iteration right here
20:59 - and
21:01 - now we can move on to the next iteration
21:05 - so first of all we want to do selection
21:07 - again so we want to walk down until we
21:10 - have reached the leaf node and now our
21:13 - root node is fully expanded so we have
21:15 - to calculate the UCB score for each of
21:18 - our children right here and then we have
21:21 - to select the child that has the highest
21:23 - qcb score right
21:25 - so when we calculate the UCB score for
21:28 - both of them we can see that the UCB
21:30 - score here is higher because we have won
21:32 - the game in this case so we just walk
21:35 - down this direction right here
21:38 - and now uh basically we can move on to
21:42 - expansion so let's expand in this
21:45 - direction here
21:47 - so let's create this new node right here
21:49 - and appended to our tree and we took
21:52 - action three actually when we expanded
21:55 - in this direction and then the state
21:59 - here should also be S3 because of that
22:02 - and now again we can declare this total
22:06 - number of points W so W3 and the visit
22:10 - count n three
22:14 - and both of them should just equal zero
22:17 - at the beginning so let's
22:19 - set them to zero
22:22 - and now again we can
22:25 - perform this
22:28 - basic can perform some new simulation
22:31 - again
22:32 - so we walk down in this direction right
22:34 - here
22:37 - and yeah
22:39 - so now let's do simulation
22:42 - and when we do that we just arrived at
22:47 - this terminal node again
22:49 - and again we can now see that in this
22:52 - case we have just lost the game after
22:55 - just playing randomly
22:57 - and because of that when we back
22:59 - propagate we only want to increase the
23:01 - visit count right here
23:03 - we're not changing the total number of
23:06 - wins
23:07 - so let's set the this account to two as
23:10 - well
23:15 - and
23:17 - then we can set
23:20 - the visit count to three
23:24 - okay
23:25 - so this is this situation right here
23:34 - great
23:36 - um so now we can just
23:39 - also clean all of this up right here
23:42 - and then
23:44 - we can move on
23:46 - so next uh when we want to keep doing
23:49 - this process right here we again have to
23:51 - do selection right so now we have to
23:54 - again calculate the UCB formula for both
23:56 - of these values right here
23:58 - and here we can see that
24:03 - the UCB formula actually gives us a
24:06 - higher results for this node just
24:09 - because of the visit count that is lower
24:12 - right here
24:13 - so when we
24:16 - now walk in this direction
24:19 - we again have to check if we have
24:21 - reached the leaf node or not so here we
24:23 - have just reached the leaf node
24:25 - so because of that we move to expansion
24:28 - and create this new node right here and
24:32 - this should just be equal to S4 then so
24:35 - we can say that we took action 4 right
24:38 - here
24:41 - and then again the W and the this
24:44 - account
24:46 - um
24:46 - should be set to zero
24:49 - I'm sorry 100 color
24:52 - so
24:56 - W4
24:58 - and and for you
25:01 - being our number of wins and our visit
25:04 - count
25:06 - and let's set them to zero meters
25:11 - okay so now we also walk in this
25:13 - direction right here
25:15 - and again after
25:18 - um expansion we can now move to
25:20 - simulation so when we move here
25:24 - um begins terminal and in this case we
25:27 - can just say that a draw has occurred
25:29 - and in what in case of a draw we just
25:32 - would like to add 0.5 to the total
25:37 - number of wins because no players one
25:40 - here or no one has one so um
25:44 - because of that and we
25:46 - pack propagate here first of all we can
25:49 - increase the visit count
25:52 - and then let's set the
25:56 - number of wins to 0.5
26:00 - and then up here we can set the wizard
26:03 - counter to
26:05 - and again we told the number of wins
26:08 - should be 0.5
26:13 - and when we back propagate all the way
26:15 - up to our root nodes then we have
26:18 - the total number of visits as being
26:21 - formed and our
26:26 - um so the number of wins should be 1.5
26:29 - so this is just a an example of how the
26:32 - multicabit research might actually look
26:34 - when you just start at the beginning and
26:37 - actually you can repeat
26:39 - the cycle for just a set number of
26:42 - iterations and you set this number
26:45 - manually at the beginning
26:47 - and alternatively you could also set the
26:50 - time and during this time you would just
26:52 - perform as many iterations as you can
26:55 - but yeah in this case for example you
26:57 - could just stop it for iterations but in
26:59 - practice you might run for a thousands
27:01 - of iterations
27:03 - okay so now we can also look at the way
27:05 - how our Monte colored research changes
27:07 - when we adapt it to this General Alpha
27:10 - zero algorithm
27:12 - so there are two key changes that have
27:14 - been made here so the first thing is
27:17 - that we also want to incorporate the
27:19 - policy that was gained from our model
27:22 - into the search process right and
27:26 - especially we want to add it to the
27:28 - selection phase right here and we can do
27:30 - this by incorporating it into this
27:33 - updated UCB formula so this way you see
27:36 - this P of I heart rate here so basically
27:40 - when we select the child and we want to
27:43 - take the
27:45 - yeah children with the highest OCD
27:47 - formula then we will also look at the
27:49 - policy that was assigned to it from its
27:52 - parent perspective so remember that the
27:55 - policy is just this distribution of
27:57 - likelihoods and basically for each
28:00 - child when we expanded we would also
28:02 - store this policy likelihood at the
28:06 - given position
28:07 - um here for the node as well
28:10 - and because of that we then also tend to
28:14 - select children more often that were
28:16 - assigned a high policy by its parent
28:18 - right so this way our model can guide us
28:21 - through the selection phase inside of
28:24 - our Monte Carlo research so yeah this is
28:26 - the first key change here so just
28:29 - generally this updated UCB formula right
28:31 - here
28:32 - and then for the next change we also
28:35 - want to use the information of the value
28:37 - that we got from our neural network and
28:40 - we can use this information by first of
28:43 - all completely getting rid of this
28:45 - simulation phase yeah right here so we
28:49 - don't want to do this random rollouts
28:52 - into the future anymore until we have
28:54 - reached the terminal state but rather we
28:57 - will just use the value that we got from
28:59 - our neural network when it basically
29:02 - evaluated a certain State and this value
29:05 - will then be used for back propagation
29:07 - right so this way we use both the policy
29:12 - for our selection and the value for our
29:15 - back propagation of our neural network
29:17 - because of that we know that our Monte
29:20 - Carlo research will approve drastically
29:22 - when we also have a model that
29:24 - understands how to play the game so this
29:27 - way at the end uh we then have a better
29:31 - search with a better model that can even
29:33 - create a much better model right so we
29:36 - can this uh yeah keep the cycle up as
29:38 - well okay so
29:40 - there's also just a small change uh here
29:43 - as well uh just a minor one and remember
29:47 - that when we get policy from our neural
29:51 - network we immediately get this
29:53 - distribution right with the
29:54 - probabilities for each potential child
29:58 - and because of that it's much more
30:00 - convenient now to expand an oil possible
30:04 - directions during the expansion phase so
30:07 - we won't only create one new node but
30:09 - rather all of the possible nodes that we
30:12 - can yeah expand on right
30:15 - so this is just a minor change and so in
30:19 - order to make this easier to understand
30:21 - we can just do some iterations here on
30:24 - the Whiteboard together so yeah this way
30:27 - it might be easier to fully understand
30:29 - how this multicolored research is
30:31 - adapted so first you might also see here
30:35 - that we also store this policy
30:37 - information here inside of our node
30:40 - and yeah this would just be the
30:42 - probability that was assigned to it from
30:44 - its parent right and for the root node
30:46 - we just have no policy probability
30:49 - because we also have no parent for other
30:52 - nodes we will also store this policy
30:54 - here instead of our nodes
30:56 - okay so let's start with an iteration
30:58 - here so first of all we want to walk
31:00 - down until we have which relief mode and
31:03 - here we can already see that our root
31:05 - node is in fact additional because we
31:08 - can say expanded in these two directions
31:10 - here so because of that we will then
31:13 - move to the next step right here which
31:15 - is the expansion phase and here we will
31:17 - actually create our new nodes
31:20 - so now when we expand we also want to
31:24 - get a policy and value from our neural
31:26 - network so because of that I will first
31:28 - of all write p and then value
31:32 - equals
31:35 - just calling our neural network it's the
31:38 - state of our root nodes
31:41 - and let's say this policy right here
31:43 - will just be
31:45 - 0.6 for the first child
31:49 - and then maybe 0.4 for the second child
31:54 - and let's say that this value so the
31:57 - estimation of how good this state of our
32:00 - root models is just 0.4 here
32:04 - okay so now when we expand we will
32:07 - create this these two
32:09 - new nodes right here
32:12 - so
32:14 - that these two and here we have action
32:17 - one right
32:18 - and here we have action 2.
32:22 - so because of that we also have state
32:24 - one here and we have state two right
32:27 - here so now we can also add the number
32:31 - of wins the visit count and the policy
32:34 - information to our notes right here so
32:36 - first of all I will create this W1 and
32:39 - set the equal to zero then I will create
32:43 - this n one and set that also equal to
32:45 - zero and now we have this policy
32:48 - probability one and this should be equal
32:50 - to the policy for action one and here
32:54 - the purpose fraction one is just 0.6 so
32:57 - yeah the policy here should be
32:59 - 0.6 because of that
33:02 - so now we can also add the information
33:04 - here to the other side so let's first of
33:07 - all set the number of wins or
33:10 - the value later on here zero this way
33:13 - for W2 and let's set n to
33:18 - 0 as well and the policy probability
33:22 - for the this child right here should now
33:25 - be equal to 0.4 right
33:28 - um
33:28 - like this
33:30 - okay so now we have expanded here
33:34 - and we skip this step here so now we
33:37 - move to the back propagation step
33:40 - and here we just want to back propagate
33:42 - this value right here
33:44 - so we will just do this by
33:49 - um
33:50 - yeah just adding it right here so
33:53 - basically in this case it doesn't matter
33:55 - much for our root notes but maybe it's a
33:58 - nice information so we can just add 0.4
34:02 - here and then because of that also
34:04 - change we visit come to one point
34:08 - okay so this was one iteration and now
34:11 - we can move on to the next iteration so
34:13 - I will just clean this up
34:15 - okay so now again we want to walk down
34:18 - until we reach a leaf node so now we
34:22 - have to decide between those two
34:23 - children right
34:25 - and for each we have to calculate the
34:27 - UCB formula and then also we were just
34:32 - walk down in the direction where the UCB
34:34 - formulas the highest
34:36 - right so when we calculate this UCB
34:39 - formula we will see that no longer can
34:42 - we
34:43 - basically get the winning yeah um
34:46 - winning probability here because we have
34:49 - a visit count that is equal to zero So
34:51 - currently the way this is implemented we
34:53 - get we would get a yeah division by zero
34:56 - error so here we have to make sure that
34:59 - we will only check for the winning
35:00 - probability if we have this account that
35:03 - is larger than zero so because of that
35:07 - oh it just set that in Brackets right
35:09 - here so we will only check for this if
35:12 - we have a this account that is larger
35:14 - than zero under other cases we will just
35:16 - mask it out so we will just check for
35:19 - this part right here okay so now we can
35:23 - I find out the child where this value is
35:26 - the highest and we can see that we would
35:29 - just pick this child right here because
35:31 - it has the higher policy probability and
35:33 - because of that it's UCB formula will
35:36 - also be higher so because of that we
35:39 - would just walk in this direction right
35:40 - here
35:41 - so now we have reached a leaf node here
35:43 - and we would like to expand this node
35:46 - here
35:47 - so let's expand in these two directions
35:50 - so first of all this should be action
35:52 - three
35:54 - and this should be action 4 right here
35:57 - so when we create this new States we
36:01 - have state three
36:04 - and we have state 4 like this
36:09 - okay so now
36:11 - again we also have to calculate the new
36:15 - policy and the new value so we will get
36:17 - this
36:18 - policy and this value
36:23 - by calling our neural network f
36:26 - with this state here so S1
36:30 - because yes this is the position we have
36:32 - walked down to and from here we want to
36:35 - find out what the policy would be for
36:37 - these children and also what the value
36:40 - would be for this note directly so let's
36:43 - say that the policy here is just
36:46 - zero point
36:48 - five and 0.5 okay
36:53 - and let's say that this value right here
36:55 - is just 0.1
36:58 - so now
37:00 - um first of all we can also add this
37:04 - information here to our nodes again so
37:06 - let's first of all set W 4 equal to 0
37:10 - and we can say n for equal to zero
37:14 - and then policy probability 4 should be
37:17 - equal to 0.5
37:23 - and then here we also have here W3 which
37:27 - should just be zero
37:30 - and we have N3 which should also be zero
37:34 - and then we have also our policy
37:36 - probability three here and this should
37:38 - be equal to 0.5 and we can read that
37:41 - here
37:42 - Okay so
37:44 - yeah now we have expanded this to
37:47 - Children right here
37:49 - and I created them and sort the policy
37:51 - information here and serve them so now
37:54 - we want to back propagate again so here
37:57 - we will just use this value of 0.1 so
38:00 - actually we will add 0.1 here
38:03 - and we will raise this visit count by
38:06 - one
38:08 - so let's change that one right here and
38:11 - remember that we want to back propagate
38:13 - all the way up to our root note so here
38:16 - we will change the value so w 0 here to
38:21 - 0.5 so also add 0.1 to it and then we
38:24 - will just erase this number of physical
38:27 - by one as well so it will be two
38:30 - afterwards
38:31 - so yeah this is just this updated tree
38:34 - after these two iterations but yeah
38:37 - again if we would predict this we would
38:39 - just first of all walk down until we
38:41 - have reached reached a leaf node then we
38:43 - would expand and use the policy
38:47 - um here when we store the policy
38:49 - information in the new children and then
38:52 - we would just use the value for back
38:54 - propagating up all the way in our trade
38:57 - to our root mode right so yeah these are
39:00 - just the smart changes that have been
39:03 - added to our Monte Carlo research
39:05 - great so now that you know what a
39:07 - multi-colored research is and how we
39:09 - adapt our multicolored research in
39:12 - during our Alpha zero algorithm we can
39:14 - actually move to the self-play part
39:16 - right here
39:17 - so basically during self-play we just
39:20 - yeah play games with ourselves from the
39:23 - beginning to end and so we start by just
39:27 - getting this initial state right here
39:28 - with this which is just a completely
39:30 - blank state
39:32 - and yeah we also are assigned to a
39:34 - player and then based on this state
39:37 - player
39:38 - um yeah position we perform a
39:42 - multi-colored research
39:43 - and yeah this Monty College research
39:45 - will then give us this distribution as a
39:48 - return which is equal to the visit count
39:50 - distribution of the children of our root
39:53 - node right
39:54 - and once we have this distribution we
39:56 - then would like to sample an action of
39:59 - this distribution
40:00 - so in this case right here we for
40:03 - example
40:04 - sample this action which is highlighted
40:06 - here because yeah it turned out to be
40:09 - quite promising during during our Monte
40:11 - colored research
40:12 - so once we have sampled this action we
40:14 - want to act based on it so we play here
40:16 - basically as player X and then yeah we
40:20 - move to the next state and we also
40:23 - switch the player around so that we are
40:25 - now player Circle or player o
40:28 - and we have this state basically as
40:31 - import at the beginning
40:32 - and then again we want to perform this
40:35 - multicolored research right here
40:37 - and we gain this MCTS distribution then
40:42 - we just yeah sample one action out of it
40:46 - we play based on this action and then we
40:51 - can basically change the perspective
40:54 - again so that we are now a player X
40:56 - and we basically yeah continuously
40:59 - perform a Monte Carlo tree search sample
41:02 - an action out of it play uh this action
41:05 - right here and then change the
41:06 - perspective so that we are now the other
41:08 - player or the opponent
41:11 - and we do that until we actually have
41:14 - reached the terminated state so once a
41:18 - player has won or a draw has occurred
41:21 - and yeah so when this is the case we
41:24 - want to store all of this information we
41:27 - gained while playing
41:29 - um to the training data so let's first
41:31 - look how this information is structured
41:35 - so
41:37 - basically for each state
41:41 - we also want to store the MCTS
41:44 - distribution
41:45 - and then we also need to find out the
41:49 - reward for the given State the reward is
41:53 - equal to the final outcome for the
41:56 - player that we are on this given state
42:00 - so basically in this example right here
42:02 - x won the game so that means that for
42:06 - all states when we were player X we want
42:09 - the final reward or final outcome be
42:12 - equal to one and in other cases when we
42:16 - were player o and we want the reward to
42:19 - be negative one because we lost the game
42:22 - so basically that means that
42:25 - you know what we want this game as play
42:27 - X right here so we might just guess that
42:30 - this state also is quite promising
42:32 - because yeah this that it led us to win
42:35 - the game eventually so this is why we
42:37 - turn change this reward to positive one
42:39 - here when we apply X and this is also
42:42 - the reason why we change the reward to
42:44 - negative one when we are player
42:47 - um player o here so this
42:50 - um combinations of the state the MCTS
42:52 - distribution and the reward will then be
42:55 - stored as two bits to our training data
42:58 - and then we can later use these
43:01 - um for training right in order to
43:04 - improve our model
43:07 - so this is great but now we have to
43:09 - understand how training works so yeah
43:12 - let's look at this right here
43:15 - so at the beginning we just take a
43:17 - sample from our training data and yeah
43:21 - you should know now that the sample is
43:23 - the state the MCTS distribution and pi
43:27 - and the reward Z right here
43:30 - then we will use the state S as the
43:33 - input for our model
43:36 - then we will get this policy and this
43:39 - value out as a return
43:42 - and now for training The Next Step
43:45 - basically is to minimize the difference
43:48 - between the policy p and the MCTS
43:52 - distribution at the given State pi
43:55 - on one hand and then we also want to
43:57 - minimize the difference of our value V
44:01 - here and the final reward or final
44:05 - outcome Z we sampled from from our
44:08 - trending data and the way we can
44:10 - minimize the difference
44:13 - basically in a loss is first of all by
44:16 - having a mean squared error between
44:18 - the reward and the value here and then
44:22 - by also having this multi-target cross
44:25 - entropy loss between our MCTS
44:28 - distribution pi and our policy P right
44:31 - here then we also have some form of a
44:34 - true regularization at the end but yeah
44:36 - so essentially we want to have this loss
44:40 - right here and then we want to minimize
44:42 - the uh the loss by back propagation
44:45 - and this way we actually update the
44:49 - weights of our model Theta right here
44:52 - and we also thus get a model that better
44:55 - understands how to play the game and
44:57 - that has been optimized then we can use
45:00 - this optimized model to again play with
45:02 - itself in order to gain more information
45:04 - in order to train again and so on right
45:08 - um so this is however zero is structured
45:11 - and now we can actually get to coding
45:14 - so let's actually start by programming
45:17 - Alpha zero so first of all we're going
45:19 - to build everything inside of a jupyter
45:21 - notebook since the interactivity might
45:24 - be nice for understanding the algorithm
45:26 - and also this might help you if you want
45:29 - to use Google collab for example to use
45:31 - quiz gpus to train a physical more
45:35 - efficiently
45:36 - and we will start everything just by
45:39 - creating a simple game of tic-tac-toe
45:42 - and then we will build the Monte colored
45:44 - research around it and after we have
45:47 - gone so far we will eventually build
45:49 - Alpha 0 on top of the multi-colored
45:52 - research we had previously and then we
45:55 - will expand our portfolio to connect
45:58 - four as well
46:00 - and not only should this be easier to
46:03 - understand
46:05 - but it should also show how flexible fs0
46:09 - really is when it comes to solving
46:11 - different environments or board games in
46:13 - this case
46:14 - so for Tic-tac-toe we want to use numpy
46:18 - as a package
46:19 - so we're just import numpy
46:21 - SMP right here and if you are wondering
46:25 - my version currently is this right here
46:29 - so let's just create a class of
46:32 - tic-tac-toe
46:34 - and simple in it
46:37 - [Music]
46:38 - and we first of all want to have a row
46:41 - count which equates three if a column
46:44 - count as well should also be three
46:47 - and then we also need the action size
46:50 - and the action size is just the amount
46:54 - of rows multiplied with the amount of
46:57 - columns
46:59 - so save dot row count time set of column
47:02 - count
47:03 - [Music]
47:05 - so that's great and now we can write a
47:08 - method to get our initial state
47:10 - so this is what we will call at the
47:12 - beginning
47:13 - [Music]
47:15 - and
47:18 - at the beginning the states just
47:21 - full of zeros so we will just return in
47:24 - Period zeros
47:25 - and we shape here is
47:29 - the row count for the number of rows and
47:32 - the column count for the number of
47:33 - columns
47:36 - [Music]
47:37 - and next we also want a method that will
47:40 - give us the next state after action
47:43 - has been taken
47:46 - so
47:47 - we write get next state here and as
47:50 - input we want the previous state the
47:53 - action and also the player
47:56 - so the action itself will be just an
48:00 - integer between 0 and 8 where 0 will be
48:03 - the corner uplift and eight will be the
48:06 - corner done right
48:07 - so actually we want to encode this
48:10 - action into a row and a column so that
48:13 - we can use it inside of our numpy array
48:17 - so the way we do this is that we Define
48:20 - the row as
48:21 - the action divided by the column count
48:26 - but we will use an integer division here
48:28 - so
48:29 - we will get no floats
48:32 - and then for the column we can use the
48:35 - modal law
48:36 - of the action and the column count as
48:38 - well so
48:40 - if our action would be 4 for example
48:42 - then our row would be a Ford integer
48:45 - division by three which we just return
48:47 - one
48:48 - and our column would be 4 modulo 3 which
48:52 - is also one so we would have Row one and
48:55 - then column one as well so the middle of
48:57 - our board which is exactly what we want
49:01 - so then we can set the state at the row
49:04 - at the given column
49:07 - to the player we had as input
49:11 - and let's also return the state
49:14 - so our code is more readable
49:16 - so that's awesome and next we also want
49:19 - a method that will tell us what moves
49:23 - are actually legal so these are just
49:27 - um
49:28 - the most weather field is equal to zero
49:31 - so let's write a method get valid moves
49:33 - [Music]
49:36 - also gives a status input and then we
49:39 - can just return
49:41 - um state DOT reshape
49:44 - negative one so we've flattened up the
49:45 - state and then we can check if the state
49:49 - is equal to zero and this will give us a
49:54 - Boolean array back so true or false but
49:56 - it's quite helpful if we have integers
49:59 - here so I will
50:00 - turns the type to np.u and 8 so unsigned
50:04 - integers
50:05 - and yeah that's awesome so now we also
50:09 - want a method that will tell us if a
50:11 - player has won after he took or he acted
50:16 - in a certain way so let's also write a
50:19 - method for that so just Dev check win
50:22 - [Music]
50:24 - and here we want the state and then
50:26 - action
50:28 - so let us first get the row again so we
50:31 - just say action integer division self
50:33 - dot column card
50:34 - [Music]
50:35 - and let's also get the column here
50:40 - um let's
50:42 - so now we can also get the player that
50:47 - played in the given position and the way
50:51 - we do the this is just that we check
50:54 - the state at the row of the column so
50:56 - basically we turn this expression here
50:59 - and the other way around
51:02 - um and actually in tic-tac-toe there are
51:06 - four different ways in which you could
51:07 - win a game so first of all you could
51:09 - have three in a row like this but you
51:11 - could have three in a column like this
51:13 - or three in this diagonal or three in
51:15 - this diagonal right here and we want to
51:18 - check for all of these four conditions
51:19 - and if one of them turns out to be true
51:21 - then we will return true as well and
51:24 - none of them are true then we're just
51:27 - returned for it so let's write out all
51:31 - of these Expressions right here
51:33 - so first of all we can just check if
51:35 - there are three in a row
51:37 - so we will use mp.sum of the state of
51:43 - the given row and all of the columns
51:47 - inside of that row and we want to check
51:49 - if that sum right here is equal to the
51:53 - player times
51:55 - the column count
51:58 - and yeah
52:02 - and then we can also check whether there
52:05 - are three and a column like this so we
52:07 - just use np.sum of state of
52:10 - all of the rows for a given column
52:13 - and then again we check if that equals
52:15 - player time set of column count
52:19 - oh row count in this case
52:21 - I mean let's be more flexible here
52:24 - and that's great so now we want to check
52:27 - for this diagonal and the way we can do
52:30 - this is by using the NP dot diag method
52:32 - so I put up the documentation right here
52:35 - and if we would have an array like this
52:37 - which we also want to have for
52:39 - Tic-tac-toe and then we can use this
52:42 - diag method right here to get the values
52:45 - from this diagonal so let's use this
52:49 - choice to check that someone has one so
52:53 - we use mp.sum of np.diac of state and we
52:58 - check if that's equal to player times
53:01 - save.row count or safe.columncard in
53:03 - this case it doesn't matter because you
53:05 - can only get this fully diagonal of
53:07 - broken and column count is the same
53:10 - and next we also want to get this
53:12 - opposite diagonal right here and there
53:15 - is no way to do it with the normal diet
53:18 - method but we can just flip our state
53:21 - and then use this old diet method again
53:23 - and since our state is flipped it would
53:27 - be as if our diac itself is flipped if
53:30 - that makes sense so we can just use all
53:32 - mp.sum of NP dot diac of np.flip
53:36 - of the state let's just say x is equal
53:39 - to zero so we flip like this and then we
53:41 - can take the opposite diode which is
53:44 - what we want
53:45 - so I also check if it equals player
53:48 - times save dot account
53:50 - [Music]
53:52 - so awesome
53:53 - um and next we always want to check if
53:55 - the
53:56 - if there has been a draw so
53:59 - um if the game terminated in any other
54:01 - way
54:02 - and since this might also be a scenario
54:06 - where we want to stop the game so let's
54:08 - just write a new method get value and
54:12 - terminate it
54:13 - [Music]
54:15 - and we want to State and an action
54:17 - [Music]
54:18 - and first of all we will check if check
54:21 - win is true in this case so if check
54:24 - when or save to check when
54:27 - [Music]
54:30 - of state of action
54:33 - then we'll just return one for the value
54:36 - and true for terminated
54:39 - and then we can also check if there has
54:41 - been a draw and that we know that there
54:43 - has been a draw of the amount of valid
54:46 - moves to zero so we can just take the
54:48 - sum of the valid moves and check if
54:50 - that's zero so I say
54:53 - if NP dot sum
54:57 - of safe.getwell moves
55:03 - of the given state
55:04 - [Music]
55:07 - equals zero then we just return
55:11 - 0 for the value since no one has one
55:14 - again and we will also turn return true
55:17 - for terminated and in all other cases
55:19 - the game must continue so let's just
55:22 - return zero and fights
55:25 - awesome so now we got a working game of
55:27 - Tic-tac-toe and additionally would also
55:30 - want to write a method to change the
55:33 - player since that might be different for
55:34 - different board games so let's just
55:36 - write a method get opponent
55:37 - [Music]
55:38 - and
55:40 - yeah take a player's input and then we
55:43 - just
55:43 - return the negative player so if our
55:46 - initial player would be negative one we
55:48 - would return one end of our initial
55:49 - player would be one then we would just
55:51 - return to a negative one
55:52 - so that's great and now we can test our
55:56 - game we built right here so I just say
55:58 - tick tock toe liquid stick
56:01 - attack top
56:03 - then I just say player equals one and I
56:06 - say State equals Tic Tac Toe dot get
56:09 - initial state
56:12 - awesome so then let's just buy it the
56:15 - while loops
56:17 - let's just say y true we'll print the
56:19 - state right here then we want to get our
56:21 - valid moves so we know where to play so
56:23 - I just say Well it moves equals
56:25 - tic-tac-toe
56:27 - dot get rid of moves
56:30 - of this state we have currently
56:32 - and we can also print it so I just say
56:35 - valid moves
56:37 - and
56:40 - you know what let's print the valid
56:42 - moves in a more readable performance
56:44 - currently we only have zeros and one but
56:46 - we actually want to get the position
56:48 - where we can play so I just create this
56:51 - array right here and I say e for e in
56:54 - range
56:54 - [Music]
56:57 - and then we can just say Tic Tac Toe dot
57:00 - action size
57:04 - So currently we get all the indices for
57:06 - the possible actions
57:09 - and then we want to check if the indices
57:12 - itself is valid so we say if valid moves
57:18 - of I
57:22 - equals one right so we want to get all
57:25 - of these indices and this can just be
57:28 - printed out like this
57:30 - and then we want to get an action so the
57:32 - action will just be an input that would
57:34 - be casted with an integer
57:37 - and let's just
57:39 - um write out the player
57:43 - like this
57:46 - um
57:48 - and get the action right here
57:51 - so then we want to check if the action
57:54 - is valid
57:56 - so we'll say
57:59 - if valid moves
58:01 - of action liquids zero and with just say
58:06 - print
58:07 - um
58:08 - action not valid
58:10 - and we will continue so we'll ask again
58:13 - this week
58:14 - and in other cases we want to move so we
58:18 - create a new get a new state and the
58:22 - news that will be created by calling
58:23 - tic-tac-toe dot get next state
58:25 - [Music]
58:27 - and
58:29 - we want to give the old set the action
58:32 - that the players should
58:34 - [Music]
58:38 - um great
58:40 - um so then we can check if the game has
58:43 - been terminated
58:44 - so we just say value is terminal equals
58:49 - tic-tac-toe dot get
58:51 - [Music]
58:53 - value and terminated
58:55 - [Music]
58:57 - and then we want to give the state and
59:00 - the action is input
59:03 - and then we say if this terminal
59:07 - print the state so we know where we are
59:10 - and then we will check if value equates
59:14 - one
59:15 - and then we'll say player has one so
59:19 - just say player
59:23 - so
59:24 - one or negative one for the player
59:27 - and in all other cases
59:31 - um we can just print that there has been
59:32 - a draw
59:35 - great and then we also want to break our
59:38 - while loop so
59:40 - we can end the game
59:42 - and in all other cases if the game
59:44 - continues we also want to flip the
59:46 - player
59:47 - um so we just say player equals Tick
59:50 - Tock toe dot get opponent
59:53 - of player
59:56 - so nice it should be working so let's
59:58 - test this out so here are the valid
60:01 - moves so it's looking nice so let's just
60:03 - pick zero for starters let's see nice
60:06 - okay we played here so we apply a
60:09 - negative one now so let's just play four
60:11 - at play Zero we can just say eight Okay
60:14 - negative one one for example
60:17 - play Zero I'd say two and play negative
60:19 - one and just say seven and nice we see
60:22 - here we got
60:24 - three negative ones inside of this
60:26 - column right here and thus we get the
60:28 - result that plan negative one has one
60:32 - perfect so now since we have cut our
60:34 - game of detect already we can actually
60:36 - build the multi-colored research around
60:38 - it
60:38 - so let's just create a new cell right
60:41 - here
60:42 - and then we want to have a class for our
60:44 - multi-colored research so I'm just going
60:46 - to call that MCTS for now
60:49 - and then we have our init here
60:52 - and we want to pass on a game so
60:55 - tic-tac-toe in this case and then also
60:58 - some arguments so these are just hyper
61:00 - parameters for our multicolored research
61:03 - so inside of our init we can say
61:05 - self.game equals game and safe.args
61:08 - equals args
61:10 - and then below we'll Define our search
61:13 - method and this is what we'll actually
61:15 - recall when we want to get our MCTS
61:19 - distribution deck
61:21 - so let's just write their search
61:24 - and for the search we want to pass a
61:27 - status input
61:29 - and inside here first of all we'd like
61:32 - to define a root node
61:34 - and then we want to do all of our search
61:37 - iterations so for now we can write for
61:41 - search in
61:43 - range of
61:45 - safe.ax num searches
61:48 - [Music]
61:50 - and then for each search we'd like to do
61:53 - the selection phase
61:55 - then the expansion fits
61:58 - then the simulation phase and then the
62:02 - back propagation phase
62:04 - and then after we have done all of these
62:07 - things for the given number of searches
62:09 - we just want to return the visit count
62:12 - distribution
62:13 - and uh it's the distribution of visit
62:16 - counts for the children of our root
62:18 - notes so let's just say a return visit
62:21 - counts
62:22 - yeah at the end
62:24 - yeah so that's the structure we have
62:26 - inside of Harmony College research
62:28 - and next we can actually define a class
62:32 - for a node as well
62:35 - that's right class node here and then
62:37 - have our init again
62:39 - and first of all we'd like to pass on
62:41 - the game and the arguments from the MCTS
62:44 - itself
62:45 - and then also we want to have a state as
62:48 - a note
62:49 - and then a parent
62:52 - but not all nodes have parents so for
62:54 - the root note we can just write none
62:56 - here as a placeholder and then we also
63:00 - want to have an action taken but again
63:02 - we have a placement of none
63:04 - um
63:05 - for the root note here as well
63:08 - so inside of our init we can just say
63:10 - surf.gaming quits games if dot arcs with
63:13 - arcs
63:14 - self.state with equal State and then
63:18 - again search for parent equals parent
63:20 - and serve.action taken liquid section
63:23 - table
63:26 - so next we also want to store the
63:28 - children inside of our node so let's
63:30 - write self.children equal it's just an
63:33 - empty list so these are the children of
63:36 - our node and then also we want to know
63:39 - in which ways we could further expand
63:42 - our node
63:43 - so for example if the root node would
63:46 - just be an initial State then we could
63:47 - expand in all nine fields for a Tic Tac
63:51 - Toe board and yeah we want to store this
63:53 - information here so let's just say save
63:55 - Dot
63:56 - expand the moves
63:58 - equals
64:00 - game dot get valid moves of State at the
64:05 - beginning then as we keep expanding here
64:08 - we will also remove this expandable
64:12 - States from our list here
64:14 - and additionally to all of these
64:16 - attributes we also want to store the
64:18 - visit count and the various sum of our
64:21 - nodes so that we can later perform our
64:22 - UCB method and get the distribution back
64:26 - at the end so let's say safe.viso
64:29 - account with 0 at the beginning and save
64:31 - the value sum should also equals zero
64:34 - when we start
64:37 - so yeah that's uh just the backboard or
64:41 - the beginning structure of our node and
64:44 - now we can Define our root node here
64:47 - so let's say root equals node
64:50 - and we have safe.game
64:53 - save dot arcs and then the state we took
64:57 - here's input
64:58 - and the apparent and action can just
65:00 - stay none then inside of our search we
65:04 - just want to say node equals root at the
65:06 - beginning
65:07 - and next we want to do selection
65:10 - so from the video you might remember
65:12 - that we want to keep selecting downwards
65:14 - the tree as long as our nodes are fully
65:17 - expanded themselves
65:19 - so
65:20 - for doing so we should write a method
65:22 - here that will tell us so def is fully
65:25 - expanded
65:26 - [Music]
65:29 - and the node is fully expanded if there
65:32 - are no expandable moves so that makes
65:34 - sense and also if the number if there
65:38 - are children right so
65:41 - um if our node would be terminated you
65:43 - obviously can't select past it because
65:46 - the game has ended already
65:48 - so let's say return
65:50 - NP dot sum of safe Dot
65:53 - expand
65:56 - able moves
65:59 - and
66:00 - [Music]
66:01 - that should be zero right and Len of
66:05 - self.children
66:08 - should be larger than zero so this way
66:11 - we can check whether node is fully
66:12 - expanded or not
66:14 - so during our selection phase we can
66:16 - just say while node is fully expanded
66:19 - [Music]
66:21 - foreign
66:22 - and here we just want to select
66:25 - downwards we say node equals node.select
66:30 - so now we can clear this right here
66:33 - and next we want to write a method that
66:36 - will actually select down so I'm going
66:38 - to write that here
66:40 - and
66:42 - the way selection works is that we will
66:44 - Loop over all of our children as a node
66:47 - and for each child we will calculate the
66:50 - UCB score
66:51 - and then additionally
66:54 - at the end we will just pick the child
66:56 - that has the highest UCB score
66:59 - so for the beginning we can just write
67:01 - best child liquids none since we haven't
67:04 - checked any of our children yet
67:07 - and then best UCB should just be
67:11 - negative MP dot Infinity
67:13 - [Music]
67:14 - and then we can write for child
67:18 - didn't save the children
67:19 - [Music]
67:21 - and then UCB equals self dot get UCP of
67:25 - this child right here
67:27 - and then we can check if UCB is larger
67:31 - than best UCB
67:34 - and if that is the case we say this
67:36 - child should be child
67:39 - and best USB should also equal UCB now
67:42 - right and at the end we can just return
67:45 - best chat here
67:48 - [Music]
67:52 - um so next we should actually write this
67:54 - method right here we use for calculating
67:56 - the UCB score
67:58 - and
68:03 - yeah we want to take a child yes it
68:05 - would
68:06 - and for the UCB score we first of all
68:09 - have our Q value and the Q value should
68:12 - be the likelihood of winning for a given
68:15 - node
68:16 - and then at the end we have our C which
68:20 - is a constant that will tell us whether
68:23 - we want to focus on exploration or
68:25 - exploitation
68:27 - and then at the end we have a math
68:30 - square root and then we have a log at
68:32 - the top
68:33 - with the visit count of the parent and
68:36 - then below we have the visit count of
68:39 - the chart so we want to first of all
68:43 - select notes that are promising from the
68:46 - win likelihood perceived by the parent
68:49 - and then also we want to select notes
68:52 - that haven't been visited that often
68:54 - compared to the total number of visits
68:57 - so first of all we should Define the Q
69:01 - value
69:02 - and that
69:04 - should generally be the visit sum of our
69:09 - child divided by its visit count uh
69:12 - sorry it's relative sum
69:13 - so the value sum of our chart divided by
69:16 - its wizard count
69:17 - [Music]
69:19 - and the way we implemented our game
69:22 - would actually be possible to have the
69:24 - negative values from
69:26 - and generally we would like our Q value
69:30 - to be in between the range of 0 and 1
69:33 - because then we could turn it into a
69:35 - probability
69:36 - so um currently it's between negative
69:39 - one and positive one so let's just add a
69:41 - 1 here and then at the end we can also
69:44 - divide everything up by two
69:46 - and now there's just one more thing we
69:49 - could we should think about
69:52 - and that is that actually
69:55 - the Q value right here is what the child
70:00 - thinks of itself and not how it should
70:04 - be perceived as by the parent because
70:07 - you should know that in tic-tac-toe the
70:10 - child and the parent generally are
70:12 - different players
70:13 - so as a parent we would like to select a
70:16 - child that itself has a very negative or
70:20 - very low value
70:22 - because as a player we would like to put
70:26 - our opponent in a bad situation
70:27 - essentially right so we want to switch
70:31 - this around
70:32 - and we do this by saying one minus our
70:36 - old Q value here so if our child has a
70:40 - now normalized value that is close to
70:42 - zero then as a parent this actually
70:45 - should give us a q value that is almost
70:47 - 1 because this is very good for us to
70:53 - worked on the tree in a way that our
70:55 - child is in a bad position
70:59 - so now we can just return our UCB right
71:01 - here so I'm just going to write Q Value
71:03 - Plus safe.args of C
71:07 - and then that should be multiplicated
71:09 - with math.s square root
71:12 - and we should also import that up here
71:14 - so that we oh sorry
71:17 - I like that
71:19 - so that we can use it and inside of our
71:23 - square root we have math.log
71:25 - and here we have the visit count as a
71:29 - node process so just save.visit count
71:33 - and then we can divide everything up by
71:35 - a chart of this account
71:36 - [Music]
71:38 - yeah so that should be it
71:40 - and perfect
71:42 - so now we actually have a working
71:44 - selection method here and
71:47 - now we can move on so
71:50 - now we have moved down all the selected
71:53 - down
71:55 - um the tree as far as we could go and we
71:58 - have reached the leaf node or a node
72:00 - that is a terminal one so before we
72:03 - would like to expand our Leaf node we
72:05 - still have to check whether our node
72:08 - will have finally selected here is the
72:10 - terminal one or not
72:11 - we can do this by writing value is
72:14 - terminal
72:16 - equals search dot game
72:18 - dot get value and terminated
72:22 - [Music]
72:23 - and inside so we are referencing this
72:26 - method right here
72:28 - um so here we just want to write State
72:29 - and H
72:31 - is the input so we can take the state of
72:34 - our node here we have selected and then
72:37 - also The Last Action that was taken so
72:40 - just no dot action take
72:42 - and we still have to remember one thing
72:44 - and that is
72:46 - um that the note always gets initialized
72:49 - this way so the action taken was the
72:52 - action that was taken by the parent and
72:54 - not this note itself so it was an action
72:56 - that was taken by the opponent from the
72:58 - notes perspective
73:00 - so if we would actually have reached a
73:03 - terminal node and if we would get the
73:05 - result that yeah a player has one here
73:09 - then the player who has one is the
73:11 - opponent and not the player of the note
73:13 - itself
73:14 - so if we want to read this value here
73:18 - from the notes perspective we would have
73:22 - to change it around
73:23 - so here inside of our game method we
73:26 - should write a new method def get
73:28 - opponent value
73:32 - and here we can just say negative value
73:35 - so if
73:37 - from the parents perspective we have got
73:40 - a one right here is a return and we
73:44 - would like to turn it into a negative
73:45 - one from the child's perspective because
73:47 - it's the opponent that has one another
73:50 - child itself
73:52 - okay so let's use it right here so let's
73:56 - say where you equates
73:59 - safe.game dot get opponent value of the
74:03 - old value
74:04 - and we should also keep one thing in
74:06 - mind and that is that we're using this
74:09 - node.action taking method right here
74:11 - right but actually at the beginning we
74:14 - just have our simple root node and we
74:16 - can see that the root node is not fully
74:18 - expanded right because it has all
74:20 - possible ways we could expanded on so we
74:23 - will call this right here
74:24 - immediately with our root note and the
74:27 - way we initiated our root nodes
74:31 - we still have action taken it's none
74:34 - right and if we call this method right
74:36 - here with action taken being none
74:40 - um so we call this with action taking B
74:43 - none and we'll move to check one with
74:45 - action being none and then we will do
74:48 - this expression right here and this will
74:49 - just give an error so we have to say if
74:52 - action equates none
74:53 - [Music]
74:55 - just return forwards right because for
74:57 - the root note we always know that no
75:00 - player has won since no player has
75:01 - played yet so we can just leave it like
75:04 - this
75:07 - um perfect so now we're finished with
75:10 - all of this and
75:12 - next we want to check if this node is
75:15 - terminal and then we code back propagate
75:18 - immediately and if not then we would do
75:21 - expansion and simulation so I'm writing
75:24 - now here if is
75:27 - if not is terminal
75:32 - then we want to do this right here
75:36 - and at the end we can just back
75:39 - propagate right
75:41 - okay so yeah now we can do expansion and
75:44 - inside
75:46 - well this if statement I can just write
75:48 - no dig with no dot expand
75:51 - and so we just like to return the note
75:54 - that was created by expanding here and
75:58 - then inside of our node class we can
76:00 - write a new method that will do the
76:02 - expansion job
76:04 - so let's say Dev expand
76:06 - [Music]
76:07 - and okay that's
76:11 - so the way expanding works is that we
76:15 - sample one expandable move out of these
76:18 - expandable moves we have defined up here
76:21 - and then once we have sampled this move
76:23 - for Action we want to create a new state
76:27 - for our child we want to act inside this
76:30 - state and then we want to create a new
76:32 - node
76:34 - um with this new Step we've just created
76:36 - and then we append this node to our list
76:39 - of children here so we can later
76:41 - reference it inside of our select method
76:44 - and yeah at the end we can just return
76:47 - the chart that was created newly
76:50 - so for now let's get an action here
76:53 - and yeah we want to sample it from above
76:56 - so let's use np.render.choice
77:00 - so this will just pick a random value
77:02 - out of a list
77:04 - on numpy array and inside we can say
77:07 - np.where
77:09 - and then uh the condition is that
77:13 - safe.expand
77:16 - it will moves
77:17 - should be equal to one right because the
77:20 - way we programmed it all values that are
77:24 - one actually legal and we could expand
77:26 - on them and with mp.where you always
77:28 - have to use the first argument you get
77:30 - when using the method so yeah this
77:33 - should work now so yeah we just first of
77:36 - all check what moves are legal then we
77:39 - usmp.where to get the indices
77:43 - um of from all legal moves and then we
77:46 - use np.random.choice to randomly sample
77:50 - one indices and this legal NDC will then
77:54 - be the action we got right
77:57 - so that's great and now we also have to
78:00 - make this move not uh expandable anymore
78:04 - right since we're already sampled it so
78:08 - we can say it's safe.expandable moves of
78:11 - action
78:12 - and that should just be zero here
78:15 - nice so now we can create the state for
78:18 - our chart and at the beginning we can
78:20 - just copy our own state
78:25 - now we would like to act instead of the
78:28 - state of our chart so we can say child
78:31 - state
78:32 - equals
78:34 - safe.game dot get next
78:37 - state
78:38 - so we reference this right here and then
78:42 - we can use State action and player
78:45 - so
78:47 - let's move up again
78:49 - so for State we have just child State
78:51 - and for Action we have the action right
78:53 - here and for the player we will use one
78:57 - so you might have noticed that we
78:59 - haven't even defined the player yet
79:02 - which is weird right because for
79:04 - Tic-tac-toe you have two players
79:05 - obviously
79:06 - but the way we will program our Monte
79:09 - Carlo research is that we will never
79:12 - change the player or never give the note
79:15 - the information of the player but rather
79:18 - we will always change the state of our
79:21 - child so that it is perceived by the
79:24 - opponent's player while the opponent's
79:27 - player still thinks that he is player
79:29 - one right even though so the parent that
79:31 - the child both think that they are
79:32 - player one but instead of changing the
79:35 - player we flip this state around so we
79:37 - turn all positive numbers into negative
79:40 - ones and vice versa and this way
79:43 - um we can actually create a child state
79:46 - that has a different player but still
79:48 - thinks it's player one so this just
79:50 - makes the logic much more easy for our
79:52 - game
79:53 - and it also makes the code
79:57 - um here valid for one player games even
80:00 - so that's very nice so now we can say
80:03 - child State equidsafe DOT game dot
80:07 - change perspective
80:10 - of child State and then the player
80:14 - should be negative one right because in
80:16 - tic-tac-toe when you change the
80:19 - perspective
80:21 - or when you create a child node you
80:23 - always want to be the opponent
80:25 - here so let's write a new method def Dot
80:30 - change perspective
80:34 - of the state
80:37 - and of a player
80:39 - and in tic-tac-toe we can just return
80:42 - the state multiplied with the player so
80:44 - like I said when we would change the
80:47 - perspective to play a negative one so
80:49 - two opponents then we would turn all
80:51 - positive ones to negative ones instead
80:53 - of our tic-tac-toe game
80:57 - um so nice so now we have come here
81:00 - and we have got on our child status
81:04 - ready now so next we can just create the
81:07 - child itself which is a new note so
81:09 - let's say child equals node and up here
81:13 - we can see that first of all we need the
81:14 - game so it should just be saved.game
81:17 - then save.ox
81:19 - and then the state which is just a child
81:22 - state
81:23 - and for the parent we can choose
81:25 - ourselves as a node and then for the
81:29 - action taken we can use this action
81:31 - right here we send it above
81:33 - so let's just take that
81:35 - and now we want to append this child to
81:38 - our children so I'm saying I'm going to
81:40 - say
81:41 - safe.children.append child right here
81:44 - and at the end of our method we can
81:46 - return the child we have just created
81:49 - so awesome that's the code finished here
81:53 - so we have gotten this expansion part
81:56 - done
81:57 - and after we have expanded we want to do
82:00 - simulation so from the video you might
82:02 - remember that we formed these rollouts
82:04 - and the way rollouts work is that we
82:07 - just perform random actions until we
82:10 - have reached a node that is terminal and
82:13 - then we will look at the value so the
82:15 - final outcome of this game so who won
82:18 - essentially
82:19 - and then we'll use this information who
82:22 - won
82:23 - um to back propagate basically right so
82:27 - nodes where the player of the node
82:31 - itself one of the randomly choosing
82:33 - actions are more promising than
82:36 - um the ones where the opponent one right
82:39 - so yeah we use this for our back
82:42 - propagation
82:43 - so here we can just say value equals
82:46 - note dot simulate
82:49 - and then we obviously need to write a
82:51 - simulate method up here
82:54 - so let's do that for now so Dev simulate
82:58 - of self
83:00 - and first of all again we have to check
83:03 - if this node that has just been expanded
83:05 - this uh terminal one
83:09 - so up here we just say value is terminal
83:13 - equal itself.game dot get value and
83:17 - terminate it
83:19 - and inside we can again use self.state
83:22 - and save dot action table
83:26 - and we have to again flip this value
83:29 - around to the opponent's perspective
83:32 - because
83:34 - um always when
83:36 - notice one one it's not because of the
83:39 - node itself but because of its parent
83:40 - that has taken this action here so we
83:46 - have to say
83:47 - value equals save Dot Game dot get
83:51 - opponent value of the value we have
83:55 - gotten here
83:56 - and yeah then we could just say if
84:02 - this terminal
84:04 - then we would like to return this value
84:06 - up here
84:08 - and in all other cases we now want to
84:11 - perform our rollout so
84:13 - in order to do that we can just say roll
84:17 - out state
84:20 - equaled self.state dot copy
84:25 - [Music]
84:26 - and then we say Roy odd player liquids
84:30 - one at the beginning and here we're
84:32 - actually going to use the player just
84:33 - that's just because we need to find out
84:35 - if the value we got at the end should be
84:38 - perceived
84:40 - by this get opponent Value method or not
84:43 - when we use it to back propagate our
84:45 - node uh it's value right here
84:48 - so inside of our product we can say why
84:51 - they're true
84:53 - and then again we want to sample an
84:55 - action like we did up here so first of
84:59 - all we can say valid moves equals Dot
85:02 - Game dot get when it moves of the raw
85:06 - out state
85:06 - [Music]
85:08 - and then here we can write action
85:10 - equates mp.random.choice
85:13 - of np.where off valid moves
85:18 - equals one
85:20 - take the first one
85:22 - yeah and yeah that should be our action
85:25 - and now we can use that action to get
85:28 - the next state so we can write raw out
85:31 - States equals
85:33 - and then we can say save.game dot get
85:36 - next state
85:38 - [Music]
85:39 - and then again we use the word state
85:43 - and the action here and then for the
85:45 - player we'll use Royal player
85:48 - [Music]
85:49 - and after we have got now a new state we
85:51 - have to check again whether this state
85:53 - is terminal
85:54 - so let's write value
85:57 - is terminal
86:00 - equality save dot game
86:03 - Dot
86:04 - get
86:05 - value
86:06 - and terminate it
86:08 - [Music]
86:10 - and we just use this newly updated real
86:14 - estate as input and also the action we
86:17 - took
86:19 - and now if this terminal is true we'd
86:23 - like to return our value so let's write
86:25 - that as an if statement
86:27 - [Music]
86:29 - for the value we again have to check
86:32 - whether we were a player one when we won
86:36 - the game or hit a terminal note
86:40 - so we say if row out player with
86:44 - negative one
86:46 - then we would flip the value up so that
86:49 - we don't get a positive return since the
86:53 - opponent one and notes we as a note
86:56 - so we can say that you include self Dot
86:59 - Game dot get opponent value of value
87:04 - and generally we can just return this
87:07 - value here like we did up here above
87:11 - so nice now in our other cases if we
87:15 - haven't reached the terminal node
87:16 - immediately we want to flip our play
87:19 - around so we can say raw out player
87:21 - equality save.game dot get
87:24 - component
87:27 - of the old old player
87:32 - yep so that should be it
87:35 - um so yeah now we can do simulation
87:37 - after we have expanded
87:39 - and the final step would now be to do
87:42 - back propagation
87:44 - and we can just write a new method here
87:48 - so let's say no dot back prop type gate
87:52 - and then we pass the value here as input
87:55 - we have gotten either here or here
87:58 - and we can write that here inside of our
88:01 - node class so Dev back propagate
88:04 - [Music]
88:06 - and then take value here's input
88:09 - so when we back propagate first of all
88:12 - we want to add the value to our value
88:14 - sum and then we want to add our Visa
88:17 - card with just a one so count that up
88:20 - and then we want to keep it propagating
88:22 - up all the way to our root node
88:25 - so let's write save dot value of sum
88:28 - plus equals value
88:31 - and then let's write safe.visit count
88:34 - plus equals one
88:36 - then when we back propagate up we again
88:40 - have to remember that our parents is a
88:43 - different player than us
88:44 - so we should write value equals search
88:48 - Dot Game dot get opponent value
88:52 - of value here and then we say if parent
88:57 - is not none
89:00 - which is always true except for the root
89:02 - note where we have this as our
89:04 - placeholder
89:06 - um then we're just going to say parent
89:08 - dot back proper
89:11 - gate that you write here
89:14 - and that should be
89:15 - so yeah we have this recursive back
89:18 - propagate method
89:20 - and that should be all we need for MCTS
89:24 - so finally we have gotten all of the
89:27 - results we have back propagated all of
89:29 - our visit counts and values and so on so
89:32 - now we would actually like to return the
89:34 - distribution of visit counts
89:36 - so to do that we should just
89:39 - um
89:39 - write a new variable action props so the
89:43 - probabilities of uh which actions look
89:46 - most promising and at the beginning that
89:49 - should just be MP dot zeros and the
89:51 - shape here can just be
89:53 - the action size of our game so save.game
89:56 - dot get
89:57 - dot action size
90:00 - so
90:01 - for Tic-tac-toe just this one right here
90:05 - and now we want to Loop over all of our
90:09 - children again
90:10 - so let's say for child and served or
90:14 - children
90:15 - and then we write action props at
90:19 - child.action taken
90:22 - and this probability should be equal to
90:24 - the visit count of our child so let's
90:26 - say child.visit count and that and now
90:31 - um
90:32 - we want to turn these into probabilities
90:34 - and the way we do that is we're just
90:36 - divided by its sum so that the sum will
90:39 - be equal to 1.
90:41 - and let's just write mp.sum of action
90:44 - cross right here and then we can return
90:46 - this
90:47 - [Music]
90:50 - so that's looking promising so let's
90:52 - just absorb
90:53 - and there is the inverted syntax here so
90:57 - I just
90:59 - should these should remove this here and
91:02 - now that's working so let's test this
91:05 - out and we can create a MCTS object here
91:10 - inside of our test script we used for
91:12 - the game
91:13 - so let's write MCTS equals MCTS and for
91:18 - the game we use tic-tac-toe
91:20 - and now we still have to Define our
91:22 - arguments so for C which we use in our
91:26 - UCB formula
91:27 - and we can just
91:30 - roughly say that we want the square root
91:33 - of 2 which is what you might use
91:36 - generally
91:37 - and then for the number of searches we
91:40 - can set that to a thousand
91:42 - [Music]
91:45 - nice um so let's also pass on the arcs
91:49 - here inside of our multi-colored
91:50 - research
91:51 - and then during our game first of all I
91:54 - should remove that um
91:57 - we should say that we only do all of
92:00 - this so acting ourselves if we are
92:02 - player one
92:04 - [Music]
92:08 - and in other cases so I take the toe
92:10 - when player is negative one we want to
92:14 - do a multi-colored research so we can
92:17 - say MCTS props quits
92:20 - mcts.search of State
92:24 - but you should remember that we always
92:26 - like to be player one here when we do
92:28 - our multi-colored research so first of
92:30 - all let's write neutral State liquids
92:33 - and then we can say safe.game
92:36 - dot change perspective and also not such
92:39 - a game by Tick Tock toner yeah
92:42 - um
92:45 - and then we'll use this General State
92:48 - here and for the player we can just set
92:50 - a player which is here always negative
92:52 - one so we always change the perspective
92:54 - then instead of MCTS search we should
92:57 - just use this Nutri State we have just
92:59 - created
93:01 - and that's great and now out of these
93:05 - probabilities we want to sample an
93:06 - action and to keep things easy we can
93:09 - just sample the action that looks most
93:11 - promising and the way we do that is by
93:13 - using mp.arkmax
93:20 - and this will just return the chart that
93:22 - has been visited most number of times
93:24 - and inside here we can use the props of
93:28 - our MCTS
93:30 - yep and that's just the action we have
93:32 - here so let's test result to see if this
93:34 - works
93:37 - okay so we still have a problem here uh
93:41 - also um for parent we need to use
93:43 - self.parent
93:48 - [Music]
93:53 - and here I have a typo
93:57 - and so
93:59 - just want to say best child
94:01 - [Music]
94:07 - and here we don't want to use save to
94:09 - Children obviously but the child
94:11 - children of our root note so instead of
94:14 - MCTS we can say
94:17 - for child in root of children
94:22 - so perfect so now we can see that this
94:25 - multi-colored research moved here inside
94:27 - the middle and now we as a player let's
94:30 - play a bit here to check this out and we
94:34 - can just play here MCT has played here
94:36 - then we can say I will play seven and
94:39 - now the LCT has played here and
94:42 - from the roots of tic-tac-toe you know
94:44 - that we're the MCTS has hit three
94:47 - negative ones here instead of this row
94:49 - here so it has one so nice so now we can
94:52 - see that this MCTS is working with okay
94:55 - so now that we have our Standalone
94:56 - multicolored research implemented right
94:59 - here
95:00 - and we can now start with building the
95:03 - neural network right so that we can then
95:06 - later use the alpha zero algorithm in
95:09 - order to train this model that can
95:10 - understand how to play these given games
95:12 - right
95:13 - and before we start uh yeah with
95:16 - building
95:18 - um let me just briefly talk about the
95:20 - architecture that we have for our neural
95:22 - network
95:23 - so yeah this is just a brief
95:27 - visualization here
95:28 - and first of all
95:31 - we have this state right here that we
95:33 - gave us give us input to our Android
95:35 - network and yeah in our case this state
95:38 - would just be a board position so for
95:40 - example a board of tic-tac-toe right
95:42 - here
95:43 - and actually we encode the state so that
95:48 - later we have these three different
95:50 - planes next to each other like in this
95:53 - image right here and actually we have
95:55 - one plane for all of the fields in which
95:58 - player negative one is played so where
96:01 - these fields will then be turned to once
96:03 - and or other fields would just be zeros
96:05 - and then we also have a plane for all
96:07 - the empty fields and this region then
96:09 - between the ones or other fields being
96:11 - zeros and then after this last plane for
96:13 - other fields in which player positive
96:15 - one played and yeah here again the these
96:18 - fields will be turned to once and all
96:20 - the other fields will be turned to zeros
96:22 - so by having these three planes right
96:25 - here it is easier for our Network to
96:28 - basically recognize patterns and
96:31 - understand how to play this game right
96:34 - so essentially we encode our board game
96:38 - position uh almost so that it looks like
96:42 - an image afterwards right because we
96:45 - have instead of these RGB planes we have
96:48 - these planes for player positive one uh
96:50 - player zero or empty fields and play a
96:53 - negative one right
96:54 - and because
96:57 - we our state looks like an image we also
97:00 - want to use neural network architecture
97:03 - that works best for images right and so
97:07 - because of that we have these
97:08 - convolutionary blocks then inside of our
97:11 - neural network right so first of all we
97:14 - have this backbone right here and this
97:16 - backbone will just be the main part of
97:18 - owner Network
97:20 - um where we do most of the computation
97:23 - and here for the backbone like I said we
97:26 - use these convolutional blocks
97:29 - and we also use this resnet architecture
97:33 - so the residual Network
97:36 - and
97:37 - this is just a research paper that
97:40 - turned out to work really yeah well with
97:44 - um understanding images
97:46 - and here basically what this resonant
97:50 - architecture does is that we have these
97:52 - skip connections like this
97:55 - so when we do the feed forward through
97:57 - our backbone we won't just always update
98:01 - the X by moving forward right but rather
98:04 - we will also store this residual so
98:07 - basically we store the x value or the
98:09 - you know value that is passed through
98:12 - and before we give it to our conf block
98:15 - and then at the end the output will just
98:17 - be the sum of the x value before conflog
98:20 - and the x value after our conf block or
98:23 - conf blocks and because of that our
98:26 - model is much more flexible because it
98:28 - could technically also mask out conf
98:30 - blocks by not changing the X at all and
98:33 - just using this residual skip connection
98:35 - X right so here's also just an image
98:38 - from the paper
98:40 - so yeah generally the main idea is to
98:43 - also store this x right here at this
98:45 - position as a residual and then have the
98:48 - skip connection later on so that the
98:50 - output will be the output from the conf
98:53 - blocks summed with the initial X so the
98:57 - skip connection right here so this is
98:59 - just the arrest net architecture that we
99:01 - use
99:02 - um instead of our backbone right
99:06 - um so after we have gone through our
99:09 - backbone
99:10 - um we split our model architecture into
99:13 - these two parts
99:14 - so first of all we have this policy head
99:18 - here above and at the beginning here we
99:20 - just have a singular conf block and this
99:23 - conf block will take the output from our
99:26 - backbone as input like this
99:28 - and then after we have gone through that
99:30 - conflog we will then flatten out the
99:33 - results here
99:35 - and then we just have this linear layer
99:38 - or fully connected layer between the
99:40 - outputs for account block and then also
99:43 - these neurons right here and at the end
99:46 - we just want to have uh yeah nine
99:49 - neurons in the case of tic-tac-toe
99:51 - because we want to have one neuron for
99:53 - each potential action right and then
99:55 - tic-tac-toe there are nine possible
99:57 - actions
99:58 - so yeah at the end we will just have
100:00 - these nine neurons right here and and
100:03 - normally we would just get these logids
100:06 - so just these standard outputs but when
100:09 - we want to get this readable
100:11 - distribution telling us where to play we
100:14 - also have to apply this soft Max
100:17 - function and this will basically turn
100:20 - the outputs from our nine neurons to
100:23 - this distribution of probabilities and
100:26 - yeah then each probability we're
100:28 - basically indicate us
100:30 - how promising a certain action is right
100:34 - okay so yeah this is why we have these
100:36 - nine neurons here and then where we also
100:38 - in practice later called the softmax
100:41 - method right
100:42 - so the second head this this value head
100:46 - right here and here we also have the
100:49 - singular conf block that is different uh
100:52 - from this one right here so we have this
100:55 - connection here as well and this also
100:57 - takes the output from our backbone as
100:59 - input and then again we would like to
101:02 - flatten the output of this conf block
101:04 - here out and we want to have this linear
101:07 - layer of fully connected layer and
101:09 - finally we just want to have one neuron
101:11 - right here because remember this that
101:14 - this value head should tell us how good
101:18 - this state right here is and it should
101:21 - just give a float
101:24 - in the range of negative 1 to positive
101:26 - one is output
101:27 - and because of that we only need none
101:30 - one neuron right and the best way to get
101:32 - this range of negative 4 into positive
101:35 - one is by also applying this 10h
101:38 - activation function
101:40 - onto this last singular neuron right
101:43 - here so here's just a visualization of
101:46 - the 10h function and basically this will
101:48 - just squish all of our potential values
101:51 - into this range of negative one to
101:53 - positive one and yeah this is exactly
101:55 - what we want for our value head
101:57 - so ultimately we have
102:00 - this model right here that takes this
102:02 - encoded status input and on one hand it
102:05 - will output the policy so just yeah this
102:08 - distribution telling us where to play
102:10 - and on the other hand uh it will give us
102:14 - this value so just this estimation
102:16 - telling us how good this state is right
102:19 - here okay
102:20 - so now we can actually build this inside
102:23 - of our jupyter notebook
102:25 - and here the first thing is that we want
102:28 - to import pytorch
102:30 - because we want to use the Deep learning
102:32 - framework pytorch so first of all I will
102:35 - also print the NP dot version so we
102:37 - always get that right here and then we
102:39 - will import torch like this let's also
102:43 - print torch
102:45 - dot version right here
102:50 - okay so let's just do that okay so this
102:54 - is my current torch version and I think
102:57 - for yeah the loss we will use later on
103:01 - we will have this multi-target cross
103:03 - entropy loss and this has been added as
103:06 - a feature just recently to pytorch so I
103:09 - would just recommend to get this version
103:10 - right here but or any newer version
103:14 - um so if you have any problems you can
103:16 - just use this one and I also have Cuda
103:19 - support right here because of my Nvidia
103:21 - GPU but you could also just get the CPU
103:24 - Standalone pytouch version
103:26 - depending on your setup right okay so
103:29 - now additionally to torch we also want
103:31 - to import torch
103:33 - dot n n
103:35 - s and n right here and then we want to
103:37 - import
103:40 - torch.nn.funk channel
103:42 - like this s this big F right capital f
103:48 - okay so now we can import these things
103:50 - right here and then I will just create
103:52 - this model and I will just create this
103:54 - new cell above our MCTS implementation
103:59 - so I will just call this class res net
104:03 - then we have an N dot module
104:06 - and inherit from that
104:08 - so here we have our init
104:12 - and first of all we have a game then we
104:15 - have a num of rest blocks so this will
104:18 - just be the length of our backbone right
104:21 - instead of our architecture
104:23 - and then we also have num hidden yes so
104:27 - this will be the hidden size for our
104:30 - conflict blocks right
104:33 - Okay so
104:34 - inside first of all we have to call it
104:36 - super dot init
104:40 - and now we can Define the start block
104:43 - and this should be equal to an N dot
104:46 - sequential
104:48 - and first of all we just have this conf
104:51 - block at the beginning so nn.conf
104:54 - 2D
104:55 - and the input your hidden size is just
104:59 - three because remember that we have
105:00 - these three planes at the beginning and
105:03 - then the output uh dim should be num
105:06 - hidden
105:08 - then for the kernel size we can just set
105:11 - that to three and also we can set
105:14 - heading to one right because when we
105:17 - have kind of sizes 3 and padding as one
105:19 - this conf block won't actually change
105:22 - the shape of our game State directly so
105:25 - it always be where the number of rows
105:29 - times number of columns for these images
105:32 - essentially right for these pixelves if
105:35 - you want to look at this look at it like
105:38 - an image
105:39 - so after our conf 2D we also want to
105:42 - have this batch Norm so NN dot patch
105:45 - Norm
105:46 - 2D
105:47 - then here we have just num hidden as our
105:50 - hidden size
105:51 - yeah in this page number will just
105:53 - increase the speed speed for us to train
105:57 - um essentially so now we can also have
105:59 - this num and Android value
106:02 - and yeah this radio will just turn all
106:05 - the negative yeah they use two zeros so
106:08 - clip them off basically and this way
106:11 - it's also faster to train
106:13 - and save up okay so this is just our
106:17 - start block right here so first of all
106:19 - just this confer to D then this batch
106:21 - number to D and then our value
106:23 - and now we can also have the
106:25 - save.backbone like this this should be
106:28 - equal to NN dot module list
106:31 - and then here we just want to have this
106:34 - array
106:35 - and basically we want to have this array
106:38 - of different rest blocks right inside of
106:42 - our backbone
106:43 - uh remember from this image right here
106:46 - and
106:48 - in order to do that we also have to
106:50 - create this rest block class so let's
106:52 - write class rest block
106:54 - like this of NN dot module
107:00 - and then first of all we have our init
107:01 - again
107:04 - and then we just have num hidden s input
107:09 - here
107:10 - and again we have to
107:12 - inert our super class as well
107:18 - um like this
107:19 - [Music]
107:20 - okay so now for each rest block we also
107:24 - want to have our first conf block so
107:27 - save dot conf1 this should just be equal
107:30 - to
107:32 - nn.conf 2D of num hidden for the input
107:36 - hidden dim then num hidden for the
107:40 - output in the
107:41 - and then Kernel's size should be three
107:43 - and then padding should be one here
107:46 - so this is our first curve block then we
107:48 - also have our first batch Norm block so
107:50 - bn1 this should be equal to NN dot batch
107:53 - Norm 2D
107:56 - of num hidden as we hidden then we kind
107:59 - of our second conf block so save to
108:01 - conf2 should be set to nn.conf 2D
108:04 - and like this and then again we have num
108:07 - hidden here and I'm heading here
108:10 - within a kernel size of
108:12 - three
108:13 - [Music]
108:14 - and a padding of one
108:18 - and now we can also Define the second
108:20 - batch from success.bn2 set that equal to
108:23 - nn.batch Norm today off num hidden as
108:28 - well
108:29 - so then let's also Define this forward
108:31 - method so let's write Dev forward the
108:35 - first one we have safe and then here the
108:38 - X so the input
108:41 - and then remember that we instead of our
108:44 - rest block want to have this residual so
108:47 - that we can later have this skipped
108:49 - connection so let's create this residual
108:52 - and set that equal to X
108:55 - and then now we want to update this x
108:58 - right here separately
109:01 - and we can do this by first of all
109:04 - calling self.conf one of X then calling
109:10 - self.bn1 on top of that
109:13 - and then also we want to have a radio
109:16 - here so just F Dot
109:19 - sorry capital f f dot value like this
109:22 - [Music]
109:26 - and so this is our X at this position
109:30 - and then we also want to feed through
109:34 - our second conf block here
109:37 - so let's update X by calling save.pn2
109:42 - of self.conf to
109:45 - of X right here
109:48 - so this way we have yeah went through
109:51 - these two call blocks right here
109:53 - and then now we can just sum up
109:58 - um
109:59 - this output with the residual right so X
110:04 - plus equals residual right here
110:06 - and then
110:09 - now we can just again call the value
110:13 - here so X should be
110:15 - f
110:17 - dot value of x
110:19 - and now we can just return that X right
110:21 - here so this way we have this nice
110:24 - residual connection here and at the end
110:26 - we just return the sum right
110:29 - okay so now we actually can
110:32 - create this backbone right here so in
110:35 - order to do that we want to create a
110:37 - rest block of num hidden
110:41 - just for I in range
110:46 - of num res logs
110:49 - like this so this way we also have our
110:52 - backbone right
110:53 - so this is great and next
110:56 - we can create this policy head so safe.
110:59 - policy head like this this should again
111:03 - be equal to nn.sequential
111:07 - and instead of the sequential first of
111:09 - all we want to have just this conf block
111:11 - so nn.conf 2D
111:14 - and here we have a num hidden as our
111:17 - input hidden size then we have just 32
111:20 - as our output in size
111:22 - and then we can also Define a kernel
111:25 - size and set that to three and have a
111:28 - padding of one
111:30 - and then again we want to call it batch
111:32 - Norm here
111:36 - and just this Ray U here
111:39 - [Music]
111:42 - okay
111:43 - so
111:44 - after we have went through this conf
111:47 - block remember that we want to flatten
111:49 - our results so let's just call it NN dot
111:51 - flatten
111:53 - here
111:54 - and now we want to have this linear
111:57 - layer right
111:59 - so nn.linear and for the input we want
112:03 - to have these 32 planes
112:07 - multiplied with the number of rows and
112:10 - multiplies with the number of columns so
112:13 - time scale dot row count times game dot
112:17 - column count right because remember that
112:20 - we have a kernel size of 3 and a padding
112:22 - of one for all of our conf blocks and
112:25 - because of that we won't change the
112:27 - actual shape of our state right
112:29 - so because of that we can just when we
112:32 - can want to get the input size of our
112:34 - flattened output here and we can just
112:37 - multiply the number of hidden planes so
112:40 - 32 with the row count with the column
112:43 - count as well then for the output of our
112:45 - linear layer we just want to have game
112:47 - dot action size right
112:50 - okay
112:51 - um yeah so that's great and now we can
112:54 - also Define a value head it's safe to
112:57 - the value head like this and this should
113:00 - be set to nn
113:02 - Dot sequentia
113:04 - and here first of all we also have
113:06 - nn.conf 2D and for the input hidden them
113:10 - we have just now hidden and for the
113:12 - outputting we have just three and for
113:15 - the kernel size we can set that to three
113:16 - and heading to one again
113:19 - and now we can also have a batch Norm
113:21 - here
113:22 - so patch node to D
113:24 - and have three as the hidden size
113:28 - and then again call it just already on
113:30 - top of it
113:30 - [Music]
113:32 - and flatten our results here
113:34 - [Music]
113:38 - okay so now we also want to have this
113:40 - linear layer at the end
113:41 - [Music]
113:44 - and again for the input we have just
113:45 - three times
113:47 - game dot row count timescame dot column
113:50 - count
113:52 - and then for the output we just want to
113:54 - have one right because we just want to
113:56 - have one neuron here and then also
113:59 - um we want to have this 10h activation
114:01 - function right so we can just add an N
114:04 - dot 10h right here
114:08 - so now
114:10 - um we have here this party finish and
114:13 - next we can write this forward method
114:15 - and for our resnet class so it's right
114:19 - there forward
114:20 - and take the
114:22 - input X here
114:25 - and then first of all we want to send
114:28 - this x through our start block so X
114:30 - should be safe dot start
114:32 - block of X right and then we want to
114:37 - Loop over
114:39 - all of our rest blocks right here
114:41 - instead of our backbone so let's write
114:43 - four rest Block in safe Dot
114:49 - um backbone
114:51 - like this
114:54 - um then here we can just set x to the
114:57 - rest block of X right
115:02 - so that's great
115:04 - and now we have these X values right
115:07 - here and next we yeah I can just get
115:10 - this policy and this value here back so
115:13 - first of all you can write policy
115:16 - and here we can just set that equal to
115:20 - save dot policy head
115:23 - of x
115:25 - then this value should be searched where
115:28 - you head of x
115:30 - and now we can just return our policy
115:34 - and our value right here
115:36 - [Music]
115:37 - okay so let's just run that for now
115:41 - and now we also want to test that this
115:43 - is working so I would just create this
115:46 - new cell right here and here let's just
115:48 - start by creating a game of tick tock so
115:52 - so this should be equal to this
115:53 - tic-tac-toe class right here and let's
115:55 - get this state by calling tick toe dot
115:58 - get initial State like this and then
116:02 - let's just update that state by first of
116:04 - all setting state to tick tock toe dot
116:07 - get next
116:09 - state of State let's just say that I
116:12 - don't know we play at the position two
116:14 - for the player negative one
116:18 - I'll post one in this case and let's
116:20 - also play at
116:24 - um position seven
116:26 - for the player negative one
116:30 - okay
116:31 - um
116:31 - [Music]
116:33 - so now we can just print the state for
116:35 - now
116:37 - and yeah we get this state right here
116:39 - okay so next we should
116:42 - um
116:42 - remember that we also have to encode our
116:45 - state when we give it to our model
116:47 - so we also have to write this new method
116:49 - right here so here we just write def get
116:53 - encoded state of safe and then just off
116:56 - the state right here
116:58 - and remember that we want to have these
117:00 - three planes right so we can get this by
117:03 - writing n coded State equals
117:08 - NP dot stick
117:11 - and then here we want to Stack first of
117:13 - all these eight voltage that are equal
117:15 - to negative one with the state of all
117:18 - empty Fields so that the fields that are
117:20 - equal to zero with these state where all
117:24 - four of the fields they are equal to
117:26 - positive form like this so yeah this
117:28 - should be our encoded State and here
117:31 - these widgets be booleans but rather we
117:33 - want to have floats so let's just set
117:36 - the type here for NP dot float 32.
117:40 - and now we can just return this encoded
117:42 - state
117:45 - okay
117:46 - um so let's test this out by writing
117:49 - encoded State and you're setting that
117:52 - equal to
117:54 - Tick Tock toe but get encoded state of
117:59 - this state right here let's just print
118:00 - that here
118:04 - okay so yeah now we see that we have
118:08 - this uh just stayed right here and this
118:11 - is our encoded state right so first of
118:13 - all we have this plane for all of the
118:15 - fields in which planet nectophone is
118:17 - played so just this field right here
118:18 - which is encoded as a one here
118:21 - and then we have these empty Fields
118:23 - right here and then we also have these
118:25 - fields where play a positive one played
118:27 - so just the sphere right here right
118:30 - okay and so this is great and now we
118:32 - want to get this policy and this value
118:34 - right
118:35 - so first of all we have to turn this
118:39 - state to a tensor now that's so because
118:42 - of that we can write tensor State and
118:44 - set that equal to
118:46 - rch dot tensor of encoded State like
118:50 - this
118:51 - then when we give a tensor to our model
118:54 - as input we also always need this batch
118:58 - Dimension right
119:01 - um but here we just have one state and
119:03 - not a whole batch of States so because
119:05 - of that we have to unsqueeze
119:08 - [Music]
119:09 - if the Zero's axis
119:13 - so what this does is that we that it
119:15 - will essentially just create
119:17 - further brackets around this encoded
119:20 - state right here and this way we can
119:22 - pass it
119:24 - um through our model right
119:26 - okay so now we have this tensor state
119:28 - right here then we can get this policy
119:30 - and this value by calling our model
119:34 - that's why I need to Define our model so
119:36 - that's right model equates res
119:38 - net and for the game we have Tick Tock
119:41 - too for the number of rest blocks we can
119:44 - just set that to four and for the num
119:46 - hidden we can set that to 64. okay so
119:48 - now we can write policy value equals
119:51 - model of this tensor state right here
119:56 - and then we want to yeah process the
119:59 - policy of the value so let's first of
120:01 - all get this float from our value by
120:03 - calling value dot item right here
120:07 - since this will just be a tensor on WE
120:09 - rather like to have the float and we get
120:11 - this by calling dot item with this
120:13 - tensor here and then we can also change
120:16 - the policy
120:17 - so here we can just write policy which
120:19 - equates torch.soft Max
120:22 - of this policy right here and for the
120:25 - access we have to choose one because we
120:28 - don't want to apply this soft mix on our
120:30 - batch access but rather than this axis
120:32 - where we have these nine neurons right
120:34 - so this x is one
120:36 - and
120:39 - now we first of all have to
120:42 - squeeze this again
120:44 - so that we remove the
120:47 - patch axis essentially and then we can
120:50 - detach that so we don't have a grad here
120:53 - when we turn this to numpy now so then
120:56 - let's also take this to CPU and let's
120:59 - call this to numpy here like this okay
121:02 - so now we have this policy and let's
121:04 - just print out both of these things so
121:06 - right after there you want to print out
121:08 - the policy and see whether this works
121:10 - okay so nice uh now first of all we get
121:13 - this value of 0.3 for this state right
121:17 - here
121:17 - and then this is our policy distribution
121:20 - telling us where to play so just this
121:22 - number of probabilities now you know
121:25 - what let's also make this a bit more
121:27 - visual so let's import
121:29 - matplot lib like this dot Pi plot
121:36 - um right here
121:38 - yep s Pat
121:41 - and then here we can just
121:43 - write plt.bar
121:45 - first of all of the range from our Tic
121:50 - Tac Toe dot action size and then these
121:53 - policy values right here
121:56 - let's just write pat.show
122:00 - okay so yeah let's uh so this is
122:02 - basically how our policy looks like so
122:05 - for each action and we have this bar
122:08 - right here
122:09 - um yeah telling us how promising this
122:11 - action is and then obviously our model
122:14 - was just initiated randomly currently so
122:16 - we can't expect too much here and so
122:19 - nothing actually but um
122:21 - when we later have this trained model we
122:24 - can expect this nice distribution of
122:27 - bars telling us where to play right okay
122:29 - so now we also have this Modi model
122:32 - built and next we can start with
122:34 - incorporating our neural network into
122:37 - this multi-colored research right
122:39 - so before we do so I I'd still like to
122:43 - apply a small change here so
122:46 - um let's just set a seat for torch so
122:49 - that we have reproducible results so
122:51 - that you can make sure that you at least
122:55 - bit something similar to what I've built
122:58 - right here so let's write torch.manual
123:03 - seat and let's just set the C to zero
123:05 - like this
123:07 - so perfect
123:09 - and then we can update
123:12 - this multicolor tree switch right here
123:15 - so the first thing is that we now want
123:18 - to give a model here's input so this
123:21 - will later on just be our res net so
123:23 - let's also declare safe. Model S model
123:27 - foreign
123:31 - we want to do
123:33 - so basically after we have reached the
123:36 - leaf node we want to predict a value and
123:39 - a policy
123:41 - um based on the state of our Leaf node
123:44 - and then we want to use this policy
123:47 - when we expand so that we can
123:51 - incorporate the probability and solve
123:55 - the policy into the UCB formula that
123:58 - makes sense so that when we choose a
124:01 - note during our selection phase we are
124:03 - more likely to choose nodes with higher
124:07 - privacy values because of these were the
124:11 - ones that seemed more promising to our
124:14 - model right so we want to choose these
124:17 - more often and work down the tree inside
124:20 - of these directions where our model
124:22 - guides us
124:23 - so this is the one uh thing we use our
124:26 - model for and the next thing is um
124:30 - that we want to use the value we get
124:32 - from our model to back propagate once we
124:36 - have reached the leaf node so actually
124:37 - we can completely remove the simulation
124:40 - part so we don't want to do any rollouts
124:44 - with random actions but instead we just
124:46 - use the value we get from our model so
124:49 - let's call this right here so we can
124:51 - write policy
124:52 - and value equals self dot model
124:57 - and here we want to pass in the state of
125:00 - our node right but remember that we want
125:02 - to encode it beforehand so that we have
125:05 - this three planes based on the players
125:09 - um we have in tic-tac-toe or just for
125:12 - the interference right so uh you might
125:15 - remember this from the last checkpoint
125:16 - so first of all we can write safe.game
125:19 - dot get encoded state
125:22 - and here we just have no dot state
125:25 - and now we want to turn this encoded
125:27 - State into a tensor
125:29 - uh instead of just a numpy array so that
125:32 - we can give it as input to our model
125:35 - directly so we can write torch.tensor
125:39 - off safe.game.gate encoded state of
125:41 - node.state
125:43 - so now we have to unsqueeze at the zeros
125:46 - axis
125:49 - and
125:50 - um so let's write unsqueeze of zero
125:53 - right here and this is because we always
125:55 - need an axis for our batch right and
125:58 - currently we aren't patching any states
126:00 - so we just create this empty batch
126:03 - basically with just this one stack right
126:06 - here
126:06 - so this is why we unscreased here
126:09 - and yeah then we get a policy and the
126:12 - value as a result and now remember from
126:15 - the model checkpoint that we want to
126:17 - turn this policy
126:20 - um which is just uh with which are just
126:23 - logits currently so just nine uh floats
126:27 - we want to turn this into a distribution
126:30 - of likelihoods so first of all we can
126:34 - write policy equates torch.soft Max
126:38 - of policy and then we have to set access
126:40 - to one because we want to apply the soft
126:43 - Max not on the batch axis but rather on
126:45 - the axis of our nine neurons right in
126:48 - the case of tic-tac-toe so after we have
126:51 - called the softmax we would like to
126:53 - squeeze again
126:55 - at the zeros and axis so that we remove
126:58 - this batch axis now so actually we want
127:01 - to read revert this statement right here
127:04 - after we have feed forwarded our state
127:06 - through our model
127:08 - and next we can call CPU in case we
127:12 - might lose Jeep use GPU later and then
127:14 - we can use numpy right here and also we
127:18 - should use The torch.nograd Decorator on
127:21 - top of our search method
127:24 - um so
127:26 - because we don't want to use this policy
127:28 - and value for training immediately
127:30 - rather we just want to use it for
127:31 - prediction
127:33 - so we don't want to store the gradients
127:36 - um yeah of this tensors right here so we
127:38 - just use no grid as a decorator so this
127:41 - way
127:42 - um yeah it's faster to run our Monte
127:44 - Carlo research
127:46 - so great so now we have our policy and
127:49 - we have turned it into a numpy array and
127:52 - we have applied the softmax activation
127:54 - function
127:55 - but next we also want to mask out all of
127:58 - the illegal States
128:00 - all of the illegal moves I'm sorry
128:02 - because uh when we expand we don't want
128:05 - to expand inside of directions where a
128:09 - player has already played for example
128:10 - right so we have to mask out our policy
128:13 - on these illegal positions so we get the
128:17 - valid moves first of all by calling
128:19 - self.game dot get valid moves
128:22 - and then for the state we can just use
128:24 - node.state so the state of our Leaf node
128:27 - right here
128:28 - and
128:30 - um
128:31 - then we want to multiply our policy with
128:34 - these valid moves
128:35 - [Music]
128:38 - so now all the illegal moves will have a
128:41 - policy equal to a zero which is great
128:44 - but again we have to rescale our
128:47 - policies so that we have percentages
128:50 - and we do this by just dividing with its
128:53 - own sum so that the sum of our policy
128:55 - will turn to one
128:58 - um and yeah this will give us this nice
129:00 - distribution we want so let's actually
129:02 - also change the value and we want to
129:05 - just get the float uh
129:07 - from our one neuron of our value head
129:11 - so we get this by calling value equals
129:13 - value dot item so in pie torch if you
129:17 - call Dot item on a tensor that has only
129:20 - one float you will just get the float as
129:23 - result
129:24 - so that's great so now we have our value
129:26 - and we have our policy so then we want
129:29 - to use this value for back propagation
129:31 - and we want to use this policy for
129:34 - expanding right so let's call
129:36 - node.expand with the policy as input and
129:38 - let's remove this simulation part
129:40 - directly so that we will use this value
129:44 - right here for back propagation later on
129:46 - in case we have reached a leaf node here
129:49 - and we expand right
129:51 - so this is great and
129:54 - now there's some smaller things we can
129:57 - just remove so first of all we can just
130:00 - completely delete the simulation method
130:02 - here
130:03 - and then there are some more things and
130:07 - first of all when we call the expand
130:10 - method we immediately expand in all
130:13 - possible directions because we just have
130:15 - this policy right here and
130:18 - um it doesn't make sense anymore to only
130:20 - expand in One Direction once you had
130:22 - reached a leaf node but rather we expand
130:24 - in all directions immediately
130:26 - so
130:28 - um this also means that we don't need
130:30 - these expandable moves right here
130:32 - anymore
130:34 - um because yeah
130:35 - we will check the expandable moves by
130:37 - calling dot Reddit moves later on
130:40 - so also
130:43 - um we can change this is fully expanded
130:45 - method here and now we can just check
130:48 - whether the land of our children is
130:50 - larger than zero because remember when
130:53 - we expand once we expanded all the
130:55 - directions so if we have one child we
130:59 - have we know that there aren't any more
131:02 - children we could have right
131:04 - so yeah this should be enough here
131:07 - so yeah now this is working but next we
131:10 - have to update
131:12 - um this node.expand method right here
131:15 - so let's do that here so first of all we
131:19 - take this policy yes input
131:22 - and then we want to Loop over our policy
131:24 - right so we can write for action
131:27 - and then prop so just the probability uh
131:30 - at a certain action in enumerate policy
131:36 - and then we can check for each action
131:39 - whether the probability at the given
131:41 - action is larger than zero so let's
131:43 - write this prop it's larger than zero
131:46 - and we can remove these two segments
131:50 - here
131:51 - and if this is the case let us just
131:55 - create this new child right here and
131:59 - let's depart it to the children
132:01 - but remember that we want to use this
132:04 - probability here inside of our UCB
132:07 - formula later on during selection so we
132:10 - want to store this probability inside of
132:13 - our node object and we do this by
132:16 - creating this new prior input here so
132:19 - let's just prior to set prior to Zero by
132:22 - default and let's define price after
132:26 - prior here as prior
132:29 - so the prayer will just be the
132:31 - probability
132:33 - um
132:34 - that was given here when we initiated
132:37 - the child and it is equal to the policy
132:41 - at the given action from the parents
132:43 - perspective
132:45 - when we expanded our parents
132:48 - so
132:50 - um yeah now we have defined this prior
132:51 - right here but we also want to
132:54 - um yeah use it here when we create a new
132:57 - node so let's just give here prop as
133:01 - input for the prior
133:03 - uh at the end like this so great so now
133:08 - we have all of this ready so
133:10 - um next we have to update the UCB
133:14 - formula
133:15 - since Alpha zero uses a different USB
133:18 - formula than just a standard
133:20 - multi-colored research
133:22 - so I just brought an image up right here
133:25 - so actually we remove the math.log and
133:29 - we just wrap the square root around the
133:32 - visit count of our parent then also um
133:36 - we just add a one to the visit count of
133:38 - our child and yeah so so that's
133:41 - basically it
133:43 - and so
133:46 - first of all we have to say that if we
133:49 - call this UCB formula on a child that
133:52 - has a visit count of zero then we can't
133:55 - calculate a q value right so let's just
133:57 - set the Q value to zero in that case as
133:59 - well so let's say if child dot wizard
134:03 - count
134:04 - equals zero then Q value should equal
134:09 - zero then edits
134:12 - I'm sorry Q value should be equal to
134:15 - this formula ratio
134:17 - so because now we don't back propagate
134:20 - immediately on a note that has just been
134:22 - created during expanding so it is
134:25 - actually possible now to call this USB
134:28 - method on here on a chart that hasn't
134:31 - been visited before
134:32 - so yeah this way we change this part
134:35 - right here and then also we want to
134:37 - update this part here below so that's
134:40 - right
134:41 - like of this image here
134:44 - so let's first of all remove this
134:46 - math.log
134:48 - from here and then
134:51 - here we want to add a one the visit
134:54 - count of our child
134:57 - and then we also want to multiply
134:59 - everything up with the prior of our
135:02 - child right so that we also use the
135:05 - policy when we um
135:08 - yeah when we select
135:10 - um downwards of the tree
135:12 - so this should be working now
135:15 - so let's see see if we got everything
135:18 - ready so yeah I think that's it so let's
135:21 - just run this right here so first of all
135:24 - I will just update C to be equal to 2.
135:27 - and then we can create a model right
135:30 - here so let's write model equals resnet
135:33 - and then for the number for the game we
135:37 - can just use sticker tool
135:39 - for the number of rest blocks let's just
135:41 - say four and for the number of our
135:43 - hidden planes we can just set that to
135:46 - 64. then you know what also evaluate our
135:49 - model right here so
135:51 - currently this model has just been
135:53 - initiated randomly right so we can't
135:56 - expect too much but let's try this out
136:01 - so oh sorry uh we're still missing the
136:04 - model that he has argument inside of
136:06 - what yes so yeah if it's
136:08 - [Music]
136:12 - so nice so now we see that with our
136:16 - updated MCTS
136:17 - um we played right here and you know
136:20 - what let's say
136:21 - um we would like to play on one
136:26 - so yeah the MCTS played here and let's
136:30 - okay uh I don't know it's just on this
136:34 - Edge right here and the MCT has played
136:36 - here so negative one and has won the
136:38 - game so now we have our updated
136:40 - multi-colored research ready and next we
136:43 - can actually start to build this main
136:46 - Alpha zero algorithm so in order to do
136:49 - that let us just create a iPhone 0 class
136:52 - and I will Define that just below our
136:54 - updated MCTS
136:56 - sorry let's remove that here
136:58 - and Define it
137:01 - down below here
137:03 - so let's write class Alpha zero
137:07 - and for the image
137:10 - we'd like to take the model
137:13 - then also an optimizer so this will just
137:18 - be a pie torch Optimizer like Adam we
137:20 - will use for training
137:21 - and then also a game and some additional
137:24 - arguments yes input
137:27 - and next we can Define all of these
137:30 - things right here so let's write self
137:31 - dot model
137:33 - its model self dot optimizer
137:37 - liquids optimizer
137:39 - and then write safe.game equals game and
137:42 - save dot arcs should equal access way
137:45 - and at the end we can now also Define
137:48 - multi-colored research object inside of
137:51 - our fs0 class so let's write safe.cgs
137:55 - should be equal to MCTS
137:57 - and when we initiate it when we initiate
138:00 - our multi-colored research we want to
138:01 - pass in the game the args and the model
138:04 - so let's just run it game arcs model
138:07 - just like this
138:10 - so yeah let's just be standard in it of
138:13 - our fs0 class and then next we want to
138:16 - define the method inside and from the
138:19 - overview you might remember that we have
138:21 - these two components which is the save
138:23 - play part and then the training part
138:26 - and outside we just have this Alpha zero
138:29 - wrapper right here so let's create our
138:32 - self-play method
138:34 - this
138:36 - and let's just pass that right now and
138:39 - let's also create our Training Method
138:41 - like this
138:43 - and let's pass that here as well so next
138:47 - we can have this learn method
138:50 - and this will be the main method we will
138:52 - call when we actually want to start this
138:55 - cycle of continuous learning where we
138:58 - run sort of play get data use this data
139:01 - for training and then optimize our model
139:03 - and then close if play again with the
139:06 - model that is much smarter right so for
139:10 - this cycle we want to first of all Loop
139:12 - over all of the iterations we have so
139:15 - let's say for iteration in safe.args of
139:18 - num iterations
139:21 - [Music]
139:24 - and then for each iteration we want to
139:27 - create this memory class so just the
139:30 - training data essentially for one cycle
139:33 - so let's write our memory list here
139:36 - let's uh so let's define memory right
139:38 - here
139:39 - and then we want to Loop over all of our
139:42 - self-play games
139:44 - so let's write four set play iteration
139:50 - in safe.args of
139:54 - save or in range sorry we need to
139:58 - refinement shift
140:01 - [Music]
140:04 - um so in range of safe.args
140:09 - of num
140:11 - save play iterations
140:14 - like this
140:17 - and then for each iteration we would
140:19 - like to extend the data we just got out
140:25 - of our self-play method and we want to
140:28 - extend this to this memory list right
140:31 - here so let's write memory plus equals
140:35 - save dot save play
140:38 - um just like this so let me extend the
140:41 - new memory and actually we'd like to
140:44 - change the mode of our model so that we
140:47 - so that the model is now an evalu mode
140:50 - so that we don't have these batch knobs
140:53 - for example during safe place
140:54 - [Music]
140:55 - and now we want to move on to the
140:58 - training part so first of all we can
140:59 - write safe.od.train like this
141:02 - [Music]
141:04 - and then we can save for Epoch in range
141:08 - of safe.args
141:10 - num epochs
141:14 - and yeah for each epochs we like to call
141:16 - this train method right here and we like
141:19 - to give the memory as input so let's
141:22 - write safe.train of memory like this
141:25 - so let's also add memory here as an
141:28 - argument
141:31 - so that's great and at the end of an
141:33 - iteration we like to store the weights
141:36 - of our model right so let's write
141:38 - torch.save of
141:41 - safe.static
141:44 - like this and for the path we can just
141:47 - write model
141:48 - and then the iteration
141:50 - [Music]
141:53 - with instead of an F string and write
141:56 - dot PT
141:58 - for the file ending
142:00 - so let's also save the static of our
142:02 - Optimizer so let's write torch.save off
142:06 - safe.optimizer
142:08 - dot State picked
142:10 - then for the path again we can write
142:14 - Optimizer and then iteration
142:17 - [Music]
142:19 - and take dot pts file ending here and we
142:22 - have to turn our string into an F string
142:25 - yeah so this is great and
142:28 - um now that we have gotten our main Loop
142:30 - ready
142:31 - um we would like to focus on this
142:33 - self-play method right here
142:35 - so
142:37 - inside uh we again have to define a new
142:40 - memory so this will just be uh yeah the
142:44 - memory inside of one own self-play game
142:48 - and then we have to define a plane at
142:51 - the beginning so let's say player should
142:53 - equal 1 at the start and then we also
142:56 - have to create an initial State and we
142:58 - get the state by calling
143:00 - save.game dot get initial State like
143:04 - this
143:06 - and then we
143:08 - want to have this true Loop and inside
143:11 - of this Loop we want to first of all
143:14 - call the multi-colored research based on
143:17 - the state we have then we want to get
143:19 - yeah these action props as result
143:22 - and next we like to sample an action
143:25 - from this distribution of action
143:27 - probabilities
143:28 - and yeah so after we have sent it an
143:31 - action we want to use this action to
143:34 - play essentially so we will get a new
143:37 - state
143:38 - um yeah by calling self.game dot get
143:40 - next State uh based on the Old State and
143:43 - then once we have this new state we want
143:45 - to check if this state is a terminal one
143:48 - so if the game has ended and if indeed
143:51 - the game is ended already then we want
143:54 - to first of all return all of this data
143:58 - to the memory here and here the data
144:01 - should be structured in this Tuple form
144:03 - where for each instance we have a given
144:07 - State we have the action probabilities
144:10 - of our MCTS and then we have the final
144:12 - outcome right so whether the player who
144:15 - played at the instance also was the
144:18 - player that won the game at the end or
144:20 - the player that lost the game or the
144:22 - player that he didn't draw right so this
144:25 - should be incorporated in the final
144:27 - outcome
144:28 - so so we can write y true like this
144:33 - and here first of all we have to
144:35 - remember that when we call mcts.search
144:38 - we always like to be player one so we
144:41 - first of all need to get this neutral
144:43 - State and we can get this by recording
144:47 - safe.game.change perspective so let's
144:50 - write it right here
144:54 - and for the state we just like to take
144:58 - this state right here and for the player
145:00 - we just want to take the player above
145:04 - and yeah so that's great so now that we
145:06 - have our neutral set we can get our
145:08 - action props back from this MCTS search
145:11 - right here
145:13 - and we call it save.mcts.search
145:18 - and we take this neutral State as input
145:23 - so now that we have our neutral State
145:25 - and our action props we can store this
145:28 - information instead of our memory so
145:30 - that we can later use this information
145:32 - to get gather this training data so
145:36 - first of all we can write self.memory
145:38 - dot append
145:40 - then we append the structure right here
145:42 - of the neutral State and the action
145:45 - props
145:47 - and then also the player
145:50 - so yeah now we want to sample an action
145:54 - of the action props right and we can do
145:57 - this by calling NP Dot random.choice
146:02 - and for the number of different options
146:04 - we can use the action size of our game
146:07 - so in this case of tic-tac-toe that is
146:10 - equal to nine so let's say
146:13 - save.game.action size
146:15 - and then for the probabilities we use
146:18 - for sampling we just want to use these
146:21 - actual props right here we got from our
146:23 - MCTS
146:25 - so like this
146:28 - so now that we have sampled in action we
146:30 - like to play based on this action rate
146:32 - so we can say stage should equal safe
146:36 - Dot
146:37 - game dot get next
146:39 - state
146:41 - and we take give this Old State the
146:44 - action and we play ideas input
146:47 - and now that we have gotten our update
146:50 - State we want to check if the state is
146:53 - terminate or not so we write value this
146:56 - terminal
146:57 - equality save Dot Game dot get
147:01 - value and terminated
147:05 - [Music]
147:06 - and we give the updated State and the
147:09 - action we use to get this updated State
147:11 - yes input
147:13 - and then we can say if this terminate is
147:16 - true so if it's terminal
147:19 - and then we want to return this data
147:22 - right so first of all we can write
147:24 - return memory should be equal to this
147:27 - new list right here
147:29 - and then we want to Loop over all of
147:31 - these instances inside of our memory or
147:34 - sorry let's not write set of memory
147:36 - about rather
147:37 - memory just like this because we want to
147:39 - use this mirror right here
147:42 - so we can write four
147:44 - hist
147:46 - portrait state
147:48 - Test action props
147:51 - and then his player
147:55 - in
147:56 - memory
147:59 - and for each instance we now want to get
148:02 - the final outcome right at the given
148:05 - instance because we need to store that
148:07 - inside of our training data so we can
148:09 - say this
148:10 - outcome
148:12 - and this outcome should be equal to this
148:14 - value right here if his player is also
148:18 - equal to the player
148:20 - um we were when we achieved this value
148:23 - right so if we were player positive one
148:27 - when we played here and have indeed won
148:32 - the game
148:33 - and then we will receive a value as
148:36 - positive one as well and we want to set
148:38 - the his outcome for all instances in
148:41 - which player positive one played to
148:43 - positive one and we want to set the his
148:46 - outcome to all instances in which player
148:49 - negative one played to negative one
148:51 - right so this outcome should be equal to
148:53 - value
148:55 - if
148:56 - his player is equal to player
149:00 - event edits we just want to receive the
149:03 - negative value right
149:06 - um and now we can
149:08 - um depend this information to our return
149:11 - memory
149:12 - so let's write return
149:15 - memory dot append
149:19 - and yeah again we need
149:23 - to append a tuple so I have brackets
149:25 - inside of these brackets right here
149:28 - and we want to attend the neutral state
149:32 - but remember that we like to encode the
149:34 - state when we give it as input to the
149:36 - model so we can already encode it right
149:38 - here so let's write
149:40 - save.game dot get encoded state of this
149:45 - neutral state
149:46 - [Music]
149:47 - for the state and then just the action
149:49 - props from our MCTS distribution
149:54 - and then just the hist outcome we cut
149:59 - here
150:01 - [Music]
150:03 - so just like this
150:05 - and sorry we went to append this to our
150:08 - original memory read but not to return
150:10 - immediately but after we have looked
150:13 - through all of this we now can actually
150:16 - return this written memory
150:19 - just like this and all in all other
150:21 - cases if we haven't reached the
150:23 - Terminator note yet then we want to flip
150:25 - this player around so that we are a
150:28 - player negative one now uh if we were
150:30 - player positive one previously so let's
150:33 - write play
150:34 - articles.game dot get component of
150:38 - player like this
150:39 - [Music]
150:41 - so there's still some minor things I
150:44 - would like to update right here so first
150:46 - of all let's make this code more General
150:48 - by changing negative value right here to
150:52 - the value as perceived by the opponent
150:56 - depending on the game we have uh inside
150:59 - of our iPhone 0 implementation so that
151:02 - we could also
151:04 - um yeah run this in games where we have
151:07 - only one player right so let's write
151:10 - save Dot game dot gets opponent value of
151:16 - the you like this so let's just more
151:18 - General if we um
151:21 - quote this method right here
151:24 - so get opponent value
151:26 - and the next thing would be that we want
151:30 - to visualize these Loops right here and
151:33 - the way we can do this is by having
151:35 - these progress paths and we can get them
151:38 - by using the tqdm package
151:43 - so let's write import TQ
151:47 - DM dot notebook
151:50 - and you know what from tqdm.notbook we
151:53 - want to import a t range like this
151:57 - so then we can just replace these range
152:01 - chords below
152:03 - with a t range so
152:07 - just a small difference right here and
152:10 - now we want to check that this iPhone 0
152:13 - implementation is working as we have
152:16 - built it so far
152:18 - so um
152:20 - we want to create an instance of alpha 0
152:23 - and um we like to do that we need a
152:25 - model an Optimizer again and some
152:28 - arguments
152:30 - so we get the model first of all by
152:33 - building a resnet but initially let's
152:35 - just create an instance of tic tac toe
152:38 - let's write tic-tac-toe liquids click
152:40 - attack toe like this
152:43 - and then we can write model that equals
152:45 - rest net
152:46 - and for the game we want to use the
152:48 - stick-tac-toe instance which is created
152:52 - and for the number of rest blocks we can
152:54 - just say 4 and for the hidden dim we can
152:57 - just say 64.
152:59 - and now we want to Define an Optimizer
153:02 - right so let's use the atom Optimizer as
153:05 - an built-in pie torch so let's write
153:09 - Optimizer equates torch.optum dot Adam
153:13 - and then for the parameters we want to
153:15 - use the parameters of our models such as
153:18 - model.parameters
153:20 - [Music]
153:21 - and for the learning rate we can just
153:22 - use
153:23 - 0.001 just a standard language for now
153:27 - and
153:29 - um
153:30 - next we also need to refine these
153:33 - arguments right here
153:35 - and we can
153:37 - do this by creating just another
153:39 - dictionary
153:40 - and for the exploration constant we can
153:44 - just choose two again where the number
153:47 - of searches we can choose 60
153:51 - and then for the number of iterations on
153:55 - the highest level we can just choose
153:57 - three for now
153:59 - and then for the number of self-play
154:02 - iterations
154:04 - um so the number of safe play games we
154:06 - play for each iteration
154:09 - um
154:12 - durations we can set that to 500 for now
154:16 - and the number of epochs can just be
154:19 - equal to four correctly
154:22 - you know what let's also lower this
154:25 - amount to 10 for now so that we can see
154:27 - that we can save our models
154:30 - and let's just run that right here
154:32 - oh sorry and now we want to create this
154:35 - instance of alpha zero
154:38 - and this should be equal to the cipher0
154:41 - class
154:42 - and the input should now be this model
154:46 - we've created the optimizer we have
154:47 - created and the game and args we have
154:49 - created so it's right model
154:53 - optimizer and for the game we can write
154:56 - tic-tac-toe
154:57 - and for the arguments we can use the
155:00 - arcs here then we can run Alpha 0.11
155:03 - like this
155:05 - let's see if this works so nice so now
155:08 - we get this a good looking bar we can
155:10 - see that we are running these safe
155:12 - player games
155:13 - great so now we can actually implement
155:15 - the Training Method inside of our Alpha
155:18 - 0 implementation so let's just change
155:21 - this method right here and the first
155:24 - thing we always want to do when we call
155:26 - this method is that we want to shuffle
155:28 - our training data so that we don't get
155:31 - the same batches all the time
155:33 - so we can do this by calling
155:35 - randle.shuffle here of the memory
155:39 - and if we want to use the use this we
155:42 - have to import random here above
155:47 - just like this
155:48 - and nice so
155:52 - now we can actually continue
155:54 - um with this implementation right here
155:56 - so next we want to Loop over all of our
156:00 - memory and we want to Loop over it in
156:03 - the batches right so that we basically
156:07 - have a batch index and for each batch
156:11 - index we can then sample a whole batch
156:13 - of different samples
156:15 - and then use these for training so the
156:19 - way we do this is by writing for batch
156:22 - index
156:23 - then range and here we want to start at
156:26 - zero we want to end at the length of our
156:28 - memory
156:29 - and then we want to step in the size of
156:33 - safe.args.batch size
156:35 - [Music]
156:39 - and now we want to take a sample from
156:42 - our memory right and we can get this
156:44 - sampled by
156:45 - uh calling the memory at the batch index
156:50 - at the beginning
156:51 - and we want to call it until batch index
156:55 - Plus safe.args
156:57 - dot batch size
157:00 - but just remember that we don't want to
157:03 - call it on any batch index that might be
157:09 - higher than our Len of the memory
157:12 - so we have to be careful here and we can
157:14 - check that we don't exceed the line of
157:17 - our memory by calling
157:19 - Min of Len of memory
157:23 - minus one
157:25 - then the batch index plus self dot arcs
157:29 - of batch size right here
157:32 - so
157:34 - this way we won't ever exceed this limit
157:37 - correctly here
157:40 - so now we want to get the states the
157:43 - MCTS props and the final rewards from
157:49 - our sample
157:50 - and we can do this by writing State and
157:54 - policy targets for the mcds
157:56 - distributions
157:58 - and value targets for the final reward
158:04 - we get from Save play
158:07 - and we get this from our sample by
158:10 - calling the zip method
158:13 - so the way
158:16 - um this works is that we basically
158:17 - transpose our sample So currently we
158:21 - have just this list of tupils and each
158:25 - Tupac contains a state a policy targets
158:27 - and a value
158:28 - but by applying this asterisk here and
158:31 - then zipping it we actually get lists of
158:34 - States lists of policy targets and list
158:37 - of value targets so we transpose this
158:40 - sample around basically
158:44 - so now the state is a list full of MP
158:47 - arrays the policy targets is a list full
158:50 - of MP arrays and value targets is a list
158:52 - of MP arrays but it's much more helpful
158:55 - if we actually have all of these things
158:57 - as NP arrays immediately
158:59 - so let's change them by saying value and
159:03 - policy targets and value targets and we
159:07 - should be
159:08 - equal to NP dot array
159:12 - of State
159:14 - and mp.array of policy targets
159:20 - and NP dot array of value targets like
159:23 - this
159:25 - but remember for the value targets that
159:28 - currently we only have this numpy Rand
159:32 - inside we immediately have all of our
159:34 - float values but it's much more helpful
159:38 - if each of these value actually is in
159:40 - its own sub array later on if we compare
159:43 - to the output of our model so we can
159:46 - change this by calling reshape here then
159:48 - we say negative one for the batch axis
159:50 - and then we just say one so that each
159:54 - value will be in its own sub array
159:56 - essentially
159:57 - [Music]
159:58 - so this is great and now we can turn all
160:00 - of these three things into tensors so
160:04 - first of all we can write state in which
160:06 - liquid storage dot tensor of state and
160:09 - for the D type we can just say
160:11 - torch Dot float32
160:14 - even though
160:16 - um
160:17 - yeah write this encoded State might be
160:20 - um
160:20 - and just adjust right now nothing float
160:23 - so that's nice to change the detail here
160:26 - and for the policy targets
160:30 - we want to
160:32 - also turn this into a tensor and change
160:35 - the detail to flow 32.
160:38 - um
160:39 - forge.tensor like this of policy targets
160:41 - here
160:45 - yeah and then the D type should be equal
160:48 - to torch
160:50 - dot float
160:51 - 32 like this
160:54 - and then we have our value targets
160:56 - [Music]
160:59 - and here we also want to turn this into
161:02 - a tensor
161:05 - and have value targets as the old input
161:09 - and the D type again should be equal to
161:13 - torch.float32
161:15 - so great
161:17 - um so now we have all of our tensors
161:19 - ready and the next step is to get the
161:22 - out policy and the out value
161:25 - from our model by letting it predict the
161:29 - state we have here
161:32 - so let's write out policy
161:35 - out value equals self dot model
161:39 - of
161:41 - the state right here
161:42 - [Music]
161:44 - and then now we want to have a loss for
161:47 - the policy and a loss for the value and
161:50 - we can first of all Define the policy
161:52 - loss
161:53 - and you might remember that this is a
161:56 - multi-target cross-entropy loss so here
161:59 - we can write F dot cross
162:01 - entropy like this
162:04 - and then first of all we have our out
162:06 - policy and then we have our policy
162:08 - targets
162:11 - and yeah
162:12 - so next we have our value loss
162:15 - and here we use the mean squared error
162:18 - so we can write value
162:20 - [Music]
162:22 - loss equals f
162:24 - dot MSE plus like this and then first of
162:28 - all we have our out value then we have
162:30 - our value
162:31 - targets
162:34 - and now we want to take the sum of both
162:36 - of these losses
162:38 - to finally get just one loss so we can
162:41 - write loss equals policy loss
162:43 - plus value loss right here
162:48 - so now we want to minimize this loss by
162:51 - back propagating
162:53 - so first of all we can write
162:56 - optimizer.0 grid
162:58 - [Music]
163:02 - then we want to call last stop equals
163:07 - and then we want to call it Optimizer
163:10 - dot step so this way pytorch does all
163:12 - the big propagation for us
163:15 - and actually optimizes this model right
163:19 - here
163:20 - so this way we now have a better model
163:23 - and then we can use this during
163:25 - self-play so let's test this out
163:29 - so here I will just uh quite this again
163:32 - you know what let's actually set number
163:35 - iterations to 500 now and I will now
163:38 - test this for the number of iterations
163:41 - that we have here
163:43 - and then afterwards we can finally see
163:47 - how this model has learned and yeah feel
163:50 - free to train this way but you don't
163:51 - have to and we will make it more
163:53 - efficient later on so we can also just
163:56 - test it here on my machine
163:58 - okay sorry so I forgot to define a batch
164:00 - size right here so let us just pick 64
164:03 - as our default batch size and then we
164:05 - can run this cell right here and we get
164:07 - these nice progress bars during training
164:10 - and I have just trained uh yeah the cell
164:13 - right here and it took roughly 50
164:15 - minutes just using the CPU of my machine
164:19 - so now we actually have trained for a
164:21 - total number of three iterations and
164:24 - remember that we save our model at each
164:27 - iteration because of this expression
164:30 - right here so now let us actually check
164:33 - what the neural network we have trained
164:35 - understands of the game and we can do
164:37 - this by moving up to this cell right
164:40 - here so this is just where we tested our
164:42 - randomly initiated model
164:45 - so here when we Define our model let us
164:48 - actually load the static
164:51 - so let's write model to upload static
164:55 - inside we can write torch.load
164:58 - and for the path we can write model and
165:00 - then two since we want to check the last
165:03 - iteration and then PT for the file
165:05 - ending and you know what let's also put
165:08 - our model in the valley mode so let's
165:10 - just run this right here
165:12 - and then for this state right here we
165:16 - get this distribution of where to play
165:19 - so our model tells us that either we
165:22 - should play in the middle right here is
165:24 - player positive one or we should play on
165:26 - this field right here and the corner is
165:28 - player positive one
165:30 - and you know what uh let's also test it
165:33 - for this state right here
165:36 - so
165:38 - um two and fours player negative form
165:44 - and then
165:46 - [Music]
165:49 - play a positive one
165:51 - we have played on the fields of eight of
165:54 - six
165:56 - [Music]
166:00 - so first of all now you can see that we
166:03 - have copied the spot state from the
166:05 - image right here
166:07 - and yeah then we also encode it and then
166:10 - we get this distribution of where to
166:12 - play so now the neural network correctly
166:14 - tells us that we should play on position
166:17 - 7 because we would win here
166:19 - and yeah for the value we get a result
166:23 - that is close to positive one because we
166:27 - would just win if we play here so we
166:29 - have a high estimation of this position
166:32 - right here as a player positive one
166:34 - so yeah and this is even better than the
166:37 - distribution we have right here so
166:39 - remember this is just what this scenario
166:42 - network is a mathematical function put
166:45 - out and we never did a Monte Carlo
166:46 - research or anything when we got this
166:49 - result here so we can perfectly see that
166:51 - this mathematical function so this
166:54 - neural network actually internalized the
166:56 - multi-colored research by playing with
166:58 - itself and now we can just call this
167:01 - neural network with a given State and we
167:03 - get this neat distribution of where to
167:05 - play and also this nice float here
167:08 - telling us whether the given state is a
167:12 - good one or not for us as a player
167:15 - great so now we can apply some tweaks to
167:18 - our Alpha zero algorithm and the first
167:21 - week we want to yeah add right here is
167:24 - that we want to also be able to use GPU
167:26 - as a device
167:28 - since that might be way faster for
167:30 - training depending on your setup
167:33 - so we can add GPU support by declaring a
167:36 - device right here
167:38 - and for the device we can say that it
167:41 - should equal to torch.device
167:44 - and the device should be Cuda so yeah
167:48 - Nvidia GPU if torch.cuda dot is
167:52 - available
167:53 - [Music]
167:55 - and then edits we can just set it to CPU
167:59 - like it was initially
168:02 - and yeah we want to
168:05 - then use this device and give it to our
168:09 - model as an argument
168:11 - [Music]
168:12 - so this also means that we have to
168:16 - edit right here to our resnet class
168:21 - so let's set device right here
168:25 - and then we will first of all
168:29 - add it as a variable here so self.device
168:33 - should equal device
168:35 - and then we also want to set our model
168:39 - to the device it was given so let's
168:43 - write save.2 device right here so that
168:47 - we turn our model to GPU for example if
168:50 - we have Cuda support on our machine
168:53 - yeah so that's the first part and now we
168:57 - also want to use this device first of
169:00 - all during set play and then second of
169:02 - all during training right so let's move
169:05 - to the safe play part right here
169:08 - and
169:10 - we use our model during self-play when
169:12 - we get these action props right here
169:15 - during our MCTS search because yeah we
169:19 - call our model right here
169:22 - so yeah when we get this policy and this
169:25 - value right here and yeah we also want
169:28 - to
169:29 - turn this tensor into a tensor that is
169:32 - on our device we have declared so we can
169:36 - do this by writing device should equal
169:38 - self dot model
169:39 - dot device
169:41 - like this
169:43 - and now we also want to use GPU for uh
169:47 - or we want to add GPU support during
169:50 - training
169:51 - so let's move to the training part right
169:53 - here and we can first of all
169:58 - yeah move this data to our device since
170:00 - we use it here's input to our model
170:04 - let's write device equals
170:06 - search.model.device
170:09 - and then we also use these policy
170:11 - targets and value targets and we compare
170:13 - them to the outputs of our model so we
170:16 - also have to yeah move them to the
170:19 - device of our model so let's write
170:21 - self.device let's write devices with
170:24 - self dot model dot device again right
170:26 - here
170:27 - and also right here down below
170:32 - okay so yeah now we should have GPU
170:35 - support as well so that was fast
170:37 - and now there are several more tweaks
170:41 - so if uh yeah the next week we can apply
170:44 - right here is that we also want to add
170:47 - weight decay since
170:50 - um we have this A2 regularization in our
170:54 - loss
170:55 - um for f for zero um like you can see on
170:58 - this image right here so we want to
171:01 - write weight decay
171:03 - and we can just set that to
171:05 - 0.001 like this so this would be fine
171:09 - and yeah so now we have way Decay and
171:12 - GPU support added to Alpha zero and next
171:16 - we also want to add a so-called
171:19 - temperature
171:20 - so remember when we
171:24 - sample an action from these action props
171:28 - right right here currently we just use
171:31 - the visit count distribution of the
171:33 - children of our root node as the
171:35 - probabilities and then we take these
171:38 - probabilities for sampling right
171:40 - but actually we want to be more flexible
171:43 - because sometimes we would rather like
171:46 - to explore more and so also uh you know
171:50 - sometimes take a visit count uh take
171:53 - children or actions more often that
171:56 - haven't been visited that often
171:58 - and
171:59 - um sometimes we also want to exploit
172:02 - more so so we want to tend to always
172:04 - just pick the actions that look the most
172:08 - promising to us
172:10 - so the way we add this flexibility is
172:13 - that we
172:14 - get some temperature action props so we
172:18 - basically tweak this distribution right
172:22 - here and then we will use the Tweet
172:23 - distribution as the probabilities when
172:26 - we sample an action
172:28 - so let's write temperature action props
172:32 - like this
172:33 - and we get these by
172:36 - first of all
172:38 - using the old action props
172:41 - then we want to power them
172:44 - by 9 divided by self dot arcs dot
172:47 - temperature
172:51 - select this
172:53 - and now we also have to Define
172:55 - temperature in our arcs right here
172:57 - let's write temperature
173:00 - and set that to 1.25 in this case so now
173:04 - we have declared this temperature right
173:05 - here and actually what this does uh in
173:09 - the case for a temperature that is
173:11 - larger than one you can see that
173:14 - you have the
173:16 - number we use for the power actually
173:19 - gets much smaller and what this then
173:22 - does as a result is that we squish all
173:25 - of our
173:26 - probabilities much closer together so
173:30 - that we actually do more of exploration
173:33 - because
173:34 - we higher our temperature actually gets
173:37 - to Infinity the more it would be as if
173:40 - we would just take completely random uh
173:43 - actions sampled from just this random
173:45 - distribution
173:47 - and on the other hand if our temperature
173:49 - would move down we can see that this
173:52 - number would actually move up
173:54 - so if our temperature would turn to zero
173:58 - for example or close to zero and not
174:00 - zero directly then it would be as if we
174:03 - would take
174:05 - just the arc Max of our distribution
174:09 - right here so we want to do exploitation
174:11 - and not exploration right so this
174:15 - temperature right here basically gives
174:16 - us the flexibility to decide whether we
174:19 - want to rather take a more random
174:22 - actions that do exploration or whether
174:26 - we just want to
174:27 - exploit and take the actions that look
174:30 - most promising at the current time
174:33 - so yeah this is great and now we also
174:37 - have uh yeah our final tweak we want to
174:40 - add to Alpha zero and that is that we
174:43 - also want to apply Some Noise to the
174:47 - policy that was given for our root node
174:50 - at the beginning of a Monte Carlo
174:52 - research
174:54 - so when we
174:57 - um
174:58 - when we give our root note a policy we
175:01 - basically want to add some noise so that
175:03 - we also explore more and move in some
175:07 - directions that maybe we haven't checked
175:10 - before so that we also make sure that we
175:13 - don't miss on any possible promising
175:16 - actions
175:18 - so the way we add this noise
175:21 - to our root note is that we first of all
175:26 - have to
175:27 - um yeah add these statements uh to the
175:30 - top right here as well so that we
175:33 - um also add the policy
175:35 - at the beginning and basically separate
175:39 - the from the steps right here
175:41 - so first of all
175:44 - we should
175:46 - get a policy and the value
175:48 - by calling save.moded
175:51 - and then here inside we have
175:53 - torch.tensor
175:55 - then we have safe.game dot get
175:59 - encoded State sorry
176:01 - [Music]
176:03 - and then for the state we can just take
176:05 - the state
176:07 - um for root but this is equal to this
176:10 - state right here so let's just save this
176:11 - one
176:13 - and for the device again we want to use
176:15 - the device of our model so that we could
176:17 - use GPU
176:19 - [Music]
176:21 - um
176:22 - so self dot model.device right here
176:25 - and then again we have to unsqueeze at
176:27 - the end so that we have a axis for our
176:30 - batch
176:31 - [Music]
176:33 - so this great
176:35 - um so now we get this policy at this
176:37 - value and actually let's just turn this
176:39 - value into a blank variable because we
176:42 - don't need it at the top so this should
176:45 - be more readable now
176:47 - and now we want to again tweak our
176:49 - policies we have a tab right here
176:52 - so let's first of all use soft Max right
176:56 - here
176:57 - so policy should be great to torch.soft
176:59 - Max
177:00 - of policy
177:02 - for the access we want to use the first
177:04 - axis
177:06 - and so that we don't use the soft mix on
177:09 - the batch axis
177:11 - then again we have to squeeze at the end
177:14 - uh turn our tensor to a CPU
177:18 - and then just get the numpy array out of
177:21 - it
177:22 - so now again we want to get some valid
177:27 - moves and then multiply our policy with
177:30 - them
177:31 - so that we mask out the illegal moves so
177:34 - the lid moves should equal to save.game
177:36 - dot get valid moves
177:39 - off state right here
177:42 - and then policy should be multiplied
177:44 - with these
177:45 - [Music]
177:48 - and then again we can now add some noise
177:52 - right here so after that we would just
177:55 - again turn our policy to this
177:57 - distribution of probability so we want
177:59 - to divide it with its own sum
178:02 - [Music]
178:05 - so now here we actually want to add this
178:09 - noise to our policy so that we yeah do
178:12 - some more exploration during a
178:14 - multi-colored research
178:16 - so the way we can add this noise is
178:19 - shown here on this image
178:21 - so first of all this should be the
178:24 - updated policy for our root node
178:28 - and then we get this updated policy by
178:31 - first of all multiplying our old policy
178:35 - with this coefficient right here and
178:37 - this coefficient is just a number
178:39 - smaller than one so basically
178:43 - we lower all of our policy values uh in
178:46 - selfie distribution and then we add
178:50 - um
178:51 - the basically opposite coefficient so
178:54 - also number smaller than one
178:57 - um
178:58 - multiply it with this noise right here
179:01 - and yeah this should just be our updated
179:04 - policy
179:05 - so this Epsilon value here is just
179:09 - floats smaller than one this n
179:14 - value right here is a noise called
179:17 - dirichlet random noise
179:19 - and we get this by just using NP dot
179:23 - random.urysled right here
179:25 - and yeah basically for us this will just
179:28 - give this a distribution of random
179:31 - values and we basically will add them to
179:34 - our policy multiplied with this
179:36 - coefficient that is smaller than one
179:39 - so this way we basically change our
179:43 - policy a bit to also incorporate some
179:47 - Randomness so that we then can explore
179:50 - more rather than just always walking
179:52 - down the tree and the way that our model
179:55 - tells us because at the beginning our
179:57 - model doesn't know that much about the
179:59 - game so we also want to make sure that
180:01 - we do some exploration as well
180:04 - so let's add this right here so policy
180:09 - should first of all be 1 minus self dot
180:13 - arcs of
180:15 - delish LED Epsilon
180:17 - [Music]
180:19 - like this and then multiplied with our
180:21 - old policy
180:24 - and then we want to add
180:26 - the second part right here
180:29 - so this should be safe.args of D Rich
180:33 - Let's Epsilon again
180:35 - [Music]
180:36 - and then we want to multiply it with
180:40 - NP Dot random.drichlet
180:45 - and yeah inside here we just
180:49 - [Music]
180:49 - um
180:50 - want to use this Alpha value so give
180:54 - that as input to our MP3 random.tree
180:57 - slat and this Alpha basically changes
181:00 - the way our random distribution looks
181:03 - like and is this based on the number of
181:06 - different actions we have for games so
181:09 - in the case of tic-tac-toe our Alpha is
181:11 - quite high so roughly equal to
181:15 - 0.3 if you just want to use it because
181:18 - that turns out to be looking well
181:21 - but it might also change depending on
181:24 - your environment
181:25 - here then we want to
181:27 - [Music]
181:29 - use hash dot arcs
181:31 - let
181:35 - Alpha now let's actually move that to
181:38 - the next line so it's more readable
181:41 - and
181:44 - then we want to multiply this array
181:49 - with the action size of our game
181:53 - [Music]
181:54 - so
181:57 - [Music]
182:03 - and actually we also have to move this
182:05 - part here below so that we mask out our
182:09 - moves after applying this random noise
182:11 - or not before so
182:14 - let's just copy them over
182:17 - below
182:22 - okay right so now we also added this
182:26 - um
182:27 - yeah noise to our policy and now we can
182:31 - also use this policy we got right here
182:34 - to expand our root node
182:36 - so let's write root dot expand
182:40 - and then
182:42 - um yeah just use this policy right here
182:47 - and now there's just a minor change we
182:50 - can still add
182:51 - and that is that currently when we
182:56 - expand our root node at the beginning
182:59 - [Music]
182:59 - um
183:01 - we won't back propagate immediately so
183:04 - that means that our root node will have
183:07 - children but the visit count of our root
183:10 - node will still be equal to zero so that
183:13 - we means that when we select right here
183:16 - we will call this UCB formula with the
183:19 - visit count of our parent being zero
183:23 - and if this wizard count of our parrot
183:25 - is zero then
183:28 - basically this means
183:30 - that
183:32 - all of this will basically move away
183:34 - because we just turned to zero as well
183:37 - so
183:39 - um this also means that we don't use our
183:41 - prior when we select a children for the
183:44 - child for the first time
183:46 - so actually in order to make this better
183:49 - we should just set the visit count of
183:52 - our root node to 1 at the beginning
183:54 - so you know what let's write visit
183:57 - cones
183:58 - let's set that one right here and
184:01 - this also means that we have to get
184:03 - visit card right here and the default it
184:05 - should just be zero but
184:07 - yeah for the root note it should be one
184:10 - at the beginning so that we um
184:12 - immediately uh use the information of
184:16 - our prior when we select the child at
184:19 - the beginning of our Monte Carlo
184:22 - research
184:23 - okay great so now we can run all of
184:27 - these
184:28 - sets again
184:32 - and then see if this is working so we
184:35 - also have to add a direct light Epsilon
184:37 - here
184:38 - [Music]
184:41 - and yeah this should just be equal to
184:43 - 0.25
184:46 - and then the result
184:48 - Alpha
184:50 - and I just set that to 0.3 like I told
184:54 - you
184:55 - okay so now let's test it out
185:02 - okay so we still have a CPU device
185:04 - somewhere
185:07 - and let's see where that could be
185:10 - oh um I haven't run this right here this
185:13 - is the arrow sorry so here we have to
185:16 - also
185:17 - add a device and for this right here you
185:21 - know what let's just say torch dot
185:23 - device of CPU
185:27 - and then we can run this again
185:30 - okay so perfect now that's running and
185:33 - yeah for
185:35 - this current Setter with tic-tac-toe it
185:38 - shouldn't be that much faster but later
185:40 - on when we use more complex games GPU
185:43 - support will be much nicer
185:45 - and also this
185:48 - these other tweaks should help to make
185:51 - our model more flexible and actually
185:53 - yeah make it easier to always get to a
185:56 - perfect model at the end
185:58 - so nice so I will just train this again
186:01 - feel free to also train it but you don't
186:03 - have to I can I've also uploaded all of
186:07 - the weights and you can find them on the
186:09 - link below in the video description
186:12 - so let's just train that and then we can
186:15 - briefly check it again and then we will
186:16 - also expand our agent so that we can
186:20 - also play the games of uh Connect Four
186:23 - as well to tic-tac-toe
186:25 - so see you soon
186:27 - okay great so now I've trained
186:30 - everything right here and now we also
186:31 - want to check that our model has learned
186:33 - how to play the game of tic-tac-toe so
186:36 - in order to do that right here we have
186:38 - to move up to this again and then you
186:41 - know what you know what let's also
186:43 - Define a device right here so let's
186:45 - write device equates torch.device of
186:48 - Cuda
186:50 - if torch.cuda dot is available right
186:53 - here
186:55 - then else we can just use CPU for our
186:58 - device
186:58 - and then first of all we want to use
187:00 - this device right here when we Define
187:02 - our model
187:04 - and then we also want to use this device
187:06 - for the tensor state that is a stack
187:10 - we'll use
187:11 - for the monitor predict the policy and
187:13 - the value so let's write device
187:16 - right here as well and then we also have
187:20 - to the USB device when we
187:23 - load the static
187:25 - from this file right here and that is
187:29 - when we have a map location
187:31 - that's right that location equates
187:33 - device here as well
187:35 - so let's now let's just run this again
187:37 - and we see that we get this nice
187:40 - distribution and this nice value again
187:42 - so we can also see that our model has
187:45 - learned and by the way um here we aren't
187:47 - even masking out our policy
187:52 - so the model has learned itself that you
187:55 - know these moves aren't that great right
187:58 - here because so you can't even play here
188:01 - because someone else has played without
188:04 - us even yeah
188:06 - masking these illegal moves out so
188:09 - that's nice and now we can move on to
188:12 - also train our model or iPhone 0 on the
188:15 - game of Connect Four great
188:18 - so let's also define a class for the
188:21 - game of Connect 4 but before we do that
188:23 - let's also add this representation
188:26 - method to our tic-tac-toe game
188:29 - so let's write Dev wrapper like this
188:31 - save and then here we can just return
188:34 - the string of tic-tac-toe
188:37 - and this string is yeah also the result
188:40 - we would get when we call this object
188:43 - here instead of a string representation
188:47 - so this means that we can use this
188:49 - information down below
188:52 - when we save our model
188:55 - for each iteration during learning so we
189:00 - can just add
189:01 - game right here
189:04 - so basically we'll also add the name of
189:08 - our game to this model and this
189:11 - Optimizer weight or Statics when we save
189:15 - them so that we won't override
189:18 - our models when we train for the game of
189:21 - Connect Four
189:23 - um yeah
189:24 - so that's great and yeah now that we
189:29 - have done this part let us actually just
189:32 - copy this
189:34 - game over right here
189:37 - [Music]
189:39 - so that's great and now let's use it to
189:43 - define the game of Connect Four
189:46 - and for the game of Connect 4 first of
189:48 - all we have a
189:49 - row count that is equal to six we have a
189:52 - column count that is equal to seven
189:54 - and then the action size is just equal
189:57 - to the column code
189:59 - [Music]
190:01 - and then also let's add a variable for
190:04 - the number of stones you need in a row
190:07 - to win in the game so let's write save
190:09 - dot in a row and let's set that equal to
190:12 - four
190:14 - and for the representation we can change
190:16 - the string to connect four here
190:19 - and yeah the initial State can stay the
190:22 - same because we just want to get this
190:24 - here blank array with row count being
190:27 - the number of rows and column count
190:29 - being the number of columns so that's
190:30 - great and now we have to change this get
190:34 - next State method right here
190:36 - because this action will just be a
190:39 - column uh so we're telling us where to
190:42 - play it and the first step now is that
190:45 - we have to check get a row back
190:48 - um
190:48 - and
190:50 - we get this row by looking at the given
190:53 - column then we want to look at all of
190:55 - the fields inside of this column that
190:58 - are equal to zero so the that are empty
191:02 - currently then we want to basically take
191:05 - the deepest empty field inside of this
191:08 - column because yeah this is where we
191:11 - would play and connect for and yeah this
191:13 - uh would just then be the row we use for
191:17 - playing
191:17 - so we get this by calling np.max
191:21 - since yeah in a number array um you
191:24 - start at zero at the top end then move
191:26 - your values get higher if you move down
191:30 - basically on the rows so we take np.max
191:34 - of np.where off this state at the given
191:38 - column
191:40 - yeah and the state here should
191:42 - equal it to zero right so yeah MP dot
191:46 - where will just again give us V indices
191:48 - of all of the empty fields and then
191:51 - empty.mp.max will have give us the
191:54 - highest in ECU because yeah this is
191:56 - where we would play and Connect Four
191:57 - because yeah the stone will just fall
191:59 - down until
192:01 - um yeah this is uh already non-empty
192:04 - field
192:05 - so now we have a row and technically we
192:08 - also have a column because of this
192:09 - action right here so we can now write
192:11 - state of
192:13 - row of column
192:16 - and set that equal to this player right
192:19 - here we have given here's input
192:23 - and yeah so now that's great and next we
192:26 - want to move over to this method right
192:28 - here so we have this this get valid
192:31 - moves right here and for the git valid
192:34 - moves we just want to check the
192:37 - um yeah row just at the top
192:39 - and then we want to see whether the
192:42 - fields at this row are equal to zero or
192:44 - not and these are all available moves so
192:48 - we can just write state
192:50 - of a zero so at the basically upmost row
192:55 - yeah and then this should be equal to
192:58 - one
192:59 - um
193:00 - if it is valid and yeah otherwise we
193:03 - just have an illegal move on this given
193:07 - action
193:08 - okay and now we want to check for the
193:11 - win and frankly I'm just going to copy
193:14 - this method over right here so
193:18 - basically where we do the same checking
193:21 - asymptote we are where first of all
193:24 - we check for the vertical
193:26 - there are possibility of a win and then
193:29 - for a horizontal one and for these two
193:32 - diagonals based on this action right
193:36 - here that was given as input it's just
193:39 - that compared to tick-tac-toe it's a bit
193:41 - more complex where we actually have to
193:44 - uh walk in both directions and then keep
193:48 - checking whether we
193:50 - have stones in these directions that are
193:53 - equal to our own Player sign also have
193:56 - this get value and terminated method and
193:58 - this can just stay the same for both
194:00 - games good opponent can stay the same
194:03 - get opponent value can stay the same
194:04 - change perspective also stays the same
194:07 - and get encoded State yes also the same
194:11 - rate here and I just noticed a small
194:13 - mistake and that is when we use get next
194:17 - step right here we can't reference
194:18 - column because here we have defined our
194:21 - columns action here from the top so
194:23 - let's just replace column with action
194:24 - like this
194:25 - and now we can check
194:28 - um that this
194:29 - is actually working in practice as well
194:32 - so we can do this by making our code
194:34 - more General and here we just replace
194:38 - tic-tac-toe with game
194:40 - then we use game inside here
194:44 - and we use game inside here as well
194:47 - use game like this
194:50 - again here
194:55 - um and that's okay right here
194:59 - so let's remove this instance of take
195:02 - your toe
195:04 - and yeah this one it's wet
195:10 - and yeah so that should be fine
195:15 - now the should be no more instances of
195:19 - Tic-tac-toe and next let's replace this
195:22 - game take that Toe with the game of
195:23 - Connect Four
195:25 - let's set the number of searches to just
195:28 - 20 so we can validate
195:31 - let's say 100 here
195:34 - and then let's change the size of our
195:36 - model to be equal to
195:39 - 9 for the number of rest blocks and 128
195:42 - for the hidden dim so that we have a
195:44 - larger model than we let the trend
195:46 - Connect Four
195:48 - and now we can check whether this is
195:50 - working or not
195:53 - so first of all we also miss the
195:55 - argument of device
195:57 - so let's add that right here as well so
196:00 - that's right torch Dot
196:02 - device
196:04 - of Cuda
196:06 - if
196:07 - torch.huda dot is
196:09 - available
196:12 - like this and then it's we can just use
196:15 - a CPU
196:18 - and yeah now we want to use this device
196:23 - instead of our model class
196:25 - so let's use it right here
196:27 - and yeah so that should be fine
196:31 - so let's run this again
196:33 - okay perfect so now we have this game of
196:36 - Connect Four let's say that we want to
196:38 - play at the position of
196:41 - four or five sorry
196:43 - and you will still have a problem
196:45 - so delete Epsilon is also not set right
196:49 - here so let's copy these values over
196:54 - um like this
196:58 - but let's add teams like Epsilon to zero
197:01 - so that we don't use noise here
197:05 - um okay so I've played here the model
197:07 - has played here you know what let's play
197:09 - as one
197:11 - that the model has played zero again
197:13 - let's play a two
197:15 - [Music]
197:16 - see well multiplied play that zero again
197:21 - and then we can just play at three like
197:24 - this and we have one
197:26 - so at least we check that the game of
197:30 - Connect Four is working now but yeah
197:33 - obviously our model is terrible because
197:35 - we don't do that many searches right
197:37 - here and also we haven't trained
197:40 - anything yet so obviously our model is
197:42 - garbage but let's change that now so
197:45 - let's also train a model for the game of
197:47 - Connect Four and then again test our
197:49 - results here and we can do this by just
197:52 - applying some minor changes right here
197:54 - so first of all we should also make this
197:56 - cell right here more generous so we
197:58 - write game and then for the game we just
198:01 - will use Connect 4 here
198:05 - and then we also have to set the game to
198:09 - just game instead of the resonate and
198:11 - you know what because Connect 4 is
198:13 - harder to train let's
198:15 - change the number of rest blocks to 9
198:17 - and let's change the num of our hidden
198:20 - Dimensions to 128.
198:23 - and then we also have to use game right
198:25 - here during our Alpha zero initiation
198:28 - and then we can also slightly tweak
198:33 - these hyper parameters here so first of
198:36 - all for the number of searches we want
198:38 - to use 600 then for the number of
198:41 - iterations we can use something like 8
198:43 - maybe for now and then for the batch
198:47 - size we can also increase that to 128
198:51 - so yeah and next we can just run that
198:55 - and this should take a lot longer
198:58 - um but we will also increase the
199:02 - efficiency of it later on so feel free
199:05 - to just use the weights that I've
199:08 - uploaded in the description down below
199:10 - so you don't actually have to train this
199:12 - all yourself since it might at least on
199:15 - my machine correctly take a couple hours
199:18 - so see you soon
199:19 - okay now I've trained Alpha zero on the
199:22 - game of Connect 4 using these arguments
199:24 - right here and frankly I've just trained
199:27 - it for one iteration since even that one
199:30 - iteration took several hours to train
199:33 - right so now we want to increase the
199:35 - efficiency so yeah that will be way
199:38 - faster for us to train also for these
199:40 - rather complex environments
199:42 - but yeah so still it was nice to train
199:46 - for this one iteration because now we
199:48 - can evaluate our model and find out
199:51 - whether we had at least learn something
199:53 - so let's move to this evaluation set
199:56 - right here
199:58 - and now when we create this model we
200:01 - also want to load the state date from
200:03 - the weights that we saved after we have
200:07 - trained for this one iteration here
200:08 - right
200:10 - so here when we create our model in this
200:13 - line right here we can write model dot
200:16 - load State stick like this then inside
200:18 - we can write torch.load and then we want
200:21 - to reference this path right here
200:24 - and so we can write model and for the
200:28 - iteration we will just use zero since we
200:30 - started zero and we have only trend for
200:32 - one iteration and for the game we will
200:35 - use Connect Four and for the file ending
200:37 - PT again
200:38 - and let's also Define a mapped location
200:41 - and set that equal to device so this way
200:44 - if your device would change during
200:47 - training compared with the duration you
200:49 - could still load the state deck like
200:51 - this right here
200:52 - so yeah this might be nice so let's run
200:54 - the cell again and you know what let's
200:57 - just play it six here so yeah now I'm
200:59 - going to play it in the middle like this
201:01 - so let's just play it six again
201:04 - and yeah our model to defend this
201:05 - position so let's just I don't know okay
201:08 - let's see right here
201:10 - and the model bit of this Tower right
201:13 - here
201:13 - let's play Let's see again
201:16 - building up the tower let's just play
201:19 - that zero again and our model finished
201:21 - this Tower right here and first it has
201:23 - won the game so we can still see that
201:26 - our model isn't playing perfectly and
201:28 - obviously I just played in the corners
201:30 - to yeah check for some simple intuition
201:32 - about the game on the modded side yeah
201:35 - but we can still see that it is much
201:37 - better compared to the initial model we
201:39 - had where it only wanted to play instead
201:42 - of this uh column right here right so
201:46 - um that's great
201:47 - um and now we also want to increase the
201:51 - efficiency of our fs0 implementation
201:53 - right
201:54 - so actually now the main part is
201:57 - completely done and you should have this
201:59 - intuitive understanding of how the
202:01 - algorithm works but like I said it would
202:03 - be nicer to increase yeah the speed we
202:07 - got when training and especially the
202:09 - speed rings have played with it
202:12 - so how can we do that and here we want
202:15 - to basically parallelize as much of our
202:18 - Alpha zero implementation as possible
202:20 - because these neural networks are built
202:23 - in a way where we could batch up our
202:25 - states to then get here parallel
202:28 - predictions for the policy and the value
202:32 - especially when we run ourselves play
202:34 - Loop
202:36 - so one maybe way in which you could
202:39 - parallelize this other implementation
202:42 - here is by using a package like Ray that
202:45 - would basically yeah create this threads
202:47 - that are independent from each other and
202:49 - then basically use that to yeah harness
202:53 - all of your GPU power
202:55 - but I'd rather like to build this
202:58 - parallelized version in a more pythonic
203:00 - way like this so just inside of our
203:04 - Jupiter notebook again
203:05 - so let's actually do that
203:08 - um
203:09 - so we will actually
203:12 - basically
203:13 - patch up
203:15 - all of these different states then get
203:18 - policy and value predictions and then
203:20 - distribute them again to our safe play
203:22 - games so that we play several safe play
203:25 - games in parallel and this way we can
203:28 - essentially drastically increase the
203:30 - speed of our Alpha zero implementation
203:32 - so the first thing we want to do here is
203:35 - that we also want to add this argument
203:39 - of non-paralleled games here to our
203:42 - dictionary
203:44 - come on
203:47 - and for now I just set that to 100 so
203:50 - the idea is that we want to play all of
203:53 - these yeah self-play games in parallel
203:55 - and then when we reference our model we
203:59 - will batch up all of our states instead
204:01 - of our MCTS search
204:03 - and this way we can drastically reduce
204:05 - the amount of times that we call our
204:08 - model in the first place
204:09 - and yeah thus on one hand we fully
204:12 - utilize our GPU capacities and on the
204:16 - other hand here we can just decrease the
204:18 - amount of times we call our model and
204:20 - thus also
204:22 - um yeah have a higher speed right
204:25 - so
204:26 - the way we can yeah change
204:30 - this or update our other implementation
204:32 - is the first of all we want to just copy
204:34 - this class here over
204:36 - so I would just create this new class
204:39 - here and I will call this iPhone 0
204:41 - parallel
204:43 - and then we also want to update the
204:45 - Monte Carlo tree search so I will also
204:48 - copy this class here over as well
204:52 - and I will call this MCTS parallel like
204:56 - this so
204:57 - let's write it like this
205:00 - so yeah now we have our parallel F0 and
205:03 - mcds classes
205:04 - and here we want to change the
205:07 - implementation of the save play Method
205:09 - and because of that also the
205:10 - implementation of this search method
205:12 - right here
205:14 - so now that we have got this class
205:17 - um
205:18 - we also want to create a new class
205:21 - and here we basically want to store the
205:24 - some information of a given self-play
205:27 - game
205:28 - so that we yeah can Outsource some
205:30 - information from our Loop when we Loop
205:33 - over all of these given games
205:34 - so let's create class and I will just
205:38 - call this SPG for set play game and then
205:41 - we can also have an inlet here
205:44 - and for the unit we want to pass in a
205:46 - game so click the tool for example or
205:48 - Connect Four
205:49 - and then first of all we want to have a
205:51 - state here so initially this should just
205:54 - be the initial state of our game right
205:56 - so we can set save.day to game dot get
205:59 - initial State like this
206:02 - and then we also want to have a memory
206:04 - inside here and this should just be an
206:07 - empty list at the beginning
206:09 - and yeah then we also want to store a
206:12 - root and that should be none at the
206:14 - beginning and we also want to store a
206:17 - note here this should also be set to
206:19 - none at the beginning
206:20 - and yeah so this is everything we need
206:23 - here for this class right here
206:25 - and yeah so this way we can later store
206:27 - some information here
206:29 - okay so now we want to update this set
206:32 - play Method right here
206:35 - um
206:35 - so first of all we want to call it less
206:38 - often since every time we call this
206:40 - method
206:41 - um
206:42 - we will call it 400
206:44 - potential games that are played in
206:47 - parallel right so here and when we Loop
206:50 - over our numset play iterations
206:53 - we want to divide that with the games we
206:56 - play apparently
206:58 - so save dot args of
207:01 - sorry and I'm parallel
207:04 - games like this and also when we
207:07 - um yeah create this if play game class
207:09 - right here I've just forgot to add those
207:11 - brackets right here so we should just
207:13 - add them yeah okay so yeah inside here
207:17 - first of all
207:18 - um we can remove this state right here
207:20 - since we store our state and we save
207:22 - click games right
207:24 - and then also we can really name this
207:26 - memory to the return memory since this
207:29 - right here won't be the actual memory um
207:32 - in which we store our
207:34 - yes State and action props and Player
207:37 - Two Bits initially but rather this will
207:40 - be the state that we will return at the
207:43 - end so
207:44 - uh the list we currently have here right
207:47 - which we won't need later on
207:49 - okay so then the player can stay just
207:52 - the same because even though we play
207:54 - games in parallel we will always just
207:57 - flip the player after we have looped
207:59 - over all of our games right
208:01 - so this way all of our games in parallel
208:03 - will also always be at the same player
208:06 - right because we always start at player
208:08 - one and then we can just flip for other
208:10 - games pyramid as well
208:13 - Okay so
208:15 - now we also want to store our self-play
208:18 - games here
208:19 - so I would just call this spgs like this
208:23 - and maybe let's just sorry maybe like
208:26 - say play games like this
208:28 - and here we want to have a list of all
208:30 - of our Circle games so one Circle game
208:32 - should be created using this SPG class
208:34 - we have defined below here and inside we
208:38 - want to pass the safe.ame we have here
208:41 - this our Alpha zero class
208:43 - and we want to create the set play game
208:45 - for
208:46 - SPG in rangeof.args
208:51 - and then nump parallel games right
208:55 - so this way we create this list for all
208:57 - of our safe play games
208:59 - and yeah we will basically give them the
209:03 - game in each case okay so now we have
209:07 - these safely games right here and next
209:10 - we want to change this file through
209:12 - um you know expression here because this
209:15 - way we would stop after one game has
209:17 - finished but rather we want to keep
209:20 - running our self-play method until we
209:24 - all of ours have play games are finished
209:26 - so we can check this by saying while the
209:31 - land of SP games
209:33 - is larger than zero so
209:35 - we implemented this way that once a
209:38 - software game is finished we will just
209:40 - remove it from RSP games list because of
209:43 - that we can just check here whether we
209:45 - have any set play games inside of our
209:47 - list right
209:49 - so uh great um so the next step right
209:52 - here would be to get all of these states
209:55 - of our self-play games first of all
209:57 - inside of our while loop
209:59 - so let's get all of the states here
210:02 - and we can get all of the states first
210:05 - of all as a list
210:06 - by saying
210:08 - SPG dot state
210:11 - for SPG in Sp games like this
210:14 - [Music]
210:16 - so yeah this way we just have a list of
210:18 - all of our states that are installed
210:20 - inside of our class
210:22 - and now we can turn these states into
210:24 - just a number array so the best way to
210:27 - do this is just by calling np.stack here
210:29 - of this list of numpy arrays and this
210:33 - way we will basically get one large
210:35 - number array right
210:37 - okay so here we have all of our states
210:40 - and next we also want to get the neutral
210:42 - States
210:43 - so first of all we can just change the
210:47 - naming right here and then here when we
210:50 - change the perspective we just have to
210:52 - write States here instead of state and
210:54 - now we will change the perspective for
210:56 - all of our states basically in just yeah
210:59 - one function call right so this is what
211:02 - we can also increase the efficiency
211:03 - since
211:05 - um yeah changing the perspective works
211:07 - the same way if we have several States
211:08 - because we would just multiply our
211:11 - values with negative one in case that we
211:13 - have player set to negative one and yeah
211:17 - here we we can just do the same right
211:20 - so this is great and
211:23 - now we can also pass these neutral
211:26 - states to our Monte colored research
211:29 - and we have basically finished this part
211:31 - right here so let's now change
211:34 - the Monte Carlo research search method
211:37 - and yeah so first of all we want to now
211:40 - give States yes input so these are the
211:42 - neutral States we've just created
211:45 - and now we can also
211:49 - first of all I change the order so let's
211:52 - just
211:53 - paste this one here
211:56 - below
211:59 - and then we can work on the policy and
212:01 - the value first
212:03 - and so
212:06 - move it like this Okay so
212:08 - yeah first of all we get this policy on
212:10 - this video and like I said we can also
212:12 - do this with all of our batched States
212:14 - so that's where we just get several
212:17 - policies and values back here as a
212:19 - return and that's great
212:21 - but we still have to keep two things in
212:23 - common so first of all we don't need to
212:25 - call it unsqueeze here because now we
212:27 - have a batch so we don't need to create
212:28 - this fake batch access right
212:31 - and then we also um have to update this
212:34 - get encoded State method right here so
212:38 - because the way it is currently
212:40 - implemented
212:41 - um
212:42 - it's only working if we pass it one
212:45 - state at a time
212:46 - because this get encoded State method
212:48 - which yeah create these three planes for
212:52 - the fields in which play negative front
212:54 - plate or these uh the fields that are
212:55 - empty or the fields and which player
212:57 - positive one is played right so it will
212:59 - always first of all give us these three
213:01 - planes then inside of these planes that
213:04 - will give us the other fields right but
213:07 - we want to change the order here because
213:09 - now the first axis we basically want to
213:12 - have here all of our different states
213:14 - and then for each state we would like to
213:16 - have these three planes right so we
213:19 - don't want to have these three planes at
213:20 - the beginning but after our batch access
213:23 - basically right if that makes sense so
213:26 - we have to basically swap the axis of
213:30 - this encoded state right here so that it
213:32 - would also work if we have several
213:34 - States and not just one that we use when
213:37 - we call this method here
213:39 - so first of all we can check whether we
213:40 - have several States or not and we can
213:42 - check this by checking whether the Len
213:45 - of state.shape
213:47 - it's really quick to three right
213:49 - because normally as the land of our of
213:52 - just one set would be two right where we
213:53 - have the rows and then the columns
213:56 - and if we also have a batch axis then
213:58 - the Len would be three because first of
214:00 - all we have a batch then we have the row
214:01 - something we have the columns
214:03 - and yeah so if this is the case then we
214:06 - want to create this new encoded state
214:08 - [Music]
214:09 - like this
214:11 - um
214:12 - and
214:14 - it should basically be equal to NP dot
214:17 - swap access
214:18 - like this
214:20 - and then here we want to pass in the old
214:23 - encoded state
214:25 - then we want to swap the axis 0 and 1
214:27 - right so that we
214:29 - don't so that the basically the new
214:31 - shape should be the size of our batch
214:34 - then three and then the number of rows
214:38 - and then the number of columns right
214:39 - instead of having three then the size of
214:41 - our batch and then V number of rows and
214:43 - number of columns right if that makes
214:45 - sense so basically we swap these X's
214:48 - right here
214:50 - um so that's great and now we can also
214:52 - copy this over to the game of
214:54 - tic-tac-toe as well
214:57 - and yeah oh sorry I just noticed that we
215:00 - had NP dot swap access using an e
215:03 - instead of it I so that's just my
215:05 - mistake and let's update this for both
215:07 - of our games
215:09 - and yeah so this should be working now
215:12 - okay so now we can keep updating
215:16 - this CTS search method
215:20 - so first of all
215:22 - um
215:23 - we now get these policies and these
215:26 - values package return right at the
215:28 - beginning we don't need our values so we
215:29 - just get these policies and we can
215:31 - rename the state here to States
215:34 - and then when we
215:37 - keep processing the policies it's nice
215:40 - that we call the soft Max this way but
215:42 - again we don't want to squeeze since we
215:44 - want to keep our batch axis the way it
215:47 - is currently
215:48 - and then when we add the noise to this
215:51 - policy we also have to make sure that
215:53 - the shape of our noise is equal to the
215:55 - shape of our policy so we can add a size
215:59 - right here
216:00 - and the size should just be equal to
216:04 - policy.shape of zero right so this way
216:07 - we basically create noise for each
216:10 - different state so for each different
216:12 - self-care game essentially
216:14 - okay so this is great and
216:17 - now we have this yeah process policy
216:20 - here ready for us
216:21 - and the next step would be to allocate
216:24 - this policy to all of our self-play
216:28 - games
216:29 - so we first of all have to also add the
216:33 - self-play games here to our MCTS search
216:36 - as the argument
216:37 - and let's also
216:40 - add them here because of that
216:44 - and now we can work with them so
216:48 - here we basically want to Loop
216:50 - um over all of our self-play games
216:53 - so I will just write for I then SPG
216:56 - in enumerate and then say play games
216:59 - like this
217:01 - and yeah inside of each separate game we
217:05 - first of all want to get the valid moves
217:09 - and we can get the valid moves from the
217:12 - state
217:13 - for the given Supply game right and we
217:16 - get this state by looking at the states
217:18 - at the position of I right here since
217:21 - this is the index we get when we
217:23 - enumerate through our self-play games
217:25 - and this is also the same index as the
217:28 - one we have when we basically stack all
217:32 - of our states up together right so we
217:34 - can just
217:35 - uh yeah call it like this so um now
217:39 - first of all we get our valid moves and
217:41 - then we multiply the policy with it but
217:44 - we don't want to multiply all of the
217:46 - policy with it but rather just the
217:49 - policy at our position right
217:51 - so let's also find the policy for the
217:54 - city to play games let's just write SPG
217:56 - dot policy like this this should be also
217:58 - equal to the policy at the position of I
218:02 - so now we can multiply this SPG policy
218:05 - with our valid moves
218:07 - and then again we also want to divide
218:09 - this SPG policy with its own sum
218:12 - so that we at the end have this
218:15 - distribution of percentages right okay
218:18 - so now we create this root right here
218:20 - and first of all we should also give
218:23 - yeah these states at the position I here
218:25 - for the state
218:26 - and then also we have to make sure that
218:30 - this route will be equal to SBG dot root
218:33 - right because we want to store it inside
218:37 - of this class right here
218:40 - okay so now we have this SPG dot root
218:42 - and we can also expand it so let's write
218:44 - SPG dot root dot expand like this
218:47 - and then we will expand it with the safe
218:49 - play game policy
218:51 - now we have here okay so that's why we
218:55 - already have saved a lot of uh yeah
218:57 - a lot of calling our model right because
219:00 - we batch up all of our states here so
219:02 - this is great and now we also want to
219:05 - keep doing this inside of our actual
219:07 - iterations right
219:09 - so here when we basically have our num
219:12 - searches we also want to Loop over our
219:16 - circular games again
219:17 - so let's again write for I then
219:22 - SPG in enumerate SP games like this
219:27 - and then we can continue with this part
219:30 - right here
219:32 - so for each set play game first of all
219:35 - we want to get the note and we get this
219:37 - note by calling spg.root right here and
219:41 - then we can just continue with these
219:43 - things right here
219:44 - and yeah so now
219:48 - um here we would call it our model again
219:51 - right and we want to basically
219:54 - yeah also parallelize this model call
219:57 - right here
219:59 - so the way we can do this is by turning
220:01 - this if statement around so instead of
220:04 - checking whether we haven't reached the
220:07 - terminal node we want to check whether
220:09 - we have reached infected terminal nodes
220:12 - so this is terminal
220:13 - and in this case we will back propagate
220:16 - directly
220:19 - so I would just copy this over right
220:21 - here
220:23 - and in all other cases
220:28 - we want to store the node
220:31 - that we have reached here inside of this
220:33 - search instead of ispg class as well
220:36 - right so edits we can just write
220:40 - SPG dot node should equal this node
220:43 - right here
220:44 - and to make sure that we always start
220:46 - with node being one so that we can later
220:48 - differentiate between the self-play
220:50 - games in which we have reached a
220:52 - terminal node and then which all the
220:54 - safe play games that are uh that we want
220:56 - to expand on here later on we also have
220:59 - to say that SPG
221:01 - dot node should be equal to none like
221:04 - this
221:05 - so because of that we can just move that
221:08 - out of
221:09 - this Loop right here
221:11 - and you know what I believe we also
221:14 - don't need this enumerate Loop so let's
221:16 - just remove it so
221:18 - it's a bit easier to read it this way
221:21 - Okay so
221:22 - now we basically
221:25 - have stored all of our expandable nodes
221:28 - instead of our SPG classes so first of
221:31 - all we now want to find out which safe
221:33 - play games are expandable or not
221:36 - so let's write expandable
221:38 - spgs
221:41 - oh that's right let's say games like
221:44 - this
221:45 - [Music]
221:46 - um
221:47 - and here we want to have the list and
221:50 - then we want to store the mapping index
221:52 - so basically the index we can later use
221:56 - when we allocate this policy and this
222:00 - value back to our list of software games
222:02 - so we want to get the index basically
222:05 - for each safe play game
222:08 - um so for each SPG where our node is
222:11 - expandable right here right so we can
222:13 - just write mapping
222:16 - index for mapping index
222:20 - then in range of
222:24 - Len
222:25 - um SPG or save play games like this and
222:29 - then we only want to get the index for
222:32 - all of our safe play games where
222:35 - um safe play game dot node is equal is
222:38 - not equal to num right
222:41 - since this will be the safety games that
222:44 - are expandable
222:46 - so if SP games
222:48 - [Music]
222:50 - of mapping index
222:51 - [Music]
222:53 - dot note
222:55 - then is not none like this
222:59 - yeah this way we get the yeah indices
223:02 - for all of our expandable safe play
223:04 - games
223:05 - and next we can check whether there are
223:08 - even any expandable games so you can
223:11 - just write if Len of expand to the SP
223:15 - games
223:16 - larger than zero and in this case we
223:20 - again want to
223:22 - yeah basically stack up all of our
223:24 - states and
223:26 - then also get these encoded States right
223:29 - here so that we can then get this policy
223:32 - at this value so first of all we can
223:34 - write states that set that equal to NP
223:36 - dot stack again
223:38 - of
223:40 - and here we can just write
223:42 - SPG
223:44 - dot note dot state
223:47 - for SPG in
223:51 - SP games like this
223:54 - and then
223:56 - oh sorry for
223:58 - again we have to change this up a bit
224:00 - sorry rather we have to write SP games
224:04 - of
224:06 - um
224:07 - I I believe
224:08 - or just mapping index like this
224:11 - if we want to reference this one again
224:13 - here
224:14 - so self-play games of mapping
224:16 - index.note.state
224:17 - form mapping index
224:21 - and then in
224:24 - this expandable SP games right here
224:27 - so in expandable
224:31 - SP games
224:32 - [Music]
224:33 - like this okay so this way first of all
224:37 - we get this list
224:39 - of yeah the state for the nodes that are
224:42 - expandable
224:44 - um yeah basically for all of our
224:47 - self-play games that we can expand right
224:49 - here right
224:51 - so okay now we have these states right
224:54 - here and we also want to encode them but
224:57 - we can just move over to this part again
224:59 - so I will just copy this over right here
225:03 - [Music]
225:05 - so now again we get this policy and this
225:09 - value back
225:11 - and inside here first of all we can use
225:14 - these expandable or these states right
225:16 - here
225:17 - and then we also don't have to unsqueeze
225:20 - again because we already have a batch
225:23 - then here we don't have to squeeze again
225:25 - because we want to keep this batch as it
225:27 - is
225:28 - okay so yeah now we don't have to have
225:31 - to add a noise right here so now we can
225:34 - again Loop over
225:36 - our
225:37 - um expanded SP G's essentially right
225:41 - so
225:43 - we don't uh have to do this and stuff
225:45 - because if statement that you could I
225:47 - mean
225:48 - not really changing much here so now we
225:50 - want to first of all have the index
225:53 - um that we can get our policy at the
225:56 - given position and then we also want to
225:58 - have this mapping index
225:59 - so that we can then allocate this policy
226:02 - at index I to the self-play game at
226:05 - index mapping index if that makes sense
226:07 - because the yeah index
226:11 - um here when we enumerate is not aligned
226:15 - with the index of our safe play games
226:16 - right because we only look for over a
226:21 - selected number of self-play games right
226:23 - so for I if the mapping index has been
226:25 - enumerate
226:28 - then we want to again enumerate over our
226:31 - expandable
226:33 - let's Speed games like this
226:36 - and then here we can just
226:41 - um yeah basically get these valid moves
226:43 - and this policy
226:46 - and then we want to expand and back
226:48 - propagate here
226:50 - to move that magnet here
226:58 - um so that's great and
227:00 - now first of all we again want to get
227:02 - the SPG policy and the SPG value so
227:06 - let's get SPG policy by yeah getting the
227:10 - policy at the position of I
227:13 - of the SPG value should also be the
227:16 - value at the position of I
227:20 - so let's call this right here
227:24 - okay so now we have our SPG policing of
227:26 - on our SPG value so now we're going want
227:29 - to get these valid moves and we get this
227:32 - well it moves by first of all
227:35 - getting the state of our node for the
227:37 - given self-player game and here we want
227:40 - to use this mapping Index right so here
227:43 - we can just call it safe play games
227:46 - at the position of mapping index
227:49 - and then here we want to
227:52 - get the node of this state right here
227:55 - and what let's also just
227:58 - um I think just throw this into a very
228:00 - big I think this is nicer so just write
228:01 - mode Eclipse
228:03 - um
228:05 - save play games of the position of
228:07 - mapping index.note
228:09 - and here we just want to use node.state
228:12 - when we um
228:14 - get these valid moves right here and
228:17 - then here we can multiply the SPG policy
228:20 - with the valid moves as the scale of
228:22 - over node and then again we want to
228:25 - divide it by its one sum
228:28 - like this
228:30 - so now we can get the item yeah I
228:34 - believe I think this
228:35 - doesn't do a big difference here
228:39 - um yeah because this already is a number
228:41 - array so I believe we don't need this
228:42 - actually
228:44 - so now first of all we want to expand
228:46 - our node using this SPG policy right
228:50 - here
228:51 - then also we want to back propagate here
228:53 - so I will just copy this over here oh
228:55 - sorry
228:56 - um copy this over from here
229:00 - and we want to back propagate using SPG
229:02 - dot value
229:04 - okay so this is great
229:05 - so basically with this implementation we
229:09 - have the Monte colored research code
229:11 - working for this number of parallel
229:14 - games now at the end we would get these
229:16 - action props right here
229:18 - but I believe this is not that helpful
229:21 - in server ircts class because here we
229:24 - and you have our
229:27 - we have this parallelized and this is
229:30 - how to parallelize these action props
229:32 - right here so red that we just copy this
229:34 - out
229:36 - and
229:39 - instead we want to work with our action
229:42 - props instead of OSF Planet method here
229:45 - so I would just paste
229:48 - this over to this part here
229:53 - so because of that we also don't get any
229:55 - action process return here but we just
229:57 - change this Visa play games that we
229:59 - passed on here right
230:01 - okay so this is great and now let's also
230:04 - just run this hook to make sure that we
230:06 - don't have any direct arrows okay so
230:08 - torch is not defined but that's just
230:10 - because I haven't run these sets above
230:12 - so let's just have quickly do that here
230:15 - [Music]
230:18 - so let's check whether we got any error
230:21 - Okay so
230:22 - okay this is working for now
230:25 - okay so now we want to update this part
230:27 - here below and then we are finished with
230:29 - this parallel implementation
230:31 - so after we have done this paralyzed
230:35 - MCTS search we again want to look over
230:38 - all of our self-play games right so we
230:40 - can just write for I in range of Len of
230:45 - s p games
230:47 - and here remember that we want to remove
230:49 - the self-play games from our list if
230:54 - they are terminal right and it's a bit
230:56 - yeah it's quite bad to remove uh yeah
231:00 - safely game from a list and then keep
231:02 - iterating over it from basically zero to
231:05 - the Len
231:07 - um because yeah then we would
231:09 - yeah basically not have a perfect
231:11 - alignment between this index here and
231:14 - yeah the actual safe play game we want
231:16 - to reach if we mutate our list instead
231:18 - of our Loop right so we can fix this by
231:22 - uh flipping this range here around okay
231:26 - so now we want to Loop over this flipped
231:29 - range of our safe play games right and
231:31 - then we can just tap all of these things
231:33 - in right here and
231:38 - the first thing we want to do is that we
231:41 - actually want to get this safe play game
231:43 - at the given position so let's just
231:44 - write SVG equals self-play games at the
231:47 - position of I
231:49 - and then when we get our action props
231:51 - here we don't want to Loop over the root
231:53 - of children directly but rather SPG dot
231:56 - root or children like this
231:59 - so yeah this should be working
232:01 - and yeah so then we have this action
232:04 - props right here
232:07 - and we don't want to add this just to
232:09 - the general memory but rather to the
232:11 - memory of our Circle again this way
232:13 - and we don't want
232:16 - this yeah when we call the neutral state
232:19 - we can just either reference this one or
232:22 - just the state of the root note
232:26 - um for our circular games which is right
232:28 - SPG dot root dot State like this to get
232:31 - this Nutri state
232:33 - and then we have these action props here
232:35 - this should be working fine and then we
232:37 - have this player right here so then we
232:39 - also get this temperature action props
232:41 - and we get this action right here so
232:44 - this is all working well and now we want
232:47 - to update the state and here we again
232:49 - want to call it spg.state so that we
232:52 - update the state instead of our SPG
232:53 - class
232:54 - and we want to update this by first of
232:57 - all having the Old State so spg.state
233:00 - right here
233:02 - and
233:04 - then by just having this action right
233:07 - here and having this player right here
233:09 - so I think this is also fine
233:11 - and now we get this value and it's
233:13 - terminated by save the game look at the
233:15 - terminal and again we have to call
233:18 - spg.state instead of here
233:21 - and now we check whether a instrument is
233:24 - true so here we don't need to Define
233:26 - this written memory like I said
233:29 - but rather when we Loop over our memory
233:32 - first of all we have to declare set this
233:34 - to SPG dot memory
233:36 - and then we want to append this to this
233:41 - General Regional memory that we have
233:44 - defined up here right
233:46 - so then we can just
233:48 - delete this Return part right here and
233:51 - yeah so this is uh that's something that
233:54 - we have finished inside here and then we
233:55 - want to flip the player
233:58 - um yeah after we have looped over all of
234:00 - ourself play games and at the end we
234:02 - just want to return this return memory
234:05 - appear above
234:07 - and also I just noticed that when we
234:10 - um yeah are finished right here with
234:11 - this play game so if our safe play games
234:13 - terminal then we also want to delete it
234:16 - from our list of soft play games right
234:17 - so that this while loop here it's
234:20 - working so here we can just write the
234:24 - and then SP gains if the position of I
234:28 - so yeah this way we just shorten our
234:30 - safe play games list and because we Loop
234:33 - in this other direction it will still
234:36 - work to just keep looping instead of our
234:38 - for Loop
234:40 - um yeah because we will basically just
234:42 - remove the circle again at this site
234:44 - when we look in this direction right and
234:46 - we also have to change this MCTS to MCTS
234:49 - or parallel here
234:51 - right oh and I also notice that we have
234:54 - this Arrow right here where we return
234:56 - these action props so obviously we want
234:58 - to remove that line right here because
235:00 - we just want to return the return memory
235:03 - and this second was the part of the
235:05 - Monte Carlo research that we removed and
235:08 - so let's just run this again and now we
235:12 - can try to train our model for Connect 4
235:15 - using this parallelized fs0
235:17 - implementation let's write F0 parallel
235:20 - instead of here
235:21 - and yeah everything else can just stay
235:23 - the same and I believe now the speed
235:27 - should be up by about 5x so this should
235:29 - be a massive Improvement so let's just
235:31 - run this cell again and we get this nice
235:34 - progress bar so I will just train this
235:36 - for these eight iterations here and yeah
235:39 - this will still take a few hours on my
235:42 - machine but after that we will have this
235:44 - a great model that has a very good
235:47 - understanding of how to play the game of
235:48 - Connect Four and then we can first of
235:50 - all evaluate it again and maybe also
235:52 - play against it in this nice visual way
235:55 - and maybe we will lose against it so
235:57 - we'll see
235:59 - okay so now I've trained in a network
236:02 - using this IFA zero parallel class for
236:05 - eight iterations right here uh yeah on
236:08 - the game of Connect Four
236:10 - so this still took a few hours I believe
236:13 - we could have been even faster if we
236:14 - would have further increased this number
236:16 - of parallel games here but still that's
236:18 - fine
236:19 - and so now we want to test this here
236:21 - instead of our evaluation set
236:24 - so first of all let's um change the path
236:28 - uh your for our weights to model 7
236:31 - connect 4. right since we trained for
236:33 - eight iterations
236:35 - and then next we can also set number of
236:38 - searches to 600 years where so yeah we
236:41 - copy this yeah value from the arguments
236:45 - above
236:46 - and yeah this is still quite a small
236:48 - number when you would compare to other
236:51 - search algorithms that I use in practice
236:53 - so let's check whether we get nice
236:56 - results here so let's run the cell
236:59 - and
237:01 - oh sorry I've accidentally
237:03 - accidentally ran this one right here
237:06 - um so let's run this right here
237:09 - so now we get this Connect Four board
237:11 - and I believe we want to play in the
237:13 - middle right here
237:15 - so let's see where the model plays and
237:17 - the model plate on top of it
237:19 - think we want to play on this side right
237:22 - here
237:24 - yeah I think that should be fine
237:26 - so let's play on the phone
237:27 - [Music]
237:31 - um yeah the model plate here I think we
237:34 - either want to play here or there or
237:37 - maybe there
237:39 - so I would just put some pressure on the
237:41 - model by playing Neon 5.
237:44 - [Music]
237:48 - so
237:51 - now I think
238:02 - I think we're doing fine when we play
238:04 - here we just have to be careful so let's
238:06 - plan for
238:09 - [Music]
238:11 - so now we could also pressure
238:14 - here and then our Motor Company here
238:17 - right because someone could defend this
238:19 - so I think
238:23 - well probably I would want to play
238:27 - here
238:34 - [Music]
238:34 - um
238:35 - so
238:36 - if again
238:40 - sorry I'm very bad at this
238:43 - yeah probably yeah would have been much
238:45 - better I guess to play anymore
238:47 - [Music]
238:48 - um
238:49 - so
238:51 - now we can't play here anymore we could
238:55 - still play here or there I think this is
238:58 - valuable here
238:59 - so
239:00 - let's move on top of you know this
239:03 - position here so let's make it three
239:06 - so we'll multiply it here
239:09 - um we want to get it we have some
239:11 - pressure here so maybe we'd like to
239:13 - clear here we also the same here so
239:16 - play on two I guess
239:19 - oh no then if we play here our model
239:22 - will play here and we would be trapped
239:24 - so we can't play as well anymore
239:28 - um
239:28 - so maybe we could play here but that's
239:32 - not that good either
239:34 - um
239:35 - maybe that's fine here
239:37 - it's not too valuable
239:40 - so yes we put it here
239:42 - okay so where did the model play so just
239:45 - play it here
239:48 - Okay so
239:50 - thing we have to definitely play here I
239:52 - don't know if we still lose them
239:57 - and since play on five now
240:00 - [Music]
240:02 - okay so where did the play now
240:06 - um there's some pressure here or so oh
240:10 - we lost this way and we're also this way
240:13 - okay we can absolutely destroy it
240:17 - um yeah I don't know we have to just
240:19 - just play the three now I guess and yeah
240:24 - took this right here and yes won the
240:28 - game because of that so nice uh I could
240:32 - have even played here as well
240:34 - um so that should be yeah just I mean I
240:37 - played quite in a bad way I guess but so
240:40 - it was super nice and so now we could
240:42 - see that even though we have around 600
240:44 - searches our model was still able to at
240:49 - least destroy me here in this game so
240:51 - this is fun and
240:54 - before we conclude with this tutorial
240:56 - here this is some minor mistakes I
240:59 - noticed
241:01 - um so first of all when we get these
241:04 - temperature action props in fs03 we
241:07 - currently aren't using them for our
241:09 - probabilities so let's change that right
241:11 - here
241:13 - and so now let's use them here and then
241:15 - also when we use our Optimizer whether
241:18 - we're gonna whether we want to call it
241:20 - safe.optimizer because then we yeah
241:23 - go for this Optimizer here instead of
241:26 - just um yeah calling this Optimizer
241:29 - right which might not work on All
241:30 - Occasions yeah so that's just a smart
241:32 - thing let's also add that to this iPhone
241:36 - 0 parallel so
241:38 - temperature action props here and then
241:40 - save.optimizer in our Training Method
241:46 - um okay so now we can just lastly let
241:50 - our model play with itself and you know
241:52 - let's also get this nicer form of
241:54 - visualization compared to just printing
241:57 - out the spot state right here so yeah
242:00 - this is what we will do next and here I
242:02 - want to use the Kegel environments
242:04 - package so we'll just write import Kegel
242:07 - environments right here then you know
242:10 - let's also print out the version so
242:11 - maybe you're interested in that
242:14 - um like this
242:16 - so yeah that's my version right here
242:18 - and then using this package we can first
242:21 - of all create an environment and we get
242:24 - this by setting nth equal to Kegel
242:27 - environments
242:29 - dot make and then here we can just use
242:31 - connect X to get this standard Connect
242:34 - Four environment I mean this also
242:36 - tweakable so but here we just get the
242:39 - standard connector environment and then
242:42 - next we would want to have some players
242:45 - so this should just be a list of our
242:46 - agents and then next we could just say
242:51 - enter run using these players we have
242:55 - defined and then end dot render
242:58 - with mode being equal to IPython instead
243:01 - of our Jupiter notebook here
243:03 - so now we also want to Define our
243:05 - players right so here I've just created
243:08 - this agent class yeah and I've copied it
243:12 - over right here so basically we just do
243:15 - some pre-processing with this date and
243:17 - then we will yeah just call our MCTS
243:20 - much search either or just get a
243:23 - prediction straight up from our model
243:25 - depending on our arguments so basically
243:27 - doing something similar than what we do
243:30 - up here in this Loop right
243:32 - okay so yeah now we want to create these
243:35 - players as Kegel agents and before we do
243:39 - that we also need this model this game
243:41 - and these arguments right here so we
243:43 - just copy them over
243:45 - um from here basically
243:47 - like this
243:48 - and then let's paste them yep
243:51 - [Music]
243:52 - so first of all we have our game of
243:54 - Connect Four then we don't need a player
243:57 - here
243:58 - um then we also have to add some
244:00 - arguments right here so first of all we
244:02 - want to search set search to true
244:07 - um and then we also need a temperature
244:10 - and the way this is built up here when
244:12 - we have temperature equal to zero we
244:15 - don't uh do this yeah division to get
244:18 - our
244:19 - um yeah updated policy but rather we
244:21 - would just get the arc mix directly so
244:23 - let's just set temperature to zero right
244:25 - here so that we always get the arc Max
244:27 - of our policy
244:29 - and well the MCTS distribution Guided by
244:33 - our model
244:34 - okay so now we have this right here
244:37 - let's also set the reflect Epsilon to
244:39 - one so that we still have some
244:40 - Randomness here
244:42 - and so next we have our device right
244:45 - here we create our model like this and
244:47 - then we use this path right here so this
244:49 - should be fine so now we can
244:52 - actually Define our players so let's set
244:54 - player one equal to calculated agent
244:58 - and like I said we first find it this
245:00 - model this game and these arguments so
245:02 - let's write model game and arcs just
245:05 - like this so let's let's also do the
245:07 - same thing for player 2.
245:09 - so I mean
245:13 - so this way we have more flexibility if
245:16 - we would want to try different players
245:18 - and then here for the players we can
245:21 - just fill this list with first of all
245:23 - setting player Wonder run and then
245:25 - player 2. run like this okay so this
245:29 - should be working now let's just run the
245:30 - cell and
245:32 - um get these nice visualizations
245:35 - also now we get this neat animation of
245:39 - our models playing against each other so
245:41 - we have two players but just one model
245:43 - here playing against itself basically
245:45 - and I believe this should come to a draw
245:48 - yeah so
245:49 - uh what it is that advanced I guess that
245:53 - it can defend against all attacks so in
245:57 - this uh just this nice animation and
246:01 - now we can also just yeah briefly do the
246:04 - same for Tic-tac-toe as well so here we
246:07 - just have to change some small things so
246:10 - first of all oh sorry
246:12 - we have to set the game to tic-tac-toe
246:17 - and event also
246:19 - um yeah we can change the arguments so
246:21 - let's just set number of searches to 100
246:23 - and here went for our resnet we can set
246:27 - the number of rest blocks to four and
246:28 - the hidden them to 64 I guess
246:31 - so then we want to update this path so I
246:34 - think the last particular terminal we
246:36 - trade was model 2.pt
246:38 - if I'm not mistaken so then also here we
246:41 - have to set this to tick tock towards
246:43 - way and then we can just run this
246:46 - okay so now we get this nice animation
246:49 - immediately of our modest plugins
246:50 - playing against each other and again we
246:52 - have gotten a draw because yeah the
246:55 - model is able to defend all possible
246:57 - attacks
246:58 - um so this is still very nice so feel
247:01 - free to do some further experiments uh
247:03 - yeah your own so maybe you could also
247:05 - set the search to fault so that just
247:08 - your neural networks directly will play
247:10 - against each other without doing these
247:12 - hundred searches in this case and yeah
247:15 - so I think we're finished with this
247:16 - tutorial this was a lot of fun and
247:20 - um I've created this GitHub repository
247:22 - where there's yeah there's a jupyter
247:24 - notebook store for each checkpoint and
247:26 - then also um I have this weights folder
247:28 - where we have the last model for
247:31 - Tic-tac-toe and the last model for
247:32 - Connect Four that we have trained
247:34 - and if any questions feel free to ask
247:38 - them either in the comments or just by
247:40 - sending an email for example
247:41 - and yeah I might do a follow-up video on
247:44 - mu zero since I've also built that
247:46 - algorithm from scratch so if you're
247:48 - interested in that so
247:50 - um would be nice to let me know so thank
247:52 - you