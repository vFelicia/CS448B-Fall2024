00:00 - hey this is Andrew Brown and I'm
00:01 - bringing you another certification
00:03 - course and this time it's the Azure AI
00:05 - fundamentals also known as the AI 900
00:08 - and if you're looking to pass a
00:10 - certification we have everything that
00:12 - you need here such as Labs lectures and
00:14 - a free practice exam so you can go ASAT
00:16 - exam get that uh certification put on
00:19 - your resume and Linkedin to go get that
00:21 - job you've been looking for um if you
00:24 - want to support more free courses like
00:26 - this one the best way is to purchase the
00:27 - additional uh paid materials where you
00:29 - can get access uh to more practice exams
00:32 - and other resources if you don't know me
00:35 - um I've taught a bit of everything uh
00:37 - here on the cloud that's been with adabs
00:39 - Azure gcp devops terraform kubernetes
00:42 - you name it I've taught it but uh you
00:44 - know you know the drill here let's get
00:47 - into it and learn more about the Azure
00:49 - AI
00:53 - fundamentals hey this is Andrew Brown
00:55 - from exam Pro and we are at the start of
00:57 - our journey here learning about the AI
00:59 - 900 asking the most important question
01:01 - which is what is the AI 900 so the Azure
01:04 - AI fundamental certification is for
01:06 - those seeking an ml role such as AI
01:08 - engineer or data scientist the
01:10 - certification will demonstrate if a
01:12 - person can Define and understand Azure
01:14 - AI services such as competive services
01:16 - and Azure applied AI Services AI
01:19 - Concepts knowledge mining responsible AI
01:22 - basics of NL pipelines classical ml
01:24 - models autom ml generative AI workloads
01:27 - which is newly added content and Azure
01:29 - AI studio so you don't need to know
01:31 - super complicated ml knowledge here but
01:33 - it definitely helps to get you through
01:34 - there so this certification is generally
01:36 - referred to by its course code the AI
01:39 - 900 and it's the natural path for the
01:41 - Azure AI engineer or Azure data
01:43 - scientist certification this generally
01:45 - is an easy course to pass and it's great
01:47 - for those new to cloud or ml related
01:49 - technology looking at our road map you
01:51 - might be asking okay well what are the
01:54 - paths and what should I learn first so
01:55 - here are a few suggested routes if you
01:58 - already have your a900 that's that's a
02:00 - great starting point before you take
02:01 - your AI 900 if you don't have your a
02:03 - z900 you can jump right into the AI 900
02:06 - but I strongly recommend you go get that
02:08 - a z900 because it gives you General
02:10 - foundational knowledge it's just another
02:12 - thing that you should not have to worry
02:13 - about which is just how to use Azure at
02:15 - a fundamental level do you need the
02:17 - dp900 to take the AI 900 no but a lot of
02:21 - people seem to like to go this route
02:22 - where they want to have that data
02:23 - Foundation before they move on to the AI
02:25 - 900 because they know that the broad
02:28 - knowledge is going to be useful they so
02:30 - it's app pairing that you see a lot of
02:31 - people getting the AI 900 and the dp900
02:34 - together for the AI 900 the path is a
02:37 - little bit more clear it's either going
02:38 - to be data scientists or AI engineer so
02:41 - for the AI engineer you have to know how
02:43 - to use the AI services in and out for
02:45 - data scientists it's more focused on
02:47 - setting up actual pipelines and things
02:49 - like that within Azure machine learning
02:50 - so you just have to decide which path is
02:52 - for you the data scientist is definitely
02:55 - harder than the AI engineer so if you
02:56 - aren't ready for the data science some
02:58 - people like taking the AI engineer first
03:00 - and then doing the data scientists so
03:02 - this is kind of like a warmup again it's
03:05 - not 100% necessary but it's just based
03:07 - on your personal learning style and a
03:09 - lot of times people like to take the
03:10 - data engineer after the data scientists
03:12 - just to round out their complete
03:13 - knowledge now if you already have the
03:16 - a900 and the administrator associate you
03:18 - can safely go to the data scientist if
03:20 - you want to risk it because this one is
03:22 - really hard so if you've passed the
03:24 - a1004 you know you're going to probably
03:26 - have a lot more confidence learning up
03:27 - about all the concepts at this level
03:29 - here but of course it's always
03:31 - recommended to go grab these
03:32 - foundational CTS because sometimes
03:34 - course materials just do not cover the
03:36 - information and so the obvious stuff is
03:38 - going to get left out okay so moving
03:40 - forward here how long should you study
03:42 - to pass for the AI 900 if you're
03:44 - entirely new to ml Ai and Cloud
03:47 - providers such as Azure you should
03:48 - anticipate dedicating around 15 hours to
03:50 - grasp the basics this estimate can vary
03:53 - base on your familiarity with these
03:54 - concepts for complete beginners the time
03:57 - commitment might extend to 20 to 30
03:58 - hours for the intermediate level so
04:01 - people that have passed the a900 or
04:03 - dp900 you're looking at around 8 to 10
04:06 - hours if you have one or more years of
04:08 - experience with Azure or another cloud
04:10 - service provider like a WS or gcp you're
04:13 - looking at about 5 hours or less the
04:16 - average stunny time is about 8 hours
04:18 - this is where you should be committing
04:19 - 50% of the time to the lecture in labs
04:21 - and 50% for the practice exams the
04:24 - recommended study time is 30 minutes to
04:26 - an hour a day for 14 days this should
04:28 - get you through it but just don't
04:29 - overstudy and just don't spend too
04:31 - little time what does it take to pass
04:33 - the exam well you got to watch the
04:35 - lectures and memorize key information do
04:38 - Hands-On labs and follow along with your
04:40 - own Azure account I'd say that you could
04:42 - probably get away with just watching all
04:43 - the videos in this one without having to
04:45 - do the labs but again it really does
04:47 - reinforce that information if you do
04:49 - take the time there is some stuff that
04:51 - is in Azure AI Studio or machine
04:53 - learning you might be wary of launching
04:55 - instances because we do have to run
04:57 - instances and they will cost money
04:59 - unless you delete the instances after
05:00 - use resulting in very small costs so if
05:03 - you feel that you're not comfortable
05:04 - with that by just watching the videos
05:06 - you should be okay but when you get into
05:08 - the associate tier you absolutely have
05:10 - to expect to pay something to learn and
05:11 - take that risk you want to do paid
05:14 - online practice exams that simulate the
05:15 - real exam as I've mentioned before I do
05:18 - provide a free practice exam and have
05:20 - paid practice exams that accompany this
05:22 - course that are on my platform exam Pro
05:24 - and that's how you can help support more
05:25 - of these free courses so can you pass
05:28 - this certification without taking a
05:30 - practice exam well azzure is a little
05:32 - bit harder if this isn't a WS exam I
05:34 - would say yes but for Azure exams like
05:36 - AI 900 dp900 and sc900 probably not it's
05:41 - kind of risky I think you should do at
05:43 - least one practice exam or go through
05:45 - the sample one there's a sample one
05:46 - probably laying around on the Azure
05:48 - website let's take a look at the exam
05:50 - guide breakdown here and then in the
05:52 - following video we'll look at in more
05:53 - detail so it's broken down into the
05:55 - following domain so the exam has five
05:58 - domains of questions and each domain has
06:00 - its own waiting which determines how
06:01 - many questions in a domain that will
06:03 - show up so 15 to 20% will be described
06:06 - Ai workloads and considerations 20 to
06:08 - 25% will consist of describe fundamental
06:11 - principles of machine learning on Azure
06:13 - 15 to 20% will consist of described
06:15 - features of computer vision workloads on
06:17 - Azure 15 to 20% will be described
06:20 - features of natural language processing
06:22 - workloads on Azure and 15 to 20% will be
06:25 - described features of generative AI
06:26 - workloads on Azure I want you to notice
06:29 - it's says describe these domains this is
06:31 - good because that tells you it's not
06:32 - going to be super hard if you start
06:34 - seeing things that say Beyond describe
06:36 - and identify then you know it's going to
06:37 - be a bit harder so where do you take
06:40 - this exam well you can take it in person
06:42 - at a test center or online from the
06:44 - convenience of your own home so there's
06:46 - two popular test centers there's CER
06:48 - aort and there's Pearson view you can
06:50 - also take it at a local test center if
06:52 - there are nearby locations the term
06:54 - Proctor means a supervisor or person
06:56 - that is monitoring you while you're
06:57 - taking the exam if I had the the option
06:59 - between in person or online I would
07:02 - always choose the in person because it's
07:03 - a controlled environment and it's way
07:05 - less stressful online there are many
07:07 - things that can go wrong but it's up to
07:08 - your personal preference and your
07:10 - situation the passing grade here is 700
07:13 - out of a th000 so that's around 70% I
07:16 - would say around because you could
07:17 - possibly fail with 70% because these
07:19 - things work on scaled scoring for
07:21 - response types there's about 37 to 47
07:24 - questions and you can afford to get
07:25 - about 10 to 13 questions wrong so some
07:28 - questions are worth more than one point
07:30 - some questions cannot be skipped and the
07:33 - format of questions can be multiple
07:35 - choice multiple answer drag and drop and
07:37 - hot area there shouldn't be any case
07:39 - studies for foundational level exams and
07:41 - there's no penalty for wrong questions
07:44 - so for the duration you get 1 hour that
07:46 - means about 1 minute per question the
07:48 - time for this exam is 60 Minutes your C
07:51 - time is 90 minutes C time refers to the
07:53 - amount of time that you should take to
07:55 - allocate for that exam so this includes
07:57 - time to review the instructions read and
07:59 - accept the NDA complete the exam and
08:01 - provide feedback at the end this
08:03 - certification is going to be valid
08:04 - forever and it does not expire Microsoft
08:07 - fundamental certifications such as the
08:08 - a900 or ms9900 do not expire as long as
08:12 - the technology is still available or
08:13 - relevant so we'll proceed to the full
08:16 - exam guide
08:17 - [Music]
08:20 - now hey this is Andrew Brown from exam
08:23 - Pro and what we've pulled up here is the
08:25 - official exam outline on the Microsoft
08:27 - website if you want to find this year s
08:29 - you just have to type in AI 900 Azure or
08:31 - Microsoft you should be able to easily
08:34 - find it the page looks like this what I
08:36 - want you to do is scroll on down because
08:38 - we're looking for the AI 900 set of
08:40 - guide and from there we're going to
08:41 - scroll on down to the skills measured
08:43 - section and you might want to bump up
08:45 - the text Azure loves updating their
08:47 - courses with minor updates that don't
08:49 - generally affect the outcome of the
08:50 - study here but it does get a lot of
08:52 - people worried because they always say
08:54 - well is your course out of date so no
08:56 - they're just making minor changes
08:58 - because they'll do this like five times
08:59 - a year and so if there was a major
09:01 - revision what would happen is they would
09:03 - change it so instead of being the AI 900
09:05 - it would be like the AI 9001 or 9002
09:08 - similar to how the AI 102 was previously
09:11 - AI 100 but now it's the AI 102 so just
09:14 - watch out for those and if it's a major
09:16 - revision then yes it would probably need
09:17 - a completely new course so there aren't
09:20 - any major changes with the new update
09:22 - other than the update for the generative
09:23 - AI workloads on Azure section A couple
09:25 - of name changes and a few things being
09:27 - removed everything else remains is
09:29 - relatively the same with very minor
09:31 - changes so the concepts and such are
09:33 - still up to date overall I think the
09:35 - exam is easier than the four so let's go
09:37 - through some of the topics and work our
09:39 - way through here so describe Ai
09:41 - workloads and considerations so here
09:43 - we're just kind of describing the
09:44 - generalities of AI so content moderation
09:47 - workloads involve filtering out
09:49 - inappropriate or harmful content from
09:51 - user generated inputs ensuring a safe
09:53 - and positive user experience
09:55 - personalization workloads analyze user
09:57 - behavior and preferences to tailor
09:58 - content recommendations or experiences
10:01 - to individual users computer vision
10:03 - workloads involve the analysis of images
10:05 - and videos to recognize patterns objects
10:08 - faces and actions identify natural
10:11 - language processing knowledge mining
10:13 - document intelligence and features of
10:15 - generative AI workloads note that these
10:17 - are all just Concepts you don't need to
10:19 - know how to use the services at a high
10:21 - level then you have the responsible AI
10:23 - section so Microsoft has these six
10:25 - principles that they really want you to
10:27 - know and they push it throughout all
10:28 - their AI services so those are the six
10:30 - you'll need to know and they're not that
10:31 - hard to
10:32 - learn moving on we have described
10:35 - fundamental principles of machine
10:36 - learning on
10:37 - Azure so here it's just describing
10:40 - regression classification clustering and
10:42 - features of deep learning we have a lot
10:44 - of practical experience with these in
10:46 - the course so you will understand at the
10:48 - end what these are used for next we have
10:50 - core machine learning Concepts we can
10:52 - identify features and labels in a data
10:54 - set so that's the data labeling service
10:56 - describe how training validation data
10:58 - sets are used in machine learning so
11:00 - we'll touch on that describe
11:02 - capabilities of Automated machine
11:04 - learning automl simplifies building and
11:06 - picking the best models while data and
11:08 - compute Services provide the power you
11:10 - need for training with Azure machine
11:12 - learning it helps with managing and
11:13 - deploying your models letting you put
11:15 - your machine learning projects into
11:17 - action smoothly under computer vision
11:19 - workloads we have image classification
11:21 - object detection optical character
11:23 - recognition facial detection and facial
11:25 - analysis
11:27 - Solutions next we have Azure AI Vision
11:29 - Azure AI face detection and Azure AI
11:32 - video indexer the Azure AI Services
11:34 - Encompass a wide range of tools designed
11:36 - to facilitate the development of
11:37 - intelligent applications these Services
11:40 - used to be called computer vision custom
11:42 - vision face service and form recognizer
11:44 - but have Evol or been grouped under
11:46 - broader service categories to streamline
11:48 - their application and integration into
11:51 - projects for NLP we have key phrase
11:53 - extraction entity recognition sentiment
11:55 - analysis language modeling speech
11:58 - recognition synthesis this one doesn't
12:00 - really appear much it's kind of a
12:01 - concept not so much something we have to
12:03 - do and then there's
12:05 - translation so now we have Azure tools
12:08 - and services for NLP workloads these
12:10 - include the Azure AI language service
12:12 - Azure AI speech service and Azure AI
12:14 - translator service these used to be
12:16 - separate Services I believe like the
12:18 - text analytics service Lewis speech
12:20 - service and translator text service but
12:22 - they have been added to the Azure AI
12:24 - umbrella of AI services and now we'll be
12:26 - moving on to the generative AI workloads
12:28 - on Azure we'll be covering features of
12:31 - generative AI models common scenarios
12:33 - for generative Ai and responsible AI
12:35 - considerations for generative Ai and
12:38 - also some of the cool features that
12:39 - Azure open ey service has to offer such
12:41 - as natural language generation code
12:44 - generation and image
12:45 - generation so that's about a general
12:48 - breakdown of the AI 900 exam
12:51 - [Music]
12:54 - guide hey this is angrew Brown from exam
12:57 - Pro and we are looking at the layers of
12:58 - machine learning so here I have this
13:00 - thing that looks like kind of an onion
13:02 - and what it is it's just describing the
13:04 - relationship between these uh ml terms
13:07 - uh uh related to Ai and we'll just work
13:09 - our way through here starting at the top
13:10 - so artificial intelligence also known as
13:13 - AI is when machines that perform jobs
13:15 - that mimic human behavior so it doesn't
13:17 - describe uh how it does that but it's
13:19 - just the fact that that's what AI is uh
13:22 - one layer underneath we have machine
13:24 - learning so machines that get better at
13:25 - a task without explicit programming uh
13:28 - then we have deep learning so these are
13:29 - machines that have an artificial neural
13:31 - network inspired by the human brain to
13:33 - solve complex problems and if you're
13:35 - talking about someone that actually
13:36 - assembles either ml or or deep learning
13:39 - uh models or algorithms that's a data
13:41 - scientist so a person with
13:42 - multi-disciplinary skills and math
13:44 - statistics predictive modeling machine
13:46 - learning to make future predictions so
13:48 - what you need to understand is that AI
13:50 - is just the outcome right and so AI
13:52 - could be using ml underneath or deep
13:55 - learning or a combination of both or
13:57 - just IFL statements okay
13:59 - [Music]
14:04 - all right so let's take a look here at
14:05 - the key elements of AI so AI is the
14:07 - software that imitates human behaviors
14:08 - and capabilities and there are key
14:10 - elements according to Azure or Microsoft
14:13 - as to what makes up AI so let's go
14:16 - through this list quickly here so we
14:17 - have machine learning which is the
14:18 - foundation of an AI system that can
14:20 - learn and predict like a human you have
14:22 - anomaly detection so detect outliers or
14:24 - things out of place like a human
14:26 - computer vision be able to see like a
14:29 - human natural language processing also
14:31 - known as NLP be able to process human
14:34 - languages in referr contexts you know
14:36 - like a human at conversational AI be
14:38 - able to hold a conversation with a human
14:41 - so you know I wrote here according to
14:45 - Microsoft and Azure because you know the
14:47 - the global definition is a bit different
14:50 - but I just wanted to put this here
14:51 - because I've definitely seen this as an
14:53 - exam question and so we're going to have
14:55 - to go with azure's definition here okay
15:02 - let's define what is a data set so a
15:04 - data set is a logical grouping of units
15:05 - of data that are closely related to or
15:08 - share the same data structure and there
15:10 - are publicly available data sets that
15:12 - are used in uh learning of Statistics
15:15 - data analytics and machine learning I
15:17 - just want to cover a couple here so the
15:19 - first is the mnist database so images of
15:21 - handwritten digits used to test classify
15:24 - cluster image processing algorithms
15:26 - commonly used when learning uh how to
15:28 - build computer vision ml models to
15:30 - translate handwritten into or
15:32 - handwriting into digital text so it's
15:34 - just a bunch of handwritten uh numbers
15:37 - and letters and then another very
15:39 - popular data set is the common objects
15:41 - in context Coco data set so this is a
15:44 - data set which contains many common
15:45 - images using a Json file Coco format
15:48 - that identify objects or segments within
15:50 - an image uh and so this data set has a
15:52 - lot of stuff in it so object
15:53 - segmentations recognition and it
15:55 - contexts super pixel stuff segmentation
15:57 - they have a lot of images and a lot of
16:00 - objects uh so there's a lot of stuff in
16:02 - there so why am I talking about this and
16:04 - in particular Coco data sets well when
16:07 - you use um Azure machine Learning Studio
16:09 - it has a DAT data labeling service and
16:12 - um the thing is is that uh it can
16:15 - actually export out into Coco formats
16:17 - that's why I wanted you to get exposure
16:18 - to what Coco was and the other thing is
16:20 - is that when you're building out Azure
16:22 - machine learning uh pipelines you uh
16:25 - they actually have open data sets which
16:26 - we'll see later in the course um that
16:28 - shows you that you can just use very
16:30 - common ones and so uh you might see mest
16:33 - and uh the other one there uh so I just
16:36 - wanted to get you some exposure
16:37 - [Music]
16:41 - okay let's talk about data labeling so
16:43 - this is the process of identifying raw
16:45 - data so images text files videos and
16:48 - adding one or more meaningful and
16:50 - informative labels to provide context so
16:52 - a machine learning model can learn so
16:54 - with supervised machine learning
16:55 - labeling is a prerequisite to produce
16:57 - training data and each piece of data
16:59 - will generally be labeled by a human the
17:01 - reason why I say generally here is
17:03 - because with azure's uh data labeling
17:06 - Service uh they can actually do ml
17:08 - assisted labeling uh so with
17:10 - unsupervised machine learning labels
17:11 - will be produced by the machine and may
17:14 - not be human readable uh and then one
17:16 - other thing I want to touch on is the
17:17 - term called Ground truth so this is a
17:20 - proper u a properly labeled data set
17:22 - that you can use as the objective
17:24 - standard to train and assess a given
17:26 - model is often called Ground truth the
17:28 - accuracy of your train model will depend
17:30 - on the accuracy of your ground Truth Now
17:32 - using um azures tools I never seen use
17:35 - the word ground truth I see that a lot
17:36 - in AWS and even this graphing here is
17:38 - from AWS but uh I just want to make sure
17:41 - you are familiar with all that stuff
17:43 - [Music]
17:46 - okay let's compare supervised
17:49 - unsupervised and reinforcement learning
17:50 - starting at the top we got supervised
17:52 - learning this is where the data has been
17:54 - labeled for training and it's considered
17:56 - Tas driven because you are trying to
17:58 - make a prediction get a value back so
18:00 - when the labels are known and you want a
18:02 - precise outcome when you need a specific
18:04 - value returned and so you're going to be
18:06 - using classification and regression in
18:08 - these cases for unsupervised learning
18:10 - this is where data that has not been
18:11 - labeled uh the ml model needs to do its
18:13 - own labeling this is considered data
18:15 - driven it's trying to recognize a
18:17 - structure or a pattern and so this is
18:19 - when the labels are not known and the
18:20 - outcome does not need to be prise when
18:23 - you're trying to make sense of data so
18:25 - you have clustering dimensionality
18:27 - reduction and associ
18:29 - if You' never heard this term before the
18:30 - idea is it's trying to reduce the amount
18:32 - of Dimensions to make it easier to work
18:33 - with the data so make sense of the data
18:36 - right uh we have reinforcement learning
18:38 - so this is where there is no data
18:39 - there's an environment and an ml model
18:41 - generates data uh and and makes many
18:44 - attempts to reach a goal so this is
18:45 - considered uh decisions driven and so
18:48 - this is for game AI learning tasks robot
18:50 - navigation when you've seen someone code
18:53 - a video game that can play itself that's
18:55 - what this is if you're wondering this is
18:57 - not all the types of machine learning uh
18:59 - and these are in specific unsupervised
19:01 - and supervised is considered classical
19:03 - machine learning because they he heavily
19:05 - rely on statistics and math to produce
19:07 - the outcome uh but there you
19:09 - [Music]
19:13 - go so what is a neural network well it's
19:16 - often described as mimicking the brain
19:17 - it's a neuron or node that represents an
19:19 - algorithm so data is inputed into a
19:21 - neuron and based on the output the data
19:22 - will be passed to one of many connected
19:25 - neurals the connections between neurons
19:27 - is weighted I really should have
19:28 - highlighted that one that's very
19:29 - important uh the network is organized
19:31 - into layers there will be an input layer
19:34 - uh one to many hidden layers and an
19:36 - output layer so here's an example of a
19:38 - very simple neural network notice the NN
19:41 - a lot of times you'll see this in ml as
19:43 - an abbreviation for neural networks and
19:45 - sometimes neural networks are just
19:46 - called neural Nets so just understand
19:48 - that's the same term here what is deep
19:50 - learning this is a neural network that
19:52 - has three or more hidden layers it's
19:53 - considered deep learning because at this
19:54 - point it's uh it's not human readable to
19:57 - understand what's going on with within
19:59 - those
20:00 - layers what is forward feed so neural
20:03 - networks where they have connections
20:04 - between nodes that do not form a cycle
20:06 - they always move forward so that just
20:07 - describes uh a a forward pass through
20:10 - the network you'll see fnn which stands
20:12 - for forward feed neural network just to
20:14 - describe that type of network uh then
20:17 - there's back back propagation which are
20:19 - in forward feed uh networks this is
20:21 - where we move backwards through the
20:22 - neural net adjusting the weights to
20:23 - improve the outcome on next iteration
20:25 - this is how a neural net learns the way
20:27 - the back propagation knows to do this is
20:29 - that there's a loss function so a
20:30 - function that compares the ground truth
20:32 - to the prediction to determine the error
20:34 - rate how bad the network performs so
20:36 - when it gets to the end it's going to
20:37 - perform that calculation and then it's
20:40 - going to do its back propagation and
20:41 - adjust the weights um then you have
20:44 - activation functions I'm just going to
20:46 - uh clear this up here so activation
20:50 - functions uh they're an algorithm
20:51 - applied to a hidden layer uh node that
20:54 - affects connected output so for this
20:56 - entire hidden layer they'll all have the
20:57 - same uh one here and it just kind of
20:59 - affects uh how it learns and like how
21:02 - the waiting works so it's part of back
21:04 - propagation and just the learning
21:05 - process there's a concept of D so when
21:08 - the next layer increases the amount of
21:09 - nodes and you have spars so when the
21:11 - next layer decreases the amount of nodes
21:13 - anytime you see something going from a
21:14 - dense layer to a sparse layer that's
21:16 - usually called dimensional
21:17 - dimensionality reduction because you're
21:19 - reducing the amount of Dimensions
21:20 - because the amount of nodes in your
21:22 - network determines the dimensions you
21:24 - have
21:27 - okay
21:28 - [Music]
21:29 - what is a GPU well it's a general
21:31 - processing unit that is specially
21:33 - designed to quickly uh render high
21:35 - resolution images and videos
21:36 - concurrently gpus can perform parallel
21:38 - operations on multiple sets of data so
21:40 - they are commonly used for non-graphical
21:42 - tasks such as machine learning and
21:44 - scientific computation so a CPU has an
21:46 - average of four to 16 processor cores a
21:49 - GPU can have thousands of processor
21:51 - cores so something that has 408 gpus
21:53 - could have as many as 40,000 cores
21:55 - here's an image I grabbed right off the
21:57 - Nvidia website and so it really
21:59 - illustrates very well uh like how this
22:02 - would be really good for machine
22:03 - learning or U neural networks because no
22:06 - networks have a bunch of nodes they're
22:08 - very repetitive tasks if you can spread
22:10 - them across a lot of cores that's going
22:11 - to work out really great so gpus are
22:13 - suited uh for repetitive and highly
22:15 - paralleled Computing tasks such as
22:16 - rendering Graphics cryptocurrency mining
22:19 - deep learning and machine
22:21 - [Music]
22:24 - learning we're talking about Cuda before
22:27 - we can let's talk about what Nvidia is
22:29 - so Nvidia is a company that manufactures
22:31 - graphical processor units for gaming and
22:33 - professional markets if you play video
22:34 - games you've heard of Nvidia so what is
22:36 - Cuda it is the compute unified device
22:39 - architecture it is a parallel Computing
22:41 - platform in API by Nvidia that allows
22:43 - developers to use Cuda enable gpus for
22:46 - general purpose Computing on gpus so
22:49 - gpg all major deep learning Frameworks
22:51 - are integrated with Nvidia deep uh
22:53 - learning SDK the Nvidia uh deep learning
22:56 - SDK is a collection of Nvidia libraries
22:58 - for deep learning one of those libraries
23:00 - is the Cuda deep neural network library
23:02 - so CNN so Cuda or CNN provides highly
23:07 - tuned implementations for standard
23:09 - routines such as forward and back uh
23:11 - convolution convolution is really great
23:12 - for um uh uh computer vision pooling
23:16 - normalization activation layers uh so
23:20 - you know in the Azure certification uh
23:22 - for the AI 900 uh they're not going to
23:25 - be talking about Cuda but if you
23:26 - understand these two things you'll
23:27 - understand why gpus uh really matter
23:30 - [Music]
23:34 - okay all right let's get a uh easy
23:37 - introduction into machine learning
23:38 - pipeline so this one is definitely not
23:40 - an exhaustive one and we're definitely
23:41 - going to see more complex ones uh
23:43 - throughout this course but let's get to
23:46 - it here so starting on the left hand
23:47 - side we might start with data labeling
23:49 - this is very uh important when you're
23:51 - doing supervis learning because you need
23:53 - to label your data so the ml model can
23:55 - learn by example during training uh this
23:57 - stage and the feature engineering stage
24:00 - are is considered pre-processing because
24:02 - we are preparing our data to be trained
24:04 - for the model uh when we move on to
24:07 - feature engineering the idea here is
24:08 - that ml models can only work with
24:10 - numerical data so you'll need to
24:12 - translate it into a format that it can
24:13 - understand so extract out the important
24:15 - data that the ml model needs to focus on
24:20 - okay uh then there's the training steps
24:22 - so your model needs to learn how to
24:23 - become smarter it will perform multiple
24:25 - iterations getting smarter with each
24:27 - iteration
24:28 - uh you might also have a hyperparameter
24:30 - tuning uh step here it says tunning but
24:33 - it should say tuning um but the ml model
24:36 - can have different parameters so you can
24:37 - use ml to try out many different
24:40 - parameters to optimize the outcome when
24:41 - you get to deep learning it's impossible
24:44 - to tweak the parameters by hand so you
24:46 - have to use hyperparameter tuning then
24:49 - you have serving sometimes known as
24:50 - deploying uh but you know when we say
24:53 - deploy we talk about the entire pipeline
24:54 - not necessarily just the the ml model
24:56 - step so we need to make an ml model
24:58 - accessible so we serve it by hosting in
25:01 - a virtual machine or container uh when
25:03 - we're talking about Azure um machine
25:06 - learning it's either going to be an
25:07 - Azure kubernetes service or Azure
25:09 - container instance and you have uh
25:11 - inference so inference is the active
25:13 - request uh of requesting to make a
25:16 - prediction so you send your payload with
25:18 - either CSV or whatever and you get back
25:21 - the results you have a real time
25:23 - endpoint and batch processing so real
25:25 - time is just they can batch can be real
25:27 - as well but generally it's slower but
25:29 - the idea is that do I am I making a
25:31 - single item prediction or am I giving
25:33 - you a bunch of data at once and again
25:35 - this is a very simplified ml pipeline
25:37 - I'm sure we'll revisit ml pipeline later
25:39 - in this
25:40 - [Music]
25:43 - course so let's compare the uh the terms
25:47 - forecasting and prediction so
25:48 - forecasting you make a prediction with
25:50 - relevant data it's great for analysis of
25:52 - Trends uh and it's not guessing and when
25:55 - you're talking about prediction this is
25:57 - where you make a prediction without
25:58 - relevant data you use statistics to
25:59 - predict future outcomes it's more of
26:01 - guessing and it uses decision Theory so
26:04 - imagine you have a bunch of data and the
26:06 - idea is you're going to infer from that
26:08 - data okay maybe it's a maybe it's B
26:10 - maybe it's C and for prediction you
26:12 - don't have really much data so you're
26:14 - going to have to uh kind of invent it
26:17 - and the idea is that you'll figure out
26:18 - what the outcome is there these are
26:19 - extremely broad terms but you know just
26:22 - so you have a highle view of these two
26:24 - things
26:26 - okay
26:29 - so what are performance or evaluation
26:31 - metrics well they are used to evaluate
26:32 - different machine learning algorithms
26:34 - the idea is uh you know when your
26:36 - machine learning makes a prediction
26:37 - these are the metrics you're using to
26:39 - evaluate to determine you know is your
26:41 - ml model working as you intended so for
26:44 - different types of problems different
26:45 - metrics matter this is absolutely not an
26:47 - exhaustive list I just want you to get
26:50 - you exposure to these uh words and
26:52 - things uh so that when you see them you
26:53 - go okay I'll come back here and refer to
26:55 - this uh but lots of these you're just
26:57 - it's not it's not necessarily to
26:58 - remember but classification metrics you
27:00 - should know so classification we have
27:02 - accuracy precision recall F1 score rock
27:05 - and a for regression metrics we have MSE
27:07 - R msce Mae ranking metrics we have MMR
27:11 - dcg and dcg statistical metrics we have
27:14 - correlation computer vision metrics we
27:16 - have psnr ssim IOU NLP metrics we have
27:20 - perplexity blue medor Rogue deep
27:22 - learning related metrics we have
27:24 - Inception score I cannot say this
27:26 - person's name but I'm assuming it's a
27:28 - person but uh this Inception distance
27:31 - and there are two categories of
27:33 - evaluation metrics we have internal
27:34 - evaluations so metrics used to evaluate
27:36 - the internals of an ml model so accuracy
27:39 - F1 score Precision recall I call them
27:41 - the famous four using all kinds of uh
27:44 - models and uh external evaluation
27:46 - metrics used to evaluate the final
27:48 - prediction of an ml model so yeah uh
27:52 - don't get too worked up here I know
27:54 - that's a lot of stuff uh the ones that
27:56 - matter we will see again again
27:58 - [Music]
28:02 - okay let's take a look at Jupiter
28:04 - notebook so these are web-based
28:05 - applications for authoring documents
28:07 - that combin live code narrative text
28:09 - equations visualizations uh so if you're
28:12 - doing data science or you're building ml
28:13 - models you absolutely are going to be
28:14 - working with jupyter notebooks they're
28:16 - always integrated into uh cloud service
28:19 - providers ml tools um uh so jupyter
28:22 - notebook actually came about from
28:23 - IPython so IPython is the precursor of
28:25 - it and they extracted that feature out
28:27 - it became jupyter notebook I IPython is
28:29 - now a kernel uh to run uh python so when
28:32 - you execute out python code here it's
28:35 - using IPython which is just a version of
28:37 - python uh jupyter notebooks were
28:39 - overhauled and better integrated into an
28:41 - IDE called Jupiter Labs which we'll talk
28:43 - about here in a moment and you generally
28:44 - want to open notebooks in Labs the
28:46 - Legacy webbased interface is known as
28:48 - Jupiter classic notebooks so this is
28:50 - what the old one looks like you can
28:51 - still open them up but everyone uses
28:53 - Jupiter Labs now okay so let's talk
28:54 - about Jupiter labs jupyter labs is the
28:56 - next generation web-based user interface
28:58 - all familiar features of the classic
29:00 - Jupiter notebook uh is in a flexible
29:02 - powerful user interface it has notebooks
29:05 - a terminal a text editor a file browser
29:07 - Rich outputs Jupiter Labs will
29:09 - eventually replace the classic uh
29:11 - Jupiter notebooks so there you
29:13 - [Music]
29:16 - go we keep mentioning regression but
29:19 - let's talk about it in uh more detail
29:21 - here so we kind of understand the
29:22 - concept so regression is the process of
29:24 - finding a function to equate a labeled
29:26 - data set notice it says labeled that
29:28 - means it's going to be for supervised
29:29 - learning into a continuous variable
29:32 - number so another way to say it is
29:34 - predict this variable in the future so
29:35 - the future is just means like that
29:37 - continuous variable doesn't have to be
29:39 - time but that's just a good example of
29:41 - regression so what will the temperature
29:44 - be next week so will it be 20 Celsius
29:47 - how would we determine that well we
29:48 - would have vectors so dots that are
29:50 - plotted on a graph that has multiple
29:53 - Dimensions the dimensions could be
29:54 - greater than just X and Y you could have
29:56 - uh many
29:58 - uh and then you have a regression Line
29:59 - This is the line that's going through
30:00 - our data set and uh and that's going to
30:03 - help us uh figure out um how to predict
30:06 - the value so how would we do that well
30:08 - we would need to calculate the distance
30:10 - of a vector from the regression line
30:11 - which is called an error and so
30:13 - different regression algorithms use uh
30:15 - the error to predict different variable
30:16 - future variables so just to look at this
30:18 - graphic here so here is our regression
30:20 - line and here is a a DOT like a a vector
30:24 - a piece of information and this distance
30:26 - from the line the the actual distance is
30:28 - what we're going to use in our ml model
30:30 - to figure out if we were to plot another
30:33 - line up here right you know we would
30:35 - compare this line to all the other lines
30:37 - okay and that's how we'd find similarity
30:40 - and what we'll commonly see for this is
30:42 - mean squared error root mean squared
30:44 - error mean absolute error so MSE mrse
30:48 - and Mae
30:50 - [Music]
30:53 - okay let's take a closer look at the
30:55 - concepts of classification so
30:57 - classification is the process of finding
30:59 - a function to divide a labeled data set
31:01 - so again this is supervised learning
31:04 - into classes or categories so predict a
31:07 - category to apply to the inputed data so
31:10 - will it rain next Saturday will it be
31:11 - sunny or rainy so we have our data set
31:14 - and the idea is we're drawing through
31:15 - this a classification line to divide the
31:17 - data set so we're regression we're
31:19 - measuring the line two or the the the
31:21 - vectors to the line and this one it's
31:23 - just what side of the line is it on if
31:25 - it's on this side then it's sunny if
31:26 - it's on this side it's rainy okay for
31:29 - classification algorithms we got log
31:31 - logistic regression decision trees
31:33 - random forests neural networks uh naive
31:36 - Bays K nearest neighbor also known as
31:39 - knnn and support Vector machines svms
31:44 - [Music]
31:47 - okay let's take a closer look at
31:49 - clustering so clustering is the process
31:51 - of grouping unlabeled data so unlabeled
31:53 - data means it's unsupervised learning
31:56 - based on similarity and differences so
31:58 - the outcome could be group data based on
32:00 - similarities or differences I guess it's
32:02 - the same description up here uh but
32:04 - imagine we have a graph and we have data
32:06 - and the idea is we draw boundaries
32:08 - around that to see uh similar groups so
32:10 - maybe we're recommending purchases to
32:12 - Windows computers or recommending
32:14 - purchase to Mac computers now remember
32:16 - this is unlabeled data so the label is
32:18 - being inferred or um or they're just
32:20 - saying these things are similar right so
32:23 - clustering algorithms we got K means k
32:25 - medoids a density Bas hierarchial
32:29 - [Music]
32:32 - okay hey this is Andrew Brown from exam
32:34 - Pro and we're looking at the confusing
32:36 - Matrix and this is a table to visualize
32:38 - the model predictions the predicted
32:39 - versus the ground truth labels the
32:41 - actual also known as an error Matrix and
32:43 - they are useful for classification
32:45 - problems to determine if our um if our
32:48 - classification is working as we think it
32:50 - is so imagine we have a question how
32:52 - many bananas did this person eat or
32:55 - these people eat and so we have this
32:57 - kind of a box here where we have
32:59 - predicted versus actual and it's really
33:01 - comparing the ground truth and what the
33:04 - model predicted right and so on the exam
33:07 - they'll ask you questions like okay well
33:10 - imagine that uh and they might not even
33:12 - say yes or no maybe like zero and one
33:15 - and so what they're saying is you know
33:17 - imagine you have you want to tell us the
33:20 - true positives right and so the idea is
33:22 - they won't show you the labels here but
33:23 - you know one and one would be a true
33:25 - positive and zero and Z would be a false
33:28 - negative okay another thing they'll ask
33:31 - you about these uh confusion matrixes is
33:34 - uh the size of them so the idea is that
33:36 - we're looking right now at a um oops
33:40 - just going to erase that there but we
33:41 - are looking at a binary classifier
33:43 - because we have one label and uh uh just
33:47 - two labels right one and two okay but
33:49 - you could have three say one two and
33:51 - three so how would you calculate that
33:52 - well there would just be a third cell
33:54 - over here uh you know and it's just
33:56 - going to be actual predicted because
33:57 - we're only going to have ground Truth
33:59 - Versus prediction and so that's how
34:01 - you'll know it will be six the size will
34:03 - be six might not say cells but it'll
34:04 - just say six
34:06 - [Music]
34:09 - okay so to understand anomaly detection
34:12 - let's define quickly what is an anomaly
34:14 - so an abnormal thing that is marked by
34:17 - deviation from the norm or standard so
34:20 - anomaly detection is the process of
34:22 - finding outliers within a data set
34:23 - called an anomaly so detecting when a
34:25 - piece of data or access pattern
34:27 - appear suspicious or malicious so use
34:31 - cases for Nom detection can be data
34:32 - cleaning intrusion detection fraud
34:34 - detection system Health monitoring event
34:37 - detection and sensory or sensor networks
34:39 - ecosystem disturbances detection of
34:42 - critical and cascading flaws Anomaly
34:45 - detections by hand is a very tedious
34:47 - process so using ml for anomaly
34:49 - detection is more efficient and accurate
34:51 - and Azure has a service called anomaly
34:53 - detector detects anomalies and data to
34:55 - quickly find uh quick identify and
34:57 - troubleshoot
34:59 - [Music]
35:02 - issues so computer vision is when we use
35:05 - machine learning neural networks to gain
35:06 - high level understanding of digital
35:08 - images or videos so for computer vision
35:11 - deep learning algorithms we have
35:13 - convolutional neural networks these are
35:15 - for image and video recognition they're
35:17 - inspired after how the human eye
35:19 - actually processes information and sends
35:21 - it back to the brain to be processed you
35:22 - have recurrent neural networks rnns
35:25 - which are generally used for handri
35:27 - recognition or speech recognition of
35:28 - course these algorithms have other
35:29 - applications but these are the most
35:31 - common use cases for them for types of
35:34 - computer vision we have image
35:36 - classification so look at an image or
35:37 - video and classify its place in a
35:40 - category object detection so identify
35:42 - objects within an image or video and
35:43 - apply labels and location boundaries
35:46 - semantic segmentation so identify
35:48 - segments or objects by drawing pixel
35:49 - masks around them so great for objects
35:51 - and movement image analysis so analyze
35:54 - uh an image or video to apply
35:57 - descriptive context uh labels so maybe
35:59 - an employee is sitting at a desk in
36:01 - Tokyo would be uh something that image
36:03 - analysis would do optical character
36:05 - recognition or OCR find texts in images
36:08 - or videos and extract them into digital
36:11 - text for editing facial detection so
36:13 - detect faces in a photo or video and
36:16 - draw a location boundary uh and label
36:18 - their expression so for computer vision
36:20 - to some things around Azure or Microsoft
36:22 - Services there's one called seeing AI
36:24 - it's an AI app developed by Microsoft
36:26 - for iOS and so you use your device
36:29 - camera to identify OB uh people and
36:31 - objects and the app is audibly describes
36:33 - those objects for people with visual
36:34 - impairments it's totally free if you
36:36 - have an IOS app I have an Android phone
36:38 - so I cannot use it but I hear it's great
36:40 - some of the Azure computer vision
36:42 - service offering is computer vision so
36:44 - analyze images and videos extract
36:46 - descriptions tags objects and texts
36:48 - custom Vision so custom uh image
36:50 - classification object detection models
36:52 - using your own images face so detect and
36:55 - identify people uh emotions and images
36:58 - form recognizer so translate scan
37:01 - documents into key value or tabular
37:03 - editable
37:04 - [Music]
37:07 - data so natural language processing also
37:10 - known as NLP is machine learning that
37:12 - can understand the context of a corpus
37:15 - Corpus being a body of related text so
37:17 - nlps enable you to analyze and interpret
37:19 - text within documents and email messages
37:21 - interpret or contextualize spoken tokens
37:24 - so for example maybe customer sentiment
37:26 - analysis whether customer is happy or
37:27 - sad synthesize speech so a voice
37:30 - assistant uh assistant talking to you
37:32 - automatically translates spoken or
37:34 - written phrases and sentences between
37:35 - languages interpret spoken or written
37:37 - commands and determine appropriate
37:39 - actions a very famous example for a
37:42 - voice assistant specifically or virtual
37:43 - assistant for Microsoft is Cortana uh it
37:46 - uses the Bing search engine to perform
37:48 - tasks such as setting reminders and
37:49 - answering questions for the user uh and
37:51 - if you're on a Windows 10 machine uh
37:53 - it's very easy to activate Cortana by
37:55 - accident uh when we're talking about
37:57 - azures MLP offering we have text and
37:59 - analytics so sentiment analysis to find
38:01 - out what customers think find topic uh
38:04 - topic relevant phrases using key phrase
38:06 - extraction identify the language of the
38:07 - text with the language detection detect
38:09 - and categorize entities in your text
38:11 - with named entity recognition for
38:13 - translator we have real-time text
38:14 - translation multilanguage support uh for
38:17 - speech service we have transcribe
38:19 - audible speech into readable searchable
38:21 - texts and then we have
38:23 - language understanding also known as
38:25 - Lewis
38:27 - uh natural language processing service
38:29 - that enables you to understand human
38:30 - language in your own application website
38:33 - chatbots iot device and more when we
38:35 - talk about conversational AI it usually
38:38 - generally uses NLP so that's where
38:40 - you'll see that overlap next
38:42 - [Music]
38:45 - okay let's take a look here at
38:47 - conversational AI which is technology
38:49 - that can participate in conversations
38:50 - with humans so we have chat Bots voice
38:52 - assistants and interactive voice
38:54 - recognition systems which is like the
38:56 - the second version to interactive voice
38:58 - response system so you know when you
39:00 - call in and they say press these numbers
39:02 - that is a response system and a
39:03 - recognition system is when they can
39:05 - actually take human uh Speech and
39:07 - translate that into action so the use
39:09 - cases here would be online customer
39:11 - support replaces human agents for uh for
39:13 - replying about customer FAQs maybe
39:16 - shipping questions anything about
39:17 - customer support accessibility so voice
39:19 - operate UI for those who are uh visually
39:21 - impaired HR processes so employee
39:24 - training onboarding updating employee
39:25 - information I've never seen it used like
39:27 - that but that's what they say as a use
39:29 - case Healthcare accessible affordable
39:30 - healthare so maybe you're doing a claim
39:32 - process I've never seen this but maybe
39:34 - in the US where you do more your claims
39:36 - and everything is privatized it makes
39:38 - more sense Internet of Things So iot
39:40 - devices so Amazon Alexa Apple Siri
39:42 - Google home and I suppose Cortana but it
39:44 - doesn't really have a particular device
39:46 - so that's why I didn't list it there
39:48 - computer software so autocomplete search
39:50 - on phone or desktop so that would be
39:52 - Cortana something it could do uh for the
39:54 - two services that are around
39:56 - conversation AI for Azure we have Q&A
39:58 - maker so create a conversational
40:00 - question and answer bot from your
40:01 - existing content also known as a
40:04 - knowledge base and Azure bot service
40:06 - intelligent serverless bot service that
40:08 - scales on demand used for creating
40:09 - publishing managing bots so uh the idea
40:12 - is you make your Bot here and then you
40:14 - deploy it with this
40:15 - [Music]
40:19 - okay let's take a look here at
40:21 - responsible AI which focuses on ethical
40:23 - transparent and accountable uses of AI
40:25 - technology Microsoft put into practice
40:27 - responsible AI via its six Microsoft AI
40:29 - principles this whole thing is invented
40:31 - by Microsoft uh and so you know it's not
40:34 - necessarily a standard but it's
40:35 - something that Microsoft is pushing hard
40:37 - to uh have people adopt okay so we the
40:40 - first thing we have is fairness so this
40:42 - is an AI system which should treat all
40:44 - people fairly we have reliability and
40:46 - safety an AI system should perform
40:47 - reliably and safely privacy and security
40:50 - AI system should be secure and respect
40:52 - privacy inclusiveness AI system should
40:55 - Empower everyone and engage people
40:56 - transparency AI systems should be
40:59 - understandable accountability people
41:00 - should be accountable for AI systems and
41:03 - we need to know these in uh uh greater
41:05 - detail so we're going to have a a short
41:07 - little video on each of these
41:09 - [Music]
41:12 - okay the first on our list is fairness
41:14 - so AI systems should treat all people
41:16 - fairly so an AI system can reinforce
41:18 - existing social social stere uh
41:21 - stereotypical bias can be introduced uh
41:24 - during the development of a pipeline so
41:27 - an A system that are used to allocate or
41:29 - withhold opportunities resources or
41:31 - information uh in domains such as
41:33 - criminal justice employee employment and
41:35 - hiring finance and credit so an example
41:37 - here would be an ml model designed to
41:39 - select a final applicant for hiring
41:40 - pipeline without incorporating any bias
41:42 - based on gender ethnicity or may result
41:44 - in unfair Advantage so Azure ml can tell
41:47 - you how each feature can influence a
41:49 - model's prediction for bias uh one thing
41:52 - that could be of use is fair learn so
41:54 - it's an open source python project to
41:55 - help data science is to improve fairness
41:57 - in the AI systems at the time of I made
41:59 - this course a lot of their stuff is
42:01 - still in preview so you know it's the
42:03 - fairness component is is not 100% there
42:05 - but it's great to see that they're
42:07 - getting that along
42:11 - okay so we are on to our second AI
42:14 - principle for Microsoft and this one is
42:16 - AI systems should perform reliably and
42:18 - safely so AI software must be rigorously
42:20 - tested to ensure they work as expected
42:22 - before release to the end user if there
42:24 - are scenarios where AI is making
42:26 - mistakes it is important to release a
42:27 - report Quantified risks and harms to end
42:29 - users so they are informed of the
42:31 - shortcomings of an AI solution something
42:33 - you should really remember for the exam
42:34 - they'll definitely ask that AI wear
42:36 - concern uh for reliability safety for
42:38 - humans is critically important
42:40 - autonomous vehicles a health diagnosis a
42:43 - suggestion prescriptions and autonomous
42:45 - weapon systems they didn't mention this
42:47 - in their content and I was just like
42:48 - doing some additional research research
42:50 - I'm like yeah you really don't want
42:52 - mistakes when you have automated weapons
42:54 - or ethnically you shouldn't have them at
42:55 - all but hey that's uh that's just how
42:57 - the world works but yeah this is this
42:59 - category
43:00 - [Music]
43:03 - here we're on to our third Microsoft AI
43:06 - principle AI system should be secure and
43:09 - respect privacy so AI can require vast
43:11 - amounts of data to train deep machine ml
43:13 - models the nature of an ml model may
43:15 - require personally identifiable
43:17 - information so
43:19 - piis uh it is important that we ensure
43:21 - protection of user data that it is not
43:23 - leak or disclosed in some cases ml
43:25 - models can be run locally on a user's
43:27 - device so their uh piis remain on their
43:30 - device avoiding the the vulnerability
43:32 - this is called this is like Edge
43:33 - Computing so that's the concept there AI
43:36 - security principles to malicious actors
43:37 - so data origin and lineage data use
43:40 - internal versus external data corruption
43:42 - considerations anomaly detection so
43:44 - there you
43:45 - [Music]
43:48 - go we're on to the fourth Microsoft AI
43:51 - principle so AI systems should Empower
43:53 - everyone and engage people if we can
43:55 - design AI Solutions for the minority of
43:56 - users they can design a solution for the
43:58 - majority of users so when we're talking
43:59 - about minority groups we're talking
44:00 - about physical ability gender sexual
44:02 - orientation ethnicity other factors this
44:04 - one's really simple uh in terms of
44:06 - practicality it doesn't 100% make sense
44:09 - because if you've worked with um uh
44:11 - groups that are deaf and blind
44:13 - developing technology for them a lot of
44:14 - times they need specialized Solutions uh
44:17 - but the approach here is that you know
44:18 - if we can design for the minority we can
44:20 - design for all that is uh the principle
44:22 - there so that's what we need to know
44:25 - okay
44:28 - let's take a look here at transparency
44:30 - so AI systems should be understandable
44:32 - so interpretability and intelligibility
44:34 - is when the end user can understand the
44:36 - behavior of UI so transparency of AI
44:38 - systems can result in mitigating
44:40 - unfairness help developers debug their
44:42 - AI systems ging more trust from our
44:44 - users those build a those who build AI
44:47 - systems should be open about why they're
44:49 - using AI open about the limitations of
44:51 - the AI systems adopting an open source
44:53 - AI framework can provide transparency at
44:56 - least from a technical perspective on
44:57 - the internal workings of an AI
45:03 - system we are on to the last Microsoft
45:06 - AI principle here people uh should be
45:08 - accountable for AI systems so the
45:09 - structure put in place to consistently
45:11 - enacting AI principles and taking them
45:13 - into account AI systems should work
45:15 - within Frameworks of governments
45:16 - organizational principles ethical and
45:18 - legal standards that are clearly defined
45:21 - principles guide Microsoft and how they
45:22 - develop sell and Advocate when working
45:24 - with third parties and this push towards
45:27 - regulation towards a principles so this
45:29 - is Microsoft saying hey everybody adopt
45:31 - our model um there aren't many other
45:33 - model so I guess it's great that
45:34 - Microsoft is taking the charge there I
45:36 - just feel that it needs to be a bit more
45:38 - welldeveloped but what we'll do is look
45:40 - at some more practical examples so we
45:42 - can better understand how to apply their
45:44 - principles
45:48 - okay so if we really want to understand
45:50 - how to apply the Microsoft AI principles
45:52 - they've great created this nice little
45:53 - tool via a free web app for practical
45:55 - scenarios so they have these cards you
45:57 - can read through these cards they're
45:58 - color coded for different scenarios and
46:01 - there's a website so let's go take a
46:02 - look at that and see what we can learn
46:04 - [Music]
46:07 - okay all right so we're here on the
46:09 - guidelines for human AI interaction so
46:11 - we can better understand the uh how to
46:14 - put into practice the Microsoft AI
46:16 - principles they have 18 cards and let's
46:19 - work our way through here and see the
46:20 - examples the first one our list make
46:22 - clear what the system can do help the
46:24 - users understand what the AI system is
46:25 - capable of doing so here PowerPoint
46:27 - quick start Builders an on uh Builds an
46:29 - online outline to help you get started
46:31 - researching a subject it display uh
46:34 - suggested topics that help you
46:35 - understand the features capability then
46:37 - we have the Bing app shows examples of
46:40 - types of things you can search for um
46:42 - Apple watch displays all metrics it
46:44 - tracks and explains how going on the
46:46 - second card we have make clear how well
46:49 - the system can do what it can do so here
46:51 - we have office new uh companion
46:53 - experience ideas dock alongside your
46:55 - work work and offers one-click
46:57 - assistance with grammar design Data
46:59 - Insights richer images and more the
47:01 - unassuming term ideas coupled with label
47:03 - previews help set expectations and
47:05 - presented suggestions the recommender in
47:08 - apple music uses language such as we
47:10 - will think you'll like to communicate
47:14 - uncertainty the help page for Outlook
47:16 - web mail explains the filtering into
47:18 - focused and other and we'll start
47:20 - working right away but we'll get better
47:21 - with use making clear the mistakes uh
47:24 - will happen and you teach the product
47:26 - and set overrides onto our red cards
47:29 - here we have time Services based on
47:32 - context time when to act or interrupt
47:34 - based on the user's current task and
47:36 - environment when it's time to leave for
47:38 - appointments Outlook sends a time to
47:40 - leave notification with directions for
47:42 - both driving and public transit taking
47:43 - into account current location event
47:45 - location real-time traffic
47:48 - information um and then we have after
47:50 - using Apple Maps routing it remembers
47:52 - when you're parked your car when you
47:54 - open the app after for a little while it
47:56 - suggests routing to the location of the
47:58 - park car all these Apple examples make
48:00 - me think that Microsoft has some kind of
48:02 - partnership with apple I guess I guess
48:04 - Microsoft or or Bill Gates did own Apple
48:08 - shares so maybe they're closer than we
48:09 - think uh show contextually relevant
48:11 - information time when to act or
48:13 - interrupt based on user's current task
48:15 - and environment powered by Machine
48:16 - learning acronyms in word helps you
48:18 - understand shorthand employed uh in your
48:21 - own work environment relative to current
48:23 - Open
48:24 - document uh on Walmart.com when the user
48:27 - is looking at a product such as gaming
48:29 - console recommends accessories and games
48:31 - that would go with it when a user
48:33 - searches for movies Google shows results
48:36 - including showtimes near the users's
48:38 - location for the current data onto our
48:40 - fifth card here match based uh we didn't
48:43 - we didn't miss this one right yeah we
48:46 - did okay so we're on the fifth one here
48:48 - match relevant social norms ensure
48:50 - experience is delivered in a way the
48:51 - users would expect given the social
48:53 - cultural context when editor identifies
48:56 - ways to improve writing style presents
48:58 - options politely consider using that's
49:01 - the Canadian way being polite uh Google
49:04 - photos is able to recognize pets and use
49:06 - the wording important cats and dogs
49:08 - recognizing that for many pets are an
49:11 - important part of one's family and you
49:13 - know what uh when I uh started renting
49:15 - my new house uh I I said you know is
49:17 - there a problem with dogs and my
49:18 - landlord said well of course pets are
49:21 - part of the family and that was
49:22 - something I like to hear uh Cortana uses
49:24 - semiformal tone apologizing when unable
49:27 - to find a uh contact which is polite and
49:30 - socially appropriate I like
49:32 - that okay mitigate social biases ensure
49:35 - AI system languages and behaviors do not
49:37 - reinforce undesirable unfair stereotypes
49:40 - and biases my anal summarizes how you
49:42 - spend your time at work then suggest
49:44 - ways to work smarter one ways to
49:46 - mitigate bias is by using gender neutral
49:48 - icons to represent important people
49:50 - sounds good to me a Bing search for CEO
49:52 - or doctor shows images of diverse people
49:55 - in terms
49:56 - of gender and an ethnicity sounds good
49:58 - to me the predictive uh keyboard for
50:01 - Android suggests both genders when
50:02 - typing a pronoun starting with the
50:04 - letter H we're on to our yellow cards uh
50:08 - so support efficient invocation so make
50:10 - it easy to invoke or request system
50:12 - Services when needed so flashfill is a
50:14 - helpful timesaver in Excel that can be
50:16 - easily invoked with on canvas
50:19 - interactions and uh that keep you in
50:20 - flow on amazon.com oh hey there got
50:24 - Amazon in addition to the system giving
50:26 - recommendations as you browse you can
50:28 - manually invoke additional
50:29 - recommendations from the recommender for
50:31 - your menu uh design ideas in Microsoft
50:34 - PowerPoint can be invoked uh with the
50:38 - with the Press of a button if needed I
50:39 - cannot stand it when that pops up I
50:41 - always have to tell it to leave me alone
50:44 - okay support efficient dismal uh
50:47 - efficient
50:49 - disle dismissal oh support efficient
50:52 - dismissal okay make it easy to dismiss
50:54 - or ignore or undesired AI system
50:57 - Services okay this sounds good to me
50:59 - Microsoft forms allows you to create
51:00 - custom surveys quizzes polls
51:02 - questionnaires and forms some choices
51:04 - questions trigger suggested options
51:06 - position beneath the relevant question
51:09 - the suggestion can be easily ignored and
51:11 - dismissed Instagram allows the user to
51:13 - easily hide or report ads that have been
51:15 - suggested by AI by tapping the ellipses
51:18 - at the top of the right of the ad Siri
51:21 - can be easily dismissed uh uh by saying
51:24 - never mind
51:27 - I'm always telling my Alexa never mind
51:30 - support efficient uh correction make it
51:32 - easy to edit refine or recover the AI
51:34 - system uh when the when the AI system is
51:36 - wrong so alt Auto alt text automatically
51:39 - generates alt text for photographs by
51:41 - using intelligent services in the cloud
51:43 - descriptions can be easily Modified by
51:44 - clicking the alt text button in the
51:46 - ribbon once you set a reminder with Siri
51:49 - the UI displays a tap to edit link when
51:52 - Bing automatically corrects spelling
51:54 - errors in search queries it provides the
51:56 - option to revert to the query as
51:58 - originally typed with one click on to
52:01 - card number
52:02 - 10 Scope Services when in doubt so
52:04 - engage in
52:07 - disambiguate
52:09 - disambiguation or gracefully degrade the
52:12 - AI system service when uncertain about a
52:14 - user's goal so when Auto replacing word
52:17 - is uncertain of a correction it engages
52:19 - in disambiguation by displaying multiple
52:22 - options you can select from Siri will
52:24 - let you know it has trouble hearing if
52:26 - you don't respond or talk or or speak
52:28 - too softly big Maps will provide
52:30 - multiple routing options when uh when
52:33 - unable to recommend best one we're on to
52:35 - card number 11 make clear why the system
52:38 - did what it did enable users to access
52:40 - an explanation of why the AI system
52:42 - behaved as it did office online
52:45 - recommends docu documents based on
52:47 - history and activity descriptive text
52:49 - above each document makes it clear why
52:51 - the recommendation is shown product
52:54 - recommendations on Amazon .c include why
52:56 - Rec recommended link that shows that
52:59 - what products in the user shopping
53:00 - history informs the recommendations
53:03 - Facebook enables you to access an
53:05 - explanation about why you are seeing
53:07 - each ad in the news
53:10 - feed onto our green cards so remember
53:13 - recent interactions so maintain
53:15 - short-term memory and allow the user to
53:16 - make efficient references to that memory
53:19 - when attaching a file Outlook offers a
53:21 - list of recent files including recently
53:23 - copied file links Outlook also remembers
53:25 - people you have interacted with recently
53:28 - and displays uh them when addressing a
53:30 - new
53:31 - email uh Bing search remembers some
53:34 - recent queries and search can be
53:35 - continued uh conversationally how old is
53:37 - he after a search for kyanu Reeves Siri
53:41 - carries over the context from one
53:43 - interaction to the next a text message
53:44 - is Created from the person you told Siri
53:46 - to message to onto card number 13 lucky
53:50 - number 13 learn from user Behavior
53:52 - personalize the user experience by
53:54 - learning from their actions over time
53:56 - tap on a search bar in office
53:57 - applications and search lists uh the top
54:00 - three commands on your screen that
54:02 - you're most likely to need to
54:03 - personalize the technology called zero
54:06 - query doesn't even need to type in the
54:08 - search bar to provide a personalized
54:10 - predictive answer amazon.com gives
54:12 - personalized product recomm
54:14 - recommendations based on previous
54:16 - purchases onto card 14 update and adapt
54:19 - Cur uh C uh cautiously limit disruptive
54:22 - changes when updating adaptive adapting
54:24 - the systems behaviors so PowerPoint
54:27 - designer improves slides for office 65
54:29 - subscribers by automatically generating
54:31 - design ideas from to choose from
54:33 - designer has integrated new capabilities
54:36 - such as smart Graphics icon suggestions
54:37 - and existing user experience ensuring
54:40 - the updates are not
54:41 - disruptive office tell office tell me
54:44 - feature shows dynamically recommended
54:46 - items and a designated try area to
54:49 - minimize disruptive changes onto card
54:52 - number 15 encourage granular feedback
54:55 - back enable the users to provide
54:56 - feedback indicating their preferences
54:58 - during regular interactions with the AI
55:00 - system so ideas and Excel empowers you
55:02 - to understand your data through high
55:04 - level visual summaries Trends and
55:05 - patterns encourages feedback on each
55:07 - suggestion by asking is this helpful not
55:10 - only does Instagram provide the option
55:12 - to hide specific ads but it also
55:13 - solicits feedback to understand why the
55:15 - ad is not relevant and Apple's music app
55:18 - love dislike buttons are prominent
55:20 - easily
55:22 - accessible number 16 convey the
55:24 - consequences of us actions immediately
55:26 - update or convey how user actions will
55:27 - impact future behaviors of the AI system
55:30 - you can get stock in G Geographic data
55:32 - types and Excel it is easy as typing
55:35 - text into a cell and converting it to
55:37 - stock data type or geograph geography
55:40 - data type when you perform the
55:41 - conversion action an icon immediately
55:43 - appears in the converted cells uh upon
55:46 - tapping the like dislike button for each
55:47 - recommendation it at in apple music a
55:50 - pop-up informs the user that they'll
55:52 - receive more or fewer similar
55:54 - recommendations onto card number 17
55:56 - we're almost near the end provide Global
55:58 - controls allow the user to globally
56:00 - customize the system system monitors and
56:04 - how it behaves so editor expands on
56:06 - spelling and grammar checking
56:08 - capabilities of word to include more
56:10 - advanced proofing and editing designed
56:11 - to ensure document is readable editor
56:14 - can flag a range of critique types and
56:16 - allow to customize the thing is is that
56:18 - in word it's so awful spellchecking I
56:21 - don't understand like it's been years
56:23 - and the the spell checking never gets
56:25 - better so they got to emplore better
56:26 - spellchecking AI I think bang search
56:29 - provides settings that impact the the
56:31 - types of results the engine will return
56:33 - for example safe
56:36 - search uh then we have Google photos
56:38 - allows user to turn location history on
56:40 - enough for future photos it's kind of
56:42 - funny seeing like Bing in there about
56:44 - like using AI because at one point it's
56:46 - almost pretty certain that Bing was
56:49 - copying just Google search indexes to
56:50 - learn how to index I don't know that's
56:53 - Microsoft for you uh we're on on to card
56:55 - 18 notify users about changes inform the
56:57 - user when AI system adds or updates his
57:00 - capabilities uh the what's new dialogue
57:02 - in office informs you about changes by
57:04 - giving an overview of the latest
57:05 - features and updates including updates
57:07 - to AI features in Outlook web the help
57:10 - tab includes a what's new section that
57:13 - covers updates so there we go we made it
57:15 - to the end of the list I hope that was a
57:18 - fun listen for you and and there I hope
57:20 - that we could kind of match up the uh
57:23 - the responsible AI I kind of wish what
57:25 - they would have done is actually mapped
57:27 - it out here and said where it match but
57:28 - I guess it's kind of an isolate service
57:30 - that kind of ties in so I guess there we
57:32 - go
57:33 - [Music]
57:37 - okay hey this is Andrew Brown from exam
57:39 - Pro and we are looking at Azure
57:41 - cognitive services and this is a
57:42 - comprehensive family of AI services and
57:44 - cognitive apis to help you build
57:46 - intelligent apps so create customizable
57:48 - pre-trained models built with
57:50 - breakthrough AI researches I put that in
57:52 - quotations I'm kind of throwing some
57:53 - shade at uh Microsoft at Azure just
57:56 - because it's their marketing material
57:58 - right uh deploy cognitive Services
58:00 - anywhere from Cloud to the edge uh with
58:02 - containers get started quickly no
58:04 - machine learning expertise required but
58:06 - I think it it helps to have a bit of
58:08 - background knowledge uh develop with
58:10 - strict ethical standards uh Microsoft
58:12 - loves talking about the responsible um
58:15 - their responsible AI stuff empowering
58:17 - responsible use with industry leading
58:19 - tools and guidelines so let's do a quick
58:21 - breakdown of the types of services in
58:24 - this family so for decision we have
58:25 - anomaly detector identify potential
58:27 - problems early on content moderator
58:30 - detect potentially offensive or unwanted
58:32 - content personalizer create Rich
58:34 - personalized experiences for every user
58:36 - for languages we have language
58:37 - understanding also known as uh Luis
58:40 - Lewis I don't know why I didn't put the
58:41 - initialism there but don't worry we'll
58:42 - see it again build natural language
58:45 - understanding into app spots and iot
58:46 - devices Q&A maker create a
58:48 - conversational question and answer layer
58:50 - over your data text analytics detect
58:53 - sentiment so sentiment is is like
58:55 - whether customers are happy sad glad uh
58:58 - keep phrases and named entities
59:00 - translator detect and translate more
59:02 - than 90 supported languages for speech
59:04 - we have speech to text so transcribe
59:06 - audible um speech into readable search
59:08 - text text to speech convert text to
59:10 - lifelike speech for natural interfaces
59:13 - speech translation so integrate realtime
59:16 - speech translation into your apps uh
59:18 - speaker recognition uh identify and
59:20 - verify uh the People speaking based on
59:23 - audio for vision uh we have computer
59:25 - vision so analyze content in images and
59:27 - videos custom Vision so analyze or sorry
59:31 - customize image Rec image recognition to
59:34 - fit your business needs um pH detect uh
59:37 - and identify people and emotions in
59:39 - images so there you
59:41 - [Music]
59:45 - go so Azure cognitive Services is an
59:48 - umbrella AI service that enables
59:50 - customers to access multiple AI services
59:52 - with an AI key and API endpoint so what
59:55 - you do is you go create a new cognitive
59:57 - service and once you're there it's going
59:59 - to generate out two keys and an endpoint
60:01 - and that is what you're using generally
60:02 - for authentication uh with the various
60:04 - AI services programmatically and that is
60:06 - something that is key to the service
60:08 - that you need to
60:09 - [Music]
60:12 - know so knowledge mining is a discipline
60:15 - in AI that uses a combination of
60:16 - intelligent services to quickly learn
60:18 - from vast amounts of information so it
60:20 - allows organizations to deeply
60:22 - understand and easily explore
60:23 - information uncover hidden insights and
60:25 - find relationships and patterns at scale
60:27 - so we have ingest enrich and explore as
60:31 - our three steps so for ingest content
60:32 - from a range of sources using connectors
60:34 - to First and thirdparty data stores so
60:37 - we might have structured data such as
60:38 - databases csvs uh the csvs would more be
60:42 - semi-structured but we're not going to
60:43 - get into that level of detail
60:45 - unstructured data so PDFs videos images
60:47 - and audio for enrich the content with AI
60:50 - capabilities that let you extract
60:52 - information find patterns and deepen
60:54 - understanding in so cognitive services
60:56 - like Vision language speech decision and
60:59 - search and explore the newly indexed
61:01 - data via search spots existing
61:03 - businesses applications and data
61:05 - visualizations enrich uh structured Data
61:07 - customer relationship management wrap
61:10 - systems powerbi this whole knowledge
61:12 - mining thing is a thing but like I
61:14 - believe that the whole model around this
61:16 - is so that Azure uh shows you how you
61:18 - can use the cognitive services to solve
61:21 - uh things without having to invent new
61:23 - Solutions so let's look at a bunch of
61:24 - use cases that Azure has uh and see what
61:27 - where we can find some in uh useful use
61:29 - so the first one here is for Content
61:31 - research so when organizations uh task
61:33 - employees review and research of
61:35 - technical data it can be tedious to read
61:37 - page after page of dense texts knowledge
61:39 - mining helps employees quickly review
61:42 - these dense materials so you have a
61:43 - document and in the enrichment step you
61:46 - could be doing printed text recognition
61:48 - key phrase extraction sharpener uh
61:50 - sharpen skills technical keyword
61:52 - sanitation format definition minor large
61:55 - scale vocabulary matcher you put it
61:57 - through a search service and now you
61:59 - have search reference library so it
62:00 - makes things a lot easier to work with
62:03 - uh now we have uh audit risk compliance
62:05 - management so developers could use
62:07 - knowledge mining to help attorneys
62:08 - quickly identify entities of importance
62:10 - from Discovery documents and flag
62:12 - important ideas across documents so we
62:14 - have documents so Clause extraction
62:17 - Clause uh classification gdpr risk named
62:20 - uh identity extraction keyphrase
62:22 - extraction language detection automate
62:24 - translation uh then you put it back into
62:26 - a search index and now you can use it a
62:28 - management platform or a word
62:31 - plugin and so we have business Process
62:33 - Management in Industries where bidding
62:35 - competition is fierce or when the
62:37 - diagnosis of a problem must be quick or
62:39 - in near real time companies use
62:41 - knowledge mining to avoid costly
62:44 - mistakes so the client Drilling and uh
62:46 - uh and completion reports document
62:48 - processor AI services and custom models
62:50 - Q for human validation intelligent
62:53 - automation yes send it to a backend
62:55 - system or a data Lake Andor a data Lake
62:57 - and then you do your analytics dashboard
63:00 - then we have customer support and
63:01 - feedback uh analysis so for many
63:04 - companies customer support is costly and
63:06 - in uh efficient knowledge mining can
63:07 - help customer support teams quickly find
63:09 - the right answers for a customer inquiry
63:12 - or assess customer sentiment at scale so
63:15 - you have your Source data you do your
63:16 - document cracking use cognitive skills
63:19 - so pre-trained services or custom uh you
63:21 - have enriched documents from here you're
63:23 - going to do your projections and have a
63:24 - knowledge store you're going to have a
63:26 - search index and then do your analytics
63:28 - something like powerbi uh we have
63:30 - digital assessment management I know
63:31 - there's a lot of these but it really
63:32 - helps you understand how cognitive
63:34 - services are going to be useful uh given
63:36 - the amount of unstructured data created
63:38 - daily many companies are struggling to
63:39 - make use of or find information within
63:42 - their files knowledge mining through a
63:43 - search index makes it easy for end
63:45 - customers and employees to locate what
63:47 - they're looking for faster so you in
63:49 - just like art metadata and the actual
63:50 - images themselves for the top ler we're
63:52 - doing geopoint extractor biographical
63:54 - enricher then down below we're tagging
63:56 - we're custom object detector similar
63:58 - image tagger we put it in a search index
64:00 - they love those search indexes and now
64:02 - you have an art
64:04 - Explorer uh we have contract management
64:07 - this is the last one here many companies
64:09 - create products for multiple sectors
64:11 - hence the business opportunities with
64:12 - different vendors and buyers increase
64:14 - exponentially knowledge mining can help
64:16 - organizations to scour thousands of
64:18 - pages of sources to create accurate bids
64:21 - so here we have RFP documents um this
64:23 - will actually probably come back later
64:25 - in the original set but we will Ex we'll
64:28 - do risk extraction print text
64:30 - recognition keyphrase extraction
64:32 - organizational extraction engineering
64:34 - standards we'll create a search index
64:36 - and put it here this will bring back
64:38 - data also Metadate extraction will come
64:40 - back here and then this is just like a
64:41 - continuous pipeline
64:43 - [Music]
64:46 - okay hey this is Andrew Brown from exam
64:49 - Pro and we are looking at face service
64:51 - and Azure face service provides an AI
64:53 - algorithm that can detect recognize and
64:55 - analyze human faces and images uh such
64:58 - as a face in an image face with specific
65:00 - attributes face landmarks similar faces
65:03 - and the same face as a specific identity
65:06 - across a gallery of images so here is an
65:08 - example of an image uh that I ran that
65:10 - will do in the follow along and uh what
65:13 - it's done is it's drawn a bounding box
65:14 - around the image and there's this ID and
65:17 - this is a unique identifier uh string
65:19 - for each detected face in an image and
65:21 - these can be unique across the gallery
65:22 - which is really useful as well another
65:24 - cool thing you can do is a face
65:26 - landmarks so the idea is that you have a
65:29 - face and it can identify very particular
65:31 - components of it and up to 27 predefined
65:34 - landmarks is what is provided with this
65:36 - face Service uh another interesting
65:39 - thing is face attribut so you can uh
65:41 - check whether they're wearing access
65:43 - accessories so think like earrings or
65:45 - lip rings uh determine its age uh the
65:48 - blurriness of the image uh what kind of
65:50 - emotion is being uh experienced the
65:52 - exposure of the image you know the
65:54 - contrast uh facial hair gender glasses
65:58 - uh your hair in general the head pose
66:01 - there's a lot of information around that
66:02 - makeup which seems to be limited like
66:04 - when we ran it here in the lab all we
66:07 - got back was eye makeup and lip makeup
66:09 - but hey we get some information whether
66:11 - they're wearing a mask uh noise so
66:13 - whether there's artifacts like visual
66:15 - artifacts or occlusion so whether an
66:18 - object is blocking parts of the face and
66:20 - then they simply have a Boolean value
66:22 - for whether the person smileing or not
66:24 - which I assume is a very common
66:26 - component so that's pretty much all we
66:28 - really need to know about the faith
66:29 - service and there you
66:30 - [Music]
66:34 - go hey this is Andrew Brown from exam
66:36 - Pro and we are looking at the speech and
66:38 - translate service so a your's translate
66:40 - service is a translation service as the
66:42 - name implies and it can translate 90
66:44 - languages and dialects and I was even
66:46 - surprised to find out that it can
66:47 - translate into kingon um and it uses a
66:51 - neural machine translation nmt replace
66:53 - reping its Legacy statistical machine
66:56 - translation smt so what my guess here is
66:59 - that statistical meaning that it used
67:01 - classical machine learning back in 2010
67:03 - and and then they decided to switch it
67:05 - over to neural networks um which of
67:08 - course would be a lot more accurate uh
67:09 - as your translate service can support a
67:12 - custom translator so it allows you to
67:13 - extend the service for translation based
67:15 - on your business domain use cases so if
67:17 - you use a lot of technical words and
67:19 - things like that then you can fine-tune
67:21 - that or particular phrases then there's
67:23 - the other service Azure speech service
67:26 - and this is a uh a speech uh synthesis
67:29 - Serv service so what can do is speech to
67:31 - text text to speech and speech
67:33 - translation so it's synthesizing
67:35 - creating new voices okay so we have
67:37 - speech to text so real time speech to
67:39 - text batch uh batching Multi-Device
67:42 - conversation conversation transcription
67:44 - and you can create custom speech models
67:46 - then you have text to speech so this
67:48 - utilizes a speech synthesis markup
67:51 - language so it's just a way of
67:52 - formatting it and it can create cust
67:54 - some
67:54 - voices uh then you have the voice
67:56 - assistant so integrates with the bot
67:58 - framework SDK and speech recognition so
68:01 - speaker verification and identification
68:03 - so there you
68:04 - [Music]
68:07 - go hey this is Andrew Brown from exam
68:10 - Pro and we are looking at text analytics
68:12 - and this is a service for NLP so natural
68:14 - language processing for text Mining and
68:17 - text analysis so text analytics can
68:19 - perform sentiment analysis so find out
68:22 - what people think about your brand or
68:23 - topics so features provide sentiment
68:25 - labels such as negative neutral positive
68:28 - then you have opinion minin which is an
68:30 - aspect based sentiment analysis it's for
68:33 - granular information about the opinions
68:34 - related to aspects then you have key
68:37 - phrase extraction so quickly identify
68:39 - the main Concepts in text you have
68:41 - language detections that detect the
68:42 - language uh of an inputed text that it's
68:45 - written in and you have name entity
68:47 - recognition so Neer so identify and
68:50 - categorize entities in your text as
68:51 - people places objects and quantities and
68:54 - subset of ner is personally identifiable
68:58 - information so piis let's just look at a
69:01 - few of these more in detail some of them
69:03 - are very obvious but some of these would
69:04 - help to have an example so the first
69:05 - we're looking at is key phrase
69:06 - extraction so quickly identify the main
69:09 - Concepts and text so keyphrase
69:10 - extraction works best when you have uh
69:12 - when you give it bigger amounts of text
69:13 - to work on this is the opposite of
69:15 - sentiment analysis which performs better
69:17 - on smaller amounts of text so document
69:20 - sizes can be 5,000 or fewer characters
69:22 - per document and you can have up to a
69:25 - thousand items per collection so imagine
69:28 - you have a movie review with a lot of
69:29 - text in here and you want to uh extract
69:31 - out the key phrases so here it
69:32 - identified Borg ship Enterprise um
69:35 - surface travels things like that uh then
69:37 - you have named entity recognition so
69:40 - this detects words and phrases mentioned
69:42 - in unstructured data that can be
69:43 - associated with one or more semantic
69:45 - types and so here's an example I think
69:48 - this is medicine based and so the idea
69:50 - is that it's identifying it's
69:52 - identifying um
69:54 - these words or phrases and then it's
69:56 - applying a semantic type so it's saying
69:58 - like this is a diagnosis this is a
70:00 - medication class and stuff like that uh
70:02 - semantic type could be more broad so
70:04 - there's location event but location
70:06 - twice your person diagnosis age and
70:08 - there is a predefined set I believe that
70:10 - is in um Azure that you should expect
70:13 - but they have a generic one and then
70:14 - there's one that's for health we're
70:16 - looking at sentiment analysis this
70:18 - graphic makes it uh make a lot more
70:20 - sense when we're splitting between
70:21 - sentiment and opinion mining but the
70:24 - idea here is that sentiment analysis
70:25 - will apply labels and confidence scores
70:27 - to text at the sentence and document
70:30 - level and so labels can include negative
70:33 - positive mixed or neutral and we'll have
70:35 - a confidence score ranging from 0 to one
70:38 - and so over here we have a sentiment
70:40 - analysis of this line here and it's
70:42 - saying that this was a negative
70:43 - sentiment but look there's something
70:44 - that's positive and there's something
70:46 - that's negative so was it really
70:47 - negative and that's where opinion mining
70:49 - gets really useful because it has more
70:51 - granular data where we have a subject
70:54 - and we have an opinion right and so here
70:56 - we can see the room was great but the
70:58 - staff was unfriendly negative so we have
71:00 - a bit of a split there
71:02 - [Music]
71:06 - okay hey this is Andrew Brown from exam
71:08 - Pro and we are looking at optical
71:10 - character recognition also known as OCR
71:13 - and this is the process of extracting
71:14 - printed or handwritten text into a
71:16 - digital and editable format so OCR can
71:19 - be applied to photos of street signs
71:21 - products documents invoices bills s
71:24 - Financial reports articles and more and
71:26 - so here's an example of us extracting
71:28 - out nutritional data or nutritional
71:30 - facts off the back of a food product so
71:34 - Azure has two different kinds of apis
71:36 - that can perform OCR they have the OCR
71:39 - API and the read API so the OCR API uses
71:42 - an older recognition model it supports
71:45 - only images it executes synchros
71:48 - synchronously returning immediately when
71:51 - uh it detects text it's suited for for
71:53 - Less texts it supports more languages
71:56 - it's easier to implement and on the
71:58 - other side we have the read API so this
72:00 - is an updated recognition model supports
72:02 - images and PDFs executes
72:06 - asynchronously paralyzes tasks per line
72:08 - for faster results suited for lots of
72:11 - texts supports fewer languages and it's
72:14 - a bit more difficult to implement and so
72:16 - when we want to use this service we're
72:18 - going to be using uh computer vision SDK
72:22 - okay
72:26 - hey this is Andrew Brown from exam Pro
72:28 - and we're taking a look here at form
72:30 - recognizer service this is a specialized
72:32 - OCR service that translates printed text
72:35 - into digital and editable content it PR
72:37 - preserves the structure and
72:39 - relationships of the form like data
72:41 - that's what makes it so special so form
72:43 - recognizer is used to automate data
72:45 - entry in your applications and enrich
72:46 - your document search capabilities it can
72:48 - identify key value pairs selection marks
72:50 - table structures uh it can produce
72:53 - output structures such as original file
72:55 - relationships bounding boxes confidence
72:58 - score and form recognizer is composed of
73:00 - a custom document processing models
73:02 - pre-built models for invoices receipts
73:04 - IDs business cards the model layouts
73:07 - let's talk about the layout here so
73:08 - extract text selection marks table
73:10 - structures along with bounding box
73:11 - coordinates from documents for
73:13 - recognizer can extract text selection
73:16 - marks and table structures the row and
73:18 - column numbers associated with the text
73:20 - using highdefinition optical character
73:22 - enhancement models
73:24 - that is totally useless that
73:52 - text
74:02 - hey this is Andrew Brown from exam Pro
74:04 - and we are looking at form recognizer
74:06 - service and this is a Specialized
74:08 - Service uh for OCR that translates
74:10 - printed text into digital edible content
74:12 - but the magic here is that preserves the
74:14 - structure and relationship of form likee
74:16 - data so there's an invoice you see those
74:18 - magenta lines it's saying identify that
74:21 - form like data so form recogn is used to
74:24 - automate data entry in your applications
74:25 - and enrich your document search
74:27 - capabilities and it can identify key
74:29 - value pairs selection marks table
74:31 - structures and it can output structures
74:33 - such as original file relationships
74:34 - bounding box boxes confidence scores
74:37 - it's composed of a customer Uh custom
74:40 - document processing model pre-built
74:42 - models for invoices receipts IDs and
74:44 - business cards it's based on this layout
74:46 - model uh and there you
74:48 - [Music]
74:52 - go so let's touch upon custom models so
74:55 - custom models allow you to extract text
74:57 - key value pair selection marks and
74:58 - tabular data from your forms these
75:00 - models are trained with your data so
75:02 - they're tailored to your forms you only
75:04 - need five samples uh sample input forms
75:06 - to start a trained document processing
75:08 - model can output structured data that
75:10 - includes the relationship in the
75:11 - original form document after you train
75:13 - the model you can test and retrain it
75:14 - and eventually use it reliably extract
75:16 - data from uh more forms according to
75:18 - your needs you have two learning options
75:20 - you have unsupervised learnings to
75:22 - understand the layout and Rel ship
75:23 - between Fields entries and your forms
75:25 - and you have supervised learning to
75:26 - extract values of Interest using the
75:28 - labeled form so we've covered
75:30 - unsupervised and supervisor learning so
75:31 - you're going to be very familiar with
75:32 - these two
75:34 - [Music]
75:37 - okay so Forum recognizer service has
75:41 - many pre-built models that are uh easy
75:43 - to get uh started with and so let's go
75:45 - look at them and see what kind of fields
75:47 - it extracts out by default so the first
75:49 - is receipts so sales receipts from
75:52 - Australia Canada great Britain India
75:53 - United States will work great here and
75:55 - the fields it will extract out is
75:56 - receipt type Merchant name Merchant
75:57 - phone number Merchant address
75:59 - transaction date transaction time total
76:01 - subtotal tax tip items name quantity
76:04 - price total price if there's information
76:06 - that is on a a receipt that you're not
76:08 - getting out of these fields and that's
76:10 - where you make your own custom model
76:11 - right for business cards it's only
76:14 - available for English business cards but
76:16 - uh we can extract out contact names
76:17 - first name last name company names
76:19 - departments job titles emails websites
76:21 - addresses mobile phones faxes work
76:23 - phones uh and other phone numbers not
76:25 - sure how many people are using uh
76:27 - business cards these days but hey they
76:29 - have it as an option for invoices
76:31 - extract data from invoices in various
76:32 - formats and return structured data so we
76:35 - have customer name customer ID purchase
76:37 - order invoice ID invoice date due date
76:39 - vendor name vendor address vendor
76:41 - address receipt customer address
76:43 - customer address receipt billing address
76:45 - billing address receipt shipping address
76:47 - subtotal total tax invoice total amount
76:50 - due service address uh remittance
76:52 - address uh start service start date and
76:54 - end date uh previous unpaid balance and
76:57 - then they even have one for line items
76:59 - so items amount description quantity
77:03 - unit price product code unit date tax
77:06 - and then for IDs which could be
77:08 - worldwide passports us driver license
77:10 - things like that um you would have
77:12 - Fields such as country region date of
77:14 - birth date of expire expiration document
77:16 - name first name last name nationality
77:18 - sex machine readable zone I'm not sure
77:22 - what that is document type and address
77:25 - and region uh and there are some
77:27 - additional features with some of these
77:29 - models we didn't really cover them it's
77:31 - not that important but um yeah there we
77:33 - [Music]
77:37 - go hey this is Andrew Brown from exam
77:39 - Pro and we were looking at natural
77:41 - understanding or LS or Louise depends on
77:45 - how You' like to say it and this is a no
77:47 - code ml service to build language uh
77:49 - natural language into Apps Bots and iot
77:52 - devices to quickly create Enterprise
77:54 - ready custom models that continuously
77:56 - improve so Lewis I'm going to just call
77:58 - it lwis cuz that's what I prefer is
78:00 - access via its own isolate domain at
78:03 - lewis. and it utilizes NLP and nlu so
78:07 - nlu is the ability to perform or ability
78:10 - to transform a linguistic statement to a
78:12 - representation that enables you to
78:14 - understand your users naturally and it
78:16 - is intended to focus on intention and
78:18 - extraction okay so where the users want
78:21 - or sorry what the users want and what
78:23 - the users are talking about so uh the
78:26 - loose application is composed of a
78:28 - schema and the schema is autogenerated
78:30 - for you when you use the Lewis AI web
78:32 - interface so you definitely AR going to
78:34 - be writing this by hand but it just
78:35 - helps to see what's kind of in there if
78:37 - you do have some programmatic skills you
78:38 - obviously you can make better use of the
78:40 - service than just the web interface but
78:42 - the schema defines intentions so what
78:44 - the users are asking for a l app always
78:47 - contains a nonone intent we'll talk
78:49 - about why that is in a moment and
78:50 - entities what parts of the intent is
78:53 - used to determine the answer then you
78:55 - also have utterances so examples of the
78:57 - user input that includes intent and
78:59 - entities to train the ml model to match
79:01 - predictions against the real user input
79:04 - so an intent requires one or more
79:06 - example utterance for training and it is
79:08 - recommended to have 15 to 30 example
79:10 - utterances to explicitly train uh to
79:13 - ignore an utterance you use the nonone
79:15 - intent so intent classifies user
79:20 - utterances and entities extract data
79:22 - from utterances so hopefully that
79:24 - understands I always get this stuff
79:25 - mixed up it always takes me a bit of
79:26 - time to understand there is more than
79:28 - just these things there's like features
79:30 - and other things but you know for the AI
79:33 - 900 we don't need to go that deep okay
79:36 - uh just to get to visualizing this to
79:38 - make a bit easier so imagine we have
79:39 - this uh this utterance here these would
79:42 - be the identities so we have two and
79:44 - Toronto this is the example utterance
79:46 - and then the idea is that you'd have the
79:47 - intent and the intent and if you look at
79:50 - this keyword here this really helps
79:51 - where it says classify it's that's what
79:52 - what it is it's a classification of this
79:55 - example utterance and that's how the ml
79:57 - model is going to learn
79:59 - [Music]
80:02 - okay hey this is Andre Brown from exam
80:05 - Pro and we are looking at Q&A maker
80:06 - service and this is a cloud-based NLP
80:09 - service that allows you to create a
80:10 - natural conversational layer over your
80:12 - data so Q&A maker is hosted on its own
80:14 - isolate domain at Q&A maker. it will
80:17 - help the most uh it will help you find
80:19 - the most appropriate answer from any
80:20 - input from your custom knowledge base of
80:22 - information so you can commonly uh it's
80:25 - commonly used to build conversation
80:26 - clients which include social apps chat
80:28 - Bots speech enabled uh desktop
80:31 - applications uh Q&A maker doesn't store
80:33 - customer data all the customer data
80:35 - stored in the region the customer
80:37 - deploys the the dependent Services
80:39 - instances within okay so let's look at
80:41 - some of the use cases for this so when
80:43 - you have static information you can use
80:45 - Q&A maker uh in your knowledge base of
80:48 - answers this knowledge base is customed
80:49 - to your needs which you've built with
80:51 - documents such as PDF and URL
80:53 - when you want to provide the same answer
80:54 - to a repeat question command when
80:56 - different users submit the same question
80:57 - the answers is returned when you want to
80:59 - filter stack information based on meta
81:02 - information so metatag data is provide
81:05 - uh provides additional filtering options
81:06 - relevant to your client application
81:08 - users and information common meded
81:09 - information includes chitchat content
81:11 - type format content purpose content
81:14 - freshness and there's a use case when
81:16 - you want to manage a bot conversation
81:18 - that includes static information so your
81:20 - knowledge base takes uh takes a user
81:22 - conversation text or command and answers
81:24 - it if the answer is part of a
81:26 - predetermined conversation flow
81:28 - represented in the knowledge base with
81:29 - multiple TurnKey context the bot can
81:32 - easily provide this flow so Q&A maker
81:35 - Imports your content into a knowledge
81:37 - base of questions and answer Pairs and
81:39 - Q&A maker can build your knowledge base
81:41 - from an existing document manual or
81:43 - website you're all docx PDF I thought
81:45 - this was the coolest thing so you can
81:47 - just basically have anyone write a doc X
81:49 - as long as it has a heading and a and
81:51 - text and I think you can even extract at
81:52 - images and it'll just turn it into uh
81:55 - the bot it just saves you so much time
81:56 - it's crazy it will use ml to extract the
81:59 - question and answer pairs the content of
82:01 - the question of answer pairs include all
82:03 - the alternate forms of the question
82:04 - metad dag tags used to filter choices
82:07 - during the search followup prompts to
82:09 - continue to search refinement uh
82:10 - refinement K maker stores answers text
82:13 - in markdown once your knowledge base is
82:16 - imported you can finetune the imported
82:17 - results by editing the question and
82:19 - answer pairs as seen here uh there is
82:22 - the chat box so you can converse with
82:24 - your Bot through a chat box I wouldn't
82:25 - say it's particularly a feature of Q&A
82:28 - maker but I just want you to know that's
82:29 - how You' interact with it so when you're
82:31 - using the Q&A maker AI the Azure bot
82:33 - service the bot composer um or via
82:36 - channels you'll get an embeddable one
82:37 - you'll see this box where you can start
82:39 - typing in your questions and and get
82:41 - back the answers to test it here an
82:43 - example is a multi-term conversation so
82:46 - somebody asked a question a generic
82:47 - question and that said hey are you
82:49 - talking about adus or Azure which is
82:51 - kind of like a follow-up prompt and
82:52 - we'll talk about multi-turn here in a
82:54 - second but uh that's something I want
82:55 - you to know about okay so chitchat is a
82:58 - feature in Q&A maker that allows you to
82:59 - easily add prepopulated sets of top Chit
83:02 - Chats into your knowledge base the data
83:04 - set has about 100 scenarios of chitchat
83:06 - in voices of multiple personas so the
83:08 - idea is like if someone says something
83:10 - random like how are you doing what's the
83:12 - weather today things that your Bot
83:13 - wouldn't necessarily know it has like
83:15 - canned answers and it's going to be
83:17 - different based on how you want the
83:19 - response to be okay uh there's a concept
83:22 - of layered ranking so uh Q&A maker
83:24 - system is a layered ranking approach the
83:26 - data is stored in Azure search which
83:28 - also serves as the first ranking layer
83:31 - the top result for uh from Azure search
83:33 - are then passed through Q&A makers NLP
83:36 - reranking model to produce the final
83:37 - results and confidence score T touching
83:40 - on multi- turn conversation is a
83:42 - follow-up prompt and context to manage
83:44 - the multiple turns known as multi-t for
83:47 - your Bot from one question to another
83:49 - when a question can't be answered in a
83:50 - single turn that is when you're using
83:52 - multi-term conversation so Q&A maker
83:55 - provides multi-term prompts and active
83:56 - learning to help you improve your
83:57 - questions based on key and answer Pairs
83:59 - and gives you the opportunity to connect
84:01 - questions and answer pairs the
84:02 - connection allows the client application
84:05 - to provide a top answer and provide more
84:07 - questions to refine the search for a
84:08 - final answer after the knowledge base
84:10 - receives questions from users at the
84:12 - publish endpoint cre maker applies
84:13 - active learning to these rule work
84:15 - questions to suggest changes to your
84:16 - knowledge base to improve the quality
84:18 - all
84:21 - right
84:22 - [Music]
84:24 - hey this is Andrew Brown from exam Pro
84:26 - and we are looking Azure bot service so
84:28 - the Azure bot service is an intelligent
84:29 - servess bot service that scales on
84:31 - demand used for creating publishing and
84:33 - managing bots so you can register and
84:35 - publish a variety of bots from the Azure
84:36 - portal so here there's a bunch of ones
84:38 - I've never heard of um probably with
84:40 - thirdparty providers partnered with
84:41 - Azure and then there's the ones that we
84:43 - would know like the Azure healthbot the
84:45 - Azure bot or the uh web app bot which is
84:48 - more of a generic one so Azure Bop
84:51 - service Bop bot service can integrate
84:53 - your Bot with other Azure Microsoft or
84:57 - thirdparty service uh Services via
84:59 - channel so you can have a direct line
85:01 - Alexa Office 365 Facebook keik line
85:06 - Microsoft teams Skype uh twio and more
85:10 - all right and two things that are
85:12 - commonly associated with the Azure bot
85:14 - service is the bot framework and Bot
85:16 - composer in fact it was really hard just
85:18 - to make make this slide here because
85:20 - they just weren't very descriptive on it
85:21 - cuz they wanted to push these other two
85:22 - things here but let's talk about the bot
85:24 - framework SDK so the bot framework SDK
85:27 - which is now version four is an open
85:29 - source SDK that enables developers to
85:31 - model and build sophisticated
85:32 - conversations the bot framework uh along
85:35 - with the Azure bot service provides an
85:37 - end-to-end workflow so we can design
85:39 - build test publish connect and evaluate
85:43 - our uh Bots okay with this with this
85:45 - framework developers can create Bots
85:47 - that use speech understand uh natural
85:49 - language handle questions answers and
85:51 - more the bot framework includes a module
85:53 - accessible SDK for building Bots as well
85:55 - as tools templates and related AI
85:57 - Services then you have bot framework
86:00 - composer and uh this is built on top of
86:02 - the bot framework SDK it's an open
86:04 - source IDE for developers to author test
86:06 - provision and manage conversational
86:08 - experiences you can download it's an app
86:10 - on Windows OS X and Linux is probably
86:12 - built uh using uh like uh web technology
86:16 - and so here is the actual uh app there
86:18 - and so you can see there's kind of a bit
86:19 - of a flow and things you can do in there
86:21 - so you can either use C or not to build
86:23 - your Bot you can deploy the bot to the
86:26 - Azure web apps or Azure functions you
86:28 - have templates to build Q&A maker bot
86:30 - Enterprise or personal assistant bot
86:32 - language bot calendar or people bot uh
86:35 - you can test and debug via the bot
86:36 - framework emulator uh and it has a
86:38 - built-in package manager there's a lot
86:40 - more to these things but again at the AI
86:42 - 900 this is all we need to know um but
86:44 - yeah there you
86:45 - [Music]
86:48 - go hey this is Andrew Brown from exam
86:51 - Pro and we are looking at Azure machine
86:53 - learning service I want you to know
86:54 - there's a classic version of the service
86:56 - it's still accessible in the portal this
86:58 - is not an exam we are going to 100%
87:00 - avoid it uh it has severe limitations we
87:03 - cannot transfer anything over from the
87:05 - classic to the new one uh so the one
87:07 - we're going to focus on is the Azure
87:08 - machine learning service you do create
87:10 - Studios within it so you'll hear me say
87:12 - Azure machine Learning Studio and I'm
87:13 - referring to the new one a service that
87:15 - simplifies running AI ml work related
87:17 - workloads allowing you to build flexible
87:19 - automated ml pipelines use python or Run
87:22 - Deep learning workloads such as
87:24 - tensorflow we can make Jer notebooks in
87:26 - here so build and document your machine
87:28 - learning models as you build them share
87:29 - and collaborate Azure machine learning
87:31 - SDK for python so an SDK designed
87:33 - specifically to interact with the Azure
87:35 - machine learning Services it does ML Ops
87:38 - machine learning operations so end to
87:40 - end automation of ml model pipelines
87:41 - cicd training inference aure machine
87:44 - learning designer uh uh so this is a
87:47 - drag and drop interface to visually
87:48 - build test deploy machine learning
87:50 - models uh technically pipelines I guess
87:52 - as a data labeling service so assemble a
87:55 - team of humans to label your training
87:57 - data responsible machine learning so
87:58 - model fairness through disparity metrics
88:01 - and mitigate unfairness at the time of
88:03 - this Ser this is not very good but it's
88:04 - supposed to tie in with the responsible
88:06 - AI that Microsoft is always promoting
88:13 - okay so once we launch our own uh Studio
88:16 - within Azure machine learning service
88:17 - you're going to get this nice big bar uh
88:19 - or navigation left hand side it shows
88:21 - you there's a lot of stuff that's in
88:22 - here so let's just break it down and
88:23 - what all these things are so for
88:24 - authoring we got notebooks these are
88:26 - jupyter notebooks and IDE to write
88:27 - python code to build ml models they kind
88:29 - of have their own preview which I don't
88:30 - really like but there is a way to bridge
88:32 - it over to jupyter notebooks or into
88:34 - Visual Studio code we have automl
88:36 - completely automated process to build
88:38 - and train ml models it's you're limited
88:40 - to only three types of models but still
88:42 - that's great we have the designer so
88:43 - visual drag and drop designer to
88:45 - construct end to-end ml pipelines for
88:47 - assets we have data sets so data you can
88:49 - upload which we will be used which will
88:51 - be used for for training experiments
88:53 - when you run a training job they are
88:55 - detailed here uh pipelines ml workflows
88:58 - that you have built or have used in the
89:00 - designer model so a model registry
89:02 - containing train models that can be
89:04 - deployed endpoints so when you deploy a
89:06 - model it's hosted on accessible endpoint
89:08 - so you're going to be able to uh access
89:10 - it via a rest API or maybe the SDK uh
89:13 - for manage we got compute the underlying
89:15 - Computing instances used uh for
89:17 - notebooks training and inference
89:19 - environments are
89:20 - reproducible uh python environment for
89:22 - machine learning experiments data stores
89:25 - a data repository where your data
89:27 - resides data labeling uh so you have a
89:29 - human with ML assisted labeling to label
89:32 - your data for supervised Learning Link
89:34 - services external service you can
89:35 - connect to the workspace such as Azure
89:37 - synapse
89:39 - [Music]
89:42 - analytic let's take a look at uh the
89:44 - types of compute that is available in
89:46 - our Azure machine Learning Studio we got
89:48 - four categories uh we have compute
89:50 - instances to uh development workstations
89:53 - that data scientists can use to work
89:54 - with data and models compute clusters to
89:57 - scalable clusters of VMS for on demand
89:59 - processing experimentation code
90:01 - deployment targets for Predictive
90:02 - Services that use your trained models
90:05 - and attach compute links to existing
90:07 - Azure compute resources such as uh Azure
90:09 - VMS uh and Azure data brick clusters now
90:13 - uh what's interesting here is like with
90:15 - this compute you can see that you can
90:16 - open it in Jupiter Labs jupyter vs code
90:18 - R studio and terminal but you can you
90:22 - can work with uh your computer instances
90:24 - your development workstations uh
90:26 - directly in the studio which that's the
90:27 - way I do it um what's interesting is for
90:30 - inference that's when you want to make a
90:32 - prediction you use Azure kubernetes
90:34 - service or Azure container instance and
90:36 - I didn't see it show up under here so
90:37 - I'm kind of confused whether that's
90:39 - where it appears U maybe we'll discover
90:41 - as we do the follow alongs that they do
90:43 - appear here but uh I'm not sure about
90:45 - that one but yeah those are the four
90:47 - there
90:50 - okay So within Azure machine Learning
90:53 - Studio we can do some data labeling so
90:55 - we create data labeling jobs to prepare
90:57 - your ground Truth for supervised
90:58 - learning we have two options human in
91:00 - the loop labeling you have a team of
91:01 - humans that will apply labeling these
91:03 - are humans you grant access to labeling
91:05 - uh machine learning assisted daily
91:07 - labeling you will use ml to perform uh
91:09 - labeling so you can export the label
91:11 - data from for machine learning
91:13 - experimentation at any time uh your
91:15 - users often export multiple times and
91:17 - train different models rather than wait
91:19 - for all the images to be labeled images
91:21 - labeled can be exported in Coco format
91:23 - that's why we talked about Coco uh a lot
91:26 - earlier in our data set section as your
91:28 - machine learning data set and this is
91:29 - the data set format that makes it easy
91:31 - to use for training and Azure machine
91:33 - learning so generally you want to use
91:34 - that format the idea is you would choose
91:36 - a labeling task type uh and that way you
91:39 - would have this UI and then people would
91:41 - go in and and just click buttons and do
91:43 - the labeling
91:47 - okay so aure ml data store securely
91:50 - connects you to storage service services
91:52 - on Azure without putting your
91:53 - authentication credentials and the
91:55 - Integrity of your original data source
91:56 - at risk so here is the example of data
91:59 - source that are available to us in the
92:00 - studio and let's just go quickly through
92:02 - them so we have Azure blob storage this
92:04 - is data that is stored as objects
92:05 - distributed across many machines Azure
92:07 - file share am mountable file share via
92:09 - SMB and NFS protocols Azure data Lake
92:12 - storage Gen 2 um this is blob storage
92:14 - design for vast amounts of big data
92:16 - analytics Azure SQL this is a fully
92:18 - managed mssql relational database azure
92:21 - postest datas this is an open source
92:22 - relational database often considered an
92:25 - object related database preferred by
92:26 - developers a your MySQL another open
92:29 - source relational database the most
92:30 - popular one and considered a pure
92:32 - relational database
92:36 - okay so aure ml data sets makes it easy
92:40 - to register your data sets for use with
92:42 - your ml workload so what you do is you'd
92:45 - add a data set and you get a bunch of
92:46 - metadata associated with it uh and you
92:48 - can also upload uh addition like the
92:50 - data set again to have multiple versions
92:53 - so you'll have a current version and a
92:54 - latest version uh it's very easy to get
92:57 - started working with them because
92:58 - they'll have some sample code that's for
92:59 - the Azure ml SDK uh to import that into
93:04 - uh into your Jupiter notebooks uh for
93:06 - data sets you can generate profiles that
93:08 - will give you summary statistics
93:09 - distribution of data and more you will
93:11 - have to use a compute instance to
93:12 - generate that data so you press the
93:14 - generate profile and you'd have that
93:15 - stored I think it's in Blob storage
93:17 - there are open data sets these are
93:19 - publicly hosted data sets that are
93:20 - commonly used for learning how to build
93:22 - ml models so if you go to open data sets
93:25 - you just choose one and so this is a
93:27 - curated list of open data sets that you
93:28 - can quickly add to your data store great
93:30 - for learning how to use autom ml or
93:32 - aszure machine learning designer or any
93:34 - kind of uh ml uh workload if you're new
93:37 - to it that's why we covered mnist and
93:40 - Coco earlier just because those are some
93:41 - common data sets there but there you
93:44 - [Music]
93:47 - go taking a look here at Azure ml
93:49 - experiments this is a logical grouping
93:51 - of azure runs and runs act uh is the act
93:53 - of running an ml task on a virtual
93:55 - machine or container so here's a list of
93:57 - them and uh it can run various uh types
94:00 - of ml tasks so scripts could be
94:01 - pre-processing autom ml uh a training
94:04 - pipeline but what it's not going to
94:06 - include is inference and what I mean is
94:07 - once you've deployed your model or
94:09 - Pipeline and you uh uh make predictions
94:12 - via a request it's just not going to
94:14 - show up under here
94:16 - [Music]
94:19 - okay okay so we have azure ml pipelines
94:22 - which is an exec workflow of a complete
94:24 - machine learning task not to be confused
94:26 - with Azure pipelines which is part of
94:28 - azure devops or data Factory which has
94:30 - its own pipelines it's a total a totally
94:32 - separate thing here so subas are
94:34 - encapsulated is a series of steps within
94:36 - the pipeline independent steps allow
94:38 - multiple data scientists to work on the
94:40 - same pipeline at the same time without
94:41 - over taxing compute resources separate
94:44 - steps also make it easy to use different
94:46 - compute type sizes for each step when
94:48 - you rerun a pipeline the Run jumps to
94:50 - the steps that need to be rerun such as
94:52 - the updated training script steps do not
94:54 - need to be rerun and they will be
94:56 - skipped after a uh pipeline has been
94:58 - published you can configure a rest
95:00 - endpoint which allows you to rerun the
95:01 - pipeline from any platform or stack
95:03 - there's two ways uh to build pipelines
95:06 - you can use the Azure ml designer or
95:08 - programmatically using Azure machine
95:10 - learning python SDK so here is example
95:13 - of some code just make a note here I
95:15 - mean it's not that important but no it's
95:17 - just you create steps okay and then you
95:19 - assemble all the steps into a pipeline
95:21 - line here all
95:23 - [Music]
95:26 - right so as your machine learning
95:29 - designer lets you quickly build as your
95:31 - ml pipelines without having to write any
95:33 - code so here is what it looks like and
95:35 - over there you can see our pipeline is
95:37 - quite Visual and on the left hand side
95:39 - you have a bunch of assets you can drag
95:41 - out that are pre-built there so uh it's
95:43 - a really fast way for building a
95:45 - pipeline so you do have to have a good
95:47 - understanding of um ml pipelines nend to
95:49 - end to make good use of it uh once you
95:51 - you trained your pipeline you can create
95:52 - an inference pipeline so you drop down
95:55 - and you say whether you want it to be
95:56 - real or batch and you can TW toggle
95:59 - between them later so I mean there's a
96:01 - lot to this service but for the AI 900
96:04 - we don't have to go uh diving too deep
96:09 - okay so Azure ml models or the model
96:13 - registry allows you to create manage and
96:15 - track your registered models as
96:16 - incremental versions under the same name
96:18 - so each time you register a model with
96:20 - the same name as an existing one the
96:22 - registry assures that it's a new version
96:24 - Additionally you can provide metadata
96:26 - tags and use tags when you search for
96:27 - models so yeah it's just really easy way
96:30 - to share and deploy or download your
96:32 - models
96:33 - [Music]
96:36 - okay Azure MLM points allow you to
96:39 - deploy machine learning models as a web
96:41 - service so the workflow for deploying
96:42 - model is register the model prepare an
96:44 - entry script prepare an inference
96:45 - configuration deploy the model locally
96:47 - to ensure everything works compute uh
96:50 - choose a compute Target redeploy the
96:52 - model uh to the cloud test the resulting
96:54 - web service so we have uh two options
96:57 - here real time endpoints so an endpoint
96:59 - that provides remote access to invoke
97:00 - the ml model uh service running on
97:03 - either Azure kubernetes service AKs or
97:06 - Azure container instance ACI then we
97:08 - have pipeline endpoints so endpoint that
97:10 - provides remote access to invoke an ml
97:12 - pipeline you can parameterize the
97:14 - pipeline endpoint for manage
97:15 - repeatability in batch scoring and
97:17 - retraining
97:19 - scenarios um and so you can deploy a
97:21 - model to an endpoint it will either be
97:23 - deployed to a AKs or ACI as we said
97:26 - earlier uh and the thing is is that when
97:28 - you do do that just understand that
97:30 - that's going to be shown under the AKs
97:32 - or ACI um within the Azure portal it's
97:35 - not Consolidated under the Azure machine
97:37 - Learning Studio when you've deployed a
97:38 - realtime endpoint you can test the
97:40 - endpoint by sending either a single
97:41 - request or batch request so they have a
97:43 - nice form here where it's single or it's
97:46 - um like here it's a CSV that you can
97:47 - send so there you
97:50 - go
97:53 - so Azure has a built-in jupyter like
97:55 - notebook editor so you can build and
97:57 - train your ml models and so here is an
97:59 - example of it I personally don't like it
98:01 - too much but that's okay because we have
98:03 - some other options to make it easier but
98:05 - what you do is you choose your compute
98:07 - uh instance to run the notebook you'll
98:08 - choose your kernel uh which is a
98:10 - pre-loaded programming language and
98:12 - programming libraries for different use
98:14 - cases but that's a Jupiter kernel uh
98:16 - concept there uh so you can open the
98:18 - notebook in a more familiar ID such as
98:20 - vs code jupyter notebook classic or
98:23 - Jupiter lab so you go there drop it down
98:25 - choose it and open it up and now you're
98:26 - in a more familiar territory the vs code
98:29 - one is exactly the same experience as
98:31 - the um the one in Azure or Azure ml
98:34 - Studio I personally don't like it I
98:36 - think most people are going to be using
98:37 - the notebooks but it's great that they
98:39 - have all those
98:40 - [Music]
98:44 - options so Azure automated machine
98:47 - learning also known as autom ml
98:48 - automates the process of creating an ml
98:50 - model so with Azure automl you supply a
98:52 - data set choose a task type uh and then
98:55 - automl will train and tune your model so
98:57 - here are task types let's quickly go
98:59 - through them so we have classification
99:00 - when you need to make a prediction based
99:02 - on several classes so binary
99:04 - classification multiclass classification
99:06 - regression when you need to predict a
99:08 - continuous number value and then a Time
99:10 - series forecasting when you need to
99:12 - predict the value based on time so just
99:14 - look at them a little bit more in detail
99:16 - so classification is a type of
99:17 - supervised learning in which the model
99:18 - learns using training data and appli
99:21 - those learnings to new data so here is
99:23 - an example uh or this is just the option
99:25 - here and so the goal of classification
99:27 - is to predict which categories new data
99:29 - will fall into based on learning from
99:31 - its training data so binary
99:32 - classification is a record uh is labeled
99:35 - out of two possible labels so maybe it's
99:37 - true or false zero or one it's just two
99:40 - values multiclass classification is a
99:42 - record is labeled out of range of out of
99:44 - a range of
99:45 - labels uh and so it could be like happy
99:47 - sad mad or rad and just you know I can
99:50 - see there's a smelling mistake there but
99:51 - yeah there should be an F so let's just
99:53 - correct that there we go uh you can also
99:56 - apply deep learning and so if you turn
99:58 - deep learning on you probably want to uh
100:00 - use a gpus uh compute instance just
100:02 - because or um compute cluster Because
100:05 - deep learning really prefers uh gpus
100:08 - okay looking at regression it's also a
100:11 - type of supervised learning where the
100:12 - model learns uh using training data and
100:15 - applies those learnings to new data but
100:16 - it's a bit different where the goal of
100:18 - regression is to predict a variable in
100:19 - the future uh then you have time series
100:22 - forecasting and this sounds a lot like
100:24 - um uh regression because it is so
100:27 - forecast Revenue inventory sales or
100:30 - customer demand an automated time series
100:32 - experiment that is treated as a
100:34 - multivariant regression problem past
100:36 - time series values are pivoted to become
100:38 - additional dimensions for the regressor
100:40 - together with other predictors and
100:43 - unlike classical time series methods has
100:45 - an advantage of naturally incorporating
100:48 - multiple contextual variables and their
100:50 - relationship to one another during
100:52 - training so use cases here or Advanced
100:54 - configurations I should say holiday
100:56 - detection and featurization time series
100:58 - uh deep learning uh neural networks so
101:01 - you got Auto arima profit forecast TCN
101:05 - uh many model supports through grouping
101:07 - rolling origin cross validation
101:09 - configurable Labs rolling window
101:11 - aggregate features so there you
101:14 - [Music]
101:18 - go so within automl we have data guard
101:21 - rails and these are run by automl when
101:24 - automatic featurization is enabled it's
101:26 - a sequence of checks to ensure
101:27 - highquality input data is being used to
101:30 - train the model so just to show you some
101:32 - information here so the idea is it could
101:35 - apply validation split handling so the
101:36 - input data has been split for validation
101:38 - to improve the performance then you have
101:41 - missing feature value uh imputation so
101:44 - no features missing values were detected
101:46 - in training data High cardinality
101:48 - feature detection your inputs were
101:50 - analyzed and no no high cardinality
101:51 - features were detected High cardinality
101:53 - means like if you have too many
101:55 - dimensions it becomes very dense or hard
101:56 - to process the data um so that's
101:59 - something good to check
102:01 - [Music]
102:04 - against let's talk about autom ml's
102:07 - automatic featurization so during model
102:09 - training with autom ML one of the
102:11 - following scaling or normalization
102:13 - techniques will be applied to each model
102:14 - the first is standard scale wrapper so
102:17 - standardized features by removing the
102:18 - mean and scaling to unit variance minmax
102:21 - scaler transform features by scaling
102:23 - each feature by the column is minimum
102:24 - maximum Max ABS scaler scale each
102:27 - feature by its maximum absolute value
102:29 - robust scar scales features by the
102:31 - quanti quantile range PCA linear
102:34 - dimensionality reduction using single uh
102:37 - value decomposition of the data to
102:39 - project it to lower dimensional space uh
102:43 - uh Dimension uh reduction is very useful
102:45 - if your data is too complex if let's say
102:46 - you have data and you have too many
102:49 - labels like 20 20 30 40 labels for per
102:52 - like for categories to pick out of you
102:54 - want to reduce the dimensions so that
102:56 - your machine learning model is not
102:58 - overwhelmed so then you have truncated
103:00 - SVD wrappers so the Transformer performs
103:02 - linear dimensionality reduction by means
103:04 - of truncated single singular value
103:07 - decomposition contrary to PCA the
103:09 - estimator does not Center the data uh
103:11 - before Computing the singular value
103:12 - decomposition which means it can work
103:14 - with spicy sparse matrices efficiently
103:18 - sparse normalization each sample that is
103:20 - each row of the data Matrix which uh
103:22 - with at least one zero component is
103:25 - rescaled independently of other samples
103:26 - so that is Norm so one L or two L2 I
103:30 - can't remember it's I2 or
103:32 - L anyway i1 and and I2 okay so the thing
103:39 - is is that on the exam they're probably
103:40 - not going to be asking these questions
103:42 - but I just like to get you exposure but
103:43 - I just want to show you that automl is
103:45 - doing all this this is like
103:46 - pre-processing stuff you know like this
103:48 - is stuff that you'd have to do and so
103:51 - it's just taking care of the stuff for
103:52 - you
103:53 - [Music]
103:56 - okay so within Azure automl they have a
103:59 - feature called Model selection and this
104:01 - is the task of selecting a statistical
104:03 - model from a set of candidate models and
104:05 - Azure autom ml will use different uh or
104:08 - many different ml algorithms that will
104:10 - recommend the best performing candidate
104:11 - so here is a list and I want to just
104:13 - point out down below there's three pages
104:16 - there's 53 models it's a lot of models
104:19 - and so you can see that the it chose as
104:21 - its top candidate was called voting
104:23 - Ensemble that's an ensemble uh um
104:26 - algorithm that's where you take two weak
104:28 - ml models combine them together to make
104:30 - a more uh uh Stronger one and notice
104:33 - here it will show us the results and
104:34 - this is what we're looking for which is
104:36 - the primary metric the highest value
104:38 - should indicate that that's the model we
104:40 - should want to use you can get an
104:42 - explanation of the model called uh
104:43 - that's known as
104:46 - explainability and now if you're a data
104:47 - scientist you might be a bit smarter and
104:49 - say well I know this one should be
104:51 - better so I'll use this and tweak it but
104:53 - you know if you don't know what you're
104:54 - doing you just go with the top one
104:56 - [Music]
105:00 - okay so we just saw that we had a top
105:03 - candidate model and there could be an
105:04 - explanation to understand as to the
105:06 - effectiveness of this this is called MXL
105:08 - so machine learning explainability this
105:11 - is the process of explaining
105:12 - interpreting ml or deep learning models
105:14 - MX mlx can help machine learning
105:18 - developers to better understand
105:19 - interpret models Behavior so after your
105:21 - top candidate model is selected by Azure
105:23 - automl you can get an explanation of
105:26 - internals of various factors so model
105:27 - performance uh data set Explorer
105:29 - aggregate feature importance individual
105:31 - feature importance so I mean yeah this
105:34 - is aggregate so what it's looking at and
105:36 - it's actually cut off here but it's
105:37 - saying that these are the most important
105:40 - ones that affect how the models outcome
105:43 - so I think this is the diabetes dat a
105:44 - data set so BMI uh would be one that
105:47 - would be a huge influencer there okay
105:50 - [Music]
105:54 - so the primary metric is a parameter
105:56 - that determines the metric to be used
105:58 - during the model training for
105:59 - optimization so for classification we
106:01 - have a few and regression and time
106:03 - series we have a few but you'll have
106:04 - these task types and underneath you'll
106:06 - choose the additional configuration and
106:08 - that's where you can override the
106:09 - primary metric uh it might just Auto
106:11 - detected for you so you don't have to
106:12 - because it might sample some of your
106:14 - data set to just kind of guess U but you
106:16 - might have to override it yourself uh
106:18 - just going through some scenarios um and
106:20 - we'll break it down into two categories
106:21 - so here we have suited for larger data
106:23 - sets that are well balanced well
106:25 - balanced means that your data set like
106:26 - is evenly distributed so if you have uh
106:29 - uh CL classifications for A and B let's
106:31 - say you have 100 and 100 they're well
106:33 - balanced right you don't have one data
106:35 - set much a subset of your dat set much
106:37 - larger than the other that's labeled so
106:39 - for accuracy this is great for image
106:41 - classification sentiment analysis CH
106:43 - prediction for average Precision score
106:45 - weighted it's for sentiment analysis
106:47 - Norm macro recall term prediction for
106:49 - precision score weighted uh uncertain as
106:51 - to what that would be good for maybe
106:52 - sentiment analysis suited for smaller
106:55 - data sets that are inbalance so that's
106:56 - where your data set like you might have
106:58 - like 10 records for one and 500 for the
107:00 - other on the label so you have Au
107:03 - weighted fraud detection image
107:04 - classification anomaly detection spam
107:07 - detection onto regression scenarios uh
107:10 - will break it down into ranges so when
107:12 - you have a very wide range uh Spearman
107:14 - correlation works really well R2 score
107:16 - this is great for airline uh delay
107:18 - salary estimation bug res resolution
107:20 - time we're looking at smaller ranges
107:23 - where you're talking about normalize
107:24 - root square mean to error so price
107:26 - predictions um review tips score
107:28 - predictions for normalized mean absolute
107:31 - error um it's going to be just another
107:33 - one here they don't give a description
107:34 - for time series it's the same thing it's
107:36 - just in the context of Time series so
107:39 - forecasting all
107:42 - [Music]
107:45 - right another option we can change is
107:47 - the validation type when we're setting
107:49 - up our ml model so Val validation model
107:51 - validation is when we compare the
107:52 - results of our training data set to our
107:54 - test data set model validation occurs
107:56 - after we train the model and so you can
107:58 - just drop it down there we have some
107:59 - options so Auto kfold cross validation
108:02 - Monte Carlo cross validation train
108:04 - validation split I'm not going to really
108:06 - get into the details of that I don't
108:08 - think it'll show up on the AI 900 exam
108:10 - but I just want you to be aware that you
108:11 - do have those options
108:13 - [Music]
108:16 - okay hey this is Andrew Brown from exam
108:19 - Pro and we are taking a look here at
108:20 - custom vision and this is a fully
108:22 - managed no code service to quickly build
108:25 - your own classification and object
108:27 - detection ml models the service is
108:29 - hosted on its own isolate domain at
108:31 - www.com vision. so the first idea is you
108:35 - upload your images so bring your own
108:36 - labeled images or custom Vision to
108:38 - quickly add tags to any unlabeled data
108:41 - images you use the labeled images to
108:44 - teach custom Vision the concepts you
108:46 - care about which is training and you use
108:48 - a simple rest API that calls
108:50 - uh to quickly tag images with your new
108:53 - custom computer vision model so you can
108:55 - evaluate
108:57 - [Music]
109:00 - okay so when we launch custom Vision we
109:03 - have to create a project and with that
109:04 - we need to choose a project type and we
109:06 - have classification and object detection
109:09 - reviewing classification here you have
109:12 - the option between multi-label so when
109:13 - you want to apply many tags to an image
109:16 - so think of an image that contains both
109:18 - a cat and a dog you have multi class so
109:20 - when you only have one possible tag to
109:22 - apply to an image so it's either an
109:24 - apple banana and orange it's not
109:26 - multiples of these things you have
109:28 - object detection this is when we want to
109:30 - detect various objects in an image uh
109:32 - and you also need to choose a domain a
109:34 - domain is a Microsoft managed data set
109:36 - that is used for training the ml model
109:38 - there are different domains that are
109:39 - suited for different use cases so let's
109:41 - go take a look first at image
109:42 - classification domains so here is the
109:45 - big list the domains being over
109:47 - here okay and we'll go through these
109:50 - here so General is optimized for a broad
109:52 - range of image classification tasks if
109:54 - none of the uh uh if none of the other
109:56 - specified domains are appropriate or
109:57 - you're unsure of which domain to choose
109:59 - select one of the general domains so G
110:02 - uh or A1 is optimized for better
110:04 - accuracy with comparable inference time
110:06 - as general domain recommended for larger
110:08 - data sets or more difficult user
110:10 - scenarios this domain requires a more
110:12 - training time then you have A2 optimized
110:15 - for better accuracy with faster advert
110:17 - times than A1 and general domains
110:20 - recommended for more most data sets this
110:23 - domain requires less training time than
110:25 - General and A1 you have food optimized
110:27 - for photographs or dishes of as you
110:29 - would see them on a restaurant menu if
110:31 - you want to classify photographs of
110:33 - individual fruits or vegetables use food
110:36 - domains uh so then we have optimize for
110:38 - recognizable landmarks both natural and
110:40 - artificial this domain works best when
110:43 - Landmark is clearly visible in the
110:44 - photograph this domain works even if the
110:46 - land mark is slightly um obstructed by
110:49 - people in front of
110:51 - it then you have retail so optimize for
110:54 - images that are found in a shopping cart
110:55 - or shopping uh website if you want a
110:58 - high Precision classifying uh
111:00 - classifying between dresses pants shirts
111:02 - use this domain compact domains
111:04 - optimized for the constraints of
111:05 - real-time classification on the edge
111:09 - okay then uh we have object detection
111:12 - domain so this one's a lot shorter so
111:14 - we'll get through a lot quicker so
111:15 - optimize for a broad range of object
111:17 - detection tasks if none of the uh other
111:19 - domains are appropriate or you're unsure
111:21 - of which domain choose the general one
111:23 - A1 optimized for better accuracy and
111:25 - comparable inference time than the
111:26 - general domain recommended for most
111:28 - accurate region locations larger data
111:30 - sets or more difficult use case
111:32 - scenarios the domain requires more
111:33 - training and results are not
111:34 - deterministic expect uh plus minus 1%
111:38 - mean average Precision difference uh
111:40 - with the same training data provided you
111:42 - have logo optimized for finding Brands
111:45 - uh logos and images uh products on
111:47 - shelves so optimized for detecting and
111:49 - classifying
111:50 - products on the shelves so there you
111:52 - [Music]
111:56 - go okay so let's get some uh more
111:58 - practical knowledge of the service so
112:00 - for image classification you're going to
112:01 - upload multiple images and apply a
112:03 - single or multiple labels to the entire
112:05 - image so here I have a bunch of images
112:07 - uploaded and then I have my tags over
112:09 - here and they could either be multi or
112:11 - singular for object detection you apply
112:13 - tags to objects in an image for data
112:15 - labeling and you hover uh your cursor
112:17 - over the image custom Vision uses ml to
112:19 - show boundaries uh bounding boxes of
112:21 - possible objects have not yet been
112:22 - labeled if it does not detect it you can
112:25 - also just click and drag to draw out
112:27 - whatever Square you want so here's one
112:29 - where I tagged it up quite a bit you
112:31 - have to have at least 50 images on every
112:32 - tag to train uh so just be aware of that
112:35 - when you are tagging your images uh when
112:38 - you're training your model is ready when
112:40 - you and you have two options so you have
112:41 - quick training this trains quickly but
112:43 - it will be less accurate you have
112:45 - Advanced Training this increases compute
112:46 - time to improve your results so for
112:49 - Advanced Training BAS basically you just
112:50 - have this thing that you move to the
112:51 - right uh with each iteration of training
112:54 - our ml model will improve the evaluation
112:56 - metric so precision and recall it's
112:58 - going to vary we're going to talk about
112:59 - the metrics here in a moment but the
113:00 - probability threshold value determines
113:02 - when to stop training when our
113:04 - evaluation metric meets our desired
113:06 - threshold so these are just additional
113:07 - options where when you're training you
113:09 - can move this left to right uh and these
113:12 - left to right
113:14 - okay and then when we get our results
113:16 - back uh we're going to get um some
113:18 - metrics here so uh we have evaluation
113:20 - metric so we have Precision being exact
113:22 - and accurate selects items that are
113:24 - relevant recall such sensitivity or
113:26 - known as true positive rate how many
113:28 - relevant items returned average
113:30 - Precision it's important that you
113:31 - remember these because they might ask
113:34 - you that on the exam so for uh cut when
113:37 - we're looking at object detection and
113:38 - we're looking at the evaluation metric
113:40 - outcomes for this one we have Precision
113:42 - recall and mean average Precision uh
113:46 - once we've deployed our pipeline it
113:47 - makes sense that we go ahead and give it
113:49 - a quick test to make sure it's working
113:50 - correctly so you press the click quick
113:52 - test button and you can upload your
113:54 - image and it will tell you so this one
113:56 - says it's Warf uh when you're ready to
113:58 - publish you just hit the publish button
114:01 - and then you'll get uh some prediction
114:03 - URL and information so you can invoke it
114:06 - uh one other feature that's kind of
114:08 - useful is the Smart labeler so once
114:09 - you've loaded some training data within
114:12 - it can now make suggestions right so you
114:14 - can't do this right away but once it has
114:16 - some data it's like it's like kind of a
114:19 - prediction that is not 100% guaranteed
114:21 - right and it just helps you build up
114:22 - your training uh data set a lot faster
114:25 - uh very useful if you have a very large
114:27 - data set this is known as ml assisted
114:29 - labeling
114:31 - [Music]
114:34 - okay hey this is Andrew Brown from exam
114:36 - Pro and in this section we'll be
114:38 - covering the newly added section to the
114:40 - AI 900 that focuses on generative AI
114:42 - generative AI including Technologies
114:44 - like chat GPT is becoming more
114:46 - recognized outside of tech circles while
114:49 - it may seem magical in its ability to
114:51 - produce humanlike content it's actually
114:53 - based on Advanced mathematical
114:54 - techniques from statistics data science
114:56 - and machine learning understanding these
114:58 - Core Concepts can help Society Envision
115:00 - new AI possibilities for the future
115:03 - first let's compare the differences
115:04 - between regular AI versus generative ai
115:07 - ai refers to the development of computer
115:09 - systems that can perform tasks typically
115:11 - requiring human intelligence these
115:13 - include problem solving decision-making
115:16 - understanding natural language
115:17 - recognizing speech and images and more
115:20 - the primary goal of traditional AI is to
115:22 - create systems that can interpret
115:23 - analyze and respond to human actions or
115:25 - environmental changes efficiently and
115:27 - accurately it aims to replicate or
115:29 - simulate human intelligence in machines
115:32 - AI applications are vast and include
115:34 - areas like expert systems natural
115:36 - language processing speech recognition
115:38 - and Robotics AI is used in various
115:41 - Industries for tasks such as customer
115:42 - service chatbots recommendation systems
115:45 - in e-commerce autonomous vehicles and
115:47 - medical diagnosis on the other hand
115:50 - generative AI is a subset of AI that
115:52 - focuses on creating new content or data
115:54 - that is novel and realistic it does not
115:56 - just interpret or analyze data but
115:58 - generates new data itself it includes
116:00 - generating text images music speech and
116:03 - other forms of media it often involves
116:05 - Advanced machine learning techniques
116:07 - particularly deep learning models like
116:09 - generative adversarial networks
116:10 - variational autoencoders and Transformer
116:13 - models like GPT generative AI is used in
116:16 - a range of applications including
116:18 - creating realistic images and videos
116:20 - generating human-like text composing
116:22 - music creating virtual environments and
116:24 - even drug Discovery some examples
116:27 - include tools like GPT for text
116:29 - generation doly for image creation and
116:31 - various deep learning models that
116:33 - compose music so let's quickly summarize
116:36 - the differences of regular AI with
116:38 - generative AI across three features
116:40 - functionality data handling and
116:42 - applications regular AI focuses on
116:45 - understanding and decision making
116:46 - whereas generative AI is about creating
116:48 - new original outputs in terms of data
116:51 - handling regular AI analyzes and bases
116:53 - decisions on existing data while
116:55 - generative AI uses the same data to
116:57 - generate new previously unseen outputs
117:00 - and for applications regular I scope
117:02 - includes data analysis automation
117:04 - natural language processing and
117:06 - Healthcare in contrast generative AI
117:08 - leans towards more creative and
117:09 - Innovative applications such as content
117:11 - creation synthetic data generation deep
117:14 - fakes and
117:18 - Design
117:20 - the next topic we'll be covering is what
117:21 - is a large language Model A large
117:23 - language model such as GPT Works in a
117:26 - way that's similar to a complex
117:27 - automatic system that recognizes
117:29 - patterns and makes predictions training
117:32 - on large data sets initially the model
117:34 - is trained on massive amounts of text
117:36 - Data this data can include books
117:38 - articles websites and other written
117:39 - material during this training phase the
117:42 - model learns patterns and language such
117:44 - as grammar word usage sentence structure
117:46 - and even style and tone understanding
117:48 - context the model's design allows it to
117:51 - consider a wide context this means it
117:53 - doesn't just focus on single words but
117:55 - understands them in relation to the
117:56 - words and sentences that come before and
117:58 - after this context understanding is
118:00 - important for generating coherent and
118:02 - relevant text predicting the next word
118:05 - when you give the model a prompt which
118:07 - is a starting piece of text it uses what
118:09 - it has learned to predict the next most
118:10 - likely word it then adds this word to
118:13 - the prompt and repeats the process
118:14 - continually predicting the next word
118:16 - based on the extended sequence
118:18 - generating text this process of
118:20 - predicting the next word continues
118:22 - creating a chain of words that forms a
118:24 - coherent piece of text the length of
118:26 - this generated text can vary based on
118:27 - Specific Instructions or limitations set
118:29 - for the model refinement with feedback
118:32 - the model can be further refined and
118:34 - improved over time with feedback this
118:36 - means it gets better at understanding
118:37 - and generating text as it is exposed to
118:39 - more data and usage in summary a large
118:42 - language model works by learning from a
118:44 - vast quantity of text Data understanding
118:46 - the context of language and using this
118:48 - understanding to predict and generate
118:50 - new text that is coherent and
118:51 - contextually appropriate which can be
118:53 - further refined with feedback as shown
118:55 - in the workflow
118:56 - [Music]
118:59 - image next let's talk about Transformer
119:02 - models so a Transformer model is a type
119:05 - of machine learning model that's
119:06 - especially good at understanding and
119:08 - generating language it's built using a
119:10 - structure called the Transformer
119:12 - architecture which is really effective
119:13 - for tasks involving natural language
119:15 - processing like translating languages or
119:17 - writing text trans Transformer model
119:20 - architecture consists of two components
119:22 - or blocks first we have the encoder this
119:24 - part reads and understands the input
119:26 - text it's like a smart system that goes
119:28 - through everything it's been taught
119:29 - which is a lot of text and picks up on
119:31 - the meanings of words and how they're
119:33 - used in different contexts then we have
119:35 - the decoder So based on what the encoder
119:37 - has learned this part generates New
119:39 - pieces of text it's like a skilled
119:41 - writer that can make up sentences that
119:43 - flow well and make sense there are
119:45 - different types of Transformer models
119:47 - with specific jobs for example Bert is
119:49 - good at understanding the language it's
119:51 - like a librarian who knows where every
119:53 - book is and what's inside them Google
119:55 - uses it to help its search engine
119:56 - understand what you're looking for GPT
119:59 - is good at creating text it's like a
120:00 - skilled author who can write stories
120:02 - articles or conversations based on what
120:04 - it has learned so that's an overview of
120:06 - a transformer model next we'll be
120:09 - talking about the main components of a
120:10 - transformer
120:15 - model the next component of a
120:17 - transformer model we'll be covering is
120:19 - the the tokenization process
120:20 - tokenization in a Transformer model is
120:23 - like turning a sentence into a puzzle
120:24 - for example you have the sentence I
120:26 - heard a dog bark loudly at a cat to help
120:29 - a computer understand it we chop up the
120:31 - sentence into pieces called tokens each
120:33 - piece can be a word or even a part of a
120:35 - word so for our sentence we give each
120:37 - word a number like this I might be one
120:40 - her might be two a might be three do
120:42 - might be four bark might be five loudly
120:45 - might be six at might be seven is
120:48 - already token is three tap might be
120:50 - eight now our sentence becomes a series
120:53 - of numbers this is like giving each word
120:55 - a special code the computer uses these
120:57 - codes to learn about the words and how
120:58 - they fit together if a word repeats like
121:01 - a we use its code again instead of
121:02 - making a new one as the computer reads
121:05 - more text it keeps turning new words
121:07 - into new tokens with new numbers if it
121:09 - learns the word meow it might call it
121:11 - nine and skateboard could be 10 by doing
121:13 - this with lots and lots of text the
121:15 - computer builds a big list of these
121:17 - tokens which it then uses to stand and
121:19 - generate language it's a bit like
121:21 - creating a dictionary where every word
121:22 - has a unique
121:27 - number the next component of a
121:29 - transformer model will be covering our
121:31 - embeddings so to help a computer
121:34 - understand language we turn words into
121:35 - tokens and then give each token a
121:37 - special numeric code called an embedding
121:39 - these embeddings are like a secret code
121:41 - that captures the meaning of the word as
121:43 - a simple example suppose the embeddings
121:45 - for our tokens consist of vectors with
121:47 - three elements for example four for dog
121:50 - has the embedding vectors 10 32 five for
121:53 - bark has the vectors 10 22 eight for cat
121:56 - the vectors are 10 3 1 nine for meow the
122:00 - vectors are 10 2 1 and 10 for skateboard
122:03 - as the vectors 3 3 one which is quite
122:06 - different from the rest words that have
122:08 - similar meanings or are used in similar
122:10 - ways get codes that look alike so dog
122:12 - and bark might have similar codes
122:13 - because they are related but skateboard
122:15 - might be off in a different area because
122:17 - it's not much related to these other
122:18 - words words this way the computer can
122:21 - figure out which words are similar to
122:22 - each other just by looking at their
122:24 - codes it's like giving each word a home
122:26 - on a map and words that are neighbors on
122:27 - this map have related meanings the image
122:30 - shows a simple example model in which
122:32 - each embedding has only three dimensions
122:34 - real language models have many more
122:36 - Dimensions tools such as word TWC or the
122:39 - encoding part of a transformer model
122:40 - help AI to figure out where each word
122:42 - dot should go on this big
122:47 - map let's go over positional encoding
122:50 - from a Transformer model positional
122:52 - encoding is a technique used to ensure
122:54 - that a language model such as GPT
122:56 - doesn't lose the order of words when
122:57 - processing natural language this is
123:00 - important because the order in which
123:01 - words appear can change the meaning of a
123:03 - sentence let's take the sentence I heard
123:05 - a dog bark loudly at a c from our
123:07 - previous example without positional
123:09 - encoding if we simply tokenize this
123:11 - sentence and convert the tokens into
123:13 - embedding vectors we might end up with a
123:15 - set of vectors that lose the sequence
123:16 - information positional encoding adds a
123:19 - positional Vector to each word in order
123:21 - to keep track of the positions of the
123:23 - words however by adding positional en
123:25 - coding vectors to each words embedding
123:27 - we ensure that each position in the
123:29 - sentence is uniquely identified the
123:31 - embedding for I would be modified by
123:33 - adding a positional Vector corresponding
123:35 - to position one labeled I 1 the
123:37 - embedding for herd would be altered by a
123:39 - vector for position two labeled herd 2
123:42 - the embedding for a would be updated
123:44 - with a vector for position three labeled
123:46 - a 3 and reused with the same positional
123:48 - vector for its second occurrence this
123:51 - process continues for each word token in
123:53 - the sentence with dog four bark five
123:55 - loudly six at seven and Cat 8 all
123:58 - receiving their unique positional
124:00 - encodings as a result the sentence I
124:02 - heard a dog bark loudly at a cat is
124:04 - represented not just by a sequence of
124:05 - vectors for its words but by a sequence
124:08 - of vectors that are influenced by the
124:09 - position of each word in the sentence
124:12 - this means that even if another sentence
124:13 - had the same words in a different order
124:15 - its overall representation would be
124:17 - different because the positional
124:18 - encodings differ reflecting the
124:19 - different sequence of words so that's an
124:22 - overview of positional
124:28 - encoding the next component of a
124:30 - transformer we'll be covering is
124:31 - attention attention in AI especially in
124:34 - Transformer models is a way the model
124:36 - figures out how important each word or
124:38 - token is to the meaning of a sentence
124:40 - particularly in relation to the other
124:41 - words around it let's reuse the sentence
124:43 - I heard a DOT bark loudly at a cat to
124:45 - explain this better self attention
124:48 - imagine each word word in the sentence
124:49 - shining a flashlight on the other words
124:51 - the brightness of the light shows how
124:53 - much one word should pay attention to
124:54 - the others when understanding the
124:55 - sentence for bark the light might shine
124:58 - brightest on dot because they're closely
124:59 - related encoder's role in the encoder
125:02 - part of a transformer model attention
125:04 - helps decide how to represent each word
125:06 - as a number or vector it's not just the
125:08 - word itself but also its context that
125:10 - matters for example bark in the bark of
125:12 - a tree would have a different
125:13 - representation than bark and I heard a
125:15 - DOT bark because the surrounding words
125:17 - are different decoder's role when
125:19 - generating new text like completing a
125:21 - sentence the decoder uses attention to
125:23 - figure out which words it already has
125:25 - are most important for deciding what
125:26 - comes next if our sentence is I heard a
125:29 - dog the model uses attention to know
125:31 - that her and dog are key to adding the
125:32 - next word which might be bark multi-head
125:35 - attention this is like having multiple
125:37 - flashlights each highlighting different
125:39 - aspects of the words maybe one
125:41 - flashlight looks at the meaning of the
125:42 - word another looks at its role in the
125:44 - sentence like subject or object and so
125:46 - on this helps the model get a richer
125:48 - understanding of the text building the
125:51 - output the decoder builds the sentence
125:52 - one word at a time using attention at
125:54 - each step it looks at the sentence so
125:56 - far decides what's important and then
125:58 - predicts the next word it's an ongoing
126:00 - process with each new word influencing
126:02 - the next so attention in Transformer
126:05 - models is like a guide that helps the AI
126:07 - understand and create Language by
126:08 - focusing on the most relevant parts of
126:10 - the text considering both individual
126:12 - word meanings and their relationships
126:14 - within the
126:15 - sentence let's take a look at the
126:17 - attention process token embeddings each
126:20 - word in the sentence is represented as a
126:22 - vector of numbers or its embedding
126:24 - predicting the next token the goal is to
126:26 - figure out what the next word should be
126:28 - also represented as a vector as signing
126:31 - weights the attention layer looks at the
126:32 - sentence so far and decides how much
126:34 - influence each word should have on the
126:35 - next one calculating attention scores
126:38 - using these weights a new Vector for the
126:40 - next token is calculated which includes
126:42 - an attention score multi-head attention
126:44 - does this several times focusing on
126:46 - different aspects of the words choosing
126:49 - the most likely word a neural network
126:51 - takes these vectors with attention
126:52 - scores and picks the word from the
126:53 - vocabulary that most likely comes next
126:56 - adding to the sequence The Chosen word
126:58 - is added to the existing sequence and
127:00 - the process repeats for each new word so
127:03 - let's use gp4 is an example for how this
127:06 - entire process works explained in a
127:08 - simplified manner a Transformer model
127:10 - like gp4 works by taking a text input
127:13 - and producing a well structured output
127:15 - during training it learns from a vast
127:17 - array of text Data understand
127:18 - understanding how words are typically
127:19 - arranged in sentences the model knows
127:22 - the correct sequence of words but hides
127:24 - future words to learn how to predict
127:25 - them when it tries to predict a word it
127:28 - Compares its guess to the actual word
127:30 - gradually adjusting to reduce errors in
127:32 - practice the model uses its training to
127:34 - aside importance to each word in a
127:36 - sequence helping it guess the next word
127:38 - accurately the result is that gp4 can
127:41 - create sentences that sound like they
127:42 - were written by a human however this
127:45 - doesn't mean the model knows things or
127:46 - is intelligent in the human sense it's
127:48 - it's simply very good at using its large
127:50 - vocabulary and training to generate
127:52 - realistic text base on word
127:54 - relationships so that's an overview of
127:56 - attention in a Transformer
127:58 - [Music]
128:02 - model hey this is Andrew Brown from exam
128:04 - Pro and in this section we'll be going
128:06 - over an introduction to Azure openai
128:08 - service Azure open AI service is a
128:10 - cloud-based platform designed to deploy
128:12 - and manage Advanced language models from
128:14 - open AI this service combines open I's
128:16 - latest language model development with
128:18 - the robust security and scalability of
128:20 - azure's cloud infrastructure Azure open
128:22 - AI offers several types of models for
128:24 - different purposes gp4 models these are
128:28 - the newest in the line of GPT models and
128:30 - can create text and programming code
128:31 - when given a prompt written in natural
128:33 - language GPT 3.5 models similar to GPT 4
128:38 - these models also create text and code
128:39 - from natural language props the GPT 3.5
128:43 - turbo version is specially designed for
128:44 - conversations making it a great choice
128:46 - for chat applications and other
128:48 - interactive AI tasks embedding models
128:51 - these models turn written text into
128:53 - number sequences which is helpful for
128:55 - analyzing and comparing different pieces
128:56 - of text to find out how similar they are
128:59 - doll e models these models can make
129:01 - images from descriptions given in words
129:03 - the doll e models are still being tested
129:05 - and are shown in the Azure Open aai
129:06 - studio so you don't have to set them up
129:08 - for use manually key Concepts in using
129:11 - Azure open AI include prompts and
129:13 - completions tokens resources deployments
129:16 - prompt engineering and various models
129:18 - prompts and completions users interact
129:21 - with the API by providing a text command
129:23 - in English known as a prompt and the
129:24 - model generates a text response or
129:26 - completion for example a prompt to C to
129:29 - five and a loop results in the model
129:30 - returning appropriate code tokens asure
129:33 - open AI breaks down text into tokens
129:35 - which are words or character chunks to
129:37 - process requests the number of tokens
129:39 - affects response latency and throughput
129:41 - for images token cost varies with image
129:43 - size and detail setting with low detail
129:46 - images costing fewer tokens and high
129:47 - detail images costing more resources
129:50 - Azure open aai operates like other Azure
129:52 - products where users create a resource
129:54 - within their Azure subscription
129:56 - deployments to use the service users
129:58 - must deploy a model via deployment apis
130:00 - choosing the specific model for their
130:02 - needs prompt engineering crafting
130:04 - prompts is crucial as they guide the
130:06 - model's output this requires skill as
130:08 - prompt construction is nuanced and
130:10 - impacts the model's response models
130:13 - various models offer different
130:14 - capabilities and pricing Dolly creates
130:17 - images from text while whisper
130:18 - transcribes and translates speech to
130:20 - text each has unique features suitable
130:22 - for different tasks so that's an
130:24 - overview of azure open ey
130:30 - service the next topic we'll be covering
130:32 - is azure open AI Studio developers can
130:35 - work with these models in Azure open AI
130:37 - Studio A web-based environment where AI
130:39 - professionals can deploy test and manage
130:41 - llms that support generative AI app
130:43 - development on Azure access is currently
130:45 - limited due to the high demand upcoming
130:47 - product improvements and Microsoft's
130:49 - commitment to responsible AI presently
130:52 - collaborations are being prioritized for
130:54 - those who already have a partnership
130:55 - with Microsoft are engaged in lower risk
130:57 - use cases and are dedicated to including
131:00 - necessary
131:01 - safeguards in Azure open AI Studio you
131:03 - can deploy large language models provide
131:06 - F shot examples and test them in Azure
131:08 - open AI Studios chat playground the
131:10 - image shows Azure open eyes chat
131:12 - playground interface where users can
131:14 - test and configure an AI chat bot in the
131:17 - middle there's a chat area to type user
131:18 - messages and see the assistant's replies
131:21 - on the left there's a menu for
131:22 - navigation and a section to set up the
131:24 - assistant including a reminder to save
131:26 - changes on the right adjustable
131:29 - parameters control the eyes response
131:30 - Behavior like length Randomness and
131:32 - repetition users into queries adjust
131:35 - settings and observe how the AI responds
131:37 - to fine-tune its performance so that's
131:39 - an overview of azure open AI
131:45 - Studio let's take a look at the pricing
131:47 - for the model models in Azure open AI
131:49 - service starting off with the language
131:51 - models we have GPT 3.5 Turbo with a
131:54 - context of 4K tokens cost
131:57 - 0.15 for prompts and
132:00 - 0.002 for completions per 1,000 tokens
132:03 - another version of GPT 3.5 turbo can
132:06 - handle a larger context of 16k tokens
132:09 - with PRT and completion costs increased
132:11 - up to
132:12 - 0.003 and
132:14 - 0.004 respectively gpt3 5 Turbo 11106
132:19 - with a 16k context has no available
132:21 - pricing gp4 turbo and gp4 turbo Vision
132:25 - both have an even larger Contex size of
132:27 - 128k tokens but also have no listed
132:30 - prices the standard gp4 model with an 8K
132:33 - token Contex costs 3 cents for prompts
132:36 - and 6 cents for completions and a larger
132:39 - context version of gp4 with 32k tokens
132:42 - cost 6 cents for prompts and 12 cents
132:44 - for
132:44 - completions there are other models such
132:47 - as the base models
132:48 - fine-tuning models image models
132:51 - embedding models and speech models they
132:54 - all have their respective pricing but we
132:56 - won't be going through each of them in a
132:57 - lot of detail but essentially they are
132:59 - all on a paper use pricing model it
133:02 - could be payer hour or paper token and
133:04 - so on the higher quality the model the
133:06 - more expensive it will likely be so
133:08 - that's an overview of azure open AI
133:10 - Service
133:12 - [Music]
133:15 - pricing hey this is Andrew Brown from
133:17 - exam Pro and the next topic will be
133:19 - going over co-pilots co-pilots are a new
133:22 - type of computing tool that integrates
133:23 - with applications to help users with
133:25 - common tasks using generative AI models
133:28 - they are designed using a standard
133:29 - architecture allowing developers to
133:31 - create custom co-pilots tailored to
133:33 - specific business needs and applications
133:35 - co-pilots might appear as a chat feature
133:37 - beside your document or file and they
133:39 - utilize the content within the product
133:41 - to generate specific results creating a
133:43 - co-pilot involves several steps training
133:46 - a large language model with a vast
133:48 - amount of data utilizing services like
133:50 - Azure open AI service which provide
133:52 - pre-trained models that developers can
133:54 - either use as his refin tune with their
133:56 - own data for more specific tasks
133:58 - deploying the model to make it available
134:00 - for use within applications building
134:02 - co-pilots that prompt the models to
134:04 - generate usable content enabling
134:06 - business users to enhance their
134:08 - productivity and creativity through AI
134:10 - generated assistance co-pilots have the
134:12 - potential to revolutionize the way we
134:14 - work these co-pilots use generative AI
134:16 - to help with first draft information
134:18 - synthesis strategic planning and much
134:21 - more let's take a look at a few examples
134:23 - of co-pilot starting with Microsoft
134:26 - co-pilot so Microsoft co-pilot is
134:28 - integrated into various applications to
134:30 - assist users in creating documents
134:32 - spreadsheets presentations and more by
134:35 - generating content summarizing
134:36 - information and aiding in strategic
134:38 - planning it is used across Microsoft
134:41 - Suite of products and services to
134:42 - enhance user experience and efficiency
134:45 - next we have the Microsoft being search
134:47 - engine which which has an integrated
134:48 - co-pilot to help users when browsing or
134:50 - searching the Internet by generating
134:52 - natural language answers to questions by
134:54 - understanding the context of the
134:56 - questions providing a richer and more
134:57 - intuitive search experience Microsoft
135:00 - 365 co-pilot is designed to be a partner
135:03 - in your workflow integrated with
135:05 - productivity and communication tools
135:07 - like PowerPoint and Outlook it's there
135:09 - to help you craft effective documents
135:11 - design spreadsheets put together
135:12 - presentations manage emails and
135:14 - streamline other tasks GitHub co-pilot
135:17 - is tool that helps software developers
135:19 - offering real-time assistance as they
135:21 - write code it offers more than
135:23 - suggesting code Snippets it can help in
135:25 - Thoroughly documenting the code for
135:26 - better understanding and maintenance
135:29 - additionally co-pilot contributes to the
135:31 - development process by providing support
135:32 - for testing code ensuring that
135:34 - developers can work more efficiently and
135:36 - with fewer errors so that's an overview
135:39 - of
135:40 - [Music]
135:43 - co-pilot hey this is Andrew Brown from
135:46 - exam Pro and the next topic will be
135:48 - covering is prompt engineering prompt
135:50 - engineering is a process that improves
135:52 - the interaction between humans and
135:53 - generative AI it involves refining the
135:55 - props or instructions given to an AI
135:57 - application to generate higher quality
135:59 - responses this process is valuable for
136:02 - both the developers who create AI driven
136:04 - applications and the end users who
136:05 - interact with them for example
136:08 - developers May build a generative AI
136:09 - application for teachers to create
136:11 - multiple choice questions related to
136:13 - text students read during the
136:14 - development of the application
136:16 - developers can add other rules for what
136:17 - the program should do with the prompts
136:19 - it receives system messages prompt
136:22 - engineering techniques include defining
136:23 - a system message the message sets the
136:25 - context for the model by describing
136:27 - expectations and constraints for example
136:30 - you're a helpful assistant that responds
136:32 - in a cheerful friendly manner these
136:34 - system messages determine constraints
136:35 - and styles for the model's responses
136:38 - writing good prompts to maximize the
136:40 - utility of AI responses it is essential
136:42 - to be precise and explicit in your props
136:44 - a well structured propt such as create a
136:47 - list of 10 things things to do in
136:48 - Edinburgh during August directs the AI
136:50 - to produce a targeted and relevant
136:51 - output achieving better results zero
136:54 - shot learning refers to an AI model's
136:56 - ability to correctly perform a task
136:58 - without any prior examples or training
137:00 - on that specific task one shot learning
137:02 - involves the AI model learning from a
137:04 - single example or instance to perform a
137:06 - task here is an example of prompt
137:08 - engineering with a user query and system
137:10 - response so the user inputs can my
137:13 - camera handle the rainy season if I go
137:14 - to the Amazon rainforest next week some
137:17 - The Prompt engineering components could
137:19 - be the weather resistance feature check
137:21 - users equipment database rainforest
137:23 - climate data product specifications
137:25 - travel tips for photographers etc for
137:28 - the llm processing the AI system
137:30 - integrates the user question with data
137:32 - about the Amazon's climate specifically
137:34 - during the rainy season and the product
137:36 - information about the camera's weather
137:37 - resistance features it also references a
137:40 - database of the user's equipment to
137:41 - ensure it's talking about the correct
137:43 - item and may include travel tips that
137:45 - are useful for photographers heading to
137:46 - similar climate
137:48 - and the output results in your current
137:50 - camera model the proot markv is designed
137:52 - with a weather sealed body suitable for
137:54 - high humidity and Rain conditions which
137:56 - matches the expected weather in the
137:57 - Amazon rainforest for next week however
138:00 - for added protection during heavy rains
138:02 - consider using a rain cover next let's
138:05 - take a look at the prompt engineering
138:06 - workflow this image describes a
138:09 - simplified step process for working with
138:10 - AI models and prompt engineering why
138:13 - task understanding know what you want
138:15 - the AI to do two craft prompts write
138:18 - instructions for the AI three prompt
138:21 - alignment make sure instructions match
138:23 - what the AI can do for optimizing prompt
138:26 - improve the instructions for better AI
138:28 - responses five AI model processing the
138:31 - AI thinks about the instructions six
138:34 - generating output the AI gives an answer
138:36 - or result seven output refinement fix or
138:39 - tweak the I's answer hey to iterative
138:42 - improvement keep improving the
138:43 - instructions and answers so that's an
138:46 - overview of prompt engineering
138:48 - [Music]
138:52 - the next topic we'll be covering is
138:54 - grounding grounding impr prompt
138:55 - engineering is a technique used in large
138:57 - language models where you provide
138:59 - specific relevant context within a
139:01 - prompt this helps the AI to produce a
139:03 - more accurate and related response for
139:05 - example if you want an llm to summarize
139:07 - an email you would include the actual
139:09 - email text in the prompt along with a
139:10 - command to summarize it this approach
139:12 - allows you to Leverage The llm for tasks
139:14 - it wasn't explicitly trained on without
139:16 - the need for training the model so
139:19 - what's the difference between prompt
139:20 - engineering and grounding prompt
139:22 - engineering broadly refers to the art of
139:24 - crafting effective prps to produce the
139:26 - desired output from an AI model
139:28 - grounding specifically involves
139:29 - enriching prps with relevant context to
139:31 - improve the model's understanding and
139:33 - responses grounding ensures the AI has
139:36 - enough information to process the prop
139:38 - correctly whereas prop engineering can
139:40 - also include techniques like format
139:42 - style and the Strategic use of examples
139:44 - or questions to guide the AI the image
139:46 - outlines of framework for grounding
139:48 - options in prompt engineering within the
139:50 - context of large language models
139:52 - grounding options these are techniques
139:54 - to ensure llm outputs are accurate and
139:56 - adhere to responsible AI principles
139:58 - prompt engineering placed at the top
140:00 - indicating its broad applicability this
140:02 - involves designing prompts to direct the
140:04 - AI toward generating the desired output
140:07 - fine-tuning a step below in complexity
140:09 - where llms are trained on specific data
140:11 - to improve their task performance
140:13 - training the most resource intensive
140:15 - process at the triangle base suggesting
140:17 - it's used in more extensive
140:19 - customization needs llm Ops and
140:21 - responsible AI these foundational
140:23 - aspects emphasize the importance of
140:25 - operational efficiency and ethical
140:27 - standards across all stages of LL and
140:29 - application development so that's an
140:31 - overview of
140:33 - [Music]
140:36 - grounding hey this is Andrew Brown from
140:39 - exampro and in this demo we'll be going
140:40 - over a short demo on what you can do
140:42 - with co-pilot with gp4 on Microsoft bang
140:45 - so to get here you'll need to search for
140:47 - something like co-pilot Bing and click
140:49 - on the TR copilot and you should be able
140:51 - to access this page so on here you have
140:54 - some suggested or popular prompts that
140:56 - people commonly use such as create an
140:58 - image of a concept kitchen generate
141:00 - ideas for whacking new products how
141:03 - would you explain AI to a sixth grader
141:05 - WR python code to calculate all the
141:07 - different flavor combination as for my
141:09 - ice cream parlor and so on you can
141:12 - choose the conversation style ranging
141:13 - from more creative for more original and
141:16 - imaginative ideas more balanced or more
141:18 - precise for more factual information
141:21 - we'll be going with somewhere in the
141:22 - middle so more balanced just for this
141:24 - example on the bottom here you can type
141:27 - in any prompt you want so for example we
141:30 - can type something simple like summarize
141:32 - the main differences between supervised
141:34 - and unsupervised learning for the AI 900
141:36 - exam you'll see that it will start
141:38 - generating an answer for you so for
141:41 - supervised learning data labeling in
141:43 - supervised learning the training data is
141:45 - pre-labeled with the correct output
141:46 - values
141:47 - and it provides other objectives and
141:49 - examples as well and for unsupervised
141:52 - learning No Labels unsupervised learning
141:54 - operates without labeled data it seeks
141:56 - to discover patterns structures or
141:58 - relationships within the raw data notice
142:01 - how it uses sources from the internet
142:03 - and if you want to learn more you can
142:04 - click on these links that it provides to
142:06 - directly go to the source of the
142:07 - information which is very convenient so
142:10 - let's quickly check one out and it seems
142:12 - like the information we got was pretty
142:14 - good and
142:16 - credible and on the bottom it also
142:18 - provides us some suggestions for
142:20 - follow-up questions you may want to ask
142:21 - in the future that is related to the
142:23 - previous
142:24 - PRT another cool feature of co-pilot is
142:27 - that it's integrated with dolly3 which
142:29 - is a image generation service so for
142:32 - example you can say something like
142:34 - create an image of a cute dog running
142:36 - through a green field on a sunny
142:38 - day so now you'll have to wait a little
142:40 - bit for it to generate the image that
142:42 - you described in your
142:45 - prompt and there we go we have an
142:47 - adorable little puppy running through
142:49 - the
142:53 - fields you also have the power to modify
142:56 - images if you're not satisfied with the
142:57 - result so they've provided a few options
143:00 - for you here so for example we can add a
143:02 - rainbow in the background change it into
143:04 - a cat or make the sky pink and purple
143:07 - let's try changing it to a cat so it's
143:10 - going to generate and change it from a
143:11 - dog to a cat and there we go it's now a
143:14 - cute little cat running through the
143:16 - field
143:17 - you could also write code using
143:20 - co-pilot so for example I can type in
143:23 - write a python function to check if a
143:25 - given number is
143:26 - prime it'll start generating a piece of
143:29 - code for
143:33 - me it can write code in multiple
143:35 - languages not just python so let's try
143:37 - out something with
143:39 - JavaScript let's try create a JavaScript
143:42 - function to reverse a
143:45 - string and of of course we'll need to
143:47 - wait for the code to
143:51 - generate so there we go here's our code
143:53 - for the function to reverse a string
143:55 - just as we asked for it so that's a
143:57 - really quick and general demo for
143:59 - co-pilot with
144:01 - [Music]
144:05 - gp4 hey this is Andrew Brown from exam
144:08 - Pro and in this follow along we're going
144:09 - to set up a studio with an Azure machine
144:12 - learning uh Service uh so that it will
144:14 - be the basis for all the follow alongs
144:16 - here so I want you to do is go all the
144:18 - way to the top here and type in Azure
144:20 - machine learning and you're looking for
144:22 - this one that looks like a science uh
144:24 - bottle here and we'll go ahead and
144:26 - create ourselves our machine learning uh
144:29 - studio and so I'll create a new one here
144:31 - and I'll just say um my
144:36 - studio and we'll hit okay and we'll name
144:39 - the workpace so we'll say my uh work
144:45 - workplace
144:49 - we'll maybe say ml workplace
144:52 - here uh for containers there are none so
144:54 - it'll create all that stuff for us I'll
144:55 - hit
144:57 - create and
145:01 - create and so what we're going to do
145:03 - here is just wait for that creation
145:06 - okay all right so after a short little
145:08 - wait there it looks like our studio set
145:09 - up so we'll go to that resource launch
145:11 - the studio and we are now in so uh
145:15 - there's a lot of stuff in here but
145:16 - generally the first thing you'll ever
145:17 - want to do is get yourself a notebook
145:19 - going so in the top left corner I'm
145:20 - going to go to notebooks and what we'll
145:22 - need to do is load some files in here
145:25 - now they do have some sample files like
145:27 - how to use uh Azure ml so if we just
145:30 - quickly go through here um you know
145:33 - maybe we'll want to look at something
145:35 - like uh Ms nist here and we'll go ahead
145:37 - and open this
145:39 - one and maybe we'll just go ahead and
145:41 - clone uh this and we'll just clone it
145:44 - over
145:45 - here
145:48 - okay and the idea is that we want to get
145:50 - this notebook running and so notebooks
145:52 - have to be backed by some kind of
145:54 - compute so up here it says no compute
145:56 - found and Etc so what we can do here and
145:59 - I'm just going to go back to my files oh
146:00 - it went back there for me but what I'm
146:02 - going to do is go all the way down
146:04 - actually I'll just expand this up here
146:05 - makes it a bit easier close this tab out
146:08 - but uh what we'll do is go down to
146:09 - compute and here we have our four types
146:11 - of comput so compute instances is when
146:13 - we're running notebooks compute clusters
146:15 - is when we're doing training and
146:16 - inference clusters is when we have uh a
146:19 - inference pipeline uh and then attach
146:22 - computer is bringing uh things like hdn
146:24 - sites or data bricks into here but for
146:26 - compute instances is what we need we'll
146:28 - get ahead and go new you'll notice we
146:30 - have the option between CPU and GPU GPU
146:32 - is much more expensive see it's like 90
146:34 - cents per hour for a notebook we do not
146:37 - need anything uh super powerful notice
146:39 - it'll say here development on notebooks
146:41 - IDs lightweight testing here it says
146:43 - classical ml model training autom ml
146:45 - pipelines Etc
146:47 - so I want to make this a bit cheaper for
146:49 - us here uh because we're going to be
146:51 - using the notebook to run uh cognitive
146:54 - services and those cost next to nothing
146:56 - like they don't take much compute power
146:58 - uh and for some other ones we might do
146:59 - something a bit larger for this this is
147:01 - good enough so I'll go ahead and hit
147:02 - next I'm just going to say my uh
147:05 - notebook uh instance
147:07 - here we'll go ahead and hit
147:09 - create and so we're just going to have
147:11 - to wait for that to finish creating and
147:13 - running and when it is I'll see you back
147:15 - here in a moment all right so after a
147:17 - short little wait there it looks like
147:19 - our server is running and you can even
147:20 - see here it shows you you can launch in
147:22 - jupyter Labs Jupiter vs code R Studio or
147:25 - The Terminal but what I'm going to do is
147:27 - go back all the way to our notebooks
147:29 - just so we have some consistency here I
147:30 - want you to notice that it's now running
147:32 - on this compute if it's not you can go
147:34 - ahead and select it uh and it also
147:36 - loaded in Python 3.6 there is 3.8 right
147:39 - now it's not a big deal which one you
147:41 - use um but that is the kernel like how
147:43 - it will run this stuff now this is all
147:45 - interesting but I don't want to uh run
147:47 - this right now what I want to do is get
147:49 - those cognitive Services uh into here so
147:53 - what we can do is just go up here and
147:56 - we'll choose editors and edit in Jupiter
147:59 - lab and what that should do is open up a
148:02 - new tab
148:03 - here uh is it
148:06 - opening if it's not opening what we can
148:08 - do is go to compute sometimes it's a bit
148:10 - more responsive if we just click there
148:11 - it's the same way of getting to it um I
148:14 - don't know why but just sometimes that
148:15 - link doesn't work uh when you're in the
148:16 - notebook and what we can do is well
148:19 - we're in here now we can see that this
148:20 - is where uh this example project is okay
148:25 - um but what we want to do is get those
148:27 - cognitive services in here so I don't
148:30 - know if I showed it to you yet but I
148:31 - have a repository I just got to go find
148:34 - it it's somewhere on my
148:36 - screen um here it is okay so I have a
148:38 - repo called the free a a uh the free a
148:42 - it should be AI 900 I think I'll go
148:45 - ahead and change that or that is going
148:47 - to get
148:49 - confusing okay so what I want you to do
148:52 - here is um we'll get this loaded in so
148:55 - this is a public directory I'm just
148:56 - thinking there's a couple ways we can do
148:58 - it we can go and uh use the terminal to
149:00 - grab it what I'm going to do is I'm just
149:02 - going to go download the
149:06 - zip and this is just one of the easiest
149:09 - ways to install it and we need um to
149:11 - place it somewhere so here are my
149:13 - downloads and I'm just going to drag it
149:16 - out here
149:18 - okay and uh what we'll do is upload that
149:21 - there so I can't remember if it lets you
149:23 - upload entire folders we'll give it a go
149:25 - see if it lets us maybe rename this to
149:27 - the free a or AI 900 there we'll say
149:31 - open uh yeah so it's individual file so
149:34 - it's not that big of a deal but we can
149:35 - go and ahead and select it like
149:39 - that and maybe we'll just make a new
149:41 - folder in here we'll say this cognitive
149:43 - [Music]
149:45 - services
149:52 - okay and uh what we'll do here is just
149:54 - keep on uploading some stuff so we have
150:02 - assets so I have a couple loose Falls
150:06 - there and I know we have a
150:10 - crew oops we'll have
150:13 - crew
150:15 - oops is not as responsive
150:18 - um we want
150:20 - OCR uh I believe we have one called
150:23 - movie uh
150:25 - reviews so we'll go into OCR here and
150:29 - upload the files that we
150:31 - have so we have a few files
150:34 - there and we'll go back a directory here
150:38 - and I know movie uh reviews are just
150:40 - static
150:45 - files and we have an objects
150:54 - folder so we will go back here to
151:01 - objects and then we'll go back and to
151:03 - crew and we need a folder called
151:06 - Warf a folder called
151:09 - Crusher a folder called data and so for
151:13 - each of these we have some
151:15 - images
151:18 - I think we're on Warf right yep we are
151:20 - okay great so we will quickly upload all
151:26 - these well technically we don't really
151:28 - need to upload any of these well these
151:30 - images we don't but I'm going to put
151:31 - them here anyway I just remember that uh
151:34 - these we just upload directly to the
151:36 - service but because I'm already doing
151:38 - anyway I'm just going to put them here
151:39 - even though we're not going to do
151:40 - anything with
151:45 - them
151:48 - all right and so now we are all set up
151:51 - to do some cognitive services so I'll
151:54 - see you in the next video all right so
151:56 - now that we have our work environment
151:57 - set up what we can do is go ahead and
152:00 - get cognitive Services hooked up because
152:02 - um we need that service in order to
152:04 - interact with it because if we open up
152:06 - any of these you're going to notice we
152:07 - have a cognitive key and endpoint that
152:09 - we're going to need so what I want you
152:11 - to do is go back to your Azure portal
152:15 - and at the top here we'll type in
152:17 - cognitive
152:18 - Services now the thing is is that all
152:20 - these services are individualized but at
152:22 - some point they did group them together
152:24 - and you're able to use them through
152:25 - unified um key and API Point that's what
152:28 - this is and that's the way we're going
152:29 - to do it so we'll say
152:32 - add and uh it brought us to the
152:34 - marketplace so I'm just going to type in
152:39 - cognitive
152:43 - services and then just click this one
152:46 - here here and we'll hit
152:48 - create and uh we'll make a new one here
152:50 - I'm just going to call my uh Cog
152:53 - Services say Okay um I prefer to be in
152:57 - Us East I will leave in US West it's
152:59 - fine and so in here we'll just say my
153:02 - Cog
153:04 - services and if it doesn't like that
153:06 - I'll just put some numbers in there we
153:08 - go we'll do standard so we will be
153:11 - charged something for that let's go take
153:12 - a look at the
153:15 - pricing
153:17 - so you can see that the pricing is uh
153:19 - quite variable here but uh it's like
153:21 - you'd have to do a thousand transactions
153:22 - before you are build uh so I think we're
153:25 - going to be okay for billing uh we'll
153:28 - checkbox this here we'll go down below
153:30 - it's telling us about responsible AI
153:31 - notice uh sometimes services will
153:33 - actually have you checkbox it but in
153:34 - this case it just tells us
153:39 - there and we'll go ahead and hit
153:45 - create
153:48 - and I don't believe this took very long
153:50 - so we'll give it a second here yep it's
153:52 - all deployed so we'll go to this
153:53 - resource here and what we're looking for
153:55 - are our keys and end
153:58 - points uh and so we have two keys and
154:00 - two end points we only need a single key
154:02 - so I'm going to copy this endpoint over
154:04 - we're going to go over to Jupiter lab
154:06 - and I'm just going to paste this in here
154:08 - I'm just going to put it in all the ones
154:10 - that need it so this one needs
154:12 - one this one needs
154:15 - one this one needs
154:18 - one and this one needs
154:20 - one and we will show the key here I
154:24 - guess it doesn't show but it copies of
154:26 - course I will end up deleting my key
154:27 - before you ever see it but this is
154:29 - something you don't want to share
154:31 - publicly and usually you don't want to
154:33 - embed Keys directly into a notebook but
154:35 - uh this is the only way to do it so it's
154:37 - just how it is with Azure um so yeah all
154:41 - our keys are installed going back to the
154:43 - cognitive Services uh nothing super
154:45 - exciting here but but it does tell us
154:47 - what services work with it you'll see
154:49 - there's an aster beside custom Vision
154:50 - because we're going to access that
154:51 - through another app um but uh yeah
154:54 - cognitive servic is all set up and so
154:56 - that means we are ready to uh start
154:58 - doing some of these Labs
155:00 - [Music]
155:03 - okay all right so let's take a look here
155:06 - at computer vision first and computer
155:07 - vision is actually used for a variety of
155:09 - different Services as you will see it's
155:11 - kind of an umbrella for a lot of
155:13 - different things but the one in
155:14 - particular that we're looking at here is
155:16 - to describe image in stream if we go
155:18 - over here to the documentation this
155:20 - operation generates description of image
155:22 - in a human reable language with complete
155:24 - sentences the description is based on a
155:26 - collection of content tags which also
155:28 - returned by the operation okay so let's
155:30 - go see what that looks like in action so
155:32 - the first thing is is that um we need to
155:34 - install this Azure cognitive Services
155:36 - Vision computer vision now we do have a
155:38 - kernel and these aren't installed by
155:40 - default they're not part of the um uh
155:44 - machine learning uh the Azure machine
155:46 - learning uh SDK for python I believe
155:49 - that's pre-installed but uh these AI
155:52 - services are not so what we'll do is go
155:53 - ahead and run it this way and you'll
155:55 - notice where it says pip install that's
155:56 - how it knows to install and once that is
155:59 - done we'll go run our requirements here
156:01 - so we have the OS which is for usually
156:04 - handling op like OS layer stuff we have
156:07 - matte matte plot lib which is to
156:10 - visually plot things and we're going to
156:12 - use that to show images and draw borders
156:14 - we need to handle images is I'm not sure
156:17 - if we're using numpy here but I have
156:18 - numpy loaded and then here we have the
156:21 - Azure cognitive Services Vision computer
156:23 - vision we're going to load the client
156:25 - and then we have the credentials and
156:27 - these are generic credentials for the
156:28 - cognitive Services credentials it's
156:30 - commonly used for most of these services
156:32 - and some exceptions they the apis do not
156:34 - support them yet but I imagine they will
156:36 - in the future so just notice that when
156:38 - we run something it will show a number
156:39 - if there's an aster it means it hasn't
156:41 - ran yet so I'll go ahead and hit play up
156:42 - here so it goes an aster and we get a
156:44 - two and we'll go ahe head and hit play
156:47 - again and now those are loaded in and so
156:49 - we'll go ahead and hit
156:51 - play okay so here we've just packaged
156:54 - our credentials together so we passed
156:55 - our key into here and then we'll now
156:58 - load in the client uh and so we'll pass
157:01 - our endpoint and our key okay so we hit
157:04 - play and so now we just want to load our
157:06 - image so here we're loading assets
157:08 - data.jpg let's just make sure that that
157:11 - is there so we go assets and there it is
157:13 - and we're going to load it as a stream
157:15 - because you have to pass streams along
157:16 - so we'll hit play and you'll see that it
157:19 - now ran and so now we'll go ahead and
157:21 - make that
157:23 - call okay great and so we're getting
157:25 - some data back and notice we have some
157:27 - properties person wall indoor man
157:29 - pointing captions it's not showing all
157:32 - the information sometimes you have to
157:33 - extract it out but we'll take a look
157:35 - here so uh this is a way of showing mat
157:37 - pla lib in line I don't think we have to
157:39 - run it here but I have it in here anyway
157:41 - and so what it's going to do is it's
157:43 - going to um show us the image right so
157:46 - it's going to print us the image and
157:48 - it's going to grab whatever caption is
157:50 - returned so see how there's captions so
157:53 - we're going to iterate through the
157:54 - captions it's going to give us a
157:56 - confidence score saying it thinks it's
157:58 - this so let's see what it comes out
158:01 - with okay and so here it says Brent
158:03 - spider Spiner looking at a camera so
158:05 - that is the actor who plays data on Star
158:07 - Trek has a confidence score of 57. 45%
158:10 - even though it's 100% correct uh they
158:13 - probably don't know contextual things
158:14 - like um uh in the sense of like pop
158:17 - culture like they don't know probably
158:18 - start Tre characters but they're going
158:20 - to be able to identify celebrities
158:21 - because it's in their database so that
158:23 - is um uh uh the first introduction to
158:27 - computer computer vision there but the
158:29 - key things you want to remember here is
158:30 - that we use this describe an image
158:32 - stream uh and that we get this
158:34 - confidence score and we get this
158:36 - contextual information okay and so
158:38 - that's the first one we'll move on to um
158:40 - maybe custom Vision
158:44 - next
158:46 - all right so let's take a look at custom
158:48 - Vision so we can do some um
158:50 - classification and object detection so
158:53 - um the thing is is that it's possible
158:57 - it's possible to launch custom Vision
158:59 - through the marketplace so if we go
159:00 - we're not going to do it this way if you
159:02 - type in custom Vision it never shows up
159:04 - here but if you go to the marketplace
159:05 - here and type in custom
159:08 - vision and you go here you can create it
159:11 - this way but the way I like to do it I
159:13 - think it's a lot easier to do is we go
159:15 - up the top here and type in custom
159:17 - vision. and you'll come to this website
159:19 - and what you'll do is go ahead and sign
159:21 - in it's going to connect to your Azure
159:22 - account and once you're in you can go
159:24 - ahead here and create a new project so
159:25 - the first one here is I'm just going to
159:27 - call this the Star Trek crew we're going
159:29 - to use this to identify different Star
159:31 - Trek members we'll go down here and uh
159:34 - we haven't yet created a resource so
159:35 - we'll go create
159:37 - new my custom Vision
159:41 - resource we'll drop this down we'll put
159:43 - this in our Cog services uh we'll go
159:47 - stick with um Us West as much as we can
159:50 - here we have fo Ando fo is blocked up
159:53 - for me so just choose so I think fo is
159:55 - the free tier but I don't get
159:58 - it and um once we're back here we'll go
160:02 - down below and choose our standard and
160:03 - we're going to have a lot of options
160:05 - here so we have between classification
160:06 - and object detection so classification
160:09 - is when you have an image and you just
160:10 - want to say what what is this image
160:13 - right and so we have two modes where we
160:15 - can say
160:16 - let's apply multiple labels so let's say
160:18 - there were two people in the photo or
160:20 - whether there was a dog and cat I think
160:21 - that's example that use a dog and a cat
160:23 - or you just have a single class where
160:25 - it's like what is the one thing that is
160:27 - in this photo it can only be of one of
160:28 - the particular categories this is the
160:30 - one we're going to do multic class and
160:32 - we have a bunch of different domains
160:34 - here and if you want to you can go ahead
160:35 - and read about all the different domains
160:37 - and their best use case but we're going
160:39 - to stick with A2 this is optimized for
160:42 - so that it's faster right and that's
160:43 - really good for our demo so we're going
160:46 - to choose General A2 I'm going to go
160:47 - ahead and create this
160:50 - project and uh so now what we need to do
160:52 - is start labeling our our our content so
160:56 - um what we'll do is I just want to go
160:58 - ahead and create the tags ahead of time
160:59 - so we'll say
161:00 - Warf we'll have uh data and we'll have
161:06 - Crusher and now what we'll do is we'll
161:08 - go ahead and upload those images so you
161:10 - know we uploaded the jupyter notebook
161:11 - but it was totally not necessary so here
161:13 - is data because we're going to do it all
161:15 - through here and we'll just apply the
161:17 - data tag to them all at once which saves
161:18 - us a lot of time I love that uh we'll
161:21 - upload now uh
161:24 - war and I don't want to upload them all
161:26 - I have this one quick test image we're
161:27 - going to use to make sure that this
161:29 - works
161:30 - correctly and I'm going to choose
161:34 - Warf and then we'll go ahead and add
161:39 - Beverly there she
161:41 - is Beverly
161:44 - Crusher okay so we have all our our
161:46 - images in I don't know how this one got
161:48 - in here but it's under worth it works
161:50 - out totally fine so uh what I want to
161:53 - do is uh go ahead and train this small
161:57 - because they're all labeled so we have a
161:59 - ground truth and we'll let it go ahead
162:01 - and train so we'll go and press train
162:03 - and we have two options quick training
162:04 - or Advanced Training Advanced Training
162:05 - where we can increase the time for
162:07 - better accuracy but honestly uh we just
162:10 - want to do quick training so I'll go
162:11 - ahead and do quick training and it's
162:13 - going to start its iterative process
162:15 - notice on the left hand side we have
162:17 - probability threshold the minimum
162:19 - probability score for a prediction to be
162:21 - valid when calcul calculating precision
162:23 - and recall so we uh the thing is is that
162:26 - if it doesn't at least meet that
162:28 - requirements it will quit out and if it
162:30 - gets above that then it might quit out
162:32 - early just because it's good enough okay
162:34 - so training doesn't take too long it
162:36 - might take 5 to 10 minutes I can't
162:38 - remember how long it takes but uh what
162:40 - I'll do is I'll see you back here in a
162:41 - moment okay all right so after waiting a
162:44 - short little while here looks like our
162:46 - results are down we get 100% um match
162:49 - here so these are our evaluation metrics
162:50 - to say whether uh the model was uh uh
162:54 - achieved its actual goal or not so we
162:56 - have Precision recall and I believe this
162:59 - is average Precision uh and so it says
163:02 - that it did a really good job so that
163:03 - means that it should have no problem um
163:06 - matching up an image so in the top right
163:07 - corner we have this button that's called
163:09 - quick test and this is going to give us
163:11 - the opportunity to uh quickly test these
163:13 - so what we'll do is browse our files
163:15 - locally here and uh actually I'm going
163:18 - to go to uh yeah we'll go here and we
163:22 - have Warf uh and so I have this quick
163:24 - image here we'll test and we'll see if
163:26 - it actually matches up to
163:27 - bwf and it says 98.7% Warf that's pretty
163:31 - good I also have some additional images
163:33 - here I just put into the repo to test
163:35 - against and we'll see what it matches up
163:37 - because I thought it'd be interesting to
163:38 - do something that is not necessarily uh
163:40 - them but it's something pretty close to
163:43 - um you know it's pretty close to what
163:45 - those are okay so we'll go to crew here
163:47 - and first we'll try
163:50 - Hugh okay and Hugh is a borg so he's
163:53 - kind of like an Android and so we can
163:55 - see he mostly matches to data so that's
163:57 - pretty good uh we'll give another one go
163:59 - Marto is a Klingon so he should be
164:01 - matched up to Warf very strong match to
164:04 - Warf that's pretty good and then palaski
164:07 - she is a doctor and female so she should
164:09 - get matched up to Beverly Crusher and
164:12 - she does so this works out pretty darn
164:14 - well uh and I hadn't even tried that so
164:16 - it's pretty exciting so now let's say we
164:18 - want to go ahead and well if if we
164:20 - wanted to um make predictions we could
164:23 - do them in bulk here um I believe that
164:26 - you could do them in bulk but
164:30 - anyway yeah I guess I always thought
164:32 - this was like I could have swore yeah if
164:34 - we didn't have these images before I
164:35 - think that it actually has an upload
164:36 - option it's probably just the quick test
164:38 - so I'm a bit confused there um but
164:40 - anyway so now that this is ready what we
164:42 - can do is go ahead and publish it uh so
164:44 - that it is publicly accessible so we'll
164:46 - just say here a crew
164:48 - model okay and we'll drop that down say
164:55 - publish and once it's published now we
164:58 - have this uh public out so this is an
164:59 - endpoint that we can go hit
165:01 - pragmatically uh I'm not going to do
165:03 - that I mean we could use Postman to do
165:04 - that um but my point is is that we've
165:07 - basically uh figured it out for um
165:10 - classification so now that we've done
165:11 - classification let's go back here to uh
165:16 - the vision here and let's now let's go
165:17 - ahead and do object detection
165:20 - [Music]
165:23 - okay all right so we're still in custom
165:26 - Vision let's go ahead and try out object
165:27 - detection so object detection is when
165:29 - you can identify a particular items in a
165:32 - scene um and so this one's going to be
165:34 - combadge that's what we're going to call
165:35 - it because we're going to try to detect
165:36 - combadge we have more domains here we're
165:38 - going to stick with the general
165:40 - A1 and we'll go ahead and create this
165:43 - project
165:44 - here and so what we need to do is add a
165:47 - bunch of images I'm going to go ahead
165:49 - and create our tag which is going to be
165:50 - called combadge uh you could look for
165:53 - multiple different kinds of labels but
165:55 - then you need a lot of images so we're
165:57 - just going to keep it simple and have
165:58 - that there I'm going to go ahead and add
166:01 - some images and we're going to go back
166:03 - um a couple steps here into our objects
166:05 - and here I have a bunch of photos and we
166:06 - need exactly 15 to train so we got one
166:08 - two 3 4 5 6 7 8 9 10 11 12 13 14 15 16
166:14 - and so I threw an additional image in
166:16 - here this is the badge test so we'll
166:18 - leave that out and we'll see if that
166:20 - picks up really well and yeah we got
166:23 - them all here and so we'll go ahead and
166:25 - upload those and we'll hit upload
166:29 - files
166:31 - okay and we'll say done and we can now
166:34 - begin to label so we'll click into here
166:36 - and what I want to do if you hover over
166:38 - it should start detecting things if it
166:40 - doesn't you can click and drag we'll
166:41 - click this one they're all com badges so
166:43 - we're not going to tag anything else
166:44 - here okay okay so go here hover over is
166:47 - it going to give me the combadge no so
166:48 - I'm just draag clicking and dragging to
166:50 - get it
166:51 - okay okay do we get this combadge
166:56 - yes do we get this one
166:59 - yep so simple as
167:03 - that okay it doesn't always get it but
167:06 - uh most cases it
167:10 - does okay didn't get that one so we'll
167:12 - just drag it
167:13 - out
167:18 - okay it's not getting that
167:22 - one it's interesting like that one's
167:24 - pretty clear but uh it's interesting
167:26 - what it picks out and what does what it
167:27 - does not grab eh so it's not getting
167:29 - this one probably because the photo
167:30 - doesn't have enough
167:33 - contrast and this one has a lot hoping
167:36 - that that gives us more data to work
167:37 - with here yeah I think the higher the
167:40 - contrast it's easier for it to uh
167:43 - um detect those
167:45 - it's not getting that
167:47 - one it's not getting that one okay there
167:50 - we
167:57 - go yes there are a lot I know I have
167:59 - some of these ones that are packed but
168:01 - there's only like three photos that are
168:02 - like
168:07 - this yeah they have badges but they're
168:09 - slightly different so we're going to
168:10 - leave those
168:11 - out oops I think it actually had that
168:13 - one but we'll just tag it
168:25 - anyway and hopefully this will be worth
168:28 - the uh the effort
168:32 - here there we
168:36 - go I think that was the last
168:38 - one okay great so we have all of our
168:41 - tagged photos and what we can do is go
168:42 - ahead and train the model same option
168:44 - quick training Advanced Training we're
168:46 - going to do a quick training here and
168:47 - notice that the options are slightly
168:49 - different we have probably threshold and
168:51 - then we have overlap threshold so the
168:52 - minimum percentage of overlap between
168:54 - predicted bounding boxes and ground
168:55 - truth boxes to be considered for correct
168:58 - prediction so I'll see you back here
169:00 - when it is done all right so after
169:02 - waiting a little bit a while here it
169:04 - looks like um it's done it's trained and
169:06 - so Precision is at 75% so Precision the
169:09 - number will tell you if a tag is
169:11 - predicted uh by your model How likely
169:13 - that it's likely to be so how likely did
169:15 - it guess right then you have recall so
169:17 - the number will tell you out of the tags
169:19 - which should be predicted correctly what
169:21 - percentage did your model correctly find
169:23 - so we have 100% uh and then you have
169:25 - mean average Precision this number will
169:27 - tell you the overall object detector
169:30 - performance across all the tags okay so
169:34 - what we'll do is we'll go ahead and uh
169:36 - do a quick test on this model and we'll
169:39 - see how it does I can't remember if I
169:41 - actually even ran this so it'll be
169:42 - curious to see the first one here um
169:45 - it's not as clearly visible it's part of
169:47 - their uniform so I'm not expecting to
169:49 - pick it up but we'll see what it does it
169:50 - picks up pretty much all of them with
169:53 - exception this one is definitely not a
169:55 - comp badge but uh that's okay only show
169:58 - suggests obious the probabilities above
170:00 - the selected
170:01 - threshold so if we increase
170:04 - it uh we'll just bring it down a bit so
170:07 - there it kind of improves it um if we
170:09 - move it around back and forth okay so I
170:12 - imagine via the API we could choose that
170:14 - let's go look at our other sample image
170:17 - here
170:18 - um I'm not seeing
170:23 - it uh where did I save it let me just
170:26 - double check make sure that it's in the
170:28 - correct directory here
170:30 - okay yeah I saved it to the wrong place
170:32 - just a
170:34 - moment
170:37 - um I will place
170:43 - it just call that bad test two one
170:50 - second okay and so I'll just browse here
170:53 - again and so here we have another one
170:56 - see if it picks up the badge right here
170:59 - there we go so looks like it worked so
171:01 - uh yeah I guess custom vision is uh
171:03 - pretty easy to use and uh pretty darn
171:05 - good so what we'll do is close this off
171:08 - and make our way back to our Jupiter
171:10 - labs to move on to um our our next uh
171:15 - lab here
171:16 - [Music]
171:20 - okay all right so let's move on to the
171:22 - face service so just go ahead and double
171:24 - click there on the left hand side and
171:25 - what we'll do is work our way from the
171:26 - top so the first thing we need to do is
171:28 - make sure that we have the computer
171:30 - vision installed so the face service is
171:33 - part of the computer vision API and once
171:36 - that is done we'll go ahead and uh do
171:38 - our Imports very similar to the last one
171:40 - but here we're using the face client
171:42 - we're still using the Cog cognitive
171:44 - service credentials we will populate our
171:47 - keys we make the face client and
171:50 - authenticate and we're going to use the
171:52 - same image we used um uh prior with our
171:55 - computer vision so the data one there
171:57 - and we'll go ahead and print out the
171:58 - results and so we get an object back so
172:00 - it's not very clear what it is but here
172:03 - if we hit
172:04 - show okay here it's data and it's
172:06 - identifying the face ID so going through
172:08 - this code so we're just saying open the
172:09 - image we're going to uh set up our
172:12 - figure for plotting uh it's going to say
172:14 - well how many faces did it detect in the
172:16 - photo and so here it says detected one
172:19 - face it will iterate through it and then
172:22 - we will create a bounding box around the
172:24 - images we can do that because it returns
172:26 - back the face rectangle so we get a top
172:28 - left right Etc and uh we will draw that
172:32 - Wrangle on top so we have magenta I
172:33 - could change it to like three if I
172:35 - wanted to uh I don't know what the other
172:37 - colors are so I'm not even going to try
172:39 - but yeah there it is and then we
172:41 - annotate with the face ID that's the
172:42 - unique identifier for the face and then
172:45 - we show the image okay so that's one and
172:47 - then if we wanted to get more detailed
172:49 - information like attribute such as age
172:51 - emotion makeup or gender uh this
172:54 - resolution image wasn't large enough so
172:56 - I had to find a different image and and
172:58 - do that so that's one thing you need to
173:00 - know as if it's not large enough it
173:02 - won't process it so we're just loading
173:03 - data
173:05 - large very similar process but it
173:08 - is uh the same thing detect with stream
173:11 - but now we're passing
173:13 - in um return face attributes and so here
173:16 - we're saying the attributes we want uh
173:19 - and there's that list and we went
173:20 - through it in the lecture content and so
173:22 - here we'll go ahead and run this and so
173:25 - we're getting more information so that
173:27 - magenta line is a bit hard to see I'm
173:28 - just going to increase that to
173:30 - three okay still really hard to see but
173:33 - that's okay so approximate age 44 I
173:36 - think the actor was a bit younger than
173:37 - that uh uh data technically is male
173:40 - presenting but he's an Android so he
173:42 - doesn't necessarily have a gender I
173:44 - suppose he actually is wearing a lot of
173:46 - makeup but all it detects is it I guess
173:48 - it's only particular on the lips and the
173:50 - eyes so it says he doesn't have makeup
173:51 - so maybe there's a color you know like
173:53 - ey Shadow stuff maybe we would detect
173:55 - that in terms of personality I like how
173:57 - it's he's a a 002 Point per SB but he's
174:01 - neutral right uh so just going through
174:03 - the code here very quickly so again it's
174:05 - the number of faces so it detected one
174:07 - face uh and then we draw a bounding box
174:10 - around the face for the detected
174:12 - attributes it's uh return back in the
174:15 - data here so we just say get the pH
174:18 - attributes turn it into a dictionary and
174:20 - then we can just uh get those values and
174:23 - uh iterate over it so that's as
174:25 - complicated as it is um and so there we
174:28 - [Music]
174:32 - go all right so we're on to uh our next
174:35 - cognitive service let's take a look at
174:37 - form recognizer all right and so form
174:40 - recognizer uh it tries to identify um
174:44 - like forums and turns them into readable
174:45 - things and so they have one for uh
174:47 - receipts in particular so at the top
174:49 - finally we're not using um the computer
174:52 - uh computer vision we actually have a
174:53 - different one so this one's Azure AI
174:55 - form recognizer so we'll run that there
174:58 - but this one in particular isn't up to
175:00 - date in terms of using it like um notice
175:03 - all the other ones they're using uh the
175:06 - cognitive service credential so for this
175:08 - we actually had to use the Azure key uh
175:10 - credential which was annoying I tried to
175:12 - use the other one to be consistent um
175:14 - but I I couldn't use it okay so what
175:16 - we'll do is run our keys like before we
175:19 - have a client very similar
175:21 - process and this time we actually have a
175:24 - receipt and so we have begin recognize
175:27 - receipt so it's going to analyze the
175:28 - receipt information and then it's what
175:31 - it's going to do is show us the image
175:33 - okay just so we have a reference to look
175:34 - at now the image isn't actually yellow
175:37 - it's a white background I don't know why
175:38 - when it renders out here it does that
175:40 - but that's just what
175:41 - happens and uh it even obscures the
175:44 - server name I I don't know why um but
175:47 - anyway if we go down below um this is
175:50 - return results up here right so we got
175:52 - our results and so if we just print out
175:55 - uh the results here we can see we get a
175:57 - recognized form back we get fields and
176:00 - some additional things and if we go into
176:01 - the uh the fields itself we see there's
176:03 - a lot more information if you can make
176:05 - out like here it says Merchant phone
176:06 - number form field label value and
176:09 - there's the number
176:11 - 512707 so for these things here like um
176:15 - the
176:17 - receipts if we can just find the API
176:19 - quickly here it has predefined
176:22 - Fields I'm not sure um yeah business
176:25 - card
176:27 - Etc um like if we just type in
176:32 - merchant I'm just trying to see if
176:33 - there's a big old list here it's not
176:35 - really showing us a full list but these
176:37 - are are predefined um things that are
176:39 - returned right so they've defined those
176:42 - uh maybe it's over here
176:44 - there we go so these are the predefined
176:46 - ones that extracts out so we have uh
176:48 - receipt type Merchant name etc etc and
176:51 - so if we go back to here you can see um
176:54 - I I have the field called Merchant name
176:56 - so we hit there it says Alm draft out
176:59 - Cinema let's say we want to try to get
177:00 - that balance maybe we can try to figure
177:02 - out which one it is I never ran this
177:04 - myself when I I made it so we'll see
177:06 - what it is but here it has total price
177:08 - what's interesting is that these this
177:10 - has a space so it's kind of unusual ual
177:14 - you think it'd be together but let's see
177:15 - if that
177:17 - works okay doesn't like that maybe
177:20 - that's just a typo on their part okay so
177:23 - we get none uh let's try
177:26 - price see what it picks
177:28 - up nope nothing um we know that the
177:33 - phone number is there so we'll give the
177:34 - phone
177:36 - number there we go so you know it's an
177:39 - okay service but uh you know uh you know
177:42 - you're you're mileage will vary based on
177:45 - uh what you do there maybe we could try
177:47 - total because that makes more sense
177:50 - right uh yeah there we go okay great so
177:53 - yeah it is pulling out the information
177:54 - um and so that's pretty much all you
177:57 - need to know about that service there
177:59 - [Music]
178:02 - okay let's take a look at some of our
178:04 - OCR capabilities here uh and I believe
178:07 - that's in computer vision so we'll go
178:08 - ahead and open that up at the top here
178:10 - we'll install computer vision as we did
178:12 - before very similar to the other
178:15 - computer vision task but this time we
178:16 - have a couple of ones here that'll
178:19 - explain that as we go through here we'll
178:21 - load our keys we'll do our credentials
178:24 - we'll load the client okay and then we
178:27 - have this um function here called
178:29 - printed text so what this function is
178:31 - going to do is it's going to uh print
178:34 - out the results of whatever text it
178:36 - processes okay so the idea is that we
178:39 - are going to feed in an image and it's
178:42 - going to give us back out the text for
178:43 - the the image so we'll run this function
178:46 - and I have two different images cuz I
178:48 - actually ran it on the first one and the
178:50 - results were terrible and so I got a a
178:52 - second image and it was a bit better
178:54 - okay so we'll go ahead and run this it's
178:55 - going to show us the image okay and so
178:58 - this is the photo and it was supposed to
178:59 - extract out Star Trek the Next
179:00 - Generation but because of the artifacts
179:02 - and size of the image we get back uh not
179:05 - English okay and so you know maybe a
179:08 - high resolution image it would have um a
179:10 - better a better time there um but that
179:13 - is what we got back okay so let's go
179:16 - take a look at our second image and see
179:18 - how it did and this one I'm surprised it
179:20 - actually extracts out a lot more
179:21 - information you can see really has a
179:23 - hard time with the Star Trek font but we
179:25 - get Deep Space 9 Nana Visitor tells all
179:27 - Life Death some errors here so it's not
179:30 - perfect um but you know you can see that
179:32 - it does something here now there is the
179:35 - O this is like for OCR where we have
179:37 - like for very simple images and text
179:39 - this is where we use the recognized
179:40 - printed text in stream but uh if we were
179:43 - doing this for larger amounts of text
179:45 - and we want to do this uh want this
179:47 - analyzed asynchronously then we want to
179:49 - use the read API and it's a little bit
179:51 - more involved um so what we'll do here
179:53 - is load a different image and this is a
179:55 - script we'll look at the image here in a
179:56 - moment um but here we read in stream and
180:00 - we create these
180:02 - operations okay and what it will do is
180:04 - it will asynchronous asynchronously send
180:07 - all the information over okay uh so I
180:10 - think this is supposed to be results
180:12 - here minor typ
180:14 - and um we will go ahead and give that a
180:18 - run okay and so here you can see it's
180:21 - extracting up the image if we want to uh
180:23 - uh see this image I thought I uh I
180:26 - thought I showed this image here but I
180:27 - guess I don't yeah it says plot image
180:30 - here to show us the
180:32 - image
180:34 - uh path it's up
180:38 - here it doesn't want to show us it's
180:41 - funny because this one up here is
180:42 - showing us no problem right
180:46 - um well I can just show you the image
180:48 - it's not a big
180:50 - deal but I'm not sure why it's not
180:52 - showing up here
180:54 - today so if we go to our assets here I
180:58 - go to
181:00 - OCR uh I'm just going to open this
181:03 - up it's opening up in Photoshop and so
181:05 - this is what it's transcribing okay so
181:08 - this is a thing this is like a guide to
181:09 - Star Trek where they talk about like you
181:11 - know what what makes St Trek Star Trek
181:13 - so just looking here it's actually
181:15 - pretty darn good okay but like read API
181:17 - is a lot more uh efficient because it
181:19 - can work uh
181:22 - umly and so when you have a lot of text
181:24 - that's what you want to do okay um and
181:27 - like it's feeding in each individual
181:28 - line right so that it can be more
181:30 - effective that way um so let's go look
181:32 - at some hand Rd and stuff so just in
181:34 - case the image doesn't pop up we'll go
181:35 - ahead and open this one and so this is a
181:38 - a a handwritten note that uh William
181:41 - Shatner wrote to a fan of Star Trek and
181:45 - it's basically incomprehensible I don't
181:47 - know if you can read that here but see
181:50 - was very something he was something
181:53 - hospital and healthy was something he
181:56 - was something I can't even read it okay
181:59 - so let's see what uh the machine thinks
182:04 - here and uh it says image path yeah it's
182:08 - called path let's just change that out
182:11 - go ahead and run that and run that there
182:15 - and we'll go ahead and run it and here
182:17 - we get the image so uh poner us very
182:21 - sick he was the hospital his Bey was Etc
182:25 - beat nobody lost his family knew Captain
182:27 - Halden so reads better than how I could
182:29 - read it honestly like it is it's really
182:31 - hard right like if you looked at
182:33 - this like that looks like difficult
182:38 - was Beady healthy I could see why it's
182:41 - guessing like that right dying that
182:42 - looks like dying to me you know what I
182:45 - mean so you it's just poorly hand
182:47 - handwritten but I mean it's pretty good
182:49 - for what it is so uh yeah there you
182:51 - [Music]
182:55 - go all right so let's take a look at
182:57 - another cognitive service here and this
182:58 - one is text
183:01 - analysis and uh so what we'll do is
183:03 - install the Azure cognitive Services
183:05 - language uh text analytics here so we go
183:07 - ahead and hit run all right and once
183:11 - that's uh installed uh this one is using
183:14 - the cognitive Services credentials so
183:15 - it's a little bit more standard with our
183:17 - other ones here we'll go ahead and run
183:19 - that there uh we'll make our credentials
183:22 - load our client and this one what we're
183:24 - going to do is try to determine
183:26 - sentiment and understand why people like
183:28 - a particular movie or not so I've loaded
183:31 - a bunch of reviews um they are again I
183:33 - can show you the data if it helps uh and
183:38 - so I'm just trying to find my right
183:39 - folder here and so if we go back look
183:43 - our movie reviews here's like a a review
183:44 - someone wrote first Contact just works
183:47 - it works as a rousing chapter in the
183:49 - Star Trek to less extent it works as a
183:50 - mainstream entertainment so different
183:52 - reviews for Star Trek first Contact
183:54 - which was a a very popular movie back in
183:56 - the day um so what we'll
183:59 - do is we will load uh the reviews so
184:02 - it's just iterating through the text
184:03 - files and showing us what the reviews
184:05 - are so here we can see all the ridden
184:07 - text had a lot of trouble getting the
184:09 - last one to display but it does get
184:10 - loaded in and so here we're using the
184:13 - the the um text analysis to show us uh
184:18 - key phrases because maybe that would
184:19 - give us an indicator and so that's the
184:21 - object back but maybe that give us an
184:23 - indicator as to like what people are
184:25 - saying as important things so here we
184:26 - see Borg ship Enterprise smaller ship
184:28 - escapes neutral zone travels contact
184:31 - damage uh co-writer Beautiful Mind
184:33 - sophisticated science fiction best
184:36 - whales Leonard neoy okay uh wealth of
184:40 - unrealized potential uh filmmaker John
184:43 - fr s okay so very interesting stuff as
184:46 - it here Borg ship again you've seen Borg
184:48 - ship a lot so that is kind of key
184:50 - phrases let's go get uh C or customer
184:53 - sentiment or how people felt about it
184:54 - did they like it or not and so here we
184:57 - just call sentiment and um what we'll do
184:59 - is if it's uh above five then it's
185:01 - positive and it's below five then it's a
185:02 - negative review I think most people uh
185:06 - thought it was a very good film uh so
185:08 - this one says it's pretty low nine so
185:10 - let's go take a look at that one uh it
185:12 - wasn't actually showing rendered there
185:14 - so maybe we'll have to open it up
185:15 - manually see if that's actually accurate
185:18 - it's empty so there you go I guess we
185:20 - had a blank one in there um I must have
185:22 - forgot to paste it in but that's okay uh
185:25 - that's a good indicator that uh you know
185:27 - that's what happens if you don't have it
185:28 - so let's look at number one then which
185:29 - is uh well actually this one is nine
185:32 - this is 04 this one here is eight so
185:36 - we'll open up eight when the Borg
185:38 - launched on Earth the Enterprise is sent
185:39 - to the neutral zone etc etc however a
185:42 - smaller ship escapes travels the
185:43 - Enterprise follows back um meanwhile the
185:47 - survivors so like this is a synopsis it
185:49 - doesn't say whether they like it or they
185:50 - don't but it was just 04 I I guess so
185:53 - there's nothing positive about it right
185:55 - um if we look at one that was this one's
185:58 - pretty low which is no no it's not it's
186:01 - one so it seems like this person
186:03 - probably really liked it or no I guess
186:06 - that's actually pretty low because it's
186:07 - one it's not nine Nine's very high let's
186:10 - take a look at this one review number
186:12 - two uh if we go up
186:15 - here the doo has improved the story mon
186:17 - turn the show but there's a wealth of
186:18 - unrealized potential so that's a fair
186:20 - one saying they maybe they don't like it
186:22 - as much I don't know if they give it two
186:24 - stars right we could probably actually
186:26 - correlate it with the actual results
186:27 - because I did get these off of IMDb and
186:29 - Rotten Tomatoes but uh yeah there you go
186:31 - that is Tex
186:34 - [Music]
186:38 - analysis all right so now we're on to
186:40 - Q&A maker and so we're not going to need
186:43 - to do anything pragmatically because Q&A
186:45 - maker is all about no code or low code
186:48 - to build out a questions and answers uh
186:50 - uh bot service so what we'll do is go
186:53 - all the way up here and I want you to
186:54 - type in Q andm maker. a because as far
186:56 - as I'm aware of it's not accessible
186:58 - through the portal sometimes you can
187:00 - find these things um again if we go to
187:03 - the
187:04 - marketplace I'm just curious I'm going
187:05 - just take a look here really quickly uh
187:08 - whenever it decides to log Us in here
187:10 - okay great so I'll go over to
187:12 - Marketplace and probably if we typed in
187:14 - Q&A maybe we'd see something here
187:19 - Q&A yeah so we go
187:22 - here um give it a second
187:25 - here seems like Azure is a little bit
187:28 - slow right
187:31 - now usually varies fast but uh you know
187:34 - the service
187:36 - varies well it's not loading for me
187:38 - right now but that's okay because we're
187:39 - not going to do it that way anyway um so
187:42 - uh again go to Q&A maker. and what I
187:45 - want you to do is go all way to the top
187:47 - in the right corner and we'll hit sign
187:48 - in and what we'll be doing is connecting
187:51 - via our single sign on with our account
187:53 - so it already knows I have an account
187:54 - there I'm going to give it a moment here
187:57 - and I'm going to go ahead and just give
187:59 - it a
188:12 - second
188:15 - there we go so it says I don't have any
188:17 - um knowledge bases which is true so
188:19 - let's go ahead and create ourselves a
188:20 - new knowledge base and here we have the
188:22 - option between stable and preview I'm
188:24 - going to stick with stable because I
188:25 - don't know what's in preview I'm pretty
188:26 - happy with uh that and so we need to
188:28 - connect uh Q&A Service uh uh Q&A service
188:32 - to our knowledge base and so back over
188:34 - here in Azure actually I guess we do
188:36 - have to make one now that I remember we
188:37 - actually have to create a Q&A maker
188:39 - service so I'll go down here and put
188:41 - this under my Cog services we'll say
188:44 - my um
188:46 - Q&A Q&A
188:49 - service might complain about the name uh
188:52 - yep so I'll just put some numbers here
188:54 - we'll pick uh free tier sounds good so
188:57 - I'll go free when I actually get the
188:58 - option that's what I will choose um down
189:01 - below we'll choose free again usse
189:03 - sounds great to me uh it generates out
189:05 - the name it's the same name as here so
189:07 - that's fine uh we don't need app
189:09 - insights but I'm going to leave it
189:10 - enabled because I think it changes it to
189:12 - standard or s zero when you uh do
189:14 - not um have it enabled
189:17 - unusually and so we will create our Q&A
189:20 - maker service give it a moment
189:23 - here and it says I remember it will say
189:26 - like even if you try it might have to
189:28 - wait 10 minutes for it to create the
189:29 - service so even though even after it's
189:31 - provisioned um it'll take some time so
189:33 - what we should do is prepare our doc
189:35 - because it can take in a variety
189:36 - different files and I just want to show
189:38 - you here that uh the Q&A they have a
189:41 - whole paper here formatting the
189:42 - guidelines
189:43 - and basically it's pretty smart about
189:45 - knowing where headings and answers is so
189:48 - for unstructured data we just have a
189:49 - heading and we have some text so let's
189:51 - write some things in here that we can
189:52 - think of since we're all about
189:53 - certification we should write some stuff
189:55 - here so how many adus certifications are
189:59 - there I believe right now there are uh
190:03 - 11 uh adus
190:05 - certifications
190:08 - okay and maybe if we use our headings
190:10 - here this would probably be a good idea
190:12 - here y
190:17 - okay another one could be um how many F
190:24 - fundamental
190:27 - Azure certifications are
190:36 - there and uh we'll give this a heading
190:38 - we'll say um there are three Azure I
190:44 - think there's three there's other ones
190:46 - right like Power Platform stuff but just
190:48 - being Azure specific there are three
190:50 - Azure uh
190:53 - fundamental certifications certification
190:56 - so we have
190:58 - um the dp900 the AI 900 um the a900 I
191:03 - guess there's four there's the sc900
191:05 - right so there are
191:07 - four
191:10 - okay we'll say which is the
191:14 - hardest
191:15 - um Azure assoc Azure Association
191:30 - certification and uh what we'll say here
191:33 - is I think I mean it's my my opinion is
191:36 - it's the Azure administrator had some
191:38 - background noise there that's why I was
191:40 - a bit pausing there but the Azure
191:41 - administrator a 104 I would say that's
191:44 - the hardest uh which is
191:50 - harder um the uh adabs or Azure
191:58 - certifications I would say uh Azure
192:01 - certifications are
192:04 - harder because they uh check uh exact
192:09 - steps for
192:11 - implementation
192:13 - where AWS focuses
192:17 - on
192:19 - Concepts okay so we have a bit of a um
192:22 - knowledge base here so I'll save it and
192:25 - assuming that this is ready because we
192:26 - need a little bit time to put this
192:27 - together we'll go back to q a get hit a
192:31 - refresh
192:32 - here give it a moment drop it down
192:37 - choose our
192:41 - service
192:43 - and uh notice here that we have chitchat
192:45 - extraction and only extraction we're
192:46 - going to do chitchat I will say uh my or
192:50 - this will be uh the reference name you
192:52 - change any time this will be like uh uh
192:54 - certification
192:57 - Q&A and so here we want to populate so
193:00 - we'll go to files here I'm going to go
193:02 - to my
193:04 - desktop and here it is I'll open
193:07 - it we will choose professional tone go
193:10 - ahead and create that and so I'll see
193:12 - you back here moment all right so after
193:14 - waiting a short little time here it
193:16 - loaded in our data so you can see that
193:17 - it it figured out which is the question
193:19 - which is the answer and also has a bunch
193:21 - of default so here if somebody was asked
193:23 - something very s uh silly like can you
193:25 - cry I'll say I don't have a body it has
193:27 - a lot of information pre-loaded for us
193:30 - which is really nice if we wanted to go
193:31 - ahead and test this we could go and say
193:33 - um we'll go here and then we'll write in
193:36 - uh we say like
193:41 - hello
193:44 - I say
193:47 - boring says good morning okay so we'll
193:50 - say um how many uh
193:53 - certifications are there we didn't say
193:56 - AWS but let's just see what
194:03 - happens and so it kind of inferred even
194:05 - though we didn't say AWS in particular
194:07 - so and notice that there's ads and Azure
194:10 - so how many fundamental Azure
194:11 - certifications things like that and so
194:12 - it chose AWS so it's not like the
194:15 - perfect service but it's pretty good I
194:17 - wonder what would happen if we um placed
194:20 - in uh one that's like Azure I don't know
194:23 - how many Azure Sears there are we'll
194:24 - just say like there's 11 12 I can't
194:25 - never remember they're always adding
194:26 - more but uh it I want to close this here
194:29 - there we go so let's just go add a new
194:31 - key pair here and we'll say how many
194:34 - Azure
194:35 - [Music]
194:37 - certification are there I should have
194:39 - said certifications I'll probably just
194:41 - set one moment so there there
194:45 - are 12 Azure
194:48 - certifications who knows how many they
194:50 - have they could like 14 or something we
194:51 - could say like between 11 and
194:55 - 14 they just add them they just update
194:57 - them too frequently I can't keep track
195:00 - so uh we'll go here and we'll just say
195:02 - certifications and we will save and
195:04 - retrain so we'll just wait here a
195:10 - moment great and so now we go ahead and
195:12 - test this again so we'll
195:14 - say how many
195:17 - certifications are
195:23 - there and see it's pulling the first
195:25 - answer if I say uh Azure if it's see if
195:28 - it gets the right one
195:31 - here how many Azure certifications are
195:37 - there okay so you know uh maybe you'd
195:41 - have to say you'd have to have a generic
195:43 - one for that match so if we go back here
195:45 - and we
195:48 - say how many
195:50 - certifications are there you say uh you
195:54 - know like uh uh
195:57 - which certification uh uh which Ser
196:02 - cloud service
196:05 - provider here we got
196:08 - ads
196:10 - Azure uh prompt you can use Guides
196:13 - Through conversational flow prompts are
196:14 - used to link Q&A Pairs and can be
196:17 - displayed um I haven't used this yet but
196:19 - I mean it sounds like something that's
196:20 - pretty good um because there is
196:22 - multi-turn in this so the idea is that
196:24 - if you had to go through multiple steps
196:26 - you could absolutely do that um we try
196:28 - this a little bit here uh follow prompt
196:30 - you can use the guide use convert
196:32 - prompts are used to link Q&A pairs
196:34 - together texture button for suggested
196:36 - action oh okay so maybe we just do like
196:38 - AWS link to Q&A and then so search an
196:41 - existing Q&A or create a new one um so
196:44 - it say like how many eight of
196:47 - us okay we're typing it
196:49 - in context only this Falls up will not
196:52 - be understood out of the context flow
196:55 - sure because it should be within context
196:58 - right and uh here we can do another one
197:00 - we say like um
197:05 - Azure we'll say how many
197:11 - azure context only
197:14 - oops it uh got away from me
197:22 - there we'll save
197:24 - that and uh what we'll do is save and
197:34 - train so we go back here and we'll say
197:37 - how
197:39 - many uh certifications are there
197:44 - enter so we have to choose AWS and so
197:48 - there we go so we got something that
197:49 - works pretty good there since I'm happy
197:51 - with it we can go ahead and go and
197:52 - publish that so we's say
198:01 - publish and now that it's published we
198:03 - could use Postman or curl to uh trigger
198:06 - it but what I want to do is create a bot
198:08 - because with Azure bot Services then we
198:10 - can actually utilize it um with other
198:11 - IND ations right it's a great way to uh
198:14 - um use your Bot or to actually host your
198:17 - Bot so we'll go over here it'll link it
198:19 - over uh if you don't click it it doesn't
198:20 - preload it in so it's kind of a pain if
198:22 - you lose it you have to go back there
198:23 - and click it again but uh let's just say
198:25 - um
198:26 - certification q and
198:29 - day and we will look through here so all
198:32 - going to go with free premium messages
198:34 - 10K 1K premium message units messages
198:38 - I'm kind of confused by the pricing but
198:39 - F0 usually means free so that's what I'm
198:41 - going to go for that SDK or nodejs I'm
198:43 - going to use NOS not that we're going to
198:44 - do anything there with it go ahead and
198:46 - create
198:48 - that and I don't think this takes too
198:51 - long we'll see
198:58 - here and just go ahead and click on that
199:02 - there I'll just wait here a bit I'll see
199:04 - you back here in a moment all right so
199:06 - after waiting I don't know about 5
199:08 - minutes there it looks like our bot
199:10 - service is deployed we'll go to that
199:12 - resour there uh you can download the bot
199:15 - source code actually never did this uh
199:17 - so I don't know what it looks like so be
199:18 - curious to see this um just to see what
199:21 - the code is I assume that because we Cho
199:23 - chose nodejs it would give us um that as
199:26 - the default there so download your c as
199:28 - you bought creating the source zip not
199:30 - sure how long this
199:32 - takes might be regretting clicking on
199:34 - that but uh what we'll do is we'll go on
199:36 - the left hand side here to channels
199:38 - because I just want to show uh here yeah
199:40 - I don't not didn't
199:43 - download uh we'll try here in a second
199:45 - but um what we'll do is we'll go back po
199:50 - profile uh unspecified bot what are you
199:52 - talking
199:54 - about yeah maybe it needs some
200:03 - time so you know maybe we'll just give
200:06 - the bot a little bit of time here I'm
200:07 - not sure why it's giving us a hard time
200:09 - because this bot is definitely deployed
200:10 - if we go over to our bot right bought
200:13 - Services it is here sometimes there's
200:17 - like latency you know with uh Azure oh
200:21 - there we go okay see it works now fine
200:23 - right and so I want to show you that
200:24 - there's different channels and these are
200:25 - just easy ways to integrate your Bot
200:27 - into different services so whether you
200:29 - wanted to use it with Alexa group me
200:32 - Skype telepon twilio Skype business
200:36 - apparently they don't have that anymore
200:38 - because I got s teams now right uh keik
200:40 - which I don't know people still use that
200:42 - slack we should had Discord telegram
200:44 - Facebook email um that's kind of cool
200:48 - but teams teams is a really good one I
200:50 - use teams uh there's a direct line
200:51 - Channel I don't know what that means and
200:53 - there's web chat which is just having
200:55 - like an ined code so if we go over we
200:57 - can go and test it over here just
200:59 - testing our web chat and so it's the
201:01 - same thing as before but we just say
201:02 - things like uh um how many
201:06 - certifications are
201:10 - there let Azure and get a clear answer
201:14 - back we'll go back up to our overview
201:18 - let's try to see if we can download that
201:19 - code again I was kind of curious uh what
201:21 - that looks
201:26 - like if it will
201:37 - download must be a lot of code
201:40 - eh
201:44 - there we go so now we can hit download
201:46 - and so there is the code I'm going to go
201:47 - ahead and open that up uh so yeah I
201:50 - guess when we chose JavaScript that made
201:52 - a lot more sense let's give it a little
201:54 - peek here I'm just going
201:56 - to uh drop this on my desktop here so
202:00 - let make a new folder here and call this
202:03 - uh bot
202:04 - code okay I know you can't see what I'm
202:06 - doing here but uh let's go here
202:09 - and d double click into here and then
202:12 - just drag that code on
202:18 - in and then what we can do is open this
202:20 - up in VSS code I should have VSS code
202:22 - running somewhere around here just going
202:24 - to go ahead and open that I'm off screen
202:27 - here I'll just show you my screen in a
202:29 - moment say show code
202:32 - oops file open
202:35 - folder bot code
202:38 - okay and uh we'll come all the way back
202:41 - here and so we got a lot of code here
202:42 - never looked at this before but you know
202:44 - I'm a pretty good programmer so it's not
202:46 - too hard for me to
202:48 - understand um so looks like you got API
202:51 - request things like that I guess it
202:53 - would just be like if you needed to
202:54 - integrate into your application then it
202:55 - kind of shows you all the code there I'm
202:58 - just trying to see our dialogue
203:00 - choices nothing super
203:05 - exciting okay you know when I go and
203:08 - make the um was it the AI or the AI 100
203:12 - whatever the data scientist course is
203:14 - I'm sure I'll be a lot more thorough
203:16 - here but I was just curious as to what
203:17 - that looks like now if we wanted to have
203:20 - an easy integration uh we can get an M
203:23 - code for this so if we go back to our
203:24 - channels I
203:27 - believe uh we can go and is it
203:31 - edit ah yes so here we have a code so
203:34 - what I'll do is go back to jupyter Labs
203:36 - I'm just going to go make a new empty um
203:39 - notebook so we'll just go up here and
203:41 - say
203:42 - notebook and this can be for our
203:45 - Q&A doesn't really matter what
203:47 - kernel uh we'll say Q and A maker just
203:52 - to show like if you wanted a very very
203:54 - simple way of integrating your Bot um we
203:57 - would go back over
204:00 - to wherever it is here ah here we are
204:03 - I'm going to go ahead and copy this
204:04 - iframe I think it's percentage
204:07 - percentage HTML so it treats this cell
204:10 - as HTML
204:12 - and I don't have any HTML to render so
204:15 - we will place that in there and notice
204:16 - we have to replace our secret key so I
204:19 - will go back here and I will show my key
204:21 - and we will copy
204:23 - that and we will paste that key in here
204:27 - and then we'll run
204:28 - this and I can type in
204:32 - here where am I just ask silly
204:39 - things uh who are you
204:43 - how many Azure
204:45 - certifications are there well I wonder
204:48 - if I just leave the are there off let's
204:49 - see if it figures it out okay cool
204:52 - so uh yeah I mean that's pretty much it
204:54 - with Q&A
204:55 - maker um so yeah that's great so I think
204:59 - we're done here and we can move on to uh
205:02 - checking out uh leis or Luis learning
205:05 - understanding to make a more uh robust
205:07 - bot
205:10 - okay
205:11 - [Music]
205:13 - all right so we are on to our last
205:15 - cognitive service and this one is going
205:17 - to be uh lwis or Louise depending on how
205:20 - you like to say it it's Luis which is
205:22 - language understanding so you type in
205:24 - luis. a uh and that's going to bring us
205:28 - up to this external website still part
205:30 - of um Azure just has its own domain and
205:33 - so here we'll choose our subscription
205:35 - and we have no author authoring source
205:38 - so I guess we'll have to go ahead and
205:39 - create one ourselves so go down here and
205:42 - we'll choose my cognitive Services asure
205:44 - resource name so my o uh service or my
205:51 - cognitive
205:55 - service create new cognitive service
205:58 - account but we already have one so I
206:00 - don't want to make another one right it
206:02 - should show up here
206:05 - right are valid in the author authoring
206:08 - region so it's possible that we're just
206:10 - in the incorrect region so we might end
206:12 - up creating two of these and that's
206:13 - totally fine I don't care it's as long
206:15 - as we get this working here because
206:17 - we're going to delete everything at the
206:18 - end anyway and so just say my Cog
206:20 - service
206:22 - 2 and uh we'll say West us because I
206:25 - think that maybe we didn't choose one of
206:27 - these regions let's go double check uh
206:30 - if we go back to our
206:32 - portal just the limitations of the
206:34 - service right so we'll go to my Cog
206:37 - Services here um I just want to go uh
206:41 - cognitive
206:43 - services so just want to see where this
206:45 - is deployed and this is in um you West
206:50 - us yes I don't know why it's not shown
206:52 - up there but whatever if that's what it
206:54 - wants we'll give it what it wants
206:58 - okay shouldn't give us that much trouble
207:00 - but hey that's how it
207:03 - goes and so we have an author authoring
207:06 - service I'm going to refresh here and
207:07 - see if it added a second one it didn't
207:10 - so all right
207:12 - that's fine so we'll just say uh my
207:14 - sample
207:16 - bot um we'll use English as our culture
207:19 - if nothing shows up here don't worry you
207:21 - can choose it later on I remember the
207:22 - first time I did this it didn't show up
207:24 - and so now we have my Cog service my
207:26 - custom vision service we want Cog
207:28 - service
207:31 - so um anyway it tells you about schema
207:34 - like how you make a schema animates
207:36 - talking about like bot action intent and
207:39 - example utterance but we're just going
207:40 - to set up something very simple here so
207:42 - we're going to create our attent the one
207:43 - that we always see is uh flight booking
207:47 - so I'll go here and do
207:49 - that and what we want to do is write an
207:52 - undering so like uh book May flight to
207:57 - Toronto okay and so if someone were to
208:00 - type that in then the idea is it would
208:02 - return back the intent this value and
208:04 - metadata around it and we could
208:06 - programmatically provide code right so
208:08 - what we need is identity identities and
208:10 - we can actually just click here and uh
208:12 - make one here so enter named identity
208:15 - we'll just call this
208:16 - location okay here we have an option
208:19 - machine learned and list if you flip
208:21 - between it this is like imagine you have
208:22 - a ticket order and you have these values
208:24 - that can uh change or you just have a
208:27 - value that always stays the same like
208:29 - list so that's our
208:30 - airport that makes sense we'll do
208:34 - that if we go over to ENT entities we
208:36 - can see it
208:39 - here all right so uh nothing super
208:41 - exciting there but what I want to show
208:43 - you is if we go ahead and um we should
208:47 - probably add fight booking should be uh
208:51 - how about book
208:53 - flight flight booking fight booking okay
208:57 - so we'll go ahead and I know there's
208:58 - only one but we'll go ahead and train
208:59 - our
209:04 - model because we don't need to know tons
209:07 - right we cover a lot in the lecture
209:09 - content uh to build a complex spot is
209:11 - more for the uh associate level um but
209:14 - now what we can do is go ahead and test
209:16 - this and we'll say book me a flight to
209:22 - Seattle okay and notice here it says
209:24 - book flight we can go inspect it and we
209:26 - get some additional data so top scoring
209:29 - so it says How likely that was the
209:31 - intent
209:33 - um okay so you get kind of an idea there
209:36 - there's additional things here it
209:38 - doesn't really matter um we'll go back
209:40 - here and we will go ahead and publish
209:42 - our model so we can put it into a
209:45 - production slot you can see we have
209:46 - sentiment analysis speech priming we
209:48 - don't care about either of those
209:49 - things we can go and see where our
209:51 - endpoint is and so now we have uh an
209:55 - endpoint that we can work with um so
209:58 - yeah I mean that's pretty much all you
210:00 - really need to learn about Lewis um but
210:03 - uh I think we're all done for cognitive
210:04 - services so we're going to keep around
210:06 - our our notebook because um we're going
210:09 - to still use our jupyter notebook for
210:10 - some other things things but what I want
210:12 - you to do is make your way over
210:14 - to um your resource groups because if
210:18 - you've been pretty clean it's all within
210:20 - here we'll just take a look here so we
210:21 - have our
210:22 - Q&A all of our stuff here I'm just
210:25 - making sure it's all there and so I'm
210:26 - just going to go ahead and delete this
210:28 - Resource Group and that should wipe away
210:31 - everything okay for the cognitive
210:33 - Services
210:36 - part all right so we're all good here
210:39 - and I'm just going to go off and I'll
210:40 - leave leave this open because it's
210:43 - always a pain to get back to it and
210:44 - reopen it but let's make our way back to
210:45 - the home here in the Azure uh machine
210:48 - Learning Studio and now we can actually
210:50 - explore building up machine learning
210:53 - [Music]
210:57 - pipelines okay so we are on to the ml uh
211:01 - uh follow alongs here so we're going to
211:03 - learn how to build some pipelines so
211:04 - first I think is the easiest would be
211:06 - Auto automated ml or also know as autom
211:08 - ml the idea here is it's going to just
211:11 - um build out the entire pipeline for us
211:13 - so we don't have to do any thinking we
211:14 - just say what kind of model we want to
211:16 - run and have it to make a prediction so
211:19 - what we'll do is a new automated ML and
211:21 - we're going to need a data set so I
211:22 - don't have one but the nicest thing is
211:24 - they have these open data sets so if you
211:26 - click here you'll see there is a bunch
211:29 - here and a lot of these you'll come
211:30 - across quite often not just on Azure but
211:33 - other places like this diabetes one I've
211:35 - seen it like everywhere okay uh and so
211:39 - like if we just go click here maybe we
211:40 - can read a bit more here so diabetes
211:43 - data set 422 samples with 10 features
211:45 - ideal for getting started with machine
211:47 - learning algorithms it's one of the
211:48 - popular pyit learn toy data sets it's
211:51 - probably where I've seen it before
211:53 - though it's not showing up there uh you
211:54 - scroll on down you can see the data uh
211:57 - you notice that it's available AZ your
211:58 - notebooks data bricks and Azure synapse
212:01 - uh the thing is we have these values so
212:03 - age sex BMI BP and the Y is trying to
212:06 - make a prediction it's trying to say
212:08 - what's the likelihood of you having
212:10 - diabetes or not and so it's not a
212:11 - Boolean value so it's not a binary
212:13 - classifier it's kind of on a uh well I
212:15 - guess you would be doing binary classif
212:18 - classification say do you have di
212:20 - diabetes or you can make a prediction to
212:22 - say what's the likelihood or this value
212:24 - if you gave another value in there but
212:27 - um anyway you this is the predicting
212:29 - value a lot of times this is X so
212:31 - everything here is X and this is
212:34 - considered y the actual prediction um so
212:37 - some sometimes it's why and sometimes
212:38 - it's actually named what it is uh but
212:40 - that's just what it is here so we'll
212:42 - close that off and so we'll choose the
212:44 - diabetes set and it will be data set
212:48 - one and so we'll worry about feedback
212:51 - later so we'll click on Sample uh
212:53 - diabetes we'll hit next and here it's
212:55 - going to try to figure out uh what kind
212:57 - of model that we want we have to create
212:59 - a new experiment it's a container to run
213:00 - the model in so we'll just say
213:03 - diabetes uh my diabetes it sounds a bit
213:06 - odd but that's what it is the target
213:07 - column we want to predict um is the
213:11 - train to predict is the Y It's usually
213:13 - the Y um we don't have a compute cluster
213:16 - so I'll go ahead and create a new
213:18 - compute we have dedicator or low
213:20 - priority technically we um it is low
213:24 - priority but I just want this done low
213:26 - priority but don't G to compute nodes
213:29 - your job may be pre- emptied um I'm
213:31 - going to stick with dedicated for the
213:33 - time being we're going to stick with
213:34 - CPU uh if we go with um this it does
213:40 - take about an hour to run so when I ran
213:42 - this took about an hour so if you don't
213:44 - mind it's only going to cost you 15
213:46 - cents but if you want this done a lot
213:47 - sooner I'm going to try to do something
213:49 - a little bit more powerful so I'm just
213:52 - trying to decide here because if it only
213:54 - takes an
213:56 - hour uh I might run it on something more
213:59 - powerful that's 90 cents that might be
214:01 - Overkill because it's not really deep
214:04 - learning uh it's just statistical
214:06 - statistical stuff so try and large data
214:09 - set I wouldn't say it's large real time
214:11 - inference other latency sensitive
214:14 - ones
214:18 - um how
214:23 - about why is this one I'm just looking
214:25 - here because this one's 29 this one's
214:27 - more expensive but it has 32 GB of RAM
214:30 - this one is 28 oh 14 GB of RAM oh it's
214:33 - storage so this one's our highest in the
214:36 - tier again you can choose this one you
214:38 - you just have to wait a a lot longer I
214:40 - just want to see if it finishes a lot
214:41 - faster okay without having to go to the
214:43 - GPU level because I don't think GPU is
214:45 - going to help too much here um the
214:47 - computer name is uh my diabetes
214:54 - machine minimum number nodes uh you want
214:58 - to provision if you want dedicated nodes
215:00 - to set the count here uh
215:02 - maximum I guess I just want one node
215:05 - right uh we will go ahead and oops uh
215:09 - complete name be2 characters
215:14 - long what doesn't is it too long okay
215:17 - there we
215:24 - go we'll give it a moment
215:28 - here yeah it's going to spin up the
215:30 - cluster so it does take a little bit
215:32 - time to start this so I'll see you back
215:33 - here when this is done
215:35 - okay great so after a short little wait
215:37 - there it looks like uh our cluster is
215:39 - running if we double check it here we
215:40 - can go to compute I believe that shows
215:42 - up under here under the compute cluster
215:45 - so there it is notice it's slightly
215:47 - different this one shows you
215:48 - applications and this one is just size
215:50 - and Etc we can click in here see nodes
215:52 - and run times we'll go make our way back
215:54 - here uh and we'll go ahead and hit next
215:57 - and notice that I think it actually will
215:59 - select what it generally because it'll
216:01 - look at your prediction value maybe
216:02 - sample a bit of it and say okay you
216:04 - probably want a regression thing so to
216:05 - predict a continuous numeric values so
216:08 - the thing is that if it was a label like
216:10 - text or if it was just zero and one it
216:12 - probably would choose classification
216:14 - because it's um you saw our our y value
216:17 - was like a number that was all over the
216:18 - place it thinks it's regression so I
216:21 - think that's a good indicator uh uh
216:23 - there so let's go with
216:27 - regression you know but you might want
216:29 - it as a binary classifier but uh yeah
216:31 - it's another story there so it's uh as
216:34 - soon as we created it just started it
216:35 - didn't give us the option to say hey I
216:37 - want to start running it notice on this
216:39 - here it's going to do featurization so
216:41 - that means it's automatically going to
216:42 - select out features for us which is what
216:43 - we wanted to do it set up to do
216:45 - regression uh we have some configuration
216:47 - here so training time is 3 hours doesn't
216:50 - mean it's going to train for three hours
216:51 - but that's I guess it's timeout for it
216:54 - um you could set a metric uh score
216:56 - threshold so it has to meet at least
216:58 - this to be successful if it's not going
217:00 - to do it it probably would quit out
217:01 - early cross number Val or cross
217:03 - validations just make sure the data is
217:05 - good you can see blocked algorithm so
217:06 - tensor flow DNN tensor flow L regression
217:09 - if it was using NN so deep learning
217:11 - neural network I probably would have
217:13 - chose the GPU to see if it would go
217:15 - faster um look at the primary metric
217:17 - it's normalized root square uh root mean
217:20 - Square AED sometimes on the exam they'll
217:22 - actually ask you like what's the prim
217:23 - metric for this thing so it's good to uh
217:26 - take a look and see what they actually
217:28 - use for that I'll probably be sure to um
217:30 - highlight that stuff in the actual
217:32 - lecture content um but this will take
217:34 - some time to run uh we have data guard
217:37 - rails it will actually not populate I
217:39 - guess until We've ran it so so we'll
217:41 - just let it run and I'll see you back
217:42 - here when it's done okay all right so
217:44 - after a very very very long wait our
217:46 - automl job is done it took 60 minutes so
217:49 - using a larger instance didn't save me
217:51 - any time I don't know if maybe if I ran
217:53 - a GPU instance it would be a lot faster
217:56 - I'd be very curious to try that out but
217:57 - not something for uh uh this
218:00 - certification course so we go into here
218:02 - and yeah the cheaper instance was the
218:04 - same amount of time so it probably just
218:05 - needs gpus it really depends on the type
218:07 - of models it's running so we have a
218:09 - bunch of different algorithms in here it
218:10 - ran uh about 42 different models I
218:14 - thought last time I ran it I saw a lot
218:16 - more but you can see there's all kinds
218:18 - of models that it's running and then
218:20 - it's going to choose the top candidate
218:21 - so it chose voting Ensemble so Ensemble
218:25 - is um uh we don't cover really in the
218:27 - course because it's gets too much into
218:29 - ml but Ensemble is when you actually use
218:31 - two different weaker models and combine
218:34 - the results in order to make a more uh
218:37 - uh powerful uh ml model okay um so here
218:41 - we'll get some explanation I tried this
218:43 - before and I didn't get really good
218:45 - information so if we go
218:48 - here uh so like I don't have anything
218:50 - under model performance so this tab
218:52 - requires array of predicted values from
218:54 - the model to be supplied we didn't
218:56 - Supply any so we don't get any data
218:58 - Explorer so select a cohort of the data
219:01 - that all the data is is we have here um
219:04 - so like here we were seeing
219:06 - age and I guess it's just giving us an
219:08 - indicator about the age information um
219:12 - use the slider to show descending
219:14 - feature important select up to three
219:16 - cohorts to see the feature important SL
219:18 - by
219:19 - side
219:22 - okay so I guess S5 and BM I don't know
219:26 - what S5 is we'd have to look up the data
219:28 - set be BMI is your body mass index so
219:30 - that's a clear indicator as to what
219:32 - affects whether you have diabetes or not
219:34 - so that makes sense age doesn't seem to
219:36 - be a huge factor which is kind of
219:39 - interesting individual feature
219:41 - importance we can go here and just kind
219:42 - of like narrow in and say okay well why
219:44 - is this outlier over here and they're
219:45 - like age 79 right so that's kind of
219:49 - interesting to see that information so
219:51 - it does give you some uh explanation as
219:54 - to to you know why things are why they
219:56 - are um over here we have a little bit
219:59 - more different data this is kind of
220:01 - interesting model
220:02 - performance uh I don't know what I'm
220:04 - looking at but like here it's over mean
220:06 - squared so it's that uh mean squared
220:08 - calculation there again
220:18 - okay so yeah it's something right uh but
220:21 - anyway the point is is that uh that we
220:23 - finally get metric so I guess we always
220:25 - had to click there because that makes
220:27 - more sense um so yeah there's more
220:30 - values here sure data
220:33 - transformation uh illustrates the data
220:36 - processing feature engine scaling
220:37 - techniques and machine learning
220:38 - algorithm automl so you know if you were
220:40 - a real data scientist all this stuff
220:42 - would make sense to you um I think just
220:45 - with time it'll it'll make sense but
220:46 - even at this point I I'm not sure and I
220:49 - don't care about the model right if
220:50 - you're building something for real I'm
220:51 - sure uh the information becomes a lot
220:54 - more valuable so this model is done uh
220:58 - and the idea is that we can deploy oops
221:00 - if we go back to the
221:02 - actual uh
221:04 - models oh because we actually went into
221:06 - them e so we go back to the um autom ml
221:11 - here I think you can deploy any model
221:14 - that you like so I think you go here and
221:16 - deploy this like if you prefer a
221:18 - different model you could deploy it um
221:20 - if we go into Data guard rails we kind
221:22 - of skipped over that this is a way it
221:24 - does automatic featurization so it's
221:26 - extracting up the feature so it how it
221:28 - handles the splitting how it handles
221:31 - missing features high card anality is
221:34 - like if you have too much data it might
221:37 - have to do dimensionality reduction so
221:40 - that's just saying like hey if this is a
221:42 - problem maybe we would do some
221:44 - pre-processing or stuff to make it
221:46 - easier to work with the data so if we're
221:48 - happy with this we can go ahead and
221:49 - deploy it so let's say um
221:53 - deploy just say infer my
221:58 - diabetes here we have AKs and E
222:02 - uh um Azure container instance let's do
222:05 - Azure kubernetes uh kubernetes service
222:08 - cuz we did the other one here um say uh
222:12 - diabetes prodad
222:15 - maybe um AKs
222:21 - diabetes oh compute name sorry
222:25 - um one of the inference ones okay so in
222:29 - order to uh deploy this we would have to
222:31 - create our pipeline I'm not sure if I
222:34 - have enough in my quota here but let's
222:35 - go give it a go so I think what it's
222:37 - wanting is one of these here
222:41 - uh I I think we'd want this wherever we
222:44 - are right I'm not
222:48 - sure where we are If This Is Us East or
222:52 - uh West here let's go
222:55 - check
222:57 - studio
222:59 - um Azure machine
223:04 - learning East
223:07 - usest no I never did this when I was um
223:10 - I just use usually Azure container
223:12 - instance but I'm just curious
223:15 - here say
223:17 - next my uh
223:22 - diabetes
223:24 - prod we
223:27 - will we need to choose some
223:31 - nodes uh the number of nodes multiply by
223:34 - the virtual machine's number of cors
223:35 - must be greater or equal to 12
223:38 - okay no again if you're not confident
223:41 - like if you're concerned about cost you
223:42 - can just again watch you don't have to
223:44 - do right um this is again a uh
223:48 - fundamental certification it's not super
223:50 - important to get all the hands-on
223:52 - experience
223:53 - yourself um but I'm just trying to
223:54 - explore this so we can see right because
223:57 - I I don't care about costs it's not a
223:58 - big deal to me on my machine here uh so
224:01 - probably I don't
224:05 - have Sy pool must use a VM SKU with more
224:08 - than two cores and four gigabytes well
224:10 - what did I
224:11 - choose did I not choose the right
224:16 - one uh we'll try this
224:21 - again oh I chose
224:24 - three yeah that's
224:26 - fair
224:33 - um uh what did it want 12 cores said
224:36 - before I
224:38 - think
224:41 - invalid parameters more
224:46 - details because it already exists based
224:48 - on that name a
224:50 - to it's given us all this trouble a this
224:54 - one we'll go ahead and delete you think
224:56 - like it wouldn't matter like I wouldn't
224:57 - have to delete it out but that's
225:01 - fine this one failed now what's the
225:04 - problem quota exceeded so I can't do it
225:07 - because I don't I'd have to go make a
225:08 - support request in reset so it's not a
225:11 - real big deal um I guess what we could
225:13 - do is instead of doing it on AKs we
225:16 - could just deploy to container instance
225:18 - if it'll let us um notice I don't have
225:20 - to fill anything additional in it'll
225:22 - just deploy I
225:26 - think great uh and so I guess we'll let
225:30 - that deploy and I'll see you back here
225:32 - in a bit okay all right so I'm back here
225:35 - checking on out on my or checking up on
225:37 - my automl here so we go over to Compu
225:39 - cute we go to inference clusters we
225:42 - don't have anything under there if we go
225:44 - uh over to our
225:46 - experiments under our diabetes
225:50 - here because we did choose to deploy the
225:56 - model right we clicked
226:03 - deploy so it should have created an ACI
226:05 - instance let's make our way over to the
226:07 - portal the reason why it might not be sh
226:09 - up is because I'm just running out of
226:11 - compute because again it's a quota thing
226:15 - um it's not a big deal for us to get a
226:17 - deploy it's not like we're going to do
226:18 - anything with it but uh yeah so we can
226:19 - see that we have a container over here
226:22 - and it's
226:23 - running so we must be able to uh see if
226:26 - we go to endpoints here ah here it is
226:29 - right I was under models that's my
226:31 - problem uh so pipeline endpoints that
226:33 - would be something I I think that if we
226:35 - had deployed our designer I thought we
226:36 - would have saw it under there but here
226:38 - we have our binary pipeline or our
226:40 - diabetes prod pipeline so if we wanted
226:42 - to like test data you know we could pass
226:45 - stuff in here um I think if we wanted to
226:47 - kind of just like see this in action I'm
226:50 - not sure if it's going to work but we'll
226:51 - give it a go so if we go into our sample
226:53 - diabetes data set and we just explore
226:56 - some of the data we should be able to
226:58 - kind of Select out some values because I
226:59 - I don't know what these values mean so
227:01 - let's just say like
227:03 - 36 oops 36 but we already know that BMI
227:07 - is the major factor here uh sex is
227:09 - either one or two so we'll say two BMI
227:13 - will say
227:14 - 25.3 the BP will be
227:18 - 83 or whatever oops 83
227:24 - here S1
227:31 - 160 S2 can be
227:38 - 99.6 uh three 45 45 and
227:45 - five
227:46 - [Music]
227:48 - 5.1 oh we only we're running out of
227:50 - metrics here uh
227:53 - 82 wonder why it doesn't give us all of
227:56 - them oh I guess it does it's up to
227:58 - six okay so let's go ahead and test that
228:01 - see what we get and we got a result back
228:02 - 168 so uh that is uh autom ml all
228:06 - complete there for
228:07 - you um yeah so there you
228:10 - [Music]
228:14 - go all right so let's take a look here
228:16 - at the uh visual designer because it's a
228:18 - great way to get started very
228:21 - easily uh with uh if you don't know what
228:23 - you're doing and you want something a
228:25 - little bit more advanced than automl and
228:26 - have some customization it's great to
228:28 - start with one of these samples let's go
228:29 - ahead and expand it and see what we have
228:31 - here we have binary classification with
228:33 - custom python script uh TB parameters
228:35 - for binary
228:37 - classification uh multiclass multi class
228:39 - classification so letter recognition
228:42 - text classification all sorts of things
228:44 - usually binary classification
228:45 - classification is pretty easy I'm
228:47 - looking for one that is pretty darn
228:49 - simple uh let's go take a look here so
228:51 - this says this sample shows how to
228:52 - filter base features selection to
228:54 - selection
228:56 - features um binary classification so how
228:59 - to predictors related to customer
229:01 - relationships using binary classes how
229:02 - to handle imbalance data sets using smot
229:05 - and modules I'm not really worried about
229:07 - balancing uh customized python script to
229:09 - perform cost sensitive binary
229:11 - classification tune parameters so you
229:14 - tune model parameters best models during
229:17 - the training process let's go with this
229:18 - one this one seems okay to me um and so
229:22 - what you can see here is that it's using
229:23 - a sample data set I believe I think this
229:25 - is a sample and if you wanted to see all
229:28 - of them you could literally drag them
229:30 - out here and do things with them uh I
229:32 - haven't actually uh built one uh end to
229:35 - end yet for uh for this again I don't
229:37 - think it's like super important for uh
229:39 - this level exam but uh this just shows
229:41 - you that there's a pre-built one if
229:43 - you've start to get the handle of ml you
229:45 - know the full pipeline this isn't too
229:47 - confusing so at the beginning here we
229:49 - have our classification data and then
229:52 - what it's going to do is say select
229:53 - columns in the data set so it says
229:56 - exclude column names work class
229:58 - occupation native country so it's doing
230:00 - some pre-processing excluding that data
230:03 - might be interesting to go look at that
230:04 - data set so if we go over to our data
230:06 - sets tab it should show up here I
230:10 - believe maybe because we haven't um uh
230:14 - uh committed or submitted this we we
230:16 - can't see that data set yet but we'll
230:17 - look at it for a moment then we want to
230:19 - clean our data so here it's saying clean
230:21 - all the columns so uh custom
230:24 - substitution
230:26 - value see if we can see what it's
230:28 - substituting
230:32 - out uh it's not saying what so clean
230:36 - missing
230:38 - data so I'm not sure what it's cleaning
230:40 - out there
230:44 - but because I would suggest that it's
230:46 - using some kind of custom script um I'm
230:48 - not sure where it is but that's okay we
230:50 - have split data pretty common to split
230:52 - your data so you would have a training
230:54 - and test data set uh it's usually really
230:56 - good to randomize it so you want to
230:58 - randomize it then split it um and that's
231:01 - that's just so you get better results
231:03 - then it has model hyperparameter tuning
231:06 - so the idea is that it's going to use ml
231:08 - to figure out the uh the best um
231:11 - parameters for tuning over here we have
231:13 - the two class decision tree where it's
231:15 - going to do some work there it's going
231:17 - to score our model and then it's going
231:18 - to evaluate our model and see if it's
231:20 - successful so this is all set up to go
231:22 - so all we got to do is go to the top
231:24 - here there's a setting wheel here and we
231:25 - need to choose some type of compute so
231:27 - I'm going to go here and we have this
231:30 - one here but I'm going to go create it's
231:31 - for my um my diabetes one but I'm going
231:34 - to go ahead and make a new one and we're
231:36 - going to say
231:38 - um uh uh we recommend using a predefined
231:40 - configuration to quickly set up compute
231:42 - training this
231:45 - one looks okay I don't know if it needs
231:48 - two nodes but uh I guess we can do this
231:51 - one so we'll just say binary we'll just
231:54 - say binary
231:56 - pipeline
231:58 - okay say save hopefully it's making a
232:02 - good suggestion and we'll have to wait
232:03 - for that to spin up it's going to take a
232:05 - little bit of time okay so I'll see you
232:06 - back here in a moment all right so I got
232:09 - message saying that that is ready so
232:11 - what we can do I think it was here my
232:13 - notebook instance no that's not it but I
232:15 - I definitely saw a popup on my screen uh
232:18 - uh you might have saw it too you'd have
232:19 - to be paying close attention for that
232:20 - but if you go over um it says that it's
232:24 - it's ready to go so what I'm going to do
232:25 - is make my way back over here we're
232:27 - going to select our compute there is our
232:30 - binary pipeline I'm going to select that
232:32 - and there are some other options we're
232:34 - not going to fill around with that we're
232:35 - going to go ahead and hit submit so we
232:37 - need a new experiment so I'm going to
232:39 - just say um binary
232:41 - pipeline we'll hit
232:49 - submit okay and so this is now running
232:51 - so after a little while here we're going
232:53 - to start seeing these go green so this
232:55 - is not started we'll give it a moment
232:57 - here just so we can see some kind of
232:59 - animation and there it goes it's Off to
233:01 - the Races there's not much to do here
233:03 - this is going to take a while I don't
233:05 - know I've have never ran this one in
233:06 - particular so I don't know if it's an
233:08 - hour or 30 minutes so I'll see you back
233:10 - when it's done running U but yeah it's
233:13 - it's not that fun to watch but it's cool
233:15 - that you get a visual uh illustration a
233:17 - so I'll see you back in a bit I just
233:19 - wanted to peek in here and take a look
233:21 - at how it's progressing here and you can
233:22 - see it's still going and it's just uh
233:24 - cleaning the data it's still not done um
233:27 - I'm not sure how long this has been
233:28 - running for if we go over to our
233:29 - experiments and we go into our I think
233:31 - it's binary Pipeline and we look at the
233:33 - run time we're about 8 minutes in and it
233:36 - hasn't done a whole lot so it's still
233:39 - cleaning the data I would have thought
233:40 - it be a little bit faster I'm kind of
233:42 - used to using like AWS and it goes um
233:44 - Sage makers uh this doesn't usually take
233:47 - this long um but I mean it's nice that
233:49 - it's it's going here but uh yeah so
233:51 - we're almost out of the pre-processing
233:53 - phase we'll be on to the uh the model
233:57 - tuning
233:58 - okay all right so after waiting a little
234:00 - while looks like our pipeline is done uh
234:03 - so if we make our way over to
234:04 - experiments and go to Binary pipeline we
234:06 - can see that it took 14 minutes and 22
234:08 - seconds
234:10 - uh we can go here and just see some uh
234:12 - additional information there's nothing
234:14 - really else to see we saw all the steps
234:15 - already ran so you can see them all here
234:18 - uh okay and so let's say we wanted to
234:21 - there's nothing under metrics but um
234:24 - enable metrics log data points compare
234:26 - these did within across runs we only did
234:28 - a single run so there's nothing to
234:29 - compare so let's say we we're happy with
234:32 - this and we want to deploy this model
234:33 - well what I'm going to do is go back to
234:35 - the designer uh click back here and so
234:38 - now in the top right corner we can
234:40 - create our inference pipeline so um I
234:44 - can't remember if submits going to run
234:46 - it I don't want to run it again um I
234:49 - just want to go ahead and create
234:50 - ourselves a realtime or batch pipeline
234:53 - we's say real time pipeline
234:55 - here and what this will do is it will
234:57 - actually create a completely different
234:58 - pipeline so here's a completely new one
235:01 - uh but it's specifically designed to do
235:04 - uh deployment okay so this is now one
235:07 - was for training the model this one is
235:08 - actually for uh uh taking in data and
235:11 - doing inference okay so what we can do
235:15 - is we can go ahead and uh just submit
235:18 - this and so we'll put this under our
235:21 - binary pipeline here we'll go ahead and
235:23 - hit
235:25 - submit and I believe that we need a
235:28 - different kind of compute here I'm
235:29 - surprised that it's even
235:30 - running um no I guess it has a compute
235:33 - there so it's going to run and once it
235:36 - uh finishes running then I believe that
235:37 - we we can go ahead head and um uh uh
235:41 - deploy it okay so let's just wait for
235:43 - that to finish all right all right so
235:45 - after a little while there We've ran our
235:47 - inference Pipeline and so uh it's
235:50 - definitely something that is ready for
235:52 - use the idea is that when we actually
235:54 - it's going to go through this web
235:55 - service input to this web service output
235:58 - but uh not so important at this level uh
236:00 - of certification let's see what it looks
236:02 - like to to go ahead and deploy it so we
236:05 - have we have the option between a
236:06 - real-time endpoint and an existing
236:08 - endpoint
236:09 - uh we don't have an endpoint yet so
236:11 - we'll just say uh binary
236:14 - pipeline okay and notice we have the
236:16 - option between oh it just it wants to
236:19 - lowercase binary
236:21 - Pipeline and we have the option between
236:23 - Azure kubernetes service and add your
236:25 - container instance um it's a lot easier
236:28 - to deploy I think to container instance
236:30 - so because and we'll be waiting forever
236:31 - for kubernetes to start up so we're
236:33 - going to do container instance uh we
236:35 - have some options like SSL and things
236:36 - like that not too worried about it so so
236:38 - we're just going to go ahead and hit
236:41 - deploy
236:43 - okay and so that is going to go ahead
236:46 - and deploy that um so we'll wait for
236:49 - this real time inference if we go over
236:51 - to our
236:53 - compute uh it should spin up so this is
236:56 - for a uh AKs so I don't know if it will
236:59 - show up here I think only I've seen
237:01 - things under here but I think this will
237:02 - be for Azure kubernetes service and I
237:05 - don't think we're going to see it show
237:07 - up under there uh however um we do not
237:10 - need to be running this anymore so we'll
237:12 - go ahead and delete the binary pipeline
237:15 - because we're not uh we don't have it
237:17 - for any use right now and we might need
237:20 - to free it up for something else okay so
237:24 - go ahead and delete it we don't need
237:26 - it and uh coming back to our
237:30 - pipeline our designer here I'm just
237:33 - trying to see where we can keep track of
237:34 - it
237:37 - um well I know it it's deploying SO
237:40 - waiting for Real Time endpoint so I'll
237:42 - see you back here when this is done okay
237:44 - just takes a little bit of time all
237:45 - right so I think our pipeline is done if
237:47 - we make our way over to endpoints there
237:49 - it is the binary pipeline if we wanted
237:51 - to go ahead there we could test the
237:53 - data um and so it actually already has
237:56 - some pre-loaded data for us if we hit
237:59 - test it's nice that it fills it in
238:02 - E uh we get some results back okay so I
238:06 - mean and then we see like scored label
238:08 - and income and score probability so
238:11 - things like that uh that is um useful so
238:14 - it's giving back all all the results but
238:15 - I don't think it has yeah it doesn't
238:18 - have scored labels and scored
238:19 - probabilities which is the value we want
238:21 - come to come back here so there are end
238:23 - points and that is the end of um our
238:27 - Exploration with designer
238:29 - [Music]
238:33 - okay all right so let's take a look at
238:35 - what it would be to actually train a job
238:37 - programmatically uh through the notebook
238:39 - so remember we saw these samples over
238:41 - here and so we saw this image
238:42 - classification mnist and this is a very
238:44 - popular data set for doing uh computer
238:47 - vision and these are really great if you
238:49 - want to really learn you should really
238:50 - go through these and just um uh uh read
238:54 - through them because they're they're
238:54 - probably very very useful uh I've done a
238:57 - lot of this before so for me it's it's
238:58 - just it's not too hard to figure out but
239:00 - I've actually never ran this one so
239:01 - let's run it together again we want to
239:03 - be in uh Jupiter lab so you can go here
239:06 - and click it there or go to the compute
239:08 - if it's being a bit finicky and just
239:10 - here we'll get a tab open here and we'll
239:13 - see how this goes so what I want to do
239:16 - and uh is just make sure we're back here
239:18 - I'm going to click into this
239:20 - one and uh we have a few so there's part
239:23 - one and then we have the deploy stage so
239:27 - let's look at training I don't know if
239:29 - we really need to deploy but we'll give
239:31 - it a read here so in this tutorial you
239:32 - train ml model on a computer resource
239:35 - resources you'll be training and uh
239:37 - training and deployment workflow via the
239:39 - Azure machine learning service in a
239:41 - notebook there's two parts to this this
239:43 - is using the mnus data set and scikit
239:46 - learn and with Azure machine learning
239:48 - proba the SDK it's a popular data set
239:50 - with 70,000 grayscale images each image
239:53 - is handwritten digits of 28 times by 28
239:55 - times pixels representing numbers from 0
239:57 - to 9 the goal is to create multiclass
240:00 - classifier to define the digits in a
240:02 - given image that represents so we're
240:04 - going to learn a few things here but
240:05 - let's just jump into it uh so the first
240:08 - thing is that we need to import our
240:09 - packages so here uh it does that map PL
240:13 - plot lib inlines just make sure that
240:15 - when we print things that we visually
240:16 - see them we're going to need numpy and
240:18 - then mat plod lib itself the Azure ml
240:21 - core uh and then we're going to import a
240:23 - workspace since we'll need one there and
240:25 - uh then I guess it just checks the
240:27 - version making sure if we have the right
240:28 - version here okay so this is 1.28 z it's
240:31 - pretty common even this an AWS they'll
240:33 - have like a script in here to update it
240:35 - in case it is out of date I'm surprised
240:37 - it didn't include it in here but that's
240:39 - okay we'll scroll on down and by the way
240:41 - we're using python 3.6 Azure ml uh if
240:44 - this is the future you know they might
240:46 - retire the old one you're using 3.8 but
240:48 - you know it should generally work if
240:49 - it's in their sample data set I assume
240:51 - they try to maintain that okay so
240:53 - connect to a workspace so create a
240:54 - workspace object from an existing
240:56 - workspace uh reads the file config.js so
241:00 - what we'll do is go run that I assume
241:01 - it's kind of like a session and so here
241:04 - it says it's fig found our our
241:06 - workplace so really it's just it's not
241:09 - creating a workspace it's just returning
241:11 - the existing one so that we have it as a
241:13 - variable here create an experiment so uh
241:16 - that's pretty clear we saw experiments
241:17 - in the automl and the designer uh so
241:19 - we'll just hit run
241:22 - there okay so we named it cor ML and we
241:26 - said experiment I wonder if it actually
241:28 - created one yet let's go over to
241:30 - experiment and see if it's there so it
241:32 - is there cool that was fast I thought it
241:34 - would like print something out but it
241:35 - didn't do anything there uh so creator
241:38 - attach an existing compute resource by
241:40 - using Azure machine compute a manage
241:42 - service data scientist etc etc yada yada
241:45 - yada so create a a compute uh uh
241:48 - creation of a compute takes about five
241:50 - minutes so let's see what it's trying to
241:53 - create so we have some environment
241:54 - variables that it wants to load in I'm
241:56 - not sure how these are getting in
241:59 - here um I'm not sure where environment
242:01 - variables are set in
242:03 - um uh Jupiter or even how they get
242:05 - feeded in but apparently they're
242:07 - somewhere but we have it doesn't matter
242:09 - because these are defaulting so here it
242:10 - says CPU
242:12 - cluster uh zero and four it's going to
242:14 - use a standard D2 V2 that is the
242:16 - cheapest one that we can run um I kind
242:19 - of want something a little bit more
242:20 - powerful just for myself uh just cuz I
242:23 - want this to be done a lot sooner but
242:24 - again you know if you're don't have a
242:26 - lot of money just stick with what's
242:27 - there okay so and this is CPU cluster so
242:32 - if we go here I just want to see what
242:34 - her options
242:35 - are
242:37 - um
242:40 - not sure why it's not showing us options
242:46 - here you don't have enough quota for the
242:48 - following VM sizes so it probably it's
242:51 - because I'm running more than one VM
242:52 - right
242:55 - now yes I've s I've hit my
242:58 - quota okay so like I probably have to
243:00 - request for more um so I think this is
243:03 - the
243:05 - one I'm
243:07 - using
243:09 - what's the difference here this standard
243:10 - dv2
243:14 - vcpus it's the same one right so request
243:17 - quote to increase I don't know if this
243:19 - is instant or not I'd have to make a
243:20 - support ticket oh that's going to take
243:22 - too long so the thing is is that uh
243:25 - because the reason is is that I'm
243:26 - running the autom ML and the design and
243:29 - the uh designer in the background here
243:30 - trying to create all the workshops or
243:33 - the uh uh the follow alongs at the same
243:35 - time but what I'll do is I'll just come
243:36 - back and when I'm not running one of
243:38 - those other ones then I will uh I'll
243:40 - come back here and continue on but uh
243:42 - we're just here at the step we want to
243:44 - create a a new uh compute okay all right
243:47 - so I'm back and I freed up uh one of my
243:49 - compute instances if I go over here now
243:51 - I just have uh the one uh cluster
243:54 - instance for my uh automl but what we'll
243:57 - do here is again just read through this
243:58 - so this will create a CPU cluster 0 to
244:00 - four nodes um standard D2 V2 I guess
244:03 - we'll just stick with what what is here
244:06 - um just reading through here look look
244:08 - like it tries to find the compute Target
244:10 - it's going to provision it it will
244:12 - create the cluster call Pool for minimum
244:14 - numbers of nodes for specific time so
244:16 - wait for completion so we'll go ahead
244:18 - and hit play and so that's going to go
244:22 - and create us a new cluster so we're
244:24 - just going to have to wait a little
244:25 - while here for it to create about 5
244:27 - minutes and I'll see you back here in a
244:28 - moment all right so uh the cluster
244:31 - started up if we go back over here we
244:32 - can see that it's confirmed I don't know
244:34 - why it uh was so quick but uh it went
244:36 - pretty quick there so we're on the next
244:38 - section here explore the data so
244:39 - download the mnist data set display some
244:41 - sample images so it's just talking about
244:44 - it being the open data set the code
244:46 - retrieves in the file data set object
244:48 - which is a subass of data set file data
244:50 - set references a single or multiple
244:52 - files of any format in your data store
244:54 - the class provides you with the ability
244:56 - to download or amount files to your
244:57 - computer by creating a reference to the
244:59 - data source location Additionally you
245:01 - register the data set to your workspace
245:03 - for easy retrieval during training
245:06 - there's a bit more how-tos but we'll
245:07 - give it good read here so we have the
245:08 - open data set mnist it's kind of nice
245:11 - that they have that reference there uh
245:13 - so we have a data folder we make the
245:15 - directory we are getting the data set we
245:18 - download it and then we are registering
245:22 - it so let's go ahead and run that not
245:24 - sure how fast that is shouldn't take too
245:26 - long as it's running we'll go over here
245:29 - the left hand side refresh and we'll see
245:31 - if it
245:33 - appears
245:35 - um uh not as of yet there it
245:39 - is go into here maybe explore the data
245:42 - I'm not sure how would look like because
245:44 - these are all images right yeah so
245:46 - they're in ubite gz so they're in
245:49 - compressed files we're not going to be
245:50 - able to see within them but they're
245:52 - definitely there we know they're there
245:54 - so that that is now registered into our
245:56 - our data set uh display some sample
245:58 - images so load the compressed into a
246:01 - files into numpy then use map plot lib
246:05 - plot 30 random images from the data set
246:07 - from above not the step requires load
246:08 - data function it's included in the utils
246:10 - pie this file is included in the sample
246:12 - folder we have it over here we just
246:15 - double click very simple file the load
246:18 - data and we'll go ahead and run
246:21 - that and it's
246:23 - pretty pretty simple here uh so load
246:26 - data X train X test it are we setting up
246:29 - our training and testing data here it
246:31 - kind of looks like it because it says
246:33 - train and test data that's when we
246:34 - usually see that kind of
246:36 - split um and again it's doing a random
246:38 - split so that sounds pretty good to me
246:41 - uh let's show some randomly chosen
246:42 - images yeah so I guess they do set up
246:45 - the training data here and then down
246:47 - below we're actually showing the images
246:49 - so here's some random images train on a
246:51 - remote cluster so for this task you
246:53 - submit the job to run on the remote
246:54 - training cluster to set up earlier
246:56 - submit your
246:57 - job um create the directory create a
247:00 - training script create a script for run
247:02 - configuration submit the job so first we
247:04 - will create our
247:06 - directory
247:08 - um and notice it created this directory
247:10 - over
247:11 - here because I guess it's going to put
247:13 - the training file in there and so this
247:14 - will actually write to a training file
247:16 - this makes uh quite a bit of sense so if
247:19 - we click into here it should now have a
247:21 - training file it'll just give it a quick
247:23 - read see what's going on here so a lot
247:25 - of times when you create these training
247:27 - files you have to do and this is the
247:28 - same if you're using AWS like when
247:30 - you're creating tra like or sagemaker um
247:33 - you create a train file because it's
247:34 - part of Frameworks it's just how the
247:35 - Frameworks work but you'll have uh these
247:37 - arguments uh so it could be like
247:40 - parameters to run for training um uh and
247:44 - there could be a whole sorts of ones
247:47 - here here they're loading in the
247:49 - training and testing data so it's the
247:51 - same stuff we saw earlier when we were
247:53 - just viewing the
247:56 - data um here it's doing a logistic
248:00 - regression it's using Li uh so linear
248:03 - maybe linear learning model there it's
248:05 - doing
248:06 - multiclass on that there and so what
248:08 - it's going to do is fit so fit is
248:10 - actually performing the training and
248:13 - then what it's going to do is make a
248:14 - prediction on the test Set uh then it's
248:17 - going we're going to get accuracy so
248:19 - we're getting kind of a score so notice
248:20 - that it's using accuracy uh as a
248:24 - evaluation metric I suppose right and
248:28 - then at the end we're going to dump the
248:29 - data a lot of times like you have to
248:31 - save the model somewhere so they're
248:33 - outputting the actual weights of the
248:35 - neural network and all other stuff it's
248:36 - a plk file I don't know what that is but
248:39 - if you're using like tensor flow you
248:40 - would use tensor flow serving at the end
248:42 - of this a lot of times uh Frameworks
248:44 - will like Pi P torch or tensor flow or
248:47 - mxnet they'll have a serving layer um
248:50 - but uh since we're just using S kit
248:52 - learn which is very simple it's just
248:53 - going to dump out uh that file into our
248:56 - outputs this is going to probably run a
248:58 - container so this outputs isn't going to
249:00 - necessarily be on um the outputs into
249:03 - here it's more like the outputs of the
249:05 - container and um
249:08 - a lot of times the container will then
249:10 - place this somewhere so like it'll be
249:11 - saved on The Container but it'll be
249:13 - passed out to the register or or
249:15 - something like that like model registry
249:17 - so anyway we ran this and so that
249:18 - generated the file we don't want to keep
249:20 - on running this multiple times I
249:21 - probably would just overwrite the file
249:22 - so it's not a big deal here it says
249:25 - notice how the script gets saved in the
249:26 - data model so here it's saying the data
249:28 - uh data folder I guess we didn't look at
249:30 - that so we go top here um I didn't see
249:35 - this is data
249:36 - folder was it wasn't really paying
249:38 - attention to where that
249:39 - was guess it looks like where more so
249:42 - it's loading the data in so here it
249:44 - saves the data outut anything written to
249:46 - the strory is automatically uploaded to
249:47 - your workspace so I guess that's just
249:49 - how it works so it probably will end up
249:51 - in here then um so util pii reference
249:54 - the training script to load the data set
249:56 - correctly and copy the file over
249:58 - so um we will run this to copy the file
250:04 - over so I'm guessing did it put it into
250:06 - here I'm just yeah so just put it in
250:08 - there because when it actually uh
250:10 - packages it for the container it's going
250:12 - to bring that file over because it's a
250:14 - dependency
250:16 - so configure the training job so create
250:19 - a script run config the directory that
250:22 - contains the script the compute Target
250:23 - the training script training file Etc
250:26 - sometimes like in other Frameworks
250:27 - they'll just call them estimators but
250:29 - here it's just called a script run
250:31 - config
250:33 - so uh I'm just trying to see what it's
250:36 - doing so sidekit learn is the dependency
250:40 - okay sure we'll just hit
250:42 - run okay and then down below here we
250:46 - have script run
250:48 - config so it looks like we're passing
250:51 - our arguments so we're saying this is
250:53 - our data folder which is apparently here
250:56 - we're mounting it and then we're setting
250:58 - regularization to
251:00 - 0.5 sometimes you'll pass inde
251:02 - dependencies in here as well I guess
251:04 - these are technically are our parameters
251:06 - that are getting configured up here at
251:08 - the top right but sometimes you'll have
251:11 - dependencies if you're in uh including
251:14 - other files here uh and I guess that's
251:17 - up here right so see where it says
251:19 - environment and then we're saying
251:21 - include the Azure ml defaults and the
251:23 - pyit learn and stuff like that and so
251:26 - then it gets passed in the EnV so that
251:28 - makes sense to me we haven't ran that
251:29 - yet because we don't see any number here
251:32 - submit the job to the Clusters let's go
251:34 - ahead and do
251:36 - that
251:38 - so it says it returns a preparing or
251:39 - running State as soon as the job is
251:41 - completed so it's in a starting
251:45 - State monitor remote run so in total the
251:49 - the first run takes 10 minutes but the
251:51 - second run uh is uh as long as the
251:53 - dependencies in Azure ml firment don't
251:55 - change the same images reused and hence
251:57 - the start here start time is much faster
252:00 - here's what's happening while you wait
252:01 - the image creation a Docker image is is
252:03 - created matching the python environment
252:05 - specified by the azl environment
252:07 - the image is built and stored in the ACR
252:10 - the Azure container registry associated
252:12 - with your workspace let's go take a look
252:14 - and see if that's the case because
252:16 - sometimes like resources aren't visible
252:18 - to you so I'm just curious do we
252:20 - actually see
252:21 - it
252:23 - okay and yep there it is okay so they
252:26 - did not lie
252:29 - um so associated with your workspace
252:32 - image creation uploading takes about 5
252:34 - minutes this stage happens once for each
252:36 - python environment since the container's
252:38 - cach subsequent runs during image
252:40 - creation logs are stem to the Run
252:41 - history you can monitor the image
252:43 - creation Pro process using these logs
252:46 - wherever those are if you if the remote
252:48 - cluster requires more nodes to execute
252:50 - the Run than currently available
252:51 - additional nodes are out added
252:53 - automatically scaling T typically takes
252:55 - about five minutes and I've seen this
252:57 - before where if you're in your compute
252:58 - here uh sometimes it'll just say like
253:00 - scaling because it's just not
253:03 - enough so uh running into the stage the
253:06 - necessary Scripts and files are sent to
253:08 - the compute Target then the data stores
253:09 - are amounted copied the entry script is
253:11 - run so entry script is actually the
253:13 - train.py file while the job is running
253:16 - SD out and the files is in the logs
253:18 - directory or stem to the Run history you
253:21 - can monitor the runs progress using
253:22 - these
253:23 - logs the dot outputs directory of the
253:26 - run is copied over to the Run history in
253:28 - your workspace so you can access these
253:30 - results you can check the progress of a
253:32 - running job in multiple ways this
253:33 - tutorial uses the Jupiter widget so
253:36 - looks like we can uh run this watch the
253:39 - progress so maybe we'll run that and so
253:42 - it's actually showing us the progress
253:43 - that's kind of cool I really like
253:45 - that so it's just a little widget
253:47 - showing us all the things that it's
253:49 - doing let's go take a look and see what
253:52 - we can see under experiments and our run
253:54 - pipeline because it was talking about
253:56 - things like outputs and things like that
253:58 - so over here in the outputs and logs I'm
254:00 - just
254:02 - curious is if this is the same
254:06 - thing
254:10 - I'm not sure if this uh is this Tails
254:13 - yeah it does tail it just moves so we
254:15 - can actually monitor it from here I
254:16 - guess that's what it was talking
254:18 - about um so here we can see that it's
254:21 - setting up Docker it's actually building
254:22 - a Docker
254:23 - image and
254:26 - then I'm not sure did it send it to I
254:29 - mean it's on ACR already I think it
254:32 - looks like it's just still uh
254:34 - downloading extracting packages so maybe
254:35 - it's actually running on the image now
254:37 - so we'll just wait there we pop back
254:39 - over here you know we can see probably
254:42 - the same information is it identical
254:43 - yeah it
254:45 - is so we're 3 minutes in uh it's
254:48 - probably not that fun to to watch it in
254:50 - real time and and talk about it so let's
254:53 - just wait until it's done I'll see you
254:54 - back then okay all right so I'm uh about
254:57 - 17 minutes in here I'm not seeing any
254:59 - more uh movement here so it could be
255:01 - that it is done it does say if you run
255:03 - this next step here it will wait for
255:05 - completion um
255:07 - specify show output to true for verbose
255:10 - log so here actually did output a moment
255:13 - ago so maybe it actually was done um but
255:16 - I just ran it twice so I'm not sure if
255:19 - that's going to cause me uh issues there
255:23 - so because I can't run the next step
255:25 - unless I stop this um can I individually
255:28 - cancel this one
255:32 - here uh I think I can just
255:35 - hit interrupt the kernel there there we
255:37 - go okay so I think that it's done okay
255:40 - because it's 18 minutes in and I don't
255:41 - see any more logging in here it's just
255:43 - not very clear and also uh the logs we
255:46 - just have a lot of stuff going on here
255:48 - like it's just so much so you know if
255:51 - we're keeping keeping Pace we probably
255:53 - would have saw all these created yeah so
255:54 - another we just had a few more outputs
255:56 - there but uh I think that it's done
256:03 - okay it's just there's nothing
256:05 - definitively saying like done
256:07 - do you know what I'm saying and then up
256:08 - here it doesn't say oh oh I guess it
256:10 - does say that it's done all right so
256:12 - yeah I just never ran it with this tool
256:14 - so I just don't know so I guess it does
256:17 - definitively say that I already ran this
256:19 - so we don't need to run that again I
256:21 - just feel like we'll get stuck there so
256:23 - let's take a look at the
256:25 - metrics so regularization rate is 0.5
256:28 - accuracy is nine so N9 is pretty good
256:31 - the last step is training the script
256:32 - wrote in the output uh uh s SK learn I
256:37 - want to see if it's actually in our
256:38 - environment
256:40 - here I don't think it is so outputs is
256:43 - somewhere it's in our workspace
256:44 - somewhere but it's just not uh we just
256:47 - don't oh it's right here okay so it
256:49 - outputed the actual model right there um
256:52 - and
256:54 - so you can see the associated files that
256:56 - are ran okay we'll run
256:59 - it register the work model in space so
257:01 - you can work with other collaborators
257:03 - sure so if I click on that here and we
257:06 - go back over to our models it is now
257:09 - registered over here
257:12 - okay and so we're done part one I don't
257:16 - want to do all these other parts um
257:17 - training is enough as it is but let's
257:19 - just take a look at the deploy stage
257:22 - okay so for
257:25 - prerequisites uh we're setting up a
257:27 - workspace we have our we are loading our
257:30 - registered
257:32 - model okay we register it we have to
257:34 - import packages we are going to
257:39 - um create scoring
257:42 - script deploy to an ACI model test the
257:45 - model if you want to do this you can go
257:47 - through all the steps it does talk about
257:48 - a confusion Matrix and that is something
257:50 - that can show up on the exam is actually
257:52 - talking about a confusion Matrix but we
257:54 - do cover that in lecture content so you
257:56 - generally understand what that is but um
257:58 - you know I'm just I'm too tired I don't
258:00 - want to run through all this there's not
258:01 - a whole lot of value other than reading
258:03 - reading through it yourself here um so I
258:05 - think we're all done here
258:08 - [Music]
258:12 - okay okay one service we uh forgot to
258:14 - check out was Data labeling so let's go
258:16 - over there and give that a go so I'm
258:17 - going to go ahead and create ourselves a
258:18 - new project I say my labeling project
258:22 - and we can say whether we want to
258:23 - classify images or text um we have
258:25 - multiclass multi label bounding box uh
258:28 - segmentation let's go with multic
258:31 - class I'll go back here for a second um
258:33 - multic class
258:35 - whoops I I don't know if we uh create
258:38 - create data set but we could probably
258:39 - upload some local
258:41 - files uh let's say uh my St Trek Data
258:48 - set it doesn't let us choose the image
258:50 - file type here be nice if these were
258:55 - images going to tell us what
258:58 - here it's very finicky this input here
259:02 - uh file dis set references a single or
259:03 - multiple files in your public data store
259:05 - or private public L okay so we'll go
259:08 - next uh if we can upload files directly
259:10 - that'd be nice oo upload a folder I like
259:12 - that so what we'll do um is we do have
259:15 - some images in the free uh AI here under
259:18 - cognitive Services
259:20 - assets uh we
259:23 - have um we'll go back here and we'll
259:27 - say I think objects would be the
259:31 - easiest oh but we just want a folder
259:33 - right so yeah we'll just take
259:35 - objects yep we'll upload the 17
259:40 - files uh yep we'll just let it stick to
259:42 - that path that seems fine to
259:47 - me we will go ahead and create
259:50 - it and so now we have a data set there
259:52 - we'll go ahead and select that data set
259:53 - we'll say next your data set is
259:55 - periodically checked for new data points
259:57 - any data points will be added as tasks
259:59 - it doesn't matter we're only doing this
260:01 - for test uh enter the list of labels so
260:03 - we have um uh TNG
260:08 - DS9 uh
260:11 - Voyager toss that's the types of Star
260:15 - Trek
260:16 - episodes um label
260:20 - which
260:22 - um Star Trek
260:27 - series the image is from say
260:30 - next I don't want enabled but you can
260:33 - have Auto uh enabled assistant labeler
260:36 - I'm going to say no we'll create the
260:41 - project okay and I'll just wait for that
260:43 - to create and I'll see you back here in
260:45 - a moment okay all right so I'm back here
260:47 - actually didn't have to wait long I
260:49 - think it instantly runs I just assumed
260:51 - like I was waiting for a state that says
260:52 - completed but it's not something we have
260:54 - to do so uh we have 0 out of 17 progress
260:57 - we're going to go in here we're going to
260:58 - go label some data we can view the
261:01 - instructions it's not showing up here
261:03 - but that's fine if we go to tasks we can
261:04 - start labeling so what season is from or
261:07 - Series this is Voyager we'll hit submit
261:09 - this is Voyager we'll hit submit this is
261:12 - toss we'll hit submit this is
261:15 - TNG this is
261:17 - TNG this is
261:19 - DS9
261:21 - DS9
261:23 - Voyager
261:25 - Voyager uh
261:28 - TNG
261:29 - DS9 you get the idea though and you got
261:32 - some options here like change the
261:33 - contrast if someone can't see the photo
261:35 - or rotate it this is
261:37 - Voyager
261:39 - Voyager uh
261:41 - TNG
261:43 - DS9 uh
261:45 - Voyager
261:47 - Voyager and we're done so we'll go back
261:50 - to our labeling job here we'll see we
261:52 - have the breakdown there uh and now our
261:55 - data set is
261:56 - labeled we can export our data set CSV
261:59 - Coco as your ml data set I believe that
262:02 - means it'll go back into the data sets
262:04 - over here which will make our Liv a
262:06 - little bit easier we go back to data
262:10 - labeling okay so you just granted people
262:13 - access to the studio they'd be able to
262:14 - just go in here and and jump into that
262:16 - job okay uh if we go over to the data
262:18 - set I believe we should have a labeled
262:20 - version of it now so my labeling project
262:23 - so I believe that is the uh the labeled
262:26 - stuff here
262:28 - right yeah so it's labeled so there you
262:31 - go we're all done aure machine learning
262:33 - uh and so all that's left is to do some
262:35 - cleanup
262:39 - [Music]
262:40 - okay so we're all done with Azure
262:42 - machine learning uh if we want to we can
262:44 - go to our compute and just uh kill the
262:47 - services we have here now if we go to
262:49 - the resource Group and delete everything
262:50 - it'll it'll take all these things down
262:51 - anyway but I'm just going to go a bit
262:54 - paranoid so I'm going to just manually
262:55 - do this
262:58 - okay hit
263:04 - delete okay and so we'll go back to
263:06 - portal. azure.com
263:09 - and uh I'm going to go to my resource
263:12 - groups and everything is contained it
263:15 - should be all contained within my studio
263:17 - just be sure to check these other ones
263:18 - for that and we can see all the stuff
263:20 - that we spun up we'll go ahead and hit
263:22 - delete Resource Group um I don't know if
263:25 - it includes like because I don't see
263:28 - like container registry right so I know
263:30 - like it puts stuff
263:32 - there I guess it does it says container
263:34 - registry so that's pretty much
263:35 - everything right
263:37 - and that'll take down everything so and
263:39 - if you're paranoid all you can do is go
263:40 - to all resources and double check over
263:42 - here because if there's anything running
263:44 - it will show up here okay um but that's
263:47 - pretty much it and so just delete and
263:49 - we're all done