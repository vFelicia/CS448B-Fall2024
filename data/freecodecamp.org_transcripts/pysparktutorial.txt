00:00 - PiSpark is an interface for Apache Spark in
Python is often used for large scale data
00:06 - processing and machine learning. Krish knack
teaches this course. So we are going to start
00:12 - Apache Spark series. And specifically, if
I talk about Spark, we will be focusing on
00:18 - how we can use spark with Python. So we are
going to discuss about the library called
00:22 - pi Spark, we will try to understand everything
why spark is actually required. And probably
00:30 - will also try to cover a lot of things, there
is something called as emblem, spark emblem,
00:36 - which will basically say that how you can
apply machine learning, you know, in Apache
00:41 - Spark itself with the help of this spark API
called as pi spark libraries. And apart from
00:47 - that, we'll also try to see in the future,
once we understand the basics of the PI spark
00:52 - library, how we can actually pre process our
data set how we can use the PI spark data
00:58 - frames, we'll also try to see how we can implement
or how we can use PI spark and cloud platforms
01:04 - like data, bricks, Amazon, AWS, you know,
so all these kinds of clouds, we'll try to
01:10 - cover. And remember, Apache Spark is quite
handy. Let me tell you just let me just give
01:16 - you some of the reasons so why Apache Spark
is pretty much good. Because understand, suppose
01:22 - if you have a huge amount of data, okay, suppose
if I say that I'm having 64, GB data, 128
01:27 - gb data, you know, we may have some kind of
systems or standalone systems, you know, where
01:33 - we can have 32 GB of RAM, probably 64 gb of
ram right now in the workstation that I'm
01:38 - working in. It has 64 gb ram. So max to Max,
it can directly upload a data set of 32, GB,
01:45 - 48 gb, right. But what if we, if we have a
data set of 1.8 gb, you know, that is a time
01:52 - guys, we don't just depend on a local system,
we will try to pre process that particular
01:57 - data or perform any kind of operation in distributed
systems, right distributed system basically
02:03 - means that all there will be multiple systems,
you know, where we can actually run this pipeline
02:08 - of jobs or process or try to do any kind of
activities that we really want. And definitely
02:13 - Apache Spark will actually help us to do that.
And this has been pretty much amazing. And
02:20 - yes, people wanted this kind of videos a lot.
So how we are going to go through this specific
02:25 - playlist is that we'll try to, first of all
start with the installation will try to use
02:29 - PI Spark, because that is also Apache Spark
is it is a spark API with Python, when you're
02:36 - actually working with Python, we basically
use PI spark library. And yes, we can also
02:42 - use spark with other programming languages
like Java, Scala R. And all right, and we'll
02:47 - try to understand from basics you know, from
basics, how do we read a data set? How do
02:52 - we connect to a data source? Probably, how
do we play with the data frames, you know,
02:57 - in in this Apache Spark, that is your PI Spark,
also, they provide you data structures, like
03:02 - data frames, which is pretty much similar
to the panda's data frame. But yes, different
03:07 - kinds of operations are supported over there,
which we'll be looking at one by one as we
03:11 - go ahead. And then we will try to enter into
emlid, the spa, Apache Spark and lib. So basically,
03:17 - it is called a spark em lib, which will actually
help us to perform machine learning, which
03:23 - will be where we'll be able to perform some
machine learning algorithm task where we will
03:27 - be able to do regression, classification clustering.
And finally, we'll try to see how we can actually
03:33 - do the same operation in cloud, where I'll
try to show you some examples where we will
03:38 - be having a huge data set, we will try to
do the operation in the clusters of system,
03:44 - you know, in a distributed system, and we'll
try to see how we can use spark in that, right.
03:49 - So all those things will basically get covered.
Now, some of the advantages of Apache Spark
03:53 - and why it is very much famous, because it
runs workloads 100 multiplied by 100 times
04:01 - faster, you know, that basically means and
if you know about big data, guys, when we
04:04 - talk about big data, we're basically talking
about huge data set right. And then if you
04:09 - have heard of this terminology called as MapReduce,
right, Trashman Apache Spark is much more
04:16 - faster 100 times faster than MapReduce also.
Okay, and it is some of the more advantages
04:22 - that it is ease of use. You can write application
quickly in Java, Scala, Python, or R. As I
04:26 - said, we will be focusing on Python, where
we will be using a library called pi Spark.
04:32 - Then you can also combined sequel streaming
and complex analytics. When I talk about complex
04:38 - analytics. I'm basically talking about this
emblem, machine learning libraries. That will
04:43 - work definitely well with Apache Spark. And
Apache sparks can run on Hadoop, Apache missiles
04:49 - Cuba net standalone in our in the clouds,
cloud, different types of clouds guys, when
04:54 - I talk about AWS data, bricks, all these things,
we can definitely work okay. And it actually
04:59 - runs In a cluster mode cluster mode basically
means in a distributed mode, right. So these
05:04 - are some of the examples. Now if I if I go
with respect to which version of pi spark
05:10 - will spark we'll be using pi spark 3.1 point
one will be using will try to work. And if
05:16 - you just go and search for here you can see
SQL and data frames and all here you can see
05:22 - Spark streaming machine emilich, that is called
as machine learning. And all right, and apart
05:27 - from that, if I go and see the overview, here,
you can see that Apache Spark is a fast and
05:33 - general purpose cluster computing system and
provides high level API's in scalar, Java
05:37 - and Python, that makes parallel job easy to
write an optimized engine that supports genuine
05:42 - competition graphs, it is basically to work
with huge amount of data in short, you know,
05:47 - and that is pretty much handy, we'll try to
work. Now if I go and search for the spark
05:52 - in Python, you know, this page will get basically
you're open. And this thing's we'll try to
05:57 - discuss how to install it. And in this video,
we'll try to install the PI spark library.
06:02 - And if I talk about pi spark library, you'll
be able to see that pi spark library is pretty
06:07 - much amazing. This library is if you really
want to work spot, if you want to work this
06:15 - spark functionality with Python, you basically
use this specific library. And let's proceed,
06:20 - and let's try to see that how we can quickly
how we can quickly you know, install the specific
06:28 - libraries and check out like, what are the
things we can actually do? Okay, so all these
06:32 - things we'll try to see. So let's begin, please
make sure that you create a new environment
06:37 - when you're working with PI Spark. So I have
created a new environment called as my envy
06:40 - here, first of all, I will try to install
the PI spark library. So I'll just write pip
06:44 - install pi Spark. Okay, and let's see, in
this will focus on installation will focus
06:52 - on reading some data sets and try to see that
what are things we can actually do, okay,
06:58 - and after doing this, what we can actually
do is that, you can see that our PI spark
07:02 - has been installed, in order to check whether
the installation is perfect or not, I'll just
07:06 - write input by Spark. So this, this looks
perfectly fine it is working, you know, we
07:11 - are able to see that the PI spark is basically
installed properly. Now, you may be facing
07:17 - some kind of problems that is with respect
to pi Spark. So that is the reason why I'm
07:21 - telling you create a new environment. If you're
facing some kind of issue, just let me know
07:25 - what is the error that you're getting? Probably
writing in the comment section. Okay, now,
07:30 - let's do one thing, I'll just open an Excel
sheet. Okay. And probably I'll just try to
07:36 - create some data sets, I'll say name, probably
I'll just say name, and age, right. And suppose
07:47 - my name over here that I'm going to write
as crash, and also 31. I'm going to say Sudan
07:55 - shoe. right shoe down shoe, I will just say
okay, 30, probably, I'll just write some more
08:02 - names like Sonny, probably, I'll also give
the data as 29. So this three data, we'll
08:09 - just try to see how we can read this specific
file. Okay, I'm just going to save it. Let's
08:15 - see, I'll save it in the same location where
my Jupyter notebooks, guys, he created a folder,
08:23 - I guess, you can save it in any location where
your notebook file is opened, right? So it
08:28 - is not necessary. And just making sure that
you don't see any of my files. Okay, and I'm
08:34 - just saving it. Okay, I'm saving it as test
one here, you can see I'm saving it as test
08:40 - one dot CSV. So I'll save it. Let's keep this
particular file saved. Okay. Now, if I probably
08:47 - want to, you know, read with the pandas, what
we write we write PD dot read underscore CSV,
08:56 - right. And I basically use this particular
data sets called as test one dot CSV, right.
09:04 - So when I'm executing this here, you will
be able to see the specific information. Now
09:08 - when I really want to work with PI Spark,
always, first of all, remember, we need to
09:12 - start a spark session. And in order to start
a spark session, first of all, let me create
09:16 - some more fields. Just see this just follow
this particular steps, or with respect to
09:21 - creating a pass session. So I'll write from
pi Spark, dot SQL, input spark session. Ok.
09:31 - And then I'll execute this, you can see that
it is exhibiting fine then All right, sorry,
09:36 - I don't know what has opened. So I'll write
I'll create a variable called a spark. And
09:41 - probably I'll use the spark session dot builder.
And I'll say app name. And here I'll just
09:48 - give my session name. Okay. So it will be
like practice. Suppose I'm practicing this
09:53 - things. And then I can say get or create.
So when I actually execute this, you'll be
09:58 - able to see us barks session will get created.
And if you're executing for the first time,
10:02 - it will probably take some amount of time.
Other than that, if I've executed multiple
10:07 - times, then you will be able to work it. Now
here, you can definitely see that, in this,
10:13 - when you're executing in a local, they'll
always be only one cluster, but when you are
10:17 - actually working in the cloud, you can create
multiple clusters and instances okay. So the
10:21 - spark version that you will be using is V
3.1. point one. Here, you can see that this
10:27 - is basically present in the master, when probably
you will be working in multiple instances
10:31 - that you will be seeing masters and cluster
one, cluster two, all those kinds of information.
10:35 - Okay, so this is with respect to spark. Now,
let's just write the F of pi Spark, where
10:41 - I will try to read a data set with respect
to spark, okay. Now in order to read a data
10:48 - set, what I can write, I can write like this
spark dot read dot, there is a lot of options
10:55 - like CSV format, JDBC, Parque qL, schema,
table text, a lot of options there. So here
11:01 - we are going to take CSV, and here I'm just
going to write tips, one, tips, one dot CSV,
11:09 - right? And if I just try to execute it, here,
I'm getting some error saying that this particular
11:16 - file does not exist. Let me see. I think this
file is present. Just let me see guys, why
11:31 - this is not getting executed tips, one, b,
f file open. Here, I can see test one dot
11:42 - CSV, okay, sorry, I did not write the CSV
file, I guess, test one dot CSV. Okay, this
11:50 - has now worked. Now if I go and see the white
dots, pi Spark, it is showing this two strings,
11:56 - right, this two column C zero and C one. Now
here you can see that there is I've created
12:01 - this particular CSV file, right. And it is
just taking this A B as a default column probably.
12:08 - So it is saying c zero and C one. So what
we can actually do is that and probably if
12:12 - you really want to see your entire data set,
you can basically see like this df underscore
12:18 - pi spark dots show you here you will be able
to see name and age, there is this there's
12:22 - this information that I really want to make
my column name or age as my main column, right.
12:29 - But when I'm directly reading the CSV file
properly, we are getting underscore Cesar
12:32 - underscore c one. So in order to solve this,
what I will do is that we have a different
12:38 - technique file, right spark dot read dot option,
there is something called as option. And inside
12:45 - this option, what you can basically give is
that there will be an option with respect
12:49 - to header, I guess, see, there'll be something
like key value that you will be providing
12:54 - an option. So what you can do, you can just
write header, comma true. So whatever value
13:01 - the first column first row value will be there,
that will be considered as your header. And
13:06 - if I write CSV with respect to test one, now
I'm just going to read this test one data
13:12 - set test one dot CSV. Now once I execute this
here, you will be able to see that I'm able
13:19 - to get now named string h string, okay, but
let's see our complete data set. So here if
13:24 - I execute this now I'll be able to see the
entire data set with this particular columns.
13:29 - Okay, so let me just quickly save this in
my df underscore pi Spark. Okay, and now let's
13:37 - go and see that type of df underscore pi spark
Okay. Now when I execute this here, you will
13:44 - be able to see guys when I was reading this
df right when I was if I go and see the type
13:49 - of this with the help of pandas, here, you'll
be able to see that there is partners or core
13:54 - dot frame dot data frame, but here you will
be seeing that when you are reading this particular
13:58 - data set, it is of type pi spark dot SQL dot
data frame dot data frame. Yes, so that is
14:03 - pandas DataFrame this SQL dot data frame Rhonda,
yes, most of the API's are almost same the
14:09 - functionalities are seen a lot of things that
we will be learning as we go ahead. But if
14:14 - I quickly want to see my probably I don't
know whether head will work let's see, yes,
14:22 - head is also working. So if I use dot head,
probably you will be able to see the rows
14:27 - information are basically shown over here.
Now if I really want to provide see the more
14:33 - information regarding my columns, I will be
able to use something called as print schema.
14:38 - Okay. Now in this this print schema is just
like a df dot info which will actually tell
14:45 - about your columns like name is string and
ages string. Okay, so all these are some basic
14:50 - operations that you have actually done after
installation. Again, the main thing why I'm
14:54 - focusing on this is that just try to install
this PI spark and keep it ready for On my
15:00 - next session, I will be trying to show you
how we can change the data type, how we can
15:04 - work with data frames, how we can actually
do data pre processing, how we can handle
15:09 - null values, missing values, how we can delete
the columns, how we can do various things,
15:14 - all those things will, basically we'll be
discussing over there, how to drop columns.
15:18 - And so guys, we will be continuing the PI
spark series. And in this tutorial, we are
15:22 - actually going to see what our PI spark data
frames, we'll try to read the data set, check
15:27 - the data types of the columns, we basically
seen pi spark color schema, then we'll see
15:33 - that how we can select the columns and do
indexing. We'll see describe functionality
15:38 - that we have similar to pandas, and then we'll
try to see that how we can add new columns
15:43 - and probably drop columns. Now, this is just
the part one. So let me just write it down
15:47 - as part one, because after this, there will
also be one more part why this video will
15:52 - be important because in PI Spark, also, if
you're planning to apply emblem, you know,
15:56 - the machine learning libraries, you really
need to do data pre processing initially,
15:59 - you know, probably in the part two, we'll
try to see how to handle missing values. And
16:03 - all, we'll try to see how to filter the rows,
how we can probably put a filter condition
16:08 - and All right, so let's proceed. Before going
ahead, what I'm going to do is that, we will
16:13 - first of all, you have a data set called test
one. So I have taken three columns. One is
16:19 - name, age and experience. And then I have
a data set like Krish Taki 110, like this
16:24 - Sudan shoe Sunday, right. So this is some
data set, which you have saved in the same
16:28 - location. Now what I'm going to do first of
all, as usual, the first step with respect
16:33 - to pi spark is to build the PI spark session.
Now in order to build the PI spark session,
16:37 - I will write the code line by line. So please
make sure that you also do along with me,
16:42 - it will definitely be helpful. So I'm going
to write from pi spark dot SQL, input, sparks
16:52 - session, and then I'll create a variable,
oops, sorry, then I'll start to create a variable
16:59 - regarding my session. So I'll write spark
is equal to sparks session.we, basically,
17:05 - like builder dot app name. And here, I'm just
going to give my app name as practice I can
17:13 - just say, or liquid data frame practice or
data frame, right, something like this, since
17:17 - we are practicing data frame dot get or create
function. And this is how you actually start
17:24 - a session. So once again, if you are executing
for the first time, it will take some time,
17:28 - otherwise, it will, it is perfect to go. So
here is my entire Spark, it is running in
17:33 - memory, the version that it is running over
here. And obviously when you're running the
17:37 - local, you basically have one master node,
okay, and the app name is data frame. So to
17:42 - begin with, we will try to read the data set
again. So let's read the data set. Now reading
17:48 - the data set, I have already shown you multiple
ways. One is to read option one is to and
17:54 - since this is a CSV file, we'll try to read
it first first option, we'll try to see how
17:58 - we can actually read it. And then I'll show
you multiple ways of reading it. Okay, so
18:03 - I'll write spark dot read dot options. And
here in this option, we basically say key
18:11 - value, right, so here, I'll just make it as
header is equal to true so that, you know
18:16 - it should be considering my first row as the
header. And here I'll write it as header that's
18:22 - true dot CSV, inside the CSV. I'll give my
dataset name that is called as test one. dot
18:31 - c is right. Now when I execute this, probably,
I think you'll be able to see the data set.
18:37 - So here you are able to see that okay, it
is a data frame, and it will have features
18:41 - like name, age experience, right? So if I
want to see the complete data set, I'll just
18:45 - write dot show. So here is my entire data
set over here very clearly I can see it. Now.
18:52 - Let me just save this in a variable called
as df underscore pi Spark. Okay, so here is
18:57 - my entire data sets. Now, first thing, how
do we check the schema, let's check the schema.
19:03 - Okay, schema basically means the data types
like how we write in pandas df dot info, similarly
19:10 - we can write over here. So here you'll be
able to see that I have written df underscore
19:14 - pi spark dot print I think it should work
print schema or none type has spring theme
19:25 - Oh sorry. So I had written dot showed and
saved in a variable I'll remove this dot show
19:29 - let me execute it once again. And now if I
write print schema, here you will be able
19:33 - to see name, age and experience, but by default
it is taking a string even though in my Excel
19:40 - sheet, what we have done is that we have written
values probably this should be string, this
19:45 - should be integers, then they should be integers,
but why it is taking in a string. The reason
19:49 - it is probably taking a string guys because
by default, unless and until we don't give
19:53 - one more option in CSV, this CSV have one
option call us infer schema, okay? If I Don't
20:00 - make this as true, right? It will by default
consider all the features as you know in the
20:06 - stream value string values. So I'll execute
this now. And now if I go and see the F underscore
20:12 - pi spark dot print, Sima you will be able
to see that I'm getting name and string age
20:16 - as integer experience as integer and a level
is equal to two that basically means it can
20:20 - have null values. Okay, so this is one way
of reading it. One more way I'll try to show
20:24 - you which is pretty much simple so I can include
both header and infer schema in one thing
20:29 - itself. So I'll write d f underscore pi spark
is a call to spark dot read dot CSV and inside
20:37 - the CSV file first of all I will provide my
test file CSV Okay, and then here I will go
20:44 - ahead and write header probably is equal to
true and I can write infer schema is equal
20:51 - to, so when I write like this and if I write
df underscore pi spark dot show you here you
20:57 - will be able to see all my data set Okay,
so here is my entire data set. Now, if I go
21:03 - and see and execute this schema again it will
probably give me the same way like how we
21:10 - have we had over here, okay. So, here you
can see name is equal to string is equal to
21:14 - integer experience indigent, indigent, perfect.
So what are things we have done, we have understood
21:19 - about this one and right, if I go and see
the type of this, if I go and see the type
21:25 - of this, this is basically a data frame. pandas
also have a data frame. So if somebody asked
21:30 - you an interview, what is a data frame, you
can basically say that data frame is a data
21:35 - structures, you know, because inside this
you can perform various kinds of operations.
21:39 - So this is also one kind of data structures.
Okay, so what are things we have actually
21:42 - done, I've given introduction of data frame
reading the data set, not checking the data
21:47 - types of the column. In order to check the
data types of the column we have already just
21:51 - written print schema. Okay, now one more thing
that I can do after this, let's see selecting
21:57 - columns and indexing. First of all, let's
understand what columns are basically present
22:02 - how you can get all the column names. So in
order to get the column names, you can just
22:06 - write dot columns, okay. And when you execute
over here, you will be able to get the column
22:11 - name like name, age experience, perfect, this
is perfectly fine. Now this is my D F. Now
22:18 - suppose if I want to pick up some head elements,
also, I will be able to pick up because in
22:23 - pandas also you haven't had suppose I see
I want to get the top three records. I will
22:27 - be getting in this particular format in the
list format. Usually in pandas when we are
22:32 - using we usually get in a data frame format.
So here you will be seeing the combination
22:36 - of name age and expedience. Okay, like this.
This is my first row, this is my second row,
22:41 - this is my third. Okay, now coming to the
next thing that we are going to discuss now,
22:45 - how do I select a column? You know, I probably
want to pick up a column and see all the elements,
22:50 - right like how we do it in pandas. So first
of all, let me just write it like this PI
22:57 - spark dot show here will be able to see all
the columns are if I really want to pick up
23:02 - on the name column. Okay, so how do I do it?
Okay, let's let's actually see. Now in order
23:07 - to pick up the name column, there is very
simple functionality that we'll write which
23:12 - is called as pi spark dot select. And here,
I will just give my name column. Now once
23:20 - I execute this, you'll be able to see that
return type is data frame. Okay, the return
23:24 - type is data frame, and name is basically
a string. Now, if I write dot show, I will
23:30 - be able to see the entire column. Okay, so
when I do this, I'll be able to see this and
23:35 - if I try to find the type of this so sorry,
if I remove this dot show and see the type
23:42 - of this this is basically a data frame by
spark dot SQL dot data frame dot data for
23:47 - not pandas dot data frame. Okay, pretty much
simple. Now, suppose I want to pick up multiple
23:52 - rows like out sorry, multiple columns, like
I want to pick up name and experience probably
23:57 - two columns I want to pick up so what I'll
do I'll just make one changes. Here. Initially,
24:01 - I was providing my one column name like this.
After this what I will do, I will provide
24:06 - another column which is like experience, and
I'll execute this now once I execute this
24:11 - over here, you can see that guys I'm getting
a data frame with two features one is name
24:15 - and experience. Now, if I go and write dot
show, here you will be able to see my all
24:21 - my elements are basically present inside this
particular data frame. Pretty simple base,
24:26 - how do you select multiple rows? And yes,
here slicing definitely will not work because
24:31 - I tried doing slicing it was not working,
okay? And, okay, whenever you have any kind
24:36 - of concerns, always try to see the documentation,
the PI spark documentation, pretty much simple.
24:41 - Okay, this is one way that how we can actually
select the columns and probably see the rows.
24:46 - Okay. Now, let's show you if I just want to
pick up there is also one way like the see
24:52 - if I write the F of pi spark off named. If
I execute this here, you'll be able to see
24:59 - column name there. return type will be column
over here, if I'm directly picking because
25:03 - in pandas we directly pick like this, right.
And when we have this kind of columns definitely
25:07 - will just not, we are just able to understand
what this particular feature it is basically
25:12 - a column it is saying, okay, nothing more,
we won't be able to get the data set, there'll
25:16 - be no show function, it will be saying that
it is basically an error. So usually what
25:20 - we do, whenever we want to get a pick up any
kind of columns and try to see it, we basically
25:25 - need to select using this particular select
operation, which is my function. Okay, so
25:31 - these things have been done, guys, what we
try to understand now let's see how we can
25:36 - check the data types. So there is a functionality
which is called D types. So here you will
25:41 - be able to see name is called a string age
is equal to end experience is equal to int.
25:45 - And again, D types is pretty much similar
because we also use this in pandas. Okay,
25:49 - most of the functionalities are similar to
pandas, guys. So what are things we have actually
25:53 - done? Let's see, pi spark DataFrame, reading
the data set checking the data type, selecting
25:58 - columns and indexing, check the describe options
similar to pandas. So, we can also check out
26:02 - the describe options. So, let's see pi spark
dot describe and if I execute this, you will
26:09 - be able to see it will give you a data frame
summary is equal to string this this this
26:13 - information is there. Now when I write dot
show okay you will be able to see all this
26:20 - this is basically in the form of data frame
you may be thinking why this null values coming
26:25 - mean and standard division because understand
even in this it will take the string column
26:29 - also basically the values that are having
the data type of string away obviously, you
26:33 - don't have anything So, min and max is basically
taken on the index because in the zeros in
26:38 - the second index you will be able to see in
crushes then after that Sonny's there, okay
26:42 - look for the next and remaining all these
information are actually present again. So
26:45 - this is basically the same like the describe
options that we are actually seen, you know,
26:51 - probably in our pandas right, so similarly,
we have actually done that. Okay, so describe
26:57 - option is also done. So, let's go and see
adding columns, dropping columns. So adding
27:03 - columns and dropping columns is very, very
simple Guys, if we need to add the columns,
27:07 - so I'm just going to write the comment over
here, adding columns in a data frame, and
27:13 - this data frame is by pi spark data frame,
okay, now in order to add the column, so we
27:19 - have an amazing function add function, which
is called as the PI spark dot, there's something
27:24 - called as width column. Okay? Now, this width
column, if I see the functionality, it returns
27:30 - a new data frame by adding a class or replacing
the existing column that has the same name.
27:34 - Okay, so here, the first parameter that I'm
going to give is my column name. Suppose I
27:38 - want to pick up let's see, I'll pick up experience.
So I'll say experience, okay. And probably
27:48 - this will be my new column. After two years,
what will happen if experience after two years,
27:52 - you know, initially, the candidate is 10 years
experience, after two years, it will become
27:56 - 12. Right, so we'll try to put now the value,
this is my new column name, and what value
28:01 - it should have. So for that, I'll write df
pi Spark. And here, I'll say, probably, I'll
28:07 - take that same experience, I will multiplied
by I will add like two because after two years,
28:13 - the experience will get added by two, just
I'm taking one, one way of actually solving
28:18 - this, I can put any values I want guys, it
is up to you. Okay, and you can actually check
28:23 - it out. Okay, now after this, this is only
the two things that is required. And now if
28:28 - I execute it, you'll be able to see that the
same operation will happen. And now you in
28:32 - this data frame, you have 123 and four feature,
if I want to see the complete data set, I
28:37 - can add dot show, once I execute it now here
we'll be seeing that experience after two
28:42 - years is nothing but 266 because 10 plus 212
you have very very simple rest and this is
28:49 - what width column is basically told us and
you can also do different different things
28:54 - with respect to this. So this is how you get
add a column in a data frame. And again guys,
28:58 - this is not an in place operation, you basically
need to assign it to a variable in order to
29:03 - get reflected. Suppose if I want to get it
reflected, I really need to assign like this.
29:07 - And here now if I go and see my sorry, first
of all, let me remove the show. The show will
29:13 - not give us proper result. Okay, oh has no
attribute with column. Okay, sorry. So, this
29:23 - there was a problem with respect to this,
I'll read this dataset, because I replaced
29:27 - it completely right. Now I will execute it.
And once again, now is fine. Now if I go and
29:34 - write dot show, here, you will be able to
see the elements all properly given. Now,
29:40 - this was with respect to adding the columns
with data frame. Now, I probably need to drop
29:47 - the columns also. So drop the columns. Let's
see how we can actually drop the columns.
29:52 - Dropping the columns is pretty much simple
like how we usually drop this drop functionality.
29:57 - By default a column names you can give a list
of columns. You can Give a single column name.
30:01 - So suppose I say experience after two years,
I want to drop this, because who knows, after
30:06 - two years, what will happen. So let's drop
this, in order, drop this, just execute like
30:10 - this and just go and see dot show. Here, you
will be able to find out without that specific
30:16 - column. Again, this is not an in place operation,
you need to assign it to a variable, very
30:20 - simple. So let me just assign it to a variable
is equal to and please make sure that you
30:26 - remove this dot show dot show is the functionality,
right? Now, if I write this dot show, here,
30:33 - you will be able to see all the elements.
But now let's go ahead and see how to rename
30:41 - the column. So we are just doing this guys
because you really need to be very good at
30:46 - data pre processing, okay, so I'll write the
hot and there is another function, which is
30:50 - called as with column rename. Okay? Nine,
this you just give your existing and the new
30:56 - column name. Suppose I have my existing column
name over here, I'll say, name. And I'll say
31:02 - new name. Okay, and just executed. And now,
if I go and just like dot show, and try to
31:09 - see the elements here, you will be able to
see instead of name, there will be something
31:12 - called as new name. Right. Now, this is what
I had to actually discuss. I am just writing
31:17 - one more point over here. We have also discussed
about renaming columns, right? Yes, this is
31:26 - just the part one or data frames, the part
two, we'll try to do something called as filter
31:30 - operation. And in filter operation, we'll
try to see various operation because it will
31:33 - be pretty much amazing, you'll be able to
learn a lot probably this is the tutorial
31:36 - three, which is the part three with respect
to data frame operations. In this particular
31:41 - video, we are going to see how we can handle
missing values, null values, you know. So
31:47 - in short, this many things we'll actually
try to do, we'll see how to drop columns,
31:51 - we'll see how to drop rows, then we'll see
when when we are dropping rows, probably based
31:58 - on a null values, we'll try to drop a rose.
And then we'll try to see what are the various
32:02 - parameters in dropping functionalities and
handling missing value by Mean, Median or
32:07 - mode. Okay, so here, I'm just going to write
it as mean, median, and more probably, right.
32:12 - So all these things we are actually going
to see, again, the main thing is that, I really
32:17 - want to show you that how we can handle the
missing values. This is pretty much important
32:22 - because in pandas, and also we try to do this
in a scale on we have some kind of inbuilt
32:27 - function. So let's proceed. Whenever we usually
start pi Spark, whenever we are working with
32:34 - PI Spark, we really need to start a PI spark
session. So I hope till now you all are familiar.
32:39 - So I'll write for pi spark dot SQL, I'm going
to import sparks session, again. And then
32:48 - I'm going to create a variable with Spark.
And then here I'm going to write spark session
32:52 - dot builder. Not happening. Okay, not app
name. Again, let me just keep this app name
33:01 - as practice, okay, because I'm just practicing
things, then I like get or create and just
33:09 - execute this. So overall, it will take some
time to get executed. Yes, it has got executed
33:15 - fine. Now for this, I've just created a very
simple data set, which looks like this. I
33:20 - have a column like name, age, experience,
salary. So these are all my names, all the
33:25 - candidate names, and probably there are some
values which are left blank. Here, you can
33:29 - see some values I've left blank. So we'll
try to see how to probably drop a null values
33:35 - or how to handle this particular missing values
or not. Okay, so let's proceed. So first of
33:40 - all, in order to read the data set, I'll just
write spark dot read dot CSV. And here, I'm
33:50 - just going to use the CSV file name that's
to dot CSV. And it is saved in the same location
33:55 - where this particular file is Anyhow, I will
be providing the you in this data also. And
34:00 - I'm going to use header is equal to true and
probably there is also info schema is equal
34:05 - to true so that I'll be able to get the data
set properly. So probably when I'm reading
34:11 - this, you'll be able to see this is my data
frame that I'm actually getting if you want
34:15 - to see the entire data set, this will be like
show us dot show. And this is your entire
34:20 - data set here you are having null values under
perfect. So what let me do one thing, let
34:24 - me just save this in a variable. So I'll write
df underscore pi Spark. So if I go and now
34:32 - check, dots show and this is my entire data
set. Okay, perfect. We are pretty much good
34:38 - 10 Here we are working fine. With respect
to this. We know we have actually read some
34:43 - kind of data set also. Now, probably First,
let's start. How do we drop the columns? dropping
34:50 - the columns is very, very simple guy. Suppose
I want to drop Name column, then I just use
34:55 - df dot drop and provide my column name like
this right? So column, right column name,
35:02 - suppose I'll write df.pi spark and here column
name will be named. So let me write it as
35:08 - name. And I can basically go and check out
my dot show. And then you will be able to
35:14 - see all the features that are actually present.
This is pretty simple, which I also showed
35:18 - you probably in the previous session also,
okay. And this is how it is basically done
35:25 - basically dropping your feature or a columns,
but our main focus is dropping the non value.
35:30 - So right now, let me just write df.pi spark
dot show. So this is my data set, right? Now
35:37 - let's see how to drop this specific rows based
on the null values. So over here, I'll just
35:44 - use df.by spark.na. Okay, there's something
called as na and then you have drop, fill
35:50 - and replace. So first of all, I'll start with
dropped. Now, inside this particular drop,
35:54 - always remember if I don't give anything okay
and just exhibited here you will be able to
35:59 - see wherever there is a null values those
all rows will get deleted. So, here we will
36:05 - be seeing that this last three rows are not
present, right? So here you can see Shibam
36:09 - this particular value is present the meaning
all the rows have been removed perfect right.
36:14 - So not a problem at all. So, he had you in
short what you're doing is that whenever you
36:19 - use.na dot drop, it is just going to drop
wherever it is going to drop those rows wherever
36:24 - none values are actually present or null values
are actually present Okay, perfect This match
36:29 - is fine. If I go and search in the drop, there
are two main features one is how and one is
36:35 - threshold and then one is subset. So, let's
try to understand this particular features.
36:40 - Now, first, I will start with how any is equal
to how I just tried like this okay. So, suppose
36:48 - if I write df.pi spark.na dot drop and if
my how the how value can have two values one
36:58 - is any one is all Okay, one is any one is
on any if the value selected as any drop a
37:05 - row if it contains any notes like even though
there is just one okay one of the rich tuners
37:12 - or there is an entire mouth you know none
by default it is going to get dropped okay.
37:18 - But how is the call to all when do we use
all that basically means suppose if in your
37:24 - future you have suppose if any rule you have
all the values as null in this case you have
37:30 - 36 one value this will not get dropped, but
if he in a record you have all the values
37:34 - and ml then only it will get dropped okay.
So, let's see whether this will work or not
37:40 - definitely is not going to work because I
know all at least one values or at least one
37:46 - values one one value one non non null values
always there right? If I'm using How is equal
37:52 - to all it is going to drop those records which
is having completely not by default this how
37:56 - value should be having any right so, by default
it is any any basically says that whether
38:01 - there is one or two now we are just going
to drop it drop those specific records right.
38:07 - Pretty much simple this was what how was and
let's go ahead and try to understand with
38:11 - respect to threshold What is this threshold
I will tell you what is this threshold Now,
38:18 - let me just use this Okay, I know how is any
But there is another one more option called
38:24 - as thrush nine Thresh what we do is that suppose
if I right let's keep the threshold as two
38:30 - it basically says that suppose over here in
this particular case if the threshold value
38:37 - is two Okay, let's let's first of all execute
it you'll be able to see that the last column
38:45 - has got deleted over here Okay, that's the
last row has got deleted why it has got deleted
38:50 - because we have kept the threshold value is
two it says that at least two non null values
38:56 - should be present okay at least two non null
values now here you have two non null values
39:02 - like more than 40,000 Okay, here you just
have one non null values. So, because of that
39:09 - it got deleted suppose if you had two non
null values over here see 34 and 10 this is
39:14 - not got deleted This is same over here 3410
right 3410 you have if I go and show you 3410
39:21 - over here and 38,000 there at least here you
add three normal values here you add to normal
39:27 - values. So here whenever we give some threshold
values to that basically it will go and check
39:32 - whether in that specific row at least two
non null values are there if it is there,
39:37 - it is just going to keep that row otherwise
it is just going to delete it that is what
39:40 - you cannot you can also check out with the
one so if I go and see one then you can see
39:45 - that all this particular rows are there because
it will just go and check okay here one non
39:50 - nine values are there Here it is there. If
I make it as three, okay, let's see what it
39:56 - will come. Now here you can see at least this
is their remaining all has been deleted right
40:01 - see over here you had only two non non values
here also you how do you add three so this
40:07 - is the 3410 38,009 so here you can see the
value that is understanding with respect to
40:13 - threshold. Now, let's go ahead with the another
one we'll just call a subset. So here I'm
40:21 - just going to write it as subset because this
is the third parameter inside my drop feature.
40:25 - And remember, these are features are pretty
much simple with respect to if you have worked
40:30 - with pandas, the same thing we are working
away subset in the subset we can actually
40:35 - provide. Suppose I'll say in the subject,
let's remove threshold I don't want to keep
40:40 - any threshold let's say I just want to drop
nine values only for my specific column probably
40:47 - only from experience column then I can basically
give it as a subset. So, from the experience
40:54 - column you can see that wherever there was
none values in the records all those that
40:59 - hold record has been deleted right. So like
this you can apply with respect to the suppose
41:03 - you want to apply it in age you can also apply
this right wherever there was none values
41:08 - that old record has got deleted in the age
column. So this is with respect to subset
41:13 - so I hope you are getting an idea guys, this
is pretty much God because the same thing
41:17 - we are trying to do right we are we are actually
trying to apply whatever things we actually
41:21 - did in pandas and this is very, very handy
when you have working with missing data. Okay.
41:28 - Let's go with the next thing. Now let's go
and fill the missing value filling the missing
41:33 - value nine order to fill the missing value
again I'll be using Vf eyespot dot fill dot
41:41 - okay sorry na dot fill okay. And inside this
this field will take two parameters one is
41:50 - value and the one is subset Okay. Now, suppose
if I go and give value like this, suppose
41:55 - I say missing value and if I go and write
dot show, then what it is going to do whenever
42:01 - there is a non valid is going to replace with
missing values. So, here you can see here
42:05 - the null value is there. So, missing value
missing value missing value. Suppose, if you
42:09 - really want to perform this missing value
handling in only a specific column, then you
42:14 - can basically write your column name also
like this. So, this will be my Excel subset,
42:21 - okay. I can also give multiple records like
this. See, I can also give multiple Gods like
42:27 - experience karma probably age, karma age in
call enlist, right, when I give like this,
42:41 - then it this kind of functionalities will
happen in two columns, right? Pretty much
42:46 - simple. So by now next step, what we are going
to do is that, we are going to take a specific
42:50 - column, and probably we are going to handle
the missing values with the help of mean of
42:55 - that specific column or median of that specific
column. So, right now, if I go and check out
43:00 - my bf got pi spark here, if I go and see my
dot show value, this is my entire data set
43:07 - over here. Now, what I'm going to do is that
I'm going to take this particular experience
43:11 - column and probably replace the null values
with the mean of the experience itself. So
43:16 - in order to do this, I'm going to use an inbuilt
function. And guys, if you know about imputing
43:22 - function, we basically use that with the help
of SK learn also in PI Spark, also, we have
43:27 - an impure function. So I'm just going to copy
and paste the code over here to make it very,
43:31 - very simple. From pi spark.ml dot feature
import in pewter, here, I'm just going to
43:36 - give my input columns that is age experience
salary, probably I want to apply for every
43:41 - column over here. And then I'm just saying
that for age experience salary, I'm just going
43:46 - to find out this dot format dot c output columns.
And then I'm going to keep the strategy as
43:52 - mean you can also change the strategy has
immediate more than everything. So I'll execute
43:56 - this this has got executed fine, and then
we are just going to right fit and transform.
44:01 - So imputed reflect df of pi spark dot transform.
So once I execute this guy's here, you will
44:06 - be able to see that we are going to create
multiple columns with underscore imputed as
44:11 - this name. So here you can see h underscore
imputed. In short, what we have done we have
44:16 - tried some kind of mean functionality over
here that basically means the null value has
44:21 - been replaced by mean. So over here you can
see this null value is replaced by 28. Similarly,
44:27 - this to null value is replaced with 10 and
five sorry, five. This is what is the experience
44:33 - imputed column. Over here you will be seeing
that wherever there is a null value it is
44:38 - being replaced by the mean of the experience
column, the mean of the age column and mean
44:43 - or the salary column. And this way, you'll
be able to do it if you really want to go
44:47 - ahead with median. Just go and change this
mean to median and just try to execute it.
44:52 - Here. Now you'll be able to see the median
value and here is your initial null columns,
44:57 - which had sorry, here are the columns which
has none values. And here are all the columns
45:02 - which has basically the imputed values right
with respect to mean median. So guys, today
45:07 - we are in the tutorial for of pi spark data
frames. And here in this particular video,
45:11 - we are going to discuss about filter operation.
A filter operation is pretty much important
45:17 - for data pre processing technique. If you
want to retrieve some of the records based
45:20 - on some kind of conditions, or some kind of
Boolean conditions, we can definitely do that
45:25 - with the help of filter operation. Now guys,
please make sure that you follow this particular
45:28 - playlist with respect to pi Spark, I will
be uploading more and more videos as we go
45:32 - ahead. And remember one more thing there was
a lot of complaints from people are telling
45:36 - to upload SQL with Python. Don't worry parallely
I'll start uploading SQL with Python. I'm
45:42 - extremely sorry, because of some delay, because
I was doing some kind of work busy with something.
45:46 - But I'll make sure that I'll try to upload
all the videos. So parallely SQL with Python
45:51 - will also get uploaded. So let's proceed.
Now first of all, let me go and make some
45:55 - cell. Now today for this app taken our data
set a small data set, which is called as test
45:59 - one dot CSV. Here I have some data set like
name a just experience and salary. And I'm
46:05 - just going to use this and try to show you
some of the example with respect to filter
46:09 - operation. Initially, whenever you want to
work with PI Spark, you have to make sure
46:13 - that you install all the libraries. So I'm
going to use plus pi spark dot SQL import
46:19 - spark session. And this will actually help
us to create a spark session, right. And that
46:24 - is the first step whenever we want to basically
work with PI Spark, right. So we'll be using
46:32 - sparks session dot builder dot app name, then
I'm just going to create my app name as data
46:40 - frame. And basically right get our create
function, which will actually help me to quickly
46:45 - create a spark session, I think this is pretty
much familiar with every one of you. And let's
46:49 - proceed unless try to read a specific data
set. So over here, what I'm going to do, I'm
46:54 - just going to create a variable, df underscore
pi Spark, and I'm going to use the spark variable
47:00 - dot read dot CSV. And here, I'm just going
to consider my data set test one dot CSV.
47:09 - And here, I'm just going to make sure that
we have this particular option selected header
47:14 - is equal to true and in for schema is equal
to true I think this all I've actually explained
47:21 - you then if I write df.pi spark dot show up,
here, you'll be able to see your data set.
47:26 - Okay, so it is reading, let's see how we will
get the output. So this is my entire output.
47:31 - Now guys, as I showed you that we will be
working on a filter operation, I will try
47:37 - to retrieve some of the records based on some
conditions. Remember, filters also are available
47:43 - in pandas. But there you try to write in a
different way. Let me just show you how we
47:48 - can perform filter operation by using pi Spark.
Okay, so filter operations, let me make this
47:55 - as a markdown. So it looks big. looks amazing.
Let me make some more cells Perfect. Now first
48:02 - step, how do I do a filter operation, suppose
I want to find out salary of the people who
48:09 - are less than probably 20,000. Okay, less
than or equal to 20,000. Again, I'd like that
48:15 - less than or equal to 20,000. Now for this,
there are two ways how we can write it first
48:22 - way, I will just try to use the filter operation.
So you have like dot filter. And here, you
48:27 - just have to specify the condition that you
want. Suppose I'll write salary is less than
48:32 - or equal to 20,000. Remember, this salary
should be the same name of the column over
48:38 - here, right? And when I write dot show, you
will be able to see this specific record.
48:44 - And you'll be able to see, okay, less than
or equal to 20,000 is this foreign for people,
48:49 - Sonny Paul has shown sober here, you will
be able to see all these things along with
48:53 - the experience right? Now this is one way,
probably I just want to pick up. After putting
48:59 - this particular condition, I want to pick
up two columns. So what I can do, I can use
49:03 - this. And then I can basically write dot select.
And here, I'm going to specify my name, probably
49:12 - I want the name and age, name, comma, age.
So dot show, I'll do this. Now this is how
49:22 - you can actually do it again. Over here, you
can see that name underscore age is actually
49:28 - there. And you are able to get that specific
information after this. Probably I want to
49:34 - do some of the operation you can actually
do less than greater than whatever things
49:37 - you want. Probably I'm going to put two different
conditions Then how should I put it? Let's
49:42 - see. Let's see for that also. So I'll write
divide df pi spark dot theta. And here I am
49:49 - going to specify my first condition. Suppose
this is one way. This is one way by using
49:55 - filter operation. Also guys, then this conditions
that I'm writing, I can also write something
49:59 - like this See this, suppose if I write df
pi spark of salary suppose salary as less
50:07 - than or equal to 20,000, I can also write
like this, I will also be able to get the
50:12 - same output. So here, you'll be able to see
the same output over here. Now, suppose I
50:16 - want to write multiple conditions, how do
I write, it's very simple, I will take this,
50:21 - this is, first of all, this is one of my condition.
So I'm just going to use this condition. And
50:27 - I can also use an AND operation you know,
so I'll say and, or, or any kind of operation
50:33 - that you want, probably, I want to say that
df underscore pi salary is great, less than
50:40 - or equal to 2000 20,000. And probably I want
a df pi spark of salary salary greater than
50:55 - or equal to 15,000. So I'll be able to get
all those specific records, okay. And again,
51:00 - I'll try to put this in another brackets,
make sure that you do this, otherwise, you
51:02 - will be getting an error. Okay? Very, very
simple, guys. So let's see how I've actually
51:08 - written it is something like this d f underscore
pi spark dot filter df or pi spark of salary
51:13 - is less than or equal to 20,000 greater than
equal to 15. If I execute, you will be able
51:17 - to see between 15,000 to 20,000, you'll be
able to find out you can also write or then
51:23 - you will be able to get all the different
different values. Now, this is our kind of
51:27 - filter operation that you can basically specify,
remember, this will be pretty much handy when
51:33 - you are probably retrieving some of the records
with respect to any kind of datasets, and
51:38 - you can try different different things. So
this is one way where you are actually directly
51:41 - providing your column name and putting a condition
internally this PI spark actually pi spark
51:48 - DataFrame understands it and you will be able
to get the output, right. So yes, this was
51:53 - it all about this particular video, I hope
you like it, I hope you liked this particular
51:56 - filter operation, just try to do it from your
side. Okay, one more operation is basically
52:00 - pending, I can also write like this serious
everybody, I can basically say that, okay.
52:07 - Probably, I can use this operation, which
is called a knot operation. Let's see how
52:12 - this knot operation will be coming. Okay.
Basically, the inverse condition operation,
52:17 - we basically say, so I'll be using this, okay.
And this, and inside this, I can put a knot
52:24 - condition which like this, so I'll say this
is a knot of df of pi spark salary is less
52:29 - than or equal to 20,000. So anything that
is greater than 20,000 will be given over
52:33 - here. Okay, so inverse operation, you can
see in words filter operation, guys, we will
52:38 - be continuing the PI spark series. And in
this particular video, we are going to see
52:42 - group by an aggregate function. Already I
have actually created somewhere around four
52:47 - tutorials on pi Spark, this is basically the
fifth tutorial. And again, this is a part
52:51 - of a data frame, why we should actually use
group by an aggregate functions again for
52:56 - doing some kind of data pre processing. So
let's begin for this particular data set.
53:01 - For this particular problem, I have created
a dataset which has three features, like name
53:05 - departments and salary, and you have some
of the data like crash data science, salary,
53:10 - right, something like this. So we're here
in short, if I want to basically understand
53:15 - about this particular data set, there are
some departments probably where crash and
53:20 - other people teach. And based on different
different departments, they get a different
53:24 - different salary. So let's see how we can
perform different different group by an aggregate
53:29 - functions and see how we can pre process or
how we can get some or retrieve some kind
53:34 - of results from this particular data. So to
begin with, what we are going to do, we are
53:39 - first of all going to import pi Spark SQL
import spark session. As usual, we have to
53:48 - create a spark session. So after this, what
we have to do, I'll create a spark variable.
53:53 - So I'll use spark session dot builder dot
happening. I think everybody must be familiar
54:00 - with this. But again, I'm trying to show you
this one, so let me write it as aggregate
54:05 - dot get auto create. So now I've actually
created a spark session. Okay, probably this
54:12 - will take some time. Now if I go and check
out my spark variable, so here is your entire
54:18 - information, okay, with respect to this particular
spark video, now let's go ahead and try to
54:23 - read the data set. Now I will just write df
underscore pi Spark. And then here I'll write
54:29 - spark dot read dot CSV. The CSV file name
is basically test three dot CSV, and remember
54:39 - I'll be giving this particular CSV file in
the GitHub also. And then I'll be using header
54:45 - is equal to true comma, infer schema is equal
to two. Now this is my df underscore pi Spark.
54:55 - Now what I will do in the next statement,
I will write df underscore pi Spark. dot show
55:00 - right. Now here you will be able to see that
I am actually being able to see all the datasets.
55:07 - Here I have named departments and salary on
all this particular information. If I really
55:13 - want to see the schema or the columns, like
which all columns where it belongs to this
55:19 - like a data type, so I can definitely use
the F underscore pi spark dot print schema,
55:26 - right. And now here you can see name is a
string, department is string, and salary is
55:32 - basically an integer. Okay, now let's perform
some group by operation First we'll start
55:37 - by group by operation, probably I want to
group by name, and probably try to see what
55:44 - will be the mean average salary. You know,
well what suppose let's let's take a specific
55:51 - example over here. So I'll write TF dot underscore
pi spark dot group by suppose I want to go
55:56 - and check who is having the maximum salary
out of all these people that are present in
56:03 - this particular data set. So, I will first
of all group by name, if I execute this, you
56:07 - can see that we will be getting a return type
of group data at some specific memory location.
56:13 - And you should always know that guys, group
by aggregate functions works together. That
56:18 - basically means first of all we are we need
to apply a group by functionality and then
56:23 - we need to apply an aggregate function. So
aggregate function Do you really want to check
56:27 - just press dot and press tab. So here, you
will be able to see a lot of different different
56:32 - function examples like aggregate average count
max mean, by better many more right? Now what
56:39 - I'm going to do, I'm just going to use this
dot sum because I really need to find which
56:43 - is the maximum salary from out of all this
particular employees who is having the maximum
56:48 - salary. So here I'll say Datsun and if I execute
it, you will be able to see that we are getting
56:54 - sequel dot data frame, which has name and
sum of salary This is very much important
56:59 - Let's sum of salary because I really want
to have the sum of the salary remember, we
57:04 - cannot apply sum on the string. So, that is
the reason it has not done over here it is
57:08 - just giving you the name because we have grouped
by name and this dot some will just get applied
57:13 - on this particular salary. Now, if I go and
write dot show here you will be able to see
57:19 - so the answer will be here is having the highest
salary of 35,000 Sonny has 12,000 Krish has
57:25 - 19,000 Mahesh has 7000 So, if you go and see
over here, so, the uncial is basically present
57:31 - here here and in big data. So, overall, his
salary should be 35,000 if you compute it,
57:38 - similarly, you can go and compute my salary
over here, over here by just calculating this
57:44 - and then you can also compute sunny salary.
And you can also see my hash and so this is
57:49 - one just an example. So here I will just write
we have grouped to find the maximum salary.
57:58 - And definitely over here from this entire
observation, we can retrieve that sudhanshu
58:03 - is having the highest salary. Okay, now let's
go to one step ahead. One more step ahead.
58:08 - Now we'll try to group by departments to find
out which department gives maximum salary
58:16 - Okay, we are going to do a group by departments
which gives maximum standard suppose this
58:23 - is my, this is my requirement, okay, and different
different types of requirement may come, I'm
58:30 - just trying to show you some examples. I'm
just going to copy this. I'm going to use
58:36 - this department Okay. And then I'm basically
going to say dot some dots show, if I execute
58:43 - it, let me see department is a wrong column
name. So I'll write departments it is department.
58:51 - So let me write this. Now if I go and see
IoT over here gives some salary around 115
58:57 - 1000 to the simplest to all the employees
right combined because we are doing the some
59:02 - big data gives somewhere around 15,000 data
science gifts around 43,000 I suppose if I
59:08 - go and see big data over here 4000 4000 8000
8013 1000 13,000 15,000 so I hope I'm getting
59:17 - yes Big Data is actually giving us 15,000
so you can go and calculate it. Suppose if
59:22 - you want to find out the mean you can also
find out the mean okay, so let me just write
59:27 - it over here. Just copy this entire thing,
paste it over here and read me right instead
59:32 - of instead of some I'll try to write mean
so by default the mean salary here you can
59:38 - see that for a particular employee somewhere
for IoT it is 7500 because this mean will
59:43 - be based on how many number of people are
working in the department right. So like this,
59:48 - you can actually find out now I can also check
one more thing I can copy this I can try to
59:53 - find out how many number of employees are
actually working based on the department so
59:59 - I can use dot Count and then if I go and execute
this properly This is a method Okay. Now here
60:07 - you will be seeing that IoT there are two
people in big data there are four people in
60:11 - data science they are four people. So, four
plus four plus eight total employees that
60:16 - are present over here is basically now, one
more way that I can basically apply a directly
60:23 - aggregate function also. Now, see these are
all some of the examples and again you can
60:27 - do different different groupbuy let me use
df pi spark suppose I say dot aggregate okay
60:34 - and inside this I will just give my key value
pairs like this suppose I say let me say that
60:40 - salary I want to find out the sum of the salaries
the overall salary that is basically given
60:47 - to the entire total expenditure inside. So,
the total expenditure that you will be able
60:54 - to see somewhere on 73,000 All right. So,
we can also apply direct aggregate function
61:00 - otherwise this all are also aggregate function
which we basically apply after after you know
61:07 - applying a group by function. Now, suppose
these are probably the salary I want to find
61:11 - out suppose I take this example I want to
find out the maximum salary that the person
61:16 - is basically getting who is getting the maximum
salary sorry. So, here instead of writing
61:21 - dot sum now I'll write max dot show. Now,
here you can see, Sudan shows basically getting
61:28 - 20,000 over here 10 10,000 crashes getting
10,000 matches getting four for 4000 right.
61:36 - So, all this particular data is there see,
Krishna is basically getting with respect
61:40 - to data science over here 10,000 So, it has
basically picked up it is not picking up both
61:46 - the records, but at least when it is grouping
by name, and then it is showing this particular
61:51 - data that time you will be able to see it,
let's see whether I will be also able to see
61:55 - this or not. So, group by if I score and write
min. So here you will be able to see minimum
62:02 - value with respect to different different
records when I'm grouping by here, you will
62:07 - be able to see that Sudan shoe, sorry. So
the answer is getting a minimum salary of
62:12 - 5000 2000 crushers getting a minimum salary
of 4000. Right, we can also get that particular
62:18 - information. Now let's see what all different
different types of operation are, their average
62:23 - is also there. So if I write a VG, it's just
like mean only guys. So this is basically
62:29 - the mean salary that probably, again, you
can check out different different functionalities,
62:34 - why these all things are basically required.
understand one thing is that you really need
62:38 - to do a lot of data pre processing a lot of
retrieving skills that you basically do, you
62:43 - can check it out this one and you can do different
different functionalities as you like it spark
62:47 - emulate also has an amazing documentation
with respect to various examples. So here,
62:52 - you can go and click on examples. And basically
check out this particular documentation, you
62:57 - can actually see different different kinds
of examples how it is basically done. But
63:01 - with respect to spark ml, there are two different
techniques. One is the RDD techniques, and
63:06 - one is the data frame API's. Now what we are
going to do, guys, data frame API is the most
63:11 - recent one, you know, and it is pretty much
famously used everywhere. So we'll be focusing
63:16 - on data frame API. That is the reason why
we learn DataFrame in PI spark very much nicely.
63:21 - So we'll try to learn through data frame API's.
And we'll try to see the technique how we
63:26 - can basically solve a machine learning use
case. Now let's go and see one very simple
63:31 - example guys. And always remember, the documentation
is pretty much amazingly given, you can actually
63:37 - check out over here and try to read all these
things. Okay. So let's proceed. And let's
63:41 - try to see, what are things we can actually
do. In this particular example, I'm just going
63:47 - to take a simple machine learning problem
statement. So let me just open a specific
63:51 - data set for you all, and then probably will
try to do it. Okay. So this is my data set.
63:57 - Guys. I know there are not many records over
here. Okay, so I have a data set, which has
64:03 - like name, age, experience and salary. And
this is just a simple problem statement to
64:08 - just show you that how powerful SPARC actually
is, with respect to M lab libraries just to
64:15 - show you a demo. From the next video I'll
be showing you detailed explanation of the
64:21 - regression algorithms how we can basically
do the implementation all theoretical and
64:26 - all guys have already uploaded you can see
over here, I'll be doing see after this tutorial
64:30 - five this is basically the tutorial six I'll
try to add it after this and then whenever
64:36 - I will be uploading the linear regression
algorithm before that please make sure that
64:40 - you watch this match infusion. Okay, I have
uploaded this specific video also in the same
64:47 - playlist. So after this tutorial 26 St. yt
saying tutorial 26 because I have also added
64:52 - this in my machine learning playlist. So after
this, you'll also be able to find out when
64:56 - we'll be discussing about linear regression
how we can implement in depth That video will
65:00 - also get uploaded. So let's proceed. And here
is my entire data set. Guys, this is my data
65:05 - set. Now what I have to do is that based on
age and experience, I need to predict the
65:11 - salary, very simple use case, not not much
data pre processing, not much transformation,
65:16 - not much standardization and all okay, I'm
just going to take up this two independent
65:21 - feature. And I will be predicting the salary
of this particular person based on age and
65:26 - experience. Okay, so this is what I'm actually
going to do. So here is a perfect example,
65:31 - again, detailee, I'll try to show you how
to basically implement line by line probably
65:37 - from the upcoming videos where I'll be discussing
about linear regression, and on and if I go
65:42 - see this particular problem, this is also
a linear regression example. Okay, so let's
65:46 - go here. First of all, as usual, I will be
creating a spark session. So I'll use from
65:51 - pi spark dot SQL import spark session. And
then I'm going to use spark session dot builder
65:55 - dot app name. Here, I'm actually creating
a spark session on missing, let me execute
66:00 - it, I think this is pretty much familiar,
you're familiar with this, then what I'm going
66:05 - to do over here is that we are just going
to read this particular data set with test
66:08 - one dot CSV header is equal to true and infer
schema is equal to true. So when I go and
66:13 - see my training dot show, these are all my
features over here, perfect, I'll be giving
66:17 - you this data set. Also, don't worry. Now,
from this particular data set, if I go and
66:22 - check out my print schema, so here, you will
be able to see that I'm getting this particular
66:26 - information. This is my entire print schema.
Over here, I have features like name, age,
66:31 - experience, and salary. Now if I go and see
train dot columns, this is my training dot
66:36 - columns. Now always remember guys in PI Spark,
we use a different fund or mechanism or a
66:43 - kind of data pre processing before See, usually
what we do is that in non by using machine
66:49 - learning algorithms that are available net
scalar, we basically do a train test split,
66:54 - right. And then we first of all, divide that
into independent features dependent features,
66:58 - right, which we use an X and Y variable, and
then we do train test split. By doing this,
67:03 - in in in PI Spark, we just do some different
techniques, what we do is that yes, we have
67:08 - to basically create a way where I can group
all my independent features. So probably I'll
67:14 - try to create vector assembler, we basically
say it as a vector assembler see where the
67:21 - class I've actually used, the vector assembler
will make sure that I have all my features
67:25 - together grouped like this group like this,
in the form of age and experience, suppose
67:31 - over here, my two main features are age and
experience, which are my independent feature
67:36 - right. So it will be grouped like this, for
every record, it will be grouped like this,
67:40 - okay, for every report, it will be grouped
like this, and then what I will be doing is
67:44 - that I will be treating this group as a different
feature. So this will basically be my new
67:51 - feature, right. And remember, this new feature
is my independent feature. So my independent
67:58 - feature will look like this in a group of
H comma experience, which will be treated
68:03 - as a new feature. And this is exactly my independent
feature. So I have to group this particular
68:09 - way. So in order to group this, what we do
is that in PI Spark, we use something called
68:13 - as vector assembler. So in this vector assembler
is basically present in pi spark.ml dot feature,
68:18 - we use this vector assembler, we use two things.
One is input column, which all column we are
68:24 - basically taking to group it. So two columns,
one is age and experience, right, we don't
68:29 - have to take name because name is fixed, it
is a string. Yes, if category features are
68:35 - there, what we do what we need to do, we will
convert that into some numerical representation
68:39 - that I'll be showing you when I'm doing some
in depth implementation, the upcoming videos
68:44 - of linear regression, logistic regression
and all. But here, you'll be able to see that
68:48 - I'm going to take input columns age come experience
in the form of a list. And then I will try
68:53 - to group this and create a new column, which
is called as independent feature over here,
68:58 - right, that is what I'm actually doing. So
if I go and execute this vector assembler,
69:03 - so here, I'm got my feature assembler, and
then I do dot transform, I do dot transform
69:08 - on my training data. So this is basically
my training data when I do this, and when
69:12 - I do output dot show here, you'll be able
to see I had this all features, and a new
69:16 - feature has been created, which is called
as independent features. Okay, so we have
69:21 - actually created an independent feature. And
you can see over here, age, and experience,
69:26 - age and experience, age and experience, so
this is my grouped rows that I've actually
69:31 - got, in short, what I've done, I've combined
this two column and made it as a single independent
69:38 - feature, okay? Now, this will be my input
feature. Okay. And this will be my output
69:44 - feature, and we'll try to train the model.
Okay, so over here now, if I go and see output
69:48 - dot columns, I have name, age experience,
salary independent feature. Now what I'll
69:52 - do out of this, let's take which all data
set I'm actually interested in. So out of
69:57 - this, I will just be interested in this two
data. Separate independent features and salary
70:01 - salary will be my output feature, the y variable,
right, and this will be my input feature.
70:06 - So what I'm going to do, I am going to select
output dot select independent features and
70:11 - salary. And I'm going to put that in my finalized
underscore data. That is what I'm actually
70:15 - doing. If I now go and see my dot show here,
you will be able to see the entire thing.
70:20 - Now, this are my independent feature, these
are my dependent feature. Now the first step,
70:26 - what we do we do it train test split, like
how we do it in a scalar. So in order to do
70:30 - a train test split, I use a function inside
my finalized data, which is called as random
70:35 - split. Remember, guys, I'll try to explain
you line by line by implementing it when I'm
70:40 - doing a bigger project. Right now, since this
is just an introduction session, I really
70:45 - want to explain you how things are actually
done. So this is basically my train test split.
70:49 - So here, let me write it down the comment,
train test split. And I will be using the
70:55 - linear regression like how we import a classroom
a scaler. And similarly, by using pi spark.ml,
71:01 - dot regression, import linear regression,
and then I'm doing a random slipped off 75
71:06 - to 25%. That basically means my training data
set will be having 75 percentage of the data,
71:11 - and my test data set will be having 25 percentage
of the data, right, then after that, I'll
71:17 - be using a surrogate linear regression in
this you have two important variables that
71:21 - we need to get one is feature columns, how
many number of feature columns are there that
71:25 - is completely present in this independent
feature. So I'm giving it over here. Similarly,
71:29 - in label column, this is my second feature
that I have to give this is my output feature.
71:33 - So after I provide both these things, and
do a fit on train data, I will be able to
71:38 - find out my coefficient, these are my coefficients.
These are my intercepts. And here, I can now
71:44 - evaluate and see my output, right. So by using
the evaluate function, we will be able to
71:51 - see the output and inside this there'll be
a prediction variable, which will have the
71:55 - output. Okay, so this is my prediction. This
is my salary, the real value. This is my other
72:00 - thing. Now, if I really want to find out the
other important part parameters or metrics,
72:06 - let's press tab here, you will be able to
see mean absolute error. pred underscore result
72:13 - dot mean squared error, suppose if I do see
this particular word value, you will be able
72:17 - to understand that how the model is actually
performing. So that's just a various a very
72:22 - simple example, guys. Don't worry, I will
be explaining in depth probably in the upcoming
72:27 - videos when we'll be starting from linear
regression. Now remember, the next video is
72:31 - about linear regression implementation in
depth implementation, right? Help, you know
72:35 - what exactly is a data bricks platform. And
this is an amazing platform where you can
72:40 - actually use PI Spark, all you can work with
Apache Spark. And one more amazing thing about
72:46 - this particular platform is that they also
provide you cluster instances. So suppose
72:51 - if you have a huge amount of data set, probably
you want to distribute the parallel processing
72:56 - or probably want to distribute it in multiple
clusters, you can definitely do with the help
73:00 - of data bricks. Now, if I really want to use
this particular platform, there are two ways
73:05 - one is for community version, and one is for
the paid version, which is like Azure, or
73:10 - AWS cloud, you can actually use in the backend,
data bricks also helps you to implement ml
73:16 - flow, okay, and this ml flow is with respect
to the CI CD pipeline. So you can also perform
73:20 - those kinds of experiments also, altogether,
an amazing platform. What I will be focusing
73:29 - in my youtube channel is that I will try to
show you both with the community version also.
73:35 - And in the upcoming videos, we'll try to execute,
try to execute with both AWS and Azure. When
73:41 - we are using AWS and Azure, what we will try
to do is that whenever we create the instances,
73:46 - multiple instances, you know, that will try
to create in this particular cloud platform
73:51 - will also try to pull the data from s3 bucket,
which is the storage unit in AWS, and try
73:56 - to show you that how we can work with huge
huge data sets, right all those things when
74:00 - we actually should as we go ahead. Now let's
understand what is data bricks is it is an
74:06 - open and unified data analytics platform for
data engineering, data science and machine
74:10 - learning analytics. Remember why data bricks
actually helps us to perform data engineering
74:15 - and when I say data engineering, probably
working with big data, it also helps us to
74:19 - execute some machine learning algorithms.
Probably any kind of data science problem
74:25 - statement will be able to do it. And Rob,
let's suppose three kinds of platform cloud
74:29 - platforms one is AWS, Microsoft Azure and
Google Cloud. Now, if you really want to start
74:34 - start with this, we'll start with the community
version. And you just have to go into this
74:39 - particular URL and just type try data bricks,
and then you just enter all your details to
74:44 - get registered for free. Now once you are
registered, when you want to get started for
74:50 - free, you'll get two options over there. On
the right hand side you will be seeing the
74:54 - community version which you really want to
use it for free. And in the left hand side
74:58 - you will be having an option where it I will
tell you that you need to work with this three
75:02 - cloud platforms. And you can select that also.
So for right now, I will try to show you a
75:07 - community version, which will be very simple,
very, very easy. So let's go to the community
75:12 - version. So this is how the community version
actually looks like. If you really want to
75:16 - go into the cloud version, you can just click
on upgrade. Okay, so just click on upgrade.
75:22 - And this is the URL of the community version
and this version of this URL you will be able
75:26 - to get when you register for the community
version tomorrow. So you think that you probably
75:31 - want to work with the cloud, you just have
to click on this upgrade now. Now in this,
75:35 - you will be able to see three things one is
explore to the Explore the Quick Start tutorial,
75:40 - important explore data, create a blank notebook,
and many more things. Over here, what kind
75:45 - of tasks you'll be able to do in the community
version one is you can create a new notebook,
75:49 - you can create a table, create a cluster,
create new ml flow experiments, I hope I actually
75:55 - showed you ml flow experience, we can also
create this ml Flow Experiment by combining
76:00 - to a database in the backend, okay, then we
can import libraries, read documentation,
76:05 - do a lot of tasks. Now first of all, what
we need to do is that probably I'll create
76:09 - a cluster. And I in order to create a cluster,
I will click on this create a cluster here,
76:15 - you can basically just write down any cluster
name. Suppose I'll say Apache, or I'll just
76:20 - say, pi Spark cluster. Suppose this is my
cluster that I want to basically create. Okay,
76:27 - and then here by default, over here, you can
see 8.2 scaler, this one spark 3.1 point one
76:34 - is selected. So we'll be working with spark
3.1 point one, if you remember, in my local
76:38 - also, I actually installed this particular
version only, okay, by default, you will be
76:43 - able to see that they will be providing you
one instance with 15 GB memory, and some more
76:49 - configuration. If you really want to upgrade
your configuration, you can basically go and
76:54 - click over here, okay. And remember, in the
free version, you will be able to work in
76:59 - an instance unless and until it is not idle
for two hours, otherwise it will get disconnected.
77:04 - So over here you can see one driver 15.3,
GB memory, two cores and one dBu. Okay, all
77:09 - these things are there, you can also understand
what the view is DB is nothing but a data,
77:14 - bricks unit. If you want to click over here,
you will be able to understand what exactly
77:18 - the view is okay. And you will be able to
select a cloud and basically work with that
77:23 - perfect. till here, everything is fine. Let's
start, let's create the cluster. Now, once
77:28 - you you will be seeing that the cluster is
basically getting created. You also have lot
77:32 - of options over here like notebook libraries,
event logs, spark UI driver logs, and all.
77:38 - It's not like you just have, you will be able
to work with Python over here. Here you have
77:43 - lots of options. Okay. So suppose if I go
and click on libraries, and if I click on
77:47 - install new here, you will be having an option
to upload the libraries, you can also install
77:52 - the libraries from pi pi from Maven, which
will basically use along with Java, and then
77:57 - you have different different workspace. So
here, what I'm going to do is that suppose
78:00 - you select by by and suppose you want to install
some of the libraries like TensorFlow, or
78:06 - probably want to go with Kara's, you can basically
write like this, probably I want a scale on,
78:10 - you know, so I can just get comma separated
and start installing them. Okay. But by default,
78:16 - I know I'm going to work with PI Spark, so
I'm not going to install any libraries. So
78:20 - let's see how much time this will probably
take. This is just getting executed over here.
78:26 - And let's go back to my home. So apart from
this year, you will be also able to upload
78:31 - the data set and that particular data will
give you an environment like how you're storing
78:35 - the data in the loop okay. So before the cluster
is getting created, okay, now the cluster
78:41 - has got created here you can see pi spark
it is in running state. Now. And remember,
78:46 - this cluster only has one instance, if you
want to create multiple clusters, we have
78:50 - to use the cloud platform one which will be
chargeable. Okay? So in here, I'm going to
78:54 - click on export the data. Now see you guys
you can upload the data you can also bring
78:59 - from s3 bucket or you can also then bring
from s3 bucket. These are all things I'll
79:03 - try to show you. Then you also have dbfs you
know, and DB FF, you will basically be storing
79:09 - inside this particular format. Then you have
other data sources like Amazon redshift, Amazon
79:14 - kinases, Amazon kinases is basically used
for live streaming data. Okay. Then you have
79:20 - Cassandra, Cassandra is also a no SQL database
and JDBC lastic search so different different
79:26 - data sets, data sources also there will also
try to see with a set of partners integration.
79:32 - So they are also like real time capture in
the data lake and many more things out there.
79:38 - So you can definitely have a look onto this.
Now what I'm going to do is that I'm just
79:41 - going to click over here and try to upload
our data. Let me just see. Let me just upload
79:50 - a data sets. I'll just go to my PI spark folder.
So here is my PI Spark. So I'm just going
79:56 - to upload the test data set probably alright.
upload this test one. Now here you can see
80:02 - that the data set has been uploaded. Now it
is saying that create table with UI CREATE
80:07 - TABLE in Node back notebook. Suppose if I
go and click this, you know. So here you will
80:12 - be able to see this is the code, this is the
entire code to basically create a table in
80:18 - the UI. But what I really want to do is that
I don't want to create a table instead, I
80:22 - will just try to execute some of the PI spark
code, which we have already learned. And now,
80:26 - okay, so what I'm going to do, I'll just remove
this, I don't want it, I'll remove this, okay.
80:35 - Okay, let me read the data set. Now for reading
the data set. Over here, you will be able
80:39 - to see that my data set path is basically
this it is a CSV file. In full schema headed
80:45 - schema, all these things are there. So let
me remove this also. So let me start reading
80:50 - the data set. So by default spark is already
uploaded. So I'll write spark dot spark dot
80:58 - read dot CSV, I hope so it will work and for
the first time, remember, this is my file
81:06 - location. file location. Okay, bye underscore
elevation. And then I will also be using two
81:16 - more option one is header is equal to true
and then I have inferred schema is equal to
81:23 - true. Once I execute this, now you will be
seeing that automatically. The first time
81:29 - when you're executing, it will say that launch,
launch and run so we are going to launch the
81:33 - cluster and run it so I'm just going to click
it failed to create reject request since the
81:37 - total number of nodes would exceed the limit
one, why this is there. Let's see if our clusters
81:43 - we just have one cluster. Okay, there was
some examples that have been taken over here.
81:50 - So let me remove one of them. Okay, let me
just execute this. Okay, I'll go over here.
81:57 - space, let me delete it. Okay. Perfect. Now,
I'll try to read this. Let's see. Again, it
82:10 - says failed to create the cluster reject request
rejected since the total number of nodes would
82:15 - exceed the limit of one and it is not allowing
us to execute more than one file I guess.
82:20 - So because of that, I'm just reloading it.
Let's see now. Now it has got executed see
82:28 - guys before they were two files. So because
of that, it was not allowing me to run I just
82:33 - real I deleted one file and I I reloaded one
file Okay. So now you can see that it is getting
82:38 - the run now. Okay, you can also press Shift
Tab to basically see some hints and all the
82:44 - same like how we do it in Jupyter Notebook.
Now here you will be able to see that my file
82:49 - will be running absolutely fine. And it shows
it shows this df it shows that okay, it is
82:54 - a PI spark dot SQL dot data frame raw data.
Now, let me just execute the other things.
82:59 - Now suppose if I want df dot read, see I'm
just using that tab feature print schema,
83:07 - if I go and see this here, you will be able
to see find out all the values right. So in
83:11 - short, this is basically now running in my
instance of the cluster right, I will be able
83:16 - to upload any huge data set, probably a 50
gb data set also from s3 bucket and not right
83:21 - that I'll try to show you how we can do it
from s3 bucket in the upcoming videos. But
83:25 - what I am going to show you guys in the upcoming
future will try to run all this kind of problem
83:29 - statements through the data bricks so that
you will be able to learn it Okay. Now, let
83:33 - me just go and do one more thing. So this
is my df dot show. Okay, so, this is my entire
83:40 - data set. So probably I will just want to
select some column, I can actually write the
83:44 - DF dot select and here I just want to say
salary dot show I'm just selecting salary
83:53 - dot show here you will be able to see so everything
that you want to do you will be able to do
83:58 - it and remember over here you will be able
to find out around 15 gb and you can definitely
84:03 - perform any kind of things okay. Here also
you have same options like how we have within
84:10 - you know in Jupyter Notebook, every option
is that you will be able to find out all this
84:15 - particular options in Jupyter Notebook also,
right. So, this is basically running in 15.25
84:20 - gb, two cores, okay in that particular cluster,
you have two cores, then you have spark 3.1
84:27 - point One Spark 2.12 and you will be able
to see all this particular information. So
84:32 - what I would like to want Guys, please try
to make a specific environment for you, and
84:37 - then try to start it, try to keep everything
ready. And from the upcoming videos, we will
84:42 - try to see how we can execute how we can implement
problem statement how we can implement different
84:48 - algorithms. I've already given you the introduction
of data bricks in the last class. I hope you
84:54 - have made your account and I hope you have
started using it. If you don't know how to
84:58 - make an account, please watch Logitech tutorial
seven, the entire playlist link will be given
85:02 - in the description. Now this is my databricks
community account. Remember, in the community
85:08 - version, we can only create one cluster. I'll
also be showing you in the upgraded versions
85:13 - probably in the future, I will be buying it.
And I will try to show you how you can also
85:17 - create multiple clusters, unlimited clusters.
But for that, you also need to use some clouds
85:22 - like AWS or Azure. Now, first of all what
data set I'm going to use. So this is the
85:26 - entire data set that I'm going to use guys,
this data set is called us tips data set.
85:32 - So that basically means people who are actually
going to the restaurant what tip they have
85:36 - actually given based on the total bill, or
I can also go and solve this particular problem
85:42 - based on all these particular parameters,
what should what probably is the total bill
85:46 - that the person is going to pay? Okay, so
this is the problem statement that I'm going
85:50 - to solve. Now here, you can see this is a
multi linear regression problem statement.
85:54 - Here, you have many, many features, right?
So let's proceed. Now first of all, what I'm
85:58 - going to do, I'm just going to click to the
browse, and I'm going to upload this particular
86:02 - data set. Now in order to upload this particular
data set, I have this particular data set
86:07 - in my path. So probably I'll also be giving
you this particular data set, so don't worry
86:12 - about it. Oh, let me just quickly, just a
second, let me just upload the data set over
86:19 - here. Okay. By Spark, okay, so here, you can
see that this is my data set, which I'm actually
86:27 - uploading tips. So let me open it right. Now
here, you will be able to see that your tips
86:33 - data set will get uploaded, you know, in this
DB Fs directory. So here you will be having
86:39 - something like file stores slash tables. Okay.
Now what you can actually do now, let's go
86:45 - and click on this dvfs. And here you can see
and file stores, probably you can also click
86:51 - on tables. Here you have the steps dot CSV,
I've also uploaded dissolve data sets in my
86:57 - previous videos, probably I was just using
this, okay, but here, I'm just focusing on
87:02 - tips dot CSV. Now what I'm going to do over
here, let's go and do the first step. Remember,
87:07 - the first step in data bricks is that we need
to create the clusters, okay? And create a
87:14 - cluster. Right now, by default in the community
version, data bricks actually helps you to
87:19 - create a cluster, just a one single cluster,
okay. But if you're using the paid version,
87:24 - the upgraded version, it will actually help
you to create multiple clusters if you have
87:28 - the access of AWS cloud. So I'm just going
to click on the cluster, let me create a new
87:33 - cluster. So I'll say this is my linear regression
cluster, okay. And then I'm going to use this
87:40 - runtime 8.2 scalar, this is there. And we're
just going to click the cluster and remaining
87:44 - all things will be almost same in this cluster.
In this instance, you'll be getting 15 GB
87:49 - memory and all the other information here,
you can check it out. You can also be getting
87:53 - two cores, and one I dB. Okay, so which I've
actually already discussed in my previous
87:58 - session, so I'll go and click on cluster.
This will take some time. And remember, guys,
88:03 - if you really want to use any kind of libraries,
just click over here and install those libraries,
88:07 - which they want. Like suppose if you want
to use Seabourn, you want to use kiraz, you
88:11 - want to use TensorFlow. So here you can basically
type this along with the versions and you
88:15 - will be able to install it okay. But right
now, I don't require any libraries under shown
88:19 - to use PI spark that is my main aim. So guys,
click on the cluster over here. And here,
88:24 - you can see that probably after a minute,
this particular cluster is actually created.
88:28 - Okay. Now, again, go to the home, what you
can do, you can create a blank notebook, I've
88:33 - already created one notebook, so that I have
the basic code written. So I'm just going
88:37 - to open this, and let's start this particular
process. Now first of all, I have something
88:42 - called as file location, I know my file location
is basically tips dot CSV, the file type is
88:49 - CSV. And then I'm just using spark dot read
dot CSV file location header is equal to true
88:52 - info, schema is equal to two. And let me just
write df dot show, this will actually help
88:58 - me to check the entire dataset. Okay, so I'm
just going to execute it in front of you.
89:04 - And let's make it line by line I'll try to
write down all the all the codes, it will
89:08 - definitely be helpful for you to understand.
So please make sure that you also type along
89:13 - with me to understand the code much more better.
Okay, so here now I'm going to execute this.
89:19 - Now here, you will be able to see my, my clusters
will start running. Okay. And then you can
89:25 - see waiting to run running the command, probably
we will be able to see it and just zoom out
89:30 - a little bit so that you'll be able to see
properly. And again, guys for the first time,
89:35 - if you're starting this particular cluster,
it will take time. Okay, so spark jobs it
89:39 - is running. And now you will be able to see
my data set. That is my tips data set, which
89:44 - is uploaded in this specific file location.
So this is my entire dataset, total bill,
89:49 - tip, sex, mocha date, time size, perfect.
Now let's go to the next step. What I'm going
89:54 - to do, I'm just going to write df dot print
schema. So I can Oh So, you stab, you know,
90:01 - it will be able to load this entire thing.
So now here you can see that this is my entire
90:06 - features total bill, tip, sex smoker day time.
So here is all your features like double,
90:13 - double sexy string smoker is string, day string
time string integer. Now remember you may
90:20 - be thinking Krish, why I am actually doing
this in databricks to just make you understand
90:25 - how this will basically run in the cluster.
Right now I just have one single cluster guys,
90:30 - that basically means that the maximum ram
in this particular cluster is somewhere around
90:33 - 15 gb. But just understand if you're working
with 100 GB of data, and what happens, this
90:37 - kind of processing will get split in multiple
clusters, right. So in this way, you'll be
90:42 - able to work with big data also in the upcoming
things right. Now this is I think that's right.
90:48 - Now let's go and try to understand over here,
which is my independent feature, my independent
90:52 - feature is my tips feature sex smoker day
time and size. And my dependent features,
90:57 - basically total bills. So based on all these
particular features, I need to create a linear
91:02 - regression algorithm, which will be able to
predict the total bill. So let's go ahead,
91:07 - now over here, I'm just going to write df
dot columns. So if I want to check my columns,
91:11 - this is my columns over here. So I can see
this is my exact columns, this many columns
91:15 - I actually have. Now, one thing about this
particular feature over here, guys, you have
91:20 - columns like sex, smoker, day, time, right?
These all are categorical features, right?
91:26 - And probably, you know, this category of features
needs to be converted into some numerical
91:31 - values, then only my machine learning algorithm
will get will be able to understand it. So
91:36 - let's see how to handle category features.
So here, I'm just going to write a comment.
91:41 - Okay, handling categorical features, right.
Now, I'll try to show you how to handle this
91:51 - kind of category features. Now, one way in
PI Spark, and obviously, we know, in normal
91:56 - SK learn, you know, we try to use one hot
encoding, we try to use ordinal encoding,
92:02 - we try to use different kinds of encodings
in this. And similarly, we can use that same
92:05 - encoding over here also with the help of pi
Spark. So for this particular process, we
92:10 - have something called a string indexer. So
I'm just going to say from pi spark radar,
92:16 - from pi spark.ml dot feature, okay, I'm going
to import something called as string indexer.
92:25 - So, I will be using the string indexer, the
string indexer will actually help us to you
92:33 - know, basically convert our string category
features into some numerical features, numerical
92:38 - features basically is ordinal encoding. Like
suppose if I have gender like male or female,
92:43 - it will be shown as zeros and ones. And over
here you will be seeing I so most of the categories
92:48 - over here are ordinal encoding. Now, you may
be thinking, one hot encoding, what is the
92:52 - process that I'll try to show you in the upcoming
videos with different different machine learning
92:55 - algorithm? The reason why I'm making it because
it is better to learn one thing at a time,
92:59 - right? So we'll I'll try to show all those
kinds of examples also, is now let's proceed
93:03 - and try to see that how we can convert this
category features like sex, smoker day and
93:08 - time, probably time is also category feature,
see over here. So if I see this all features
93:13 - over here, let me do one thing, okay. Let
me just write df dot show. So this is my entire
93:22 - features. Quickly, if I go and see this is
time is also category feature. So quickly,
93:28 - let's go ahead and try to see how we can basically
use this, let me delete this thing also, or
93:33 - let me just write it once again. So I have
actually, you know, imported this library
93:38 - called a string indexer. Now what I'm going
to do over here is that, let me write our
93:42 - indexer object saying as this and I'll write
string indexer. And here, first of all, I
93:49 - really need to provide which all our category
features. Now remember, in this string indexer,
93:55 - if I go and press Shift Tab, probably over
here, here, you will be able to see I have
93:58 - to give input columns. So let me touch here,
I have to give input columns, and I have to
94:03 - give output columns. I also have options of
providing input columns as multiple columns,
94:08 - and the output columns as multiple columns.
So let me try both the thing Okay, so over
94:14 - here, first of all, let me try with input
columns. So here in the input columns, I will
94:18 - provide my first value. Now suppose I want
to really convert the sex column into into
94:24 - my category feature. So here I'll write output
column. Okay. And here, I'll say, sex underscore
94:30 - indexed. Now here what we are actually doing
guys, here, I'm actually giving my sex column
94:35 - and this sex column will be converted into
an ordinal encoding with the help of the string
94:40 - indexer. Okay, now in the next step, what
I will do, I will just write df, okay, probably
94:44 - I'll just use df. Or what I can do, I can,
I can just create another, probably I can
94:50 - create another data frame. So I'll write df
underscore are probably c because I don't
94:54 - want to change the DF and again, run that
particular code. Now, I'll say indexer dot
95:00 - fit, okay, so I can definitely use fit. And
then I can use transform. So here also, it
95:06 - is almost same like that only guys fit underscore
transform. and here also i'm going to use
95:11 - df. Okay. And then if I go and see df.df underscore
r dot show, here, now you'll be able to see
95:19 - that the sex column, one more sex underscore
index column will be created, and it will
95:25 - have the ordinal encoded values in this particular
column. So let's go and see this. So once
95:31 - I execute it, perfect, I think it is running
properly, it will take time. Now here, you
95:37 - can see that I'm having one more column, which
is called a sex underscore index, wherever
95:41 - the female value is that the value is one
wherever the male value is that the value
95:45 - is zero, right? So we have handled this particular
column. And we have basically converted this
95:49 - character feature into the ordinal encoding.
Now, still, we have many features. So what
95:54 - I'm going to do, I'm just going to use this
indexer again, okay. And probably I'm just
95:59 - going to write over here, multiple columns,
I will specify. So first column, I've already
96:04 - changed it. So I'm going to change this into
something else, sex instead of sex that will
96:10 - become smoker. Okay, smoker. But I showed
you guys, instead of writing input columns,
96:16 - now I have to write input columns, right.
So in this multiple columns when I'm giving,
96:21 - so this is smoker, then I have one more feature.
If I see day, day and time, day, and time
96:29 - is more two features. So I'm just going to
right over here, day underscore. So guys,
96:35 - now how I've written smoker day and time,
similarly, I will be writing three more columns
96:40 - over here. So the first column should be because
I'm going to create the ordinal encoding.
96:44 - And I probably create a new column over here.
So this will be my smoker underscore indexed.
96:49 - Okay, I'll close the braces over here. My
second feature will basically be de underscore
96:54 - index, right? Or, and my third feature will
probably be our time underscore index. So
97:02 - here, I'm just going to create three more
features. And then I'm giving index dot fit
97:07 - or df underscore R. Okay, because now I have
my new data frame, and then I'm going to say
97:14 - df underscore dot show. Now once I execute
it guys, I hope should not give us an error,
97:19 - okay, it is saying that invalid parameter
value given for Param output columns could
97:24 - not convert last restore, so I have to make
this as output columns. Okay, so that was
97:29 - the issue. Right? So now you'll be able to
see that it's executing perfectly fine. Now,
97:34 - here you have all the features available six
underscore index smoken underscore index,
97:39 - de underscore index and time underscore index
and all you can see over here this ordinal
97:43 - encodings like 012 right and we have now converted
this all string values into this kind of all
97:51 - category values that are available in this
feature into numerical values. Now, my model
97:56 - will definitely be able to understand Okay,
now we have done this guys, now, let's proceed
98:01 - what is the next step that we basically do
that we are going to discuss now the other
98:05 - steps are pretty much easy because we have
already created this specific data set. Now
98:09 - what we have to do is that there is something
called as vector assembler. Now always remember
98:15 - guys in PI Spark, you know, whenever we have
all this particular feature, we need to group
98:20 - all the independent features together, and
the dependent feature separately, okay, so
98:24 - guys, we'll write from from pi spark.ml dot
feature, I'm going to import something called
98:34 - as vector assembler. Right? So I'm just going
to use this vector assembler, this will actually
98:40 - help us to group independent features together
and the dependent features separately, so
98:46 - let me just go ahead and write vector assembler
and then I'm going to initialize this the
98:50 - first parameter I have to provide is basically
my input columns, here are my input columns,
98:56 - what are the input columns I have? Let me
just see before this, let me quickly do one
99:03 - thing is that let me make a cell okay. Cell
up create a cell let me just removed this
99:13 - and probably just let me write you know, df
underscore r dot columns. Okay, so how many
99:23 - number of columns we have. So I have all my
information with respect to the columns. So
99:27 - my input column over here, the first thing
that I am definitely going to provide is my
99:31 - tip column. Because tip is required tip is
the first independent feature and it is a
99:36 - numerical feature then I have something like
six underscore indexed Okay, so I'm just going
99:41 - to copy this paste it over here. And this
is my another input feature. And remember
99:47 - guys, we really need to follow the order.
So now my third feature is basically smoke
99:52 - index. And before this also I can also specify
size okay. So I will be specifying size, size
99:58 - six index smoker index. Okay, and then probably
I'll also create a index. Okay, they index,
100:07 - comma, I'm just going to use time index. Okay,
so this is our, these are all my independent
100:14 - features. And with respect to this, now remember,
this will be grouped together. And I also
100:20 - have to say that if this is grouped together,
let's create a new feature, and untie and
100:25 - name this entire group. Okay, so here, I'm
just going to say, output column is equal
100:31 - to, and here I'm just going to specify this
are my independent features. So I'm going
100:35 - to name this entire thing as my independent
feature pretty much simple. Now, let me do
100:42 - one more thing, let me create a variable called
as feature assembler, so that we will be able
100:47 - to transform this value. Okay, so feature
assembler is equal to vector assembler, and
100:51 - here I have to provide my input columns and
the output columns pretty much simple, pretty
100:55 - much easy. Now the next step that I'm going
to do is that right output is equal to I'm
101:01 - just going to say dot transform, because I
really need to transfer and this needs to
101:05 - be transformed from my df underscore art,
okay. So, let me just execute it. Now. Now,
101:11 - here, it has got executed here you can see
the entire output, all these things are created,
101:16 - these are my independent features. Now in
the independent and why we need to create
101:20 - this independent features that is the that
is the specification that is given in PI Spark,
101:24 - always remember, we need to create a group
of features and probably a list, all these
101:28 - independent features will be done together.
Now, if I go and see my output dot show, here,
101:36 - now you will be able to see, I will be able
to see one more feature, which has this, or
101:42 - let me just write output dot select, because
probably all the features have been grouped
101:48 - together. And it is very difficult to see
all the features in just one screen, I'm just
101:52 - going to take this independent features and
just click on dot show. Now once I do this,
101:57 - here, you will be able to see all this particular
features are there. Remember, this needs to
102:05 - be shown in the same order. The first feature
is tip, then size six, underscore index, smoker
102:10 - underscore index, de underscore index, time
underscore index. So these are my independent
102:14 - feature. Now I just have one feature. And
over here, you will be able to see that it
102:18 - is just having a list of all the features
like this. And this is the first major step
102:23 - to create. Okay, now let's go to the next
one. Now I know what is my output. Now what
102:28 - I'm going to do is that out of this entire
output, if I go and see my output, output
102:34 - dot show here, you will be able to see all
the features are available right in output
102:39 - dot show. So here we'll be able to see all
the features are available. Now you know,
102:43 - which is the output feature, right? So this
is my dependent feature. And this independent
102:49 - features are my independent feature. So what
I'm going to do now I'm just going to select
102:53 - output, or I'll say this is basically my finalized
data. And I'm just going to pick up two columns,
103:00 - that is output dot select. And inside this,
I'm just going to give my two features, which
103:07 - I'm going to say one is independent features.
Okay, indie band features, I hope that the
103:15 - name is right. Otherwise, I'll just confirm
it once again. So let me click it over here.
103:23 - Independent features, and one is total underscore
bill. Perfect, comma, total underscore bill.
103:32 - Okay. Now if I just go and execute this, now
I'm just picking up to features right from
103:37 - this. Now if I go and find out finalized data
dot show, now I'll be able to see two important
103:43 - features, that is independent features and
total bill. Remember, this is all my independent
103:49 - features. And this is my dependent feature.
This is very much simple till here. If it
103:54 - is done, guys, the next step is basically
I'm just going to copy and paste some code,
103:58 - I'm just going to apply the linear regression.
So first of all, from pi spark.ml dot regression,
104:02 - I'm going to import the linear regression.
And then I'm going to take this entire finalized
104:06 - data and then do a random split of 75 and
25%. Okay, and then in my linear regression,
104:12 - I am just going to provide my independent
features as my feature column. This is two
104:17 - parameters which to be given in linear regression,
one is feature column here I'll be providing
104:21 - independent features. The second one is basically
total bill which is my dependent feature.
104:25 - And now I will just do a fit on train data.
So once I do it, my regressor model will get
104:31 - created. And probably this will take time.
Now here you can see all the information,
104:35 - amazing information you are getting with respect
to train and test. Okay, and remember guys,
104:40 - whatever group you have made this independent
feature, this is in the format of UDT. Okay,
104:45 - you can see the full form of UDP, that is
not a big problem, I thought, okay, now I
104:49 - have my regressor. So what I'm going to do,
I'm just going to say regressor dot coefficient,
104:55 - since this is a multiple linear regression,
so I'm just going to use regular So dot coefficient
105:00 - and these are all my parameters are different
different coefficients because I have around
105:05 - six parameters. So these are all the six different
different coefficients. Always remember in
105:09 - a linear regression, you will be having coefficient
based on the number of features. And you will
105:13 - also be having intersect. So here I'm just
going to intercept. Okay, so this is basically
105:22 - my intercept, that is point 923. Okay, so
here you have both the information, now is
105:28 - the time that we will try to evaluate, evaluate
the test data. So here, I'm just going to
105:35 - say test. And this is basically my predictions,
right? So here, let me write it as something
105:41 - like this. So predictions, okay. prediction.
And what I'm going to do, I'm just going to
105:50 - write pred underscore results. Results is
equal to this one. And this will basically
105:57 - be my results, okay? Or test is not defined
why test is not defined, because there should
106:04 - be test data. I'm actually sorry, okay. But
it's okay, you will be able to get so small,
106:10 - small errors, okay. Now, if I really want
to see my price results, just go and see red
106:15 - dot predictions, they'll be something like
predictions dot show, okay. If you write like
106:22 - this, here, you will be able to get the entire
prediction. Okay. So remember, in this prediction,
106:28 - this is your independent feature, this is
your total bill, this is your actual value.
106:31 - And this is a prediction value, actual value
prediction value, actual value and prediction
106:35 - value here, you can actually compare how good
it is, you know, by just seeing your total
106:40 - bill and the prediction value, pretty much
good, pretty much amazing, you are able to
106:43 - see the data, I'm just going to write my final
comparison. Okay, final comparison. Perfect.
106:52 - I'm very much good at it, you can see it.
Now let's see some of the other information
106:56 - like what information we can basically check
it out. From this, we can we have a lot of
107:01 - things go probably you want to check out the
R square. So what you can write, you can basically
107:06 - write a regression.if, I press tab, this coefficient
intercept, then you have lost, then there's
107:13 - also something called an R squared. If I go
and execute this, this is basically my r squared.
107:18 - Or let me just write it down. I think, prediction
predictions. I don't think so r square is
107:29 - where Let's see whether we'll be able to see
the R squared value or not. In just a second,
107:35 - I'm just checking out the documentation page.
Okay, oh, sorry, I don't have to use regressor
107:44 - over here. So here I will be using prediction
dot results. And let me compute the R square.
107:49 - So this is my r squared. Similarly, you can
also check out prediction results dot mean
107:55 - absolute error. So you have mean absolute
error. You also have prediction underscore
108:00 - result dot mean, squared. So all these three
values, you can definitely check it out. So
108:06 - here is your mean absolute error here is your
mean squared error. So these are my performance
108:10 - metrics that I can definitely have. And whenever
you guys whenever you face any kind of problems,
108:17 - just make sure that you check the documentation
in Apache Spark em lib documentation. Now
108:22 - in this way, you can definitely do this entire
problem statement. Now I'll give you one assignment
108:26 - just try to Google it and try to see how you
can save this particular file probably in
108:31 - a pickle format or probably in a temporary
model pickle file. You know, it's very, very
108:36 - simple, you just have to use the regression
dot save but try to have a look and try to
108:40 - see that how you can save this particular
pickle file. Now this was all about this particular
108:44 - video. I hope you like this now just try to
solve it any other problem statement. Try
108:48 - to do this. In the upcoming videos. I'll also
try to show how you can do one hot encoding
108:52 - and probably will be able to learn that too.
So I hope you liked this particular video.
108:55 - Please do subscribe the channel if you're
not subscribed to either make sure to have
108:58 - a great day. Thank you. Bye bye