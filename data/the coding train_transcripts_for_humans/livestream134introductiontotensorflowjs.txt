With timestamps:

00:03 - good morning welcome here we go oh boy
00:10 - it's another Friday and that means it's
00:14 - time for the coding train now what's
00:17 - going on today here's the thing I have
00:19 - something to admit something to confess
00:23 - it the date is April 27 to Friday April
00:27 - 27th I as you may or may not know happen
00:31 - to have this full-time job working Here
00:33 - I am in a closet because I have a
00:36 - building inside of New York City which
00:38 - is part of New York University Tisch
00:41 - School of the Arts ITP and soon to be
00:45 - ima next year as well and this is
00:48 - probably our busiest time and so this
00:52 - week has been a little bit crazy next
00:54 - week will be crazier still the week
00:56 - after crazier so the things are gonna be
00:58 - a little bit in flux here for the next
01:01 - three or four weeks I'm just checking
01:03 - the chat few people are actually seeing
01:05 - me but I'm excited about something which
01:10 - I have a plan for this summer which is
01:13 - not to break this elbow you've been
01:16 - watching my channel you might know that
01:18 - this elbow was broken last summer now it
01:20 - works pretty well I would say 80 90 90
01:24 - some percent of what it used to be and
01:27 - so I actually took a two-month hiatus
01:30 - last summer but I don't plan to take
01:31 - that hiatus this summer I spent I plan
01:33 - to take to spend June and July focusing
01:37 - on something called
01:51 - can you even hear that I don't know if
01:52 - you can hear that machine learning a
01:58 - JavaScript and primarily the primary
02:01 - they're the primary vehicle for this
02:03 - that I'm going to use is oh no no no
02:06 - don't look at that spoiler alert I have
02:09 - a little little stick planned that I
02:11 - didn't rehearse I probably should
02:12 - reverse that tensorflow gjs so what I'm
02:17 - going to try to do today I have a
02:19 - limited amount of time I have about an
02:21 - hour to and I'm gonna try to just sort
02:24 - of introduce a little bit about what
02:25 - tensorflow Jas is and my sense of it in
02:30 - this world as well as talk about this
02:34 - other library called ml5 which is a
02:36 - project being developed here at at NYU
02:40 - and ITP and Tisch School of the Arts
02:42 - have to remember to name everything and
02:45 - so on today I really want to set the
02:47 - stage for what I hope will be many
02:49 - tutorials and adventures throughout this
02:53 - summer okay so where to begin well the
03:00 - other thing is I'm kind of I'm wrapping
03:03 - up here this course called the nature of
03:06 - code spring 2018 I was kind of like
03:09 - trying to remember is there anything on
03:11 - here that I meant to do a coding
03:14 - challenge or tutorial about oh and I
03:16 - didn't put my cloaking device on this
03:18 - extra laptop let me see if I can find
03:20 - that oh here it is
03:24 - by the way so I have there's a little
03:26 - mystery we have to solve which is that
03:31 - there's been some activity in this room
03:34 - some things are in different places this
03:39 - should go here oh boy but this I need
03:45 - another piece of paper my my original
03:49 - cloaking device is missing
03:54 - but here is some more I'm very tired
04:04 - today by the way yesterday was national
04:07 - take your children to work day I had a
04:14 - really fun time learning about all the
04:17 - different things that happen here at
04:18 - Tish kids were here they were doing like
04:22 - teepees stuff I learned about making
04:24 - sound effects for the movies oh it was
04:26 - great get anything done or get do any of
04:31 - the preparation that I had hoped and
04:33 - imagined to do for this life so this is
04:37 - gonna be a little bit of a bust this
04:38 - morning I would like to do I was trying
04:40 - to come up with like could I at least do
04:42 - like some interesting short coding
04:44 - challenge oh I think I might have an
04:46 - idea for one so somebody should should
04:48 - give me a knock on the head virtually at
04:52 - about I shouldn't do this towards the
04:55 - end I was gonna say maybe like 11:45 in
04:57 - an hour and I should maybe move
04:59 - stretches to a kind of a fun coding
05:01 - challenge alright so I was saying how
05:03 - things in this room by the way yes I was
05:07 - learning fully sound effects yesterday I
05:10 - was trying to remember what the name of
05:11 - it was fully I learned up that we had
05:13 - like a thing of dirt and there was like
05:17 - fake grass stuff and there was we made a
05:20 - raindrop sound effect with like a little
05:23 - like piece of leather and just doing
05:24 - this on it I don't really think like I
05:26 - want to go to the film school here
05:28 - that's really fun what else we learned
05:33 - it but some about photography there
05:36 - drama department we did some improv
05:38 - games that was fun a bunch of seven to
05:40 - twelve year olds doing improv games okay
05:45 - and oh wait hold up so I'm looking at
05:49 - Twitter here supposedly the people are
05:51 - tweeting that I'm here to talk about
05:53 - tensorflow DJ s which I am I should give
05:56 - a fair warning I have a very bad habit
06:00 - of wasting about 45 minutes at the
06:02 - beginning of my live streams before
06:04 - getting to any actual useful education I
06:06 - don't know if there is
06:07 - any actual useful educational content
06:08 - ever but if there would at least when
06:11 - I'm attempting to do so because I want
06:13 - it but so before I'm gonna try to get to
06:15 - this in a minute but before I begin so I
06:20 - was saying how like things are in
06:21 - different places here in this room
06:23 - it was the this I found here written on
06:28 - the whiteboard dan Shipman is a figment
06:31 - of our collective imagination also
06:34 - kittens so now we have a mystery who
06:47 - wrote this was this written this week is
06:52 - that really how my name is spelled where
06:56 - are the kittens are there kittens I
06:58 - would like to see the kittens are they
07:02 - small kittens cute kittens
07:04 - [Music]
07:06 - am I really a figment of your collective
07:09 - imagination to exist we will solve this
07:14 - mystery the mystery of who repetition
07:16 - though on the whiteboard in this room
07:18 - who has access to this room there's a
07:22 - door here the door opens people can come
07:25 - in people can go out but it is locked in
07:30 - order to enter the room you must have an
07:37 - NYU ID keycard that is authorized for
07:45 - the thing that you put the ID on so that
07:47 - clicks open yeah it didn't keep going
07:51 - you didn't get to the good part oh oh
07:55 - well oh the bad house good ideas come on
07:58 - bad okay do you want even know what that
08:06 - I that reference is so dated but it's
08:09 - still in my head look no idea what that
08:12 - what that was but wait why did that hold
08:15 - on a sec here I didn't go to the we
08:19 - didn't really uh what does it say they
08:28 - know hold on let me just put this back
08:29 - here
08:30 - my here my all right well I'm gonna
08:34 - workshop that you know I took this like
08:35 - theater class yesterday with a bunch of
08:37 - seven to twelve your I didn't actually
08:38 - take a class I sat in the corner and
08:39 - watched because I was a parent volunteer
08:48 - okay um so let me move on now and let's
08:53 - try to let's erase this note let's erase
08:58 - this note I'll read it to you one more
09:01 - time Dan Schiffman is a figment of our
09:05 - collective imagination also kittens
09:17 - alright okay here we go
09:22 - oh boy come back over here now I need to
09:28 - get set up with a few things here I want
09:30 - this website this website right what I
09:38 - need to actually look for here is let's
09:40 - go to this YouTube channel called the
09:42 - coding train and let's go down to it's
09:49 - not here
09:51 - neural networks and machine learning so
09:57 - this is what I've got so far and what I
10:01 - have always said in working on these
10:04 - video tutorials what's the idea here is
10:08 - that I want to talk about the idea of
10:15 - artificial intelligence in the broadest
10:17 - sense machine learning in the broadest
10:21 - sense deep learning in the well I think
10:27 - people are turning out for a second
10:28 - artificial intelligence and machine
10:29 - learning in the broadest sense then I
10:31 - want to create a set of tutorials where
10:35 - I implement simple examples to
10:38 - demonstrate particular topics in the 11
10:45 - neuro evolution playlist should be in
10:46 - here as well and so that's what these
10:48 - playlists are doing so for example if if
10:51 - I come here into this playlist 10 for
10:56 - the don't whoa this is very confusing
11:01 - and I look at this playlist this is an
11:05 - entire set of video tutorials that I
11:07 - have made to discuss the idea of a
11:10 - neural network building something called
11:13 - a simple perceptron which is a model of
11:15 - a single neuron no network what it means
11:18 - to then take that perceptron and make a
11:20 - MOLLE multi-layer perceptron with
11:22 - say Network then look at some of the
11:25 - matrix math that is behind the different
11:29 - algorithms that happen when data is fed
11:31 - forward through a network as well as
11:34 - when the error is sent backward through
11:37 - the network to adjust the network and
11:40 - then finally build a neural network
11:42 - library and ultimately then make some
11:48 - examples for example this I made an X or
11:53 - coding challenges in that neural network
11:55 - library
11:56 - I made this doodle classifier which is
11:58 - using the Google quick-draw data sets so
12:01 - these are the things that I have done to
12:03 - date and more recently if I come here to
12:08 - playlist number 11 boy what you will see
12:13 - here is I went off on a slight different
12:17 - direction for look at training a neural
12:19 - network with something called the
12:20 - genetic algorithm for the purpose of
12:24 - reinforcement style learning to Train
12:26 - and so I what example and actually there
12:30 - are five parts here so those have to get
12:31 - added to train an agent to play this
12:37 - flappy bird clone game thing okay so
12:40 - that's what I've done so far but I
12:44 - really wish I could have gotten
12:45 - organized yesterday look at this I'm
12:47 - logged in as myself so it has a playlist
12:49 - of liked videos fascinating let me go
12:52 - back one more time
12:53 - and so here's the thing it is time now
12:57 - for session 6 section 6 technically so
13:04 - chapter 11 should be here open what's
13:06 - really confusing about this let me try
13:08 - to I'm gonna try to map this out for
13:10 - myself I have two courses that I'm
13:14 - designing in my head now all these video
13:15 - tutorials that you know I want them to
13:17 - kind of stand alone and let people mix
13:19 - and match them as they please but in my
13:21 - head I have two courses I have nature of
13:25 - code and I have a course that I called
13:29 - it did a sort of version of this course
13:31 - at NYU
13:32 - called intelligence and learning okay
13:37 - so nature of code is a course where the
13:41 - first half
13:43 - I look at physics simulation and then to
13:49 - some extent also some like generative
13:52 - algorithms then this course moves to
14:00 - looking at neural networks oh I should
14:04 - put genetic algorithm this is a
14:06 - generative algorithms not to be confused
14:08 - with genetic algorithms and then after
14:13 - this the last topic of this course is
14:16 - now neuro evolution so this is playlist
14:22 - number nine for nature of code this is
14:25 - playlist number ten for nature code this
14:27 - is playlist number eleven for nature of
14:29 - code okay now my intelligence and
14:33 - learning course which doesn't really
14:34 - exist yet but I hope that maybe teach it
14:36 - next year and prepare a lot of and make
14:38 - a lot of video content for that over the
14:41 - summer it starts with this is kind of a
14:44 - we actually look so what it was like
14:47 - calling this so let me try to find it
14:49 - when I did it last year spring sixteen
14:55 - intelligence learning so this was a
15:01 - version of it that I did but it's it's
15:05 - really not okay so what did I algorithms
15:09 - and graphs so I started with also and I
15:12 - made a bunch of videos for these already
15:14 - this idea of algorithms graphs search
15:27 - and like kind of thinking about
15:30 - something called Big O notation I don't
15:33 - know if that's really if I do this
15:34 - course again if this fits exactly right
15:36 - but this is a sort of first part of that
15:39 - course so the reason why I put that
15:40 - there is with this idea of taking the
15:43 - broadest view
15:44 - of the idea of artificial intelligence
15:46 - what does it mean to create the illusion
15:48 - of intelligence or the actual practice
15:51 - and thoughtfulness of intelligence in a
15:54 - in computer software in programming with
15:56 - computation and so things like the a
15:58 - star path maze a star path finding
16:03 - algorithm to find the shortest path from
16:06 - bits start to end in a maze like graph
16:09 - structure is something that can fit into
16:11 - this
16:11 - so that was one then I did genetic okay
16:17 - okay okay
16:18 - so people are complaining in the chat
16:27 - that's fine uh maybe I should one of the
16:31 - things I actually did this is why people
16:32 - are complaining probably because I kind
16:35 - of promoted this a little bit I never
16:36 - really schedule or promote the live
16:39 - streams because I kind of I'm afraid for
16:41 - people actually tune in because I and I
16:44 - sort of said tensorflow dodgy a switch
16:46 - probably so I'm gonna get to that really
16:48 - soon
16:49 - what time is it 11:00 okay so the other
16:53 - part of this course then let me just
16:54 - move on is genetic algorithms and neural
16:58 - networks so these this crosses over and
17:01 - now this is what leads me to believe to
17:04 - what I want to do today and so if I look
17:08 - back at here session one session to
17:14 - session three session four is somewhere
17:17 - sufficient five who knows this by the
17:19 - way this needs a little work I need to
17:21 - clean up these organizational structures
17:24 - what I want to do is all of this stuff
17:28 - about genetic algorithms neural networks
17:30 - in neuro evolution all of that used my
17:32 - own JavaScript library which were in
17:36 - these two files neural net and NDIS and
17:39 - matrix chess and so that what I want to
17:42 - do here is now well I want to look at
17:47 - actual oh boy that turned off
17:55 - yeah let me just write it here
17:57 - tensorflow das so the point of me doing
18:02 - a lot of this stuff with just my own
18:05 - javascript code well a lot of that for
18:07 - me was my own just like let me try to
18:09 - figure it out try stuff make some
18:10 - educational content have people look at
18:12 - it see what what gets made
18:14 - this should give me a foundation and
18:17 - basis with which to understand and start
18:23 - to work with a more sophisticated
18:27 - machine learning framework made by
18:29 - someone else and the machine learning
18:32 - framework library made by someone else
18:35 - that I am going to use it's tensorflow
18:37 - das so the very first thing I'm going to
18:39 - do is talk about what is tensorflow Jas
18:43 - the sort of core API this layers API and
18:47 - also this other library that I'm working
18:50 - on with some folks here at ITP called
18:52 - ml5 so I'm gonna erase this in a second
18:55 - and get started then what I would like
18:56 - to do I mean I don't know if I can get
18:58 - through all this today but I want to
19:00 - look at the basic oh you know just boils
19:05 - are alert the idea here is that ml 5 is
19:07 - the sort of highest level layer let's go
19:11 - to speak on top of the layers api and
19:15 - tetra fluro Jas which will provide some
19:17 - simple and easy to use examples to do
19:21 - some common machine learning tasks but I
19:24 - and so in some sense the reason for the
19:26 - ml 5 library is to and I'm gonna repeat
19:28 - myself in a second so I should probably
19:29 - stop talking about this is - I know I
19:34 - want to make a lot of standalone videos
19:36 - and tutorials with just ml 5 but for me
19:39 - what I want to do is actually start with
19:41 - looking at some of the lower level stuff
19:43 - that's actually in tensorflow da Jas and
19:45 - build some examples so the two things
19:47 - that I want to do is I want to redo the
19:49 - XOR example which is a little bit of a
19:53 - trivial example but shows the sort of
19:55 - full story of a machine learning project
19:58 - where there's training data a model gets
20:02 - built and trained and then there is new
20:05 - data that comes in and the model makes
20:07 - guesses according to that
20:08 - new data we could visualize that and
20:10 - then I also want to do this that doodle
20:12 - classify are over again but using
20:14 - tensorflow das rather than my kind of
20:17 - goofy ridiculous toy javascript neural
20:21 - network implementation okay let me turn
20:23 - off notifications on my watch because
20:26 - they're bothering me let me come back
20:29 - over here and check the chat oh this
20:38 - turn this on and so we're going to get
20:42 - started so I'm gonna take a this is I'm
20:49 - gonna just want to leave this here so I
20:50 - can have this later to remember and now
20:54 - I am going to get started so first I
20:57 - need to erase this
21:05 - I needed to sorry than I if anybody
21:12 - watching was to do this part I needed to
21:15 - do that for my own sanity also Maxia
21:18 - who helps with video editing and
21:20 - managing the channel hopefully this
21:23 - might help you understand my thinking
21:25 - here so that we can reorganize some of
21:27 - these you know the websites and the
21:31 - different orders and playlists of
21:33 - different things okay
21:41 - okay it is 11 o'clock
21:55 - so seriously though this would be
21:59 - session six now I'm gonna consider this
22:17 - other place where you can find this
22:20 - stuff let's seal this show up here so we
22:25 - got some work to do on this website okay
22:28 - never mind if you by the way if you all
22:41 - haven't seen this emoji scavenger hunt
22:44 - project it's totally awesome I kind of
22:46 - want to play it right now but oh I'll do
22:51 - that a little aim here I'll do that the
22:52 - end okay so hold on there's actually a
23:01 - couple other things that I think would
23:03 - be useful to bring up here tensorflow
23:08 - dev summit tensorflow
23:12 - Jess
23:16 - YouTube I'm looking for the here we go
23:22 - this is what I'm looking for there we go
23:41 - okay okay and then we're gonna go do a
23:52 - little history lesson here okay what
23:54 - what what what
24:00 - this can't possibly be true oh I know
24:04 - what it is
24:05 - ah god I check this I was having some
24:09 - issues on this computer and I shut off
24:12 - WebGL which will really be a problem for
24:15 - this set of tutorials that I'm an
24:16 - attempt to do today where is that in in
24:21 - Chrome settings anybody know where are
24:31 - the worst of WebGL settings advanced
24:35 - system no wait isn't this all right in
24:39 - the right place hold on let's go to pro
24:42 - maybe I want preferences ah yeah no
24:44 - that's the right thing
24:49 - GL no somebody's gonna tell me I know
24:58 - GPU thank you I turns it off what I was
25:03 - ok GPU GPU where do I just where do I
25:08 - enable it no this is just the report
25:12 - [Music]
25:14 - hardware acceleration Hardwick van
25:16 - settings hardware acceleration okay
25:21 - advanced where's it van advanced
25:28 - advanced hardware Excel
25:37 - relaunch Chrome all right thank you
25:40 - everybody for your patience we need a
25:50 - picture of both Nikhil Daniel okay there
25:55 - we go we're back we're back everybody
25:58 - okay all right everyone flags that would
26:08 - have been another place thank you
26:17 - okay all right here we go everyone hello
26:37 - and welcome to another session of my
26:41 - street oddly organized perhaps hopefully
26:44 - somewhat organized set of videos about
26:47 - intelligence and learning so this where
26:49 - are we right now
26:50 - I am in the moment where I have finished
26:53 - a whole set of tutorials about neural
26:55 - networks and some basic machine learning
26:58 - types of things that one might do with
27:00 - the neural network comprehensive but a
27:03 - few demonstrations I built a little
27:05 - neural network library in JavaScript and
27:07 - went through some matrix math stuff so
27:08 - I'm ready finally at the time I've been
27:10 - saying this all along that I'm making
27:13 - this neural network library and kind of
27:14 - looking at how things work and trying to
27:16 - make some creative examples but later
27:18 - eventually someday I will use a more
27:23 - robust thoughtful well-designed
27:25 - framework as the guts as the as the
27:28 - foundation from which I will build all
27:31 - of my examples and projects I'll use
27:33 - somebody else's machine learning code
27:35 - and so today is the day that I'm going
27:38 - to start talking about doing that and
27:40 - the foundation that I will be using for
27:42 - almost all of these videos is a project
27:45 - called tensorflow tasks so let's discuss
27:49 - for a moment where where did tensorflow
27:53 - jas come from so you might have heard of
27:56 - something called tensors weird to write
28:01 - this down ten sir flow now first of all
28:06 - you might even be asking yourself huh
28:08 - why is it even called tensor flow what
28:12 - is this thing called a tensor and now
28:14 - this is gonna be really important
28:16 - because when I start to actually look at
28:17 - the code for tensor flow gosh yes
28:20 - there's going to be stuff in there
28:21 - called tensors and the tensor is
28:23 - actually a
28:25 - mathematical thing it's a structure that
28:28 - holds numbers in it and it's really
28:31 - basically I mean I should look at the
28:33 - Wikipedia page for tensor probably and
28:35 - it'll give you a good I'll link to that
28:37 - in this video's description
28:38 - but we've been I've been referring to
28:40 - things as vectors well we have scalars
28:43 - which is like a single number like three
28:46 - we have this idea of vectors which is a
28:50 - list of numbers like 3 1 4 etc and we
28:58 - also have this idea of a matrix or
29:00 - matrices if I'm being consistent about
29:02 - singular or plural this idea of a matrix
29:04 - and a 2-dimensional matrix might have a
29:08 - grid of numbers like 3 4 1 5 so a tensor
29:15 - is a structure a data structure
29:19 - essentially that really can store any
29:22 - n-dimensional version of these types of
29:24 - things so this is the and because the
29:27 - building blocks of any machine learning
29:29 - algorithm are matrices of numbers this
29:34 - idea of tensorflow
29:35 - let's flow with the tensors insert
29:39 - animation of me flowing down the river
29:41 - of tensors well that happened in
29:43 - post-production I seriously doubt it
29:46 - this is where the name tensor flow comes
29:48 - from so tensor flow is Google's open
29:52 - source machine learning library it is
29:56 - written you might be surprised to hear
29:58 - this because you might think ah tensor
30:00 - flow it's Python right well yes kind of
30:04 - sure there were people who are watching
30:06 - this who know more about me so if you're
30:09 - watching the recorded version of this
30:10 - check the video's description for called
30:12 - the correction I'll go look for all I'll
30:14 - try to met make any Corrections at the
30:16 - end but the tensor flow is actually a
30:18 - library written in C++
30:20 - it is a low-level C++ library with a lot
30:26 - of functionality for doing machine
30:28 - learning now the reason why you might
30:31 - have thought to yourself oh isn't it
30:35 - Python well there simply is a
30:38 - sort of bindings for pythons so to speak
30:41 - a Python a wrapper so to speak for pi so
30:44 - python being a programming language
30:45 - that's primarily used not primarily use
30:48 - but it's very popular in the world of
30:50 - data science it makes sense if you're a
30:53 - data scientist and working with data and
30:56 - you want to do some stuff with machine
30:57 - learning that you would and you're
30:59 - already in Python you'd want to be able
31:00 - to access something like tensorflow so
31:02 - every most all in every example that you
31:05 - would see working with tensorflow
31:07 - is you're just kind of operating the
31:10 - low-level tensorflow stuff from Python
31:12 - in fact there are also Java bindings for
31:17 - tensorflow and probably other languages
31:20 - as well and in another youniverse if all
31:23 - this JavaScript stuff had never happened
31:25 - oh it's travel back in time and stopped
31:27 - JavaScript from happening maybe what
31:29 - would our life be like should we try
31:31 - that but I don't know a better or worse
31:32 - I would probably be investigating here
31:35 - right now talking about the Java
31:37 - bindings for tensorflow in an attempt to
31:39 - maybe go and use them with processing
31:41 - and actually this is something that I
31:43 - really I know that Gough read hater who
31:45 - is the creator of the Raspberry Pi arm
31:49 - version of processing has done some
31:51 - investigation of this and this is
31:52 - actually something I really would like
31:53 - to do but that aside this project
31:59 - tensorflow has been around for quite a
32:01 - while let me go look and find out how
32:03 - long it's been around and then I'll come
32:04 - back all right thank you apparently
32:10 - there was a super chat from Sam Graham
32:12 - taking a little break here let's look at
32:14 - first look let's look at tensor
32:17 - Wikipedia to see if I got that right
32:25 - geometric objects that describe linear
32:28 - relationship between geometric vector
32:29 - scalars and other tensors examples of
32:32 - dot product okay so I kind of got that
32:34 - right I kind of got that right
32:38 - then tensor flow let's let's look that
32:43 - up open source math library machine
32:47 - learning bubble developed by Google
32:48 - brain for internal Google
32:50 - Zeus was released on November 9th 2015
32:52 - okay and let's say C++ where it where
32:58 - does it provides a Python ia API as well
33:02 - C++ but am i right about that that
33:05 - ultimately tensorflow itself is written
33:09 - in C++ even if you're using it in Python
33:10 - you're just kind of controlling the
33:13 - low-level math operations from Python is
33:15 - that right that's my understanding of it
33:26 - Haskell Java go rust boa and look at
33:29 - this
33:29 - March 30th 2018 okay all right I don't
33:34 - see anybody yelling at me Oh tensorflow
33:37 - documentation is oh wait okay first of
33:39 - all the keel is in the chat welcome
33:42 - thank you hello Nikhil oh boy oh no oh
33:44 - no I'm in trouble now Nikhil is one of
33:47 - them with danuel smoke off and many
33:50 - others that are part of the Google brain
33:53 - team and the big picture research group
33:55 - up in Cambridge at Google are the
33:57 - developers and creators of tensorflow
33:59 - digest which I'm going to get to in a
34:00 - minute anyway oh yes Alcott gives me a
34:08 - quote Google built the underlying
34:10 - tension flow software with the C++
34:11 - programming language but I know I'm
34:14 - usually not so here's the thing I'm so
34:15 - unsure of myself with all of this
34:17 - machine learning stuff it's very
34:19 - different for me to do these tutorials
34:21 - like the nature of code physics
34:24 - simulation and steering behavior stuff
34:25 - like I had worked on that those examples
34:27 - and educational materials for like years
34:31 - I'm flying blind oh I was really just
34:34 - trying to look up the year sorry matcha
34:37 - for creating this edit 2015 okay so it
34:43 - was started in 2011 and was open sourced
34:47 - in 2015 okay okay
34:53 - Nikhil is giving me positive positive
34:55 - feedback I very much appreciate that
34:58 - okay thank you
34:59 - thank you all right okay I'll have to go
35:06 - look that up tensorflow actually started
35:08 - as a proprietary project at Google in
35:10 - 2011 and it was open source under the
35:13 - Apache License in 2015 so I don't know
35:17 - we can say open sourced in 2015 and it's
35:21 - a it's a project developed by the Google
35:22 - brain team who as I've learned recently
35:24 - I am NOT an official representative of
35:27 - Google in any way so don't get all this
35:30 - stuff wrong you could be up there doing
35:32 - a fine job over there I thought that can
35:39 - stay from the live street but I just now
35:41 - I'm not gonna turn red in a second I'm
35:43 - getting very embarrassed let me let me
35:46 - let's let's just do that little bit over
35:48 - again sorry for you live viewers I
35:50 - really try to not do this but since I
35:55 - had this like break moment anyway okay
36:01 - so I'm back I had to look that up
36:04 - tensorflow was open sourced in 2015 so
36:08 - tasteful actually is a project according
36:09 - to Wikipedia started in 2011 was a
36:13 - proprietary machine learning library
36:15 - used at Google for doing all sorts of
36:17 - stuff with neural networks and deep
36:18 - learning and more and then it was open
36:21 - sourced in 2015 under the Apache License
36:24 - so here's the thing last year I actually
36:29 - spent some time making some Python
36:31 - examples in tensorflow and I wanted them
36:35 - to talk to JavaScript so what I actually
36:38 - did is I wrote something called a flask
36:40 - server which is a Python flaps kiss kind
36:45 - of like flask is to Python as node is to
36:48 - JavaScript I'm sure that's wrong in many
36:49 - ways and then what I did is I had my p5
36:53 - sketch talked to that flask server the
36:58 - flask server did Python stuff with
37:00 - tensorflow and then I could do machine
37:02 - learning tasks from within p5 and this
37:05 - is what I want to do I want to be able
37:07 - to demonstrate and make examples and
37:09 - show things about how in a
37:12 - beginner-friendly programming library
37:14 - like p5 or just didn't native vanilla
37:16 - JavaScript or using three Jas or
37:18 - whatever JavaScript world you live in I
37:21 - lived in the p5 world most of the time I
37:23 - want to be able to try to do some
37:25 - tensorflow East stuff and so this was
37:29 - the way I was doing it last year in the
37:31 - nature of code intelligence and learning
37:33 - course that I attempted to teach over
37:36 - the summer I think it was last summer a
37:39 - project appeared a project appeared and
37:43 - it was called deep learned is now this
37:47 - is a my sense of this project is that
37:50 - this was a speculative project the idea
37:52 - behind deep learned digests is haha can
37:55 - we do this kind of stuff
38:00 - in JavaScript and if so how so one of
38:06 - the things that's special about doing
38:09 - machine learning in today's modern era
38:11 - with tensorflow
38:13 - is in addition to this whole landscape
38:15 - of all this stuff where these operations
38:19 - that are written in C++ actually get
38:21 - executed they you have this question of
38:24 - do they get executed on the cpu or do
38:27 - they get executed on the GPU and why
38:30 - should we care about this
38:31 - well the CPU the processing unit the
38:36 - computer's processing unit is the C
38:38 - stands for go out so is a little thing
38:42 - that chugs along and kind of does most
38:44 - of the work that your computer has to do
38:45 - some time some time in days of your
38:49 - video games and special effects and
38:52 - graphics needed more and more processing
38:55 - and computing power so graphics
38:58 - processing units were created graphics
39:00 - processing units were created and
39:02 - optimized to work with pictures images
39:09 - pixels what are images they are matrices
39:14 - of pixels remember though is talking
39:16 - about how matrices are important to
39:19 - tensors and deep learning all of the
39:22 - mathematical operations that happen in
39:24 - Network our matrix based operations
39:26 - multiply these matrices together add
39:29 - these matrices sum these matrices past
39:31 - this activation function over this
39:32 - matrix that sort of stuff so the fact
39:35 - that over years and years and years that
39:37 - graphics processing units got optimized
39:40 - heavily to work with two-dimensional
39:42 - arrays of color information pixels it so
39:46 - happens that all this matrix stuff could
39:48 - be used with GPUs as well so this is
39:50 - really it's is why we deep loop the term
39:54 - deep learning from my point of view it's
39:55 - kind of in a way of like a rebrand of
39:57 - neural net machine learning with neural
40:00 - networks but now we live in an age of
40:02 - big data sets and really powerful GPUs
40:04 - and a lot of this modern research is
40:07 - coming from the fact that these older
40:09 - algorithms that we didn't think could do
40:10 - as much can do more now in the context
40:12 - of where we live now ok why am I saying
40:15 - this so how is this gonna work if we
40:19 - have a JavaScript implementation of
40:21 - tensorflow
40:22 - is the idea to just have another set of
40:24 - bindings so you're really just
40:26 - controlling C++ from JavaScript well
40:29 - that is certainly a possibility and I
40:31 - believe that exists or at least is in
40:33 - development there is a node dot J S
40:36 - package for working with tensor flow
40:39 - that actually connects directly to the
40:41 - C++ implementation it has a relationship
40:44 - to the attention flow just enough that
40:46 - I'm going to talk about here but that's
40:47 - not what I'm talking about here what the
40:48 - creators of deep learned is Nikhil and
40:51 - Daniel more information about them and
40:54 - the rest of the research teams that they
40:56 - work with in this video's description
40:58 - and want to miss credit anybody
41:00 - important they didn't actually write
41:02 - something to control native C++ GPU they
41:05 - actually just rewrote all the C++
41:07 - algorithms loosely I don't know about
41:10 - all what's implemented so far not in
41:12 - JavaScript and isn't that gonna be
41:17 - really slow isn't that a terrible idea
41:18 - well first of all if you're me I like
41:21 - things from slow who cares I just want
41:23 - the stuff to run I want to play with it
41:24 - I want to learn about it I can always
41:26 - use something else to get it to run
41:27 - faster later but maybe in JavaScript
41:31 - alone it would run just way too slow
41:33 - there happens to be something in the
41:35 - world
41:36 - javascript called WebGL WebGL is the
41:40 - browser's interface to OpenGL for doing
41:45 - operations on the graphics card for
41:48 - drawing and making graphic stuff
41:49 - happened the browser so if the math
41:52 - operations of tensorflow in c++ can run
41:55 - on the GPU why can't the math operations
41:58 - inside of this thing called deep learn
42:01 - one via the GPU via WebGL so that's
42:04 - really the magic in my mind of what was
42:07 - accomplished with this original project
42:09 - called deep learned uh chance so let's
42:10 - go look at that website for deep
42:12 - learning as first second look here it is
42:14 - this is deep learn J's or don't go there
42:17 - why because deep learned digest has
42:20 - become tensorflow dot yes so this
42:22 - speculative project that I believe was
42:23 - started last summer 2017 while getting
42:26 - that slightly wrong looking in the chat
42:28 - clean Oh Nikhil was there central
42:30 - processing unit oh shoot a little
42:33 - correction here C is for central central
42:37 - processing unit Thank You Simon
42:40 - oh I've written stuff off the screen to
42:42 - hold on would I right off the screen oh
42:45 - it's just some question marks that's
42:47 - fine
42:49 - hold on looking in the chat here on
42:56 - March 30th less than a month ago deep
43:01 - learn Jack J s became adopt this
43:03 - speculative project of doing these
43:05 - machine learning stuff in JavaScript was
43:09 - adopted by the larger tensorflow project
43:11 - itself and there and and has become this
43:14 - vert this project called tensorflow j s
43:17 - woof so tensorflow j ass this is now the
43:23 - project oh we can write that over here
43:25 - by the way if you can't see what's
43:27 - written up there it's just some question
43:28 - marks sorry about that
43:30 - tensor flow is so we're gonna circle
43:33 - that we're gonna put some hearts on it
43:35 - and a few stars on this is now the
43:39 - framework that I am planning to use in
43:43 - my ended set of tutorials that you may
43:47 - or may not choose to
43:48 - and I may or may not choose to make coz
43:51 - eyes of right now I haven't made them
43:52 - yet but that's my plan there's some more
43:55 - stuff I want to say about this I wonder
43:56 - if I should move on to another video
43:59 - because I want to talk about the let me
44:03 - just mention it briefly here I don't
44:06 - know if you want to edit around this
44:07 - match yet but so there's two more things
44:11 - that I want to mention about this and
44:12 - I'm gonna get to a lot of this a lot
44:14 - more later but tensorflow Tijs isn't
44:18 - actually just once I mean it is one
44:20 - thing it's a project but there are two
44:22 - pieces of it oh yeah actually this is
44:23 - important for this video you might have
44:26 - also heard of something called Karis
44:27 - have you heard of Karis where can I put
44:29 - that let me put a Karis up here can you
44:35 - see that no you cannot let's try this
44:39 - again okay I probably should just point
44:45 - this camera a little bit higher since I
44:47 - tend to write alright alright what is
44:49 - there something I can erase here what
44:51 - I'm gonna do is I want to talk about
44:53 - Karis the layers API and ml5 all right I
45:01 - think I can erase some stuff let me just
45:03 - let this sit in a video for a second so
45:05 - they're gonna be a picture of it case I
45:06 - want to the question parts are very
45:08 - important yes okay where are we timewise
45:19 - here 11 oh my god so little time all
45:26 - right
45:30 - alright there's something else that's
45:32 - important as part of the picture here
45:34 - that I want to talk about and to do so
45:36 - I'm gonna just erase this area over here
45:39 - so okay so we have this terminology in
45:45 - programming high level versus low level
45:48 - and I actually saw a discussion about
45:49 - this going on in the chat there are low
45:51 - level programming languages there are
45:53 - high level programming languages one way
45:55 - to think about that is low level is
45:57 - actually you're manipulating the RAM and
45:59 - the data in the central processing unit
46:01 - like you just you're all the way in
46:03 - there and the deepest part of the
46:05 - computer moving the numbers around
46:06 - yourself versus high level is something
46:09 - like really high level is like the
46:10 - scratch programming environment for kids
46:12 - where I'm like moving puzzle pieces and
46:13 - blocks around to try to create an
46:15 - algorithm so that's one way of thinking
46:17 - about high level low level so it's kind
46:19 - of there could be this sort of a level
46:21 - of abstraction so tensorflow if I were
46:25 - to make I guess I should put low on the
46:26 - bottom tensorflow in terms of working
46:30 - with machine learning operations
46:32 - tensorflow is a low-level library to do
46:36 - the actual matrix math and gradient
46:39 - descent learning training algorithms all
46:42 - yourself
46:42 - written into the code yourself with
46:45 - tential it's common operations that are
46:46 - implemented for you but this is really
46:48 - low-level control of the algorithm
46:50 - itself you could invent new machine
46:52 - learning models by writing them in
46:54 - tensorflow yourself then in between that
46:58 - there previously was a proper still is
47:01 - sorry this project called Charis Charis
47:04 - Charis Charis I don't have pronounce it
47:06 - can't laughs I always think everything
47:07 - is French for some reason so and that's
47:12 - probably not even a French pronunciation
47:13 - of that word but that aside sorry sorry
47:16 - that you had to watch that caris was
47:18 - meant to be a higher level API built on
47:23 - top of tensorflow and in fact Kerris was
47:27 - actually originally designed to be a
47:28 - higher level API that could sit on top
47:31 - of a variety of other low-level machine
47:33 - learning frameworks so for example
47:36 - there's something called Theano is that
47:40 - what is called I think it's called
47:42 - Theano there's like pi/2
47:44 - which is maybe well pi/2 is torch and
47:47 - then there's pie tortoises Python story
47:49 - so there's all these other lower-level
47:50 - machine learning frameworks that clearly
47:52 - am NOT an expert on we just say that
47:58 - again there's all these other machine
48:00 - lower-level machine learning frameworks
48:01 - I clearly have not an expert on but
48:03 - Karis the idea of the countess's you
48:05 - could kind of write your code make a
48:07 - machine learning thing and it could it
48:10 - could operate on top of any of these so
48:12 - Karis though however more recently
48:15 - became part of the tensorflow project
48:17 - itself and so Karis is actually
48:21 - intensive lower linked together so this
48:24 - is a higher level API that's written on
48:27 - that's built on top of tension flow and
48:28 - it exists as part as tensorflow Jas and
48:31 - it's so in tension flow digest there's
48:34 - no actual concept of Karis specifically
48:37 - but there is the core API and then
48:40 - what's called the layers API and the
48:42 - layers API is something that I'm gonna
48:44 - use much more in my video tutorials
48:46 - although I'm gonna start with a few that
48:48 - just look at the core stuff because it's
48:50 - kind of important to have a sense of
48:51 - what that is and how that works but
48:53 - layers so layers intensive logic is the
48:55 - equivalent of this thing called Kerris
48:57 - now a project that's being developed
49:00 - here at new york university with some
49:02 - collaborators from ITP and guests and
49:05 - researchers and students is a project
49:07 - called ml 5 the 5 in ml 5 is an omage
49:12 - homage that's French right to the 5 in
49:16 - p5 in the sense that I mean this is
49:18 - flawed for many reasons but in p5 you
49:22 - could think of as like a wrapper on top
49:24 - of canvas and Dom to jeju's like common
49:29 - creative coding functions to make
49:30 - drawing and making pictures and doing
49:32 - creative sketching projects a bit easier
49:35 - and friendlier in JavaScript ml 5 is yet
49:39 - another layer on top of well sorry I
49:43 - shouldn't put this it's it's only for
49:45 - JavaScript as a layer on top of
49:47 - tensorflow Jas to do some common to
49:50 - allow to kind of like even abstract the
49:54 - concepts even a bit further
49:56 - and you know I think one of the goals of
49:57 - ml5 is for it to be a library that high
50:00 - school class could use a kind of weekend
50:03 - workshop for artists could use these
50:06 - sort of context of people wanting to get
50:08 - a basic understanding and try some
50:10 - machine learning stuff out so anyway so
50:12 - this is all the stuff whether it was a
50:14 - long introduction to all these pieces
50:16 - this is all the stuff that I'm hoping to
50:18 - cover over the next several months here
50:21 - on this youtube channel called the
50:24 - coding tre it's really it's like
50:33 - embarrassing that I keep blowing this
50:34 - train whistle you know that's what I do
50:36 - and so that's the plan so you can kind
50:40 - of pick and choose ultimately you might
50:42 - be able see what's available for you at
50:44 - the time of watching this but ultimately
50:47 - you might want to skip ahead and look at
50:49 - some of these ml5 tutorials because you
50:51 - don't necessarily to do the ml5 examples
50:54 - you don't necessarily need to have a
50:56 - knowledge of the core api of tensorflow
50:59 - chasse or the layers api even but I'm
51:02 - gonna start even though I might but the
51:04 - goal for the ml5 Biggs librarian
51:07 - examples is to give people a starting
51:08 - place that you don't need to have gone
51:10 - through all the lower-level stuff for my
51:12 - own kind of sanity and figuring this
51:14 - stuff out
51:15 - also ml5 doesn't actually have a public
51:17 - release yet whatever that means but the
51:19 - goal is sort of like have a
51:20 - quote-unquote public release in June
51:22 - with more documentation examples and
51:24 - features my I'm gonna start very first
51:29 - thing I'm gonna do in the next video is
51:30 - just look at the core API in attentive
51:32 - flow yes and see like what some of the
51:34 - things you can make do with it what some
51:36 - of the functionality is and that kind of
51:37 - stuff all right
51:38 - how am i doing I think good I probably
51:41 - made a bunch of mistakes and missed a
51:43 - bunch of things so check the video's
51:45 - description cuz I'll write corrections
51:48 - and stuff in there and also I will in
51:50 - the next video if you continue on
51:52 - whatever the Google's YouTube machine
51:55 - learning algorithm tells you to watch
51:57 - we'll hopefully have some anything that
51:59 - needs to be corrected alright thanks for
52:00 - watching this I hope this was a helpful
52:02 - picture of my of all these pieces and my
52:05 - thinking as it relates to them look Wow
52:08 - okay goodbye
52:10 - all right um well this camera went off
52:16 - oh okay all right so I don't know I'm
52:25 - looking for questions now don't be so
52:28 - okay thank you thank you to what the web
52:31 - who said that I presented the
52:33 - information very well don't be so hard
52:35 - on yourself
52:36 - I appreciate that all right it's helpful
52:44 - to have the friendly feedback in the
52:46 - chat I appreciate it oh this computer
52:48 - the one that's invisible to you is about
52:50 - to die no that's not it for today I'm
52:55 - gonna keep going I have I have about
52:57 - another hour yes so maybe there's some
53:01 - new viewers I do a little bit I have a
53:05 - sort of weird workflow it's actually not
53:08 - that weird anywhere I've been noticing
53:09 - other people do something similar but
53:12 - which is that I do these live streams
53:14 - and the live streams are what used to be
53:18 - my own private recording sessions
53:20 - basically where I would like record a
53:22 - tutorial and then upload it later so now
53:25 - I just broadcast those live and then
53:27 - though the pieces of this that are and
53:31 - have some value will get edited down to
53:34 - something slightly shorter and uploaded
53:36 - and put into a playlist so that it could
53:38 - be used as part of a course or us just
53:40 - for you know for people when it does
53:42 - watch it just watch that that particular
53:44 - part so that so whatever all that the
53:46 - live stream will remain archives forever
53:48 - well as long as not forever I mean I'll
53:53 - be dead but if you're like really really
53:55 - let's say it's like the year 3000 and
53:57 - you're watching this maybe you could
53:59 - revive me somehow I don't know so I
54:02 - couldn't know that you're watching this
54:03 - somehow in the year 3000 with your maybe
54:06 - it's like actually like appearing you
54:08 - don't even have eyes anymore anyway I'm
54:10 - talking to a creature from the year 3000
54:14 - that I had
54:15 - now on YouTube that makes no sense at
54:16 - all so the live stream would be archived
54:21 - and then also the edited videos will be
54:23 - in different playlists okay don't forget
54:26 - any semicolons today
54:28 - Ricardo asks will you teach web assembly
54:33 - in the future I guess I would have to so
54:38 - two options there are one I learned web
54:41 - assembly and then teach it option number
54:43 - two is I just attempt to like look at it
54:46 - during the live stream without knowing
54:47 - if it and they think about it cuz I
54:48 - don't really know whether sembly which
54:49 - is actually what I'm kind of doing with
54:51 - those machine learning stuff it was like
54:52 - honestly like I as much as I've been
54:55 - looking at tensorflow das and talking
54:57 - about it and I did try to make a project
55:00 - with it actually earlier this week so
55:02 - this isn't totally true I haven't really
55:03 - done the stuff that I'm about to talk
55:04 - about okay
55:10 - okay ah different Nikhil in the chat
55:14 - asks Dan will you go for dialogue flow
55:16 - dialogue flow is something I'm planning
55:19 - to cover next fall so I have a course
55:22 - that I teach called let's see if this
55:25 - will program from A to Z ooh look at
55:30 - this oh boy
55:31 - Oh electron that's another thing oh so
55:34 - many things anyway this is a course that
55:39 - I teach I'm gonna be teaching it again a
55:41 - tizzy
55:41 - - f-18 doesn't exist yet that'll be this
55:44 - coming fall and I'm planning to add
55:45 - dialogue flow as a module for that
55:47 - course so whenever I get to it I'm
55:50 - definitely gonna use it
55:54 - alright alright alright looking at the
56:00 - chat I'm getting messages from the
56:05 - future which is that maybe somebody
56:07 - traffic could travel back in time no no
56:10 - here look did you invent time travel and
56:12 - somehow you invent a time travel and
56:14 - you're watching this video travel back
56:17 - in time go to well maybe I shouldn't
56:23 - reveal where I am right now that sound
56:25 - like a wait you're smart you're from the
56:27 - future you know where I am
56:29 - now it's 11:35 a.m. Eastern Daylight
56:32 - Time finally knock on the door if you
56:36 - have an issue with security downstairs
56:37 - tell them you're here to see you're from
56:38 - the future and you're here to see how do
56:41 - you pronounce you gotta get this very
56:43 - tricky to pronounce my name it's Daniel
56:45 - she's fun
56:47 - say you're here to see Dan Schiffman on
56:49 - and and they'll let you up you could
56:53 - figure out my phone number because
56:54 - you're from the future
56:55 - you could look it up in the archives
56:57 - alright so maybe somebody will knock at
56:59 - the door we'll see Wow suspense I could
57:03 - try my wait not this one okay never mind
57:20 - I was trying to get the serial music
57:22 - again okay um so let's see what do I
57:28 - want to do now I wanted to do a coding
57:30 - challenge today just to have the code
57:32 - maybe I could come back this afternoon
57:34 - maybe I'll so I'm gonna keep going with
57:36 - this oh I forgot to mention the whole
57:39 - point of this was I was gonna mention I
57:41 - had this video here for people to watch
57:42 - oh okay hold on alright this is the
57:50 - thing that I forgot I'll just do it in
57:57 - the beginning of the next video let me
58:04 - answer two quick questions that I see in
58:06 - the chat bridge fee asks how do you tell
58:09 - someone to learn Java scripts from
58:10 - scratch using what resources well I mean
58:14 - I don't want to toot my own train
58:20 - whistle but I do have a set of videos
58:23 - that are for total beginners to learn
58:26 - JavaScript and the p5 library acts as
58:29 - kind of the conduit for that maybe
58:32 - that's not for everybody but that
58:33 - certainly would be an option there's a
58:36 - book this is not really for beginners
58:38 - but a book that I really love
58:39 - is uh maybe this for beginners is
58:41 - eloquent JavaScript this is a wonderful
58:45 - book and I always used this as a
58:47 - reference to try to learn about how
58:48 - stuff in JavaScript actually works those
58:50 - would be two thoughts oh there's another
58:51 - question Jack goes too fast I was like
58:55 - there was a second question I was gonna
58:56 - answer but I don't remember what it was
58:57 - what was it we learn JavaScript and I
59:02 - can't remember okay okay so let's move
59:14 - on here
59:15 - and let's try to do a little tutorial
59:25 - about so I think I can get rid of all of
59:32 - this yeah so the next thing I'm going to
59:37 - do is just look at the core tensorflow
59:40 - jsapi
60:01 - so do you think one thing I want to do
60:03 - is let's find some better definitions
60:06 - what a tensor is and that was yeah let's
60:13 - so that'll be good for starting us out
60:15 - here let me get basic books p5 sketch
60:26 - going oh I'm in the wrong screen p5
60:39 - tensor flow and Adam
61:01 - and
61:10 - oops what is this here the whiteboard
61:19 - totally needs a fresh coat of paint I
61:21 - know let's open this up in the Adam
61:29 - editor and let's run where am I
61:47 - let's run a little web server should
61:53 - really I don't know should just use the
61:58 - node one I should use the live server
62:01 - two but it's the reason the live server
62:04 - kind of makes me a little bit crazy and
62:12 - then okay so this is good actually I'm
62:21 - gonna I need a new template for the p5
62:25 - manager really badly
62:27 - v5 CDN let's just do I mean I'm probably
62:35 - not gonna use p5 so I don't know even
62:37 - why I'm bothering with this very silly
62:41 - what I'm doing very very very silly
62:57 - one of these days I'm gonna just spend
63:00 - some time
63:07 - oh no so now this has to be 8080 and
63:14 - here we go yeah look at this
63:25 - okay alright so I think I'm just about
63:29 - ready here now okay so what do I need I
63:31 - want tensorflow touch yes and now five
63:37 - orange reference this okay
63:50 - alright alright somebody told me that
63:57 - the tensorflow what is a tensor the
64:01 - tensor flow documentation itself well
64:05 - look at this I would like to know this
64:06 - what is the difference between a matrix
64:09 - and a tensor Oh Oh general eyes matrix
64:16 - that's what I thought ah I wish I had
64:20 - said that but I think I sort of said
64:21 - that okay all right fine this is good
64:25 - this is helpful for me though this makes
64:26 - sense okay I'm good with I think I'm
64:30 - fine but I explained all right
64:33 - okay it's 11:45 timer that thing I
64:38 - wanted to do yeah maybe I'll come back
64:41 - this afternoon if I can I have to leave
64:44 - well hold on let me just determine what
64:45 - time I have to leave okay
64:53 - Oh interesting you're interesting so I
64:56 - could okay okay all right that's fine
65:07 - okay sorry no this is sorry I'm like now
65:13 - I'm looking at my email which I was
65:14 - trying to check this - where my schedule
65:16 - today so it's possible today between
65:20 - like three and four that I might be able
65:22 - to come back for a coding challenge
65:23 - let's see yes come in L you through the
65:25 - type of thing I don't I yes thank you -
65:28 - Ricardo in the chat who is giving me
65:31 - helpful tips that I will never probably
65:34 - follow because I'm an old person with a
65:35 - calcified brain and I can't do anything
65:38 - but just type clear okay so we want to
65:41 - be here want to be here won't be here
65:44 - want to be here all right all right here
65:50 - we go
65:53 - so it's guilt Liz de asked is it
65:57 - possible to use GPU with tensorflow
65:59 - dodge ass not only is it possible it is
66:03 - impossible to not use the GPU tensorflow
66:06 - dodge yes as far as I know only works
66:08 - with the GPU so if you are working with
66:10 - a web browser does is that that does not
66:12 - support WebGL it will not it will not
66:17 - you will not be able to do it okay hello
66:23 - I'm back to for another video where I'm
66:25 - going to look at the core API of
66:28 - tensorflow dodge as and in my
66:30 - introductory video I totally forgot to
66:32 - mention and link to this particular
66:35 - announcement video machine learning in
66:37 - JavaScript from the tensorflow dev
66:39 - summit where Nikhil Thorat and Daniel
66:42 - smilk off the creators of tensorflow TAS
66:44 - talked about the project and you should
66:47 - watch that because it'll give you a lot
66:48 - more background and also that show
66:50 - interesting demos and other things that
66:52 - people are working on with it okay so I
66:54 - wanted to mention that now okay so if
66:58 - you didn't watch my previous video which
67:00 - is kind of an overview of the landscape
67:01 - of all the pieces around working with
67:04 - machine learning in JavaScript you could
67:06 - go watch that or you could just be right
67:08 - here because what I'm gonna do in this
67:09 - video is I'm just gonna go to this API
67:11 - reference and in this API reference what
67:14 - I want to do is talk about the basic
67:17 - building blocks of tensorflow jazz now
67:21 - here's the thing again I just want to
67:22 - mention I just want to mention that you
67:24 - don't have to be here right now anywhere
67:27 - else in the world than watching this
67:28 - video and ultimately I am going to be
67:31 - using something called the layers API
67:35 - which is part of tension flow digest
67:39 - which is a bit of a higher level
67:40 - abstraction then the stuff I'm going to
67:42 - start to look at in this video and I am
67:44 - also going to use a project called ml 5
67:46 - which is a separate project being
67:49 - developed here at New York University
67:51 - which is built on top oh I guess oh boy
67:55 - this why what timeout timeout
67:58 - not time up
68:11 - we'll write that lower down ultimately
68:14 - I'm going to start using something
68:16 - called the layers API which is built on
68:22 - top which is part of tensorflow Jas so
68:26 - there's court enter flow Jas ultimately
68:33 - I'm going for projects that I'm gonna
68:34 - make I'm too useful
68:35 - the layers API which is a higher level
68:37 - abstraction and then even higher if I'm
68:39 - going inverse like down is higher level
68:42 - eventually I'm also going to use
68:44 - something called ml 5 which is a project
68:47 - being developed here at New York
68:48 - University which is a JavaScript library
68:50 - separate project from tensorflow GS but
68:52 - uses tensorflow J's behind the scenes to
68:55 - do some to show demonstrations of common
68:57 - machine learning tasks like image
68:59 - classification poetry generation and and
69:04 - things like that ok so but for me for my
69:07 - own sanity I would like to learn the
69:10 - basics of the core API of tensorflow yes
69:13 - and I think it's useful as foundational
69:15 - knowledge for moving along and looking
69:18 - at some of this other stuff but if you
69:19 - just want to get to like getting your
69:21 - webcam and trying to classify the images
69:24 - that are in your webcam you can you
69:25 - could find some videos that aren't made
69:27 - yet but they will be made maybe I don't
69:28 - know it's very confusing I'm gonna make
69:30 - them within they'll fly when we get it
69:32 - to that ok core API what is the core
69:37 - core core building block of everything
69:40 - in tensorflow it is something called a
69:43 - tensor and I talked about this in my
69:45 - previous video but just to recap we have
69:49 - this idea of a scalar which is a single
69:52 - number we have this idea of a vector
69:54 - which is a one-dimensional list of
69:56 - numbers and we have this idea of a
69:58 - matrix which is really a two dimensional
70:01 - grid of numbers a tensor is a sort of
70:05 - more generic term that refers to any
70:08 - n-dimensional thing of numbers and also
70:13 - the operations associated with those
70:15 - things like matrix multiple
70:16 - caishen and so this idea of this idea of
70:19 - a tensor being this building about it's
70:21 - a thing you can make and perform the
70:24 - common mathematical operation it's
70:25 - actually quite similar to in p5 if you
70:28 - watch my p5.js tutorials I use this
70:31 - create vector function so this create
70:34 - vector function is making a vector and
70:36 - actually three to two or three
70:38 - dimensional vector so it's always just
70:40 - like an XY inventory it's always just an
70:43 - X or Y and sometimes a Z and so then you
70:46 - can make these create vectors and you
70:48 - can get their magnitudes and normalize
70:49 - them two common mathematical operations
70:51 - the tensor is exactly the same thing so
70:54 - if we now go and look at the at the API
70:59 - here we can see here it is TF tensor so
71:02 - how do we even make a tensor let's just
71:04 - make a tensor so first of all I have
71:07 - some code I have some code not very
71:09 - little code I have a single file called
71:11 - sketched out JavaScript and you would
71:13 - think like okay let me just go and take
71:15 - this and let me just put it in here I'm
71:18 - gonna hit save then I'm gonna go back to
71:20 - the browser I'm gonna hit refresh and
71:22 - uncaught reference error TF is not
71:24 - defined so why is TF not defined well
71:27 - it's not defined because I need to
71:28 - import the tensor flow dot J's library
71:31 - so a way that I can do that I'm actually
71:34 - just going to Google I'm sure it's like
71:36 - in an obvious place but tensorflow J s
71:38 - CDN and then oh it's actually right here
71:42 - on the home page right here this is what
71:45 - I'm looking for
71:46 - I'm gonna grab this bit of code right
71:50 - here because what I want to do is
71:52 - reference the tensile voce has library
71:54 - file via a CD and a CDN is a content
71:57 - delivery network I could download the
71:59 - library include potential ojs javascript
72:02 - file in my project it's a little easier
72:04 - for me right now just to go into
72:06 - index.html you can see I'm actually
72:08 - referencing P 5 and P 5 Dom libraries to
72:12 - see the ends I don't know that I've been
72:13 - actually to use the P 5 library in this
72:15 - video but I can just add one more here
72:18 - and now I have tensorflow tachi s
72:25 - imported as part of my project and I'm
72:28 - gonna go over here I'm gonna hit refresh
72:30 - I look at that ha ha it's our first
72:33 - tensor we made a tensor we paid a tensor
72:37 - just take a break now for a minute I'll
72:39 - be right back okay yeah so people are
72:50 - mentioned require I mean maybe it's
72:53 - worth noting oh there is a cpu back-end
72:58 - I'm told
72:59 - ok other laptop could use a small
73:01 - adjustment oh thank you oh yeah
73:06 - where's is it here there we go so better
73:10 - I think that's better but I can't really
73:12 - see it now ok so where are we now
73:27 - all right so I'm taking a break ok
73:28 - always what I'm going to do next a
73:34 - scalar 1d 2d
73:36 - ok ok shaped candy go with the shape now
73:46 - if I make a 2d 3d what I'm confused
73:53 - about here is or D buffer hold on a
74:00 - nested array of numbers flat array or
74:03 - type 2 right okay shape
74:12 - so what's interesting about this is I
74:14 - understand the idea of a shape and
74:20 - datatype well let me let me experiment
74:26 - for a second here I'm gonna come back to
74:32 - the video in a second but let's say if I
74:34 - make actually let me not make this a
74:44 - Const cuz I'm gonna wanna like alright
74:48 - actually I'm just do something
74:50 - ridiculous cuz I'm gonna want to like do
74:52 - this over and over again oh all right TF
74:55 - dot tensor so that's the whole thing so
75:01 - this we can see the datatype the shape
75:04 - is just to the size is 2 so I can okay
75:10 - hold on
75:10 - so if I make a tensor 2d see this is
75:21 - what I meant to do yesterday I was going
75:22 - to like look through all this stuff more
75:24 - detail tensor 2d requires the shape to
75:28 - be provided when values are a flat type
75:31 - oh so it has to have a shape because the
75:33 - values are not got it that makes sense
75:38 - so the shape could be because it's 2d it
75:42 - could be like 2 by 2 and then it would
75:45 - be this right so this would be a 2 by 2
75:49 - tensor with four numbers right that
75:56 - makes sense and then we're seeing it
75:57 - like this that makes sense so what I
75:59 - don't understand is what if I want to
76:02 - have a because I could have like four of
76:04 - these now what work I see so if I wanted
76:09 - to have a 2 by a 2 D tensor but I wanted
76:12 - four of them so there could be multiple
76:15 - dimensions but ultimately the data
76:17 - itself is 2 by 2 is that sort of the
76:20 - idea so like for example if I want
76:24 - have two of these this would work and
76:33 - I've got yeah
76:35 - two to two by two tensors that makes
76:39 - sense and right because otherwise if I
76:45 - didn't give it if I just said it was two
76:47 - by two it would give me an error I see
76:50 - and then and then sorry I'm just talking
76:57 - myself through this before I go back
76:59 - into this like try to like make sure I
77:01 - kind of understand this oh oh
77:05 - nikhil is giving me feedback thank you
77:08 - Alka for pasting those in okay so I'm
77:14 - just and then I see so if I want it will
77:17 - be it could be inferred the shape can't
77:26 - be inferred right if it's flat but if I
77:29 - did this this would work
77:34 - no I would actually if I wanted to infer
77:40 - the shape completely I've got to
77:41 - actually do this just curious just
77:45 - trying to understand this will be okay
77:54 - I know I don't need to go into all this
77:58 - detail I just know that didn't work I
78:01 - guess you really need to you really need
78:04 - to pass the shape in the product of the
78:08 - entries in the shape must equal the
78:09 - number of values okay wait Nikhil is
78:12 - giving me actual information the product
78:14 - let's write that down
78:21 - the product and the number all right
78:23 - well that is our the project so I just
78:26 - was curious like when can it infer the
78:28 - shape it really can only infer the shape
78:32 - if you're making a single tensor that
78:36 - matches 2d 3d right r 1d I think okay
78:46 - all right so let's come back here and
78:56 - we're I forgot where I was leaving this
78:58 - off all right oh I need that thing where
79:05 - I copy/paste tensor TF tensor 2d all
79:09 - right yeah yeah okay so this
79:20 - yeah cool okay all right it can infer if
79:29 - you do this yeah yeah okay thank you
79:31 - all right shape is important I'm not I'm
79:34 - gonna not worry about the the the the
79:36 - okay so okay here we go let's let's keep
79:46 - moving now
79:53 - okay so I've made my first tensor now
79:56 - one of the things you'll notice if I go
79:58 - to the tensor flow documentation whoops
80:00 - is that in addition to just TF dot
80:03 - tensor and there's also by the way oh
80:05 - look at this I need to talk about shape
80:07 - and datatype but just for a moment I
80:10 - want to just look there also is this o
80:11 - TF scaler we're going to talk about that
80:13 - TF tensor 1d 2d okay so here's the thing
80:18 - this funk this idea of a tensor TF
80:21 - tensor is a generic concept that will
80:24 - work for any n-dimensional tensor but if
80:28 - you're working with tensor flow Jas you
80:30 - want it to the extent possible use the
80:32 - functions that specify something about
80:35 - the shape so what do I mean by shape oh
80:37 - boy so here's something a really
80:40 - important concept a shape this shape
80:45 - refers to the dimensions of the tensor
80:49 - so for example we might this right here
80:54 - this has a shape that's 2 by 2 I could
81:02 - write this this now has a shape because
81:08 - you always say the rose first with
81:10 - matrix this is 2 rows by 3 columns 2 by
81:14 - 3 so one thing that's important when
81:17 - creating tensors is to also specify the
81:20 - shape like why does this even matter
81:21 - what we're just kind of in the low-level
81:23 - land here just to get a sense of how
81:24 - these things are the reason why this
81:26 - matters for example imagine if what I'm
81:29 - ultimately going to do is feed in
81:31 - image data into a neural network for
81:34 - some tasks like image classification
81:36 - well I want and what I have are lots of
81:39 - images maybe all those images are 28 by
81:42 - 28 pixels so my shape is going to be 28
81:47 - comma 28 and let's say that I actually
81:51 - have a hundred of them I have a hundred
81:52 - 28 by 28 images this is exactly what I'm
81:55 - gonna have for the doodle classifier
81:56 - example that I'll make at some point
81:57 - then the shape of all of the data is 100
82:02 - comma 28 28 because I have 28 by 28
82:06 - images and I have a hundred of them so
82:09 - this is a really sort of important piece
82:11 - this idea of the shape is a really
82:13 - important piece of defining a tensor so
82:16 - let's take a look at that real quick so
82:18 - if I come back to but I have no plan for
82:26 - what up to it here really should play it
82:28 - these out so let's come back and let's
82:33 - go back to here and let's just do this
82:35 - in the console so I'm gonna say by the
82:38 - way one thing I didn't I mean I guess I
82:41 - could keep doing this in my code let's
82:43 - keep doing in my code so I have a record
82:45 - of it so um one thing I didn't mention
82:47 - by the way is why am I saying TF dot
82:50 - tensor and first of all I'm gonna write
82:53 - this in a different way I'm going to say
82:54 - constant nums
82:57 - let me call it data data equals let's
83:03 - write this out like this and then data
83:05 - dot print so let's look at this so one
83:09 - thing that's a key here that I'm using
83:11 - the the way of declaring a variable
83:14 - called Const for constant which means I
83:17 - can't reassign this object and so this
83:22 - is what you're gonna commonly see in the
83:24 - tensor flow to tutorials I could say let
83:29 - data but in a sense I'm using
83:30 - consequence I'm protecting myself from
83:32 - reassigning the variable data to
83:34 - something else later so I'm gonna say
83:36 - Const data then data dot print will go
83:39 - back here and we'll see there's the
83:41 - tensor now incidentally what if I say
83:43 - console dot log
83:45 - data so the tensor that print function
83:49 - is a helper function in there to let you
83:51 - sort of see just the information the
83:53 - data that's in the tensor but if I
83:55 - actually say console dot log data what
83:57 - you're actually gonna see is look at
83:59 - this whole thing so a tensor itself is
84:01 - actually this complex object it has
84:04 - functions associated with it look at
84:06 - this there's that shape the shape is
84:08 - just for right it's one dimensional and
84:11 - there's just four things in it its size
84:13 - is four meaning there's four right so
84:16 - this is really an and somewhere in here
84:18 - I probably could find the data if I
84:22 - looked hard enough rank one Strides
84:24 - object boy where is that data time out
84:28 - for a second right if you so if you pass
84:35 - a flat alright by the way this is great
84:37 - that Nikhil isn't the chat correcting
84:39 - everything so hopefully people are
84:41 - actually learning something from the
84:43 - Kiehl's comment I appreciate that and
84:45 - thank you Alka for pasting in the
84:47 - Kiehl's comments yeah I'm actually just
84:49 - curious here is can I find the data
84:51 - somewhere in here where would it be
84:56 - [Music]
84:58 - values no this is interesting is it just
85:06 - like pointing to like a memory address
85:07 - in WebGL or something like that
85:09 - this shows how little I know all right
85:12 - well I'm not going to worry about this
85:16 - ten start to strain will gives you that
85:19 - pretty print from print oh that's good
85:20 - to know
85:23 - okay okay okay
85:33 - what time is it twelve o'clock oh I'm
85:36 - doing terribly fine this is gonna take
85:38 - me a long time to get through all this
85:39 - stuff alright alright I'm gonna move on
85:46 - so so this is useful for us to be able
85:50 - to see those are a properties of it but
85:52 - most of the time we just want to look at
85:54 - the data so another thing that I could
85:56 - do here is I could say console dot log
85:58 - data to string so data dot print just
86:01 - takes the string version of the tensor
86:03 - and puts it into the console so if I do
86:05 - this hit refresh we can see there's that
86:07 - tensor as well okay now here's the thing
86:11 - what if what I want to do is this these
86:15 - are pixel values oh let's actually say
86:17 - these are pixel values like 0 0 127 255
86:23 - and what I want is for these pixel
86:27 - values to be a 2 by 2 image to represent
86:30 - a 2 by 2 image so I want to create now I
86:32 - want to create the tensor with a 2 by 2
86:36 - shape time out for a second my calendar
86:43 - is giving me alerts cuz I have meetings
86:44 - and things coming up it's very hard to
86:49 - make all this stuff happen ok ok so now
86:54 - let's look at what we see in the console
86:56 - now that I've defined a shape look at
87:00 - that we can see that it's basically a
87:02 - two-dimensional array and what I could
87:06 - do is what's interesting about this
87:07 - let's say I had two of these images so
87:11 - now there's eight values and we'll make
87:13 - these like totally different numbers now
87:17 - what would happen if I try to turn eight
87:20 - numbers into a 2x2 a tensor a 2x2 tensor
87:27 - look at this I'm getting an error so
87:29 - this by the way this is an error you're
87:31 - probably gonna see throughout your life
87:33 - if you go down this road now not exactly
87:36 - this error but constructing tensor of
87:39 - shape for should match the length of
87:41 - values 8 so this means like hey you gave
87:43 - me eight values but you're trying to
87:45 - make a sense
87:45 - a tensor that only has four values I
87:47 - can't do that and this is actually quite
87:49 - a common error I was trying to work on a
87:51 - doodle classifier with tensorflow da
87:53 - Chasse earlier this week and I kept
87:55 - getting all these errors because I was
87:56 - trying to train my dataset what didn't
87:59 - match the size of what the machine
88:01 - learning model expected so we're getting
88:02 - a little baby steps into that this idea
88:04 - of if I'm preparing data as a big of I'm
88:08 - loading it from a file a spreadsheet and
88:10 - I have a big list of numbers I better
88:11 - have the right and a lot of numbers to
88:13 - put it in the right size tensor and so
88:15 - this can now be corrected in the code
88:18 - because I can say well actually what I
88:20 - want and it's awkward that all these
88:22 - numbers are just two but hopefully
88:24 - you're following me here is I want to
88:26 - have two by two and I want two of them
88:29 - so now if I add the shape is really and
88:32 - let's well I'm gonna give leave it as an
88:36 - exercise to you try to redo this but use
88:38 - like a three by five and so now I'm
88:42 - actually gonna do that myself I'm gonna
88:44 - hit refresh you can see there we go
88:46 - there's my essentially this is the shape
88:48 - now it's two by two by two and actually
88:52 - what's interesting here also is I really
88:55 - am thinking of these as two dimensional
88:57 - tensors so one thing away hold on no no
89:02 - stop roll that back for a second I'm
89:06 - gonna go in a slightly different order
89:11 - okay yes size means total number of
89:15 - elements I believe all right
89:22 - okay okay so let's do a couple more
89:27 - things here let's say console dot log
89:30 - data just so I can see now so what I
89:34 - have here is that the sort of pretty
89:36 - version of the data there it is
89:38 - those are all my numbers now shaped into
89:40 - these arrays now also here look at this
89:43 - I can see that the shape is two by two
89:46 - by two the size is eight meaning there's
89:48 - eight total numbers the type this is
89:51 - something I haven't talked about type is
89:53 - important I'm putting floating-point
89:54 - numbers in there
89:55 - so this that's the default type for
89:58 - example this could be 1 27 point 5 and
90:01 - if I'd run this again we're gonna see
90:03 - one twenty seven point five is in there
90:05 - however I could have changed the type to
90:09 - if we go look at the documentation int
90:13 - 32 so for example I could say you know
90:17 - what I want to be more I don't need to
90:19 - have floating-point numbers so I could
90:21 - change the data type to in 32 and that's
90:23 - just one more argument for me to add
90:25 - here in 32 now let me run this again and
90:31 - you could see it actually worked fine it
90:33 - didn't complain that I tried to give it
90:35 - a floating-point number but it just took
90:37 - off the decimal place and my assumption
90:39 - here is it's not going to round it like
90:41 - if I make this one twenty seven point
90:42 - nine 99 we're still gonna see 127 there
90:45 - it's always going to floor that value
90:47 - meaning taking take off the decimal
90:49 - place so we can see now in making it
90:52 - what's it why is this off
91:02 - there we can see now in making a tenser
91:07 - we have three important properties
91:09 - essentially we have the values these are
91:12 - the numbers that are going to go in the
91:15 - tensor we have the shape which is
91:18 - defining the dimensionality of the
91:21 - arrays of data basically and then we
91:23 - also have D type or data type which is
91:28 - saying what goes in that tensor and the
91:30 - only possibilities are floats intz or
91:34 - boolean z' so you can imagine just like
91:38 - if you know you only need integers
91:39 - you're gonna save some memory or some GP
91:42 - Younis by using integers instead of
91:44 - floats so this is what it means to make
91:46 - a tensor so I want to do two more things
91:48 - before I move on to the next video we'll
91:50 - start to look at some operations on
91:52 - mathematical operations on these tensors
91:54 - and and and also I need to talk about
91:56 - the difference being at so let's do a
92:00 - couple more things number one is let's
92:02 - let's um let's do something a little
92:04 - more interesting let's make a a tensor
92:07 - that had that is um five rows by three
92:11 - columns five by three so I need fifteen
92:14 - numbers so let's let's make an array
92:21 - well I'm gonna I'm gonna be a human who
92:25 - uses the constant at values is an array
92:30 - and I'm gonna just say I equals zero
92:33 - eyes less than fifteen i plus plus I'm
92:35 - going to make up some pretend data and
92:37 - I'm going to use P five random function
92:39 - I could say math dot random if I wasn't
92:41 - using P five between zero and 100 and
92:47 - then what I want to do is and then I'm
92:50 - gonna then I'm going to make a shape and
92:51 - I'm gonna say the shape is five by three
92:55 - then I'm going to say Const data equals
93:00 - TF tensor with the values and the shape
93:06 - so this is perhaps a bit more like how
93:09 - you might want to do it right for
93:10 - example I load this is me loading in a
93:13 - lot of data from a spreadsheet or
93:15 - another API or loading image files and
93:17 - converting them to pixels there's
93:18 - actually from pixels function intention
93:20 - flow digest that will just take pixel
93:22 - data and turn it into a tensor but so
93:24 - basically so so I have 15 numbers I set
93:29 - the shape and now I have this tensor and
93:31 - now we're gonna look at it down here
93:33 - let's take a look and see if this worked
93:37 - so now we can see now there's a little
93:39 - bit awkward to look at cuz of all but
93:40 - there we go look at this it is five by
93:44 - three five rows three columns so this is
93:48 - working this is good now what happens if
93:51 - I were to have 30 numbers I get that
93:57 - error but again this could be 2 by 5 by
94:02 - 3 and now you can see I have two chunks
94:06 - of 5 by 3 data so now at least we're
94:09 - able to see how this kind of shape stuff
94:11 - works and if I wanted to I could also
94:13 - add back in int 32 because maybe what I
94:17 - want are just integers and you can see
94:19 - it looks a little nicer there now here's
94:21 - the thing all this time I have been
94:24 - using just a generic tensor TF tensor
94:28 - but if we look at the API and this is
94:30 - the last thing that I wanted to point
94:32 - out here if we look at the API what
94:34 - you'll actually see is first of all it's
94:36 - scalar so this is rank of 0 meaning a
94:40 - single number rank 0 there's a idea of a
94:43 - ranking here tensorflow 1d is rank 1
94:46 - that means a vector tensor flute atf
94:50 - tensor 2d is a matrix that means a
94:53 - matrix and 3 so even though you can just
94:56 - use TF tensor it's going to make your
95:00 - code more readable and you're gonna
95:02 - protect yourself for more errors if you
95:04 - use in here the actual if you actually
95:08 - specify the rank that you intend so for
95:11 - example if I just go back to the console
95:13 - for a second and I hit clear I could say
95:17 - scalar 4 so this now num dot print is
95:23 - just a tensor with a single number
95:26 - that's scalar now again I could have
95:28 - said
95:28 - num T equals TF tensor four and then I
95:34 - could say dumb t dot print this is
95:37 - exactly the same right
95:39 - TF tensor for or TF scalar but I've have
95:43 - possibly more legible readable code by
95:45 - saying TF dot scaler that's good right
95:48 - okay now let's think about what's going
95:52 - on here well this is really I really
95:55 - thinking of this as a 2 D tensor but I
95:59 - have a rank two but I have more than one
96:02 - of them
96:03 - will this work for me if I say tensor 2d
96:06 - let's take a look
96:08 - it does now if I were to say tensor 3d
96:13 - with this work yeah why is that bit but
96:19 - not if I went back to 15 right oh no I
96:24 - guess it doesn't wait hold on
96:28 - yeah okay well hold on let me think
96:30 - about this well I have to stop for a
96:33 - second this is where I oh wait hold on I
96:37 - got a look at the chat here yeah oh
96:45 - people are talking about in 32 versus
96:48 - float in 32 versus float I should have
96:51 - mentioned that let me let me mention
96:53 - that in the chat so wait I'm I don't
96:55 - know why I've really confused myself
96:56 - about 2d or 3d so let's look at the
97:02 - let's look at the documentation here for
97:03 - a second let's look at 3d example
97:18 - yes so this is really okay so really 4d
97:26 - yeah so really I'm really I'm like dude
97:33 - so that's my question here is that's my
97:43 - question here let me think about this
97:46 - for a second which is correct in this
97:53 - instance let me go back which is correct
98:05 - here I think this is really this is
98:14 - correct right because it's rank three I
98:20 - guess it just works anyway because I'm
98:23 - specifying the shape I see so if I
98:25 - didn't give it the shape but no it
98:38 - whoops oh did I oh whoops I lost the I
98:45 - think you found a bug sensor 2d the
98:49 - shape should have two values tensor 3d
98:51 - the shape should have three vows oh good
98:53 - I have woohoo I did something useful
98:58 - today
99:01 - should we file it as a github issue
99:04 - let's follow does a kid hub issue
99:06 - everybody even though I have to go in
99:07 - about ten minutes I haven't even
99:08 - finished making this one tutorial okay
99:10 - let's file this is a github issue let's
99:13 - go let's be good citizens everybody
99:14 - tensorflow J ass github let's go to
99:20 - issues and let's figure out this bug so
99:26 - hold on a sec we're on the server here
99:36 - [Music]
99:38 - so write this 30 30 30 30 30 30 30 all
99:49 - right hold on so okay so let's let's go
99:56 - back to putting the shape oh no three
100:02 - shape to be okay it requires shape to be
100:04 - provided that makes sense when the
100:06 - values are a flat array of course
100:08 - there's no way it could infer the shape
100:10 - if it's a flat array so that's fine so a
100:12 - shape shape let's do this so that's this
100:20 - is correct and this is the expected
100:21 - behavior this is incorrect this should
100:28 - give me an error yes okay oh they're
100:35 - already working on a fix
100:36 - all right so should I not file the issue
100:38 - is this good this is good it's good this
100:41 - is good to file an issue I know you're
100:42 - already working on a fix let's do it
100:43 - okay new issue oh my goodness
100:48 - okay 2d 2d were tensor 2d works with a
100:57 - shape of with a with a rank three shape
101:04 - okay so the tensor flow version is this
101:13 - one so by the way this if you're filing
101:15 - an issue this is wonderful the tensor
101:16 - flow J's project has a template for the
101:19 - issue so I want to say TF just version
101:26 - browser Chrome and then I'm going to go
101:31 - to my browser and we get the version
101:38 - version is chrome okay okay while making
101:48 - a video tutorial
101:50 - I found that TF tensor 2d does not throw
101:58 - an error if I pass it a rank three shape
102:07 - ie so let's go grab this code now and
102:19 - let's paste this in here let's clean it
102:22 - up a little bit let's do this because
102:24 - this code won't work without p5 so I'm
102:28 - going to change it to this
102:39 - so by the way a couple little github
102:41 - tidbits you haven't seen this these back
102:43 - ticks are for a code block and if you
102:45 - say what language the code blocks in it
102:47 - will syntax highlight it for you we can
102:49 - go to preview here so I put my TF jazz
102:52 - version version of Chrome I wrote a
102:55 - little explanation okay so I think this
103:01 - is probably pretty good and so I'm going
103:04 - to file this issue file this issue okay
103:08 - there we go all right this is fun times
103:12 - okay so where was I in this tutorial I'm
103:19 - gonna shoot let's let's I remember I was
103:28 - so this is where I was okay alright Sam
103:44 - the krail thank you for your super chats
103:47 - it is very kind of you I appreciate them
103:50 - advise that the YouTube has this feature
103:51 - it's very nice that people want to
103:53 - support while they're watching live
103:54 - stream I also have a patreon that you
103:57 - can join if you're so inclined and that
104:00 - will get you an invite to a slack
104:01 - channel where we'll discuss stuff
104:04 - what's commented out in my bug report
104:07 - shoot oh sorry let me thank you let me
104:18 - fix that okay great
104:20 - okay thank you Sam the Grail okay
104:24 - actually Sam yeah Sam the Grail okay
104:29 - alright let's go back to here
104:32 - alright everybody all right I got it now
104:37 - so
104:39 - I'm gonna come over here hours I talking
104:44 - about this I don't remember not yeah I'm
104:46 - sorry again so even though I have I know
104:55 - I know I remember I think okay so even
104:57 - though I am talking about I'm making
105:00 - these things as a generic tensor if I
105:02 - come back and look at the documentation
105:09 - if I come back and look at the
105:12 - documentation I will actually find oh we
105:16 - don't know I talked about that then I
105:17 - did the scalar thing okay sorry I did
105:19 - all this already then I went let me go
105:21 - back from where here yep this and okay
105:45 - this is where I was this is where I want
105:50 - to go from okay so now if I go back and
105:56 - look here even though I'm also just
105:59 - using the generic TF tensor this is
106:02 - really a rank three tensor a three
106:05 - dimensional tensor so to make my code
106:08 - more readable what I want to do is and
106:12 - and also to protect myself from errors
106:14 - what I want to do is come here and use
106:17 - TF tensor 3d so again if I know I'm just
106:21 - making a one dimensional vector I'm
106:23 - gonna want to use tensor dot 1d if I'm
106:25 - just making a single number
106:26 - TF dot scalar 1d so I'm just gonna
106:29 - change that here
106:30 - TF dot tensor 3d I am now going to go
106:34 - back to the sketch I'm gonna hit refresh
106:37 - and we can see there it is here is my
106:40 - tensor which is 5 2 by 5 by 3 okay so
106:46 - this I think this concludes this
106:48 - particular video tutorial where all I
106:50 - have done is show you what is it what is
106:52 - it
106:53 - answer how do I make a tensor what is
106:55 - shape and what is data type now that I
106:59 - have this building block finished in the
107:02 - next video I need to actually talk about
107:03 - something these tensors are some are
107:05 - known as spooky music immutable whoo
107:09 - spooky whatever I'm not gonna bother
107:11 - this music they're immutable I cannot
107:13 - change the value so there is something
107:15 - called TF variable and the distinction
107:17 - between TF tensor and TF variable is
107:19 - important and I probably want to look at
107:21 - reshaping tensors although as well as I
107:26 - want to look at the operations so what
107:28 - does it mean to multiply tensors add
107:30 - stuff to tensors square tensors what are
107:33 - the kinds of operations mathematical
107:34 - operations just like there are a whole
107:37 - bunch of mathematical operations with a
107:39 - p5 Ector what are some of the
107:41 - mathematical operations with a tense
107:42 - below GS tensor ok so that's what will
107:44 - be coming in the next couple videos
107:48 - alright everybody I did not get nearly
107:52 - through nearly as much as I had hoped to
107:54 - get through today but this was good I
107:58 - think I have a better sense now actually
108:00 - of what a tensor is and and how to
108:04 - create them I do oh and I want to look
108:07 - at one all these things well I think
108:09 - though mostly things random oh this is
108:11 - so useful random uniform I got to come
108:14 - up with this but the most that stuff is
108:16 - gonna come up zeros as I that's really
108:20 - so you there's all sorts of stuff here
108:22 - by the there's all I would encourage you
108:24 - I have to go in a minute so I'm gonna
108:27 - I'm definitely gonna be back next week
108:29 - doing a lot more of this but if I can
108:31 - make it I was gonna come back this
108:33 - afternoon
108:33 - I have this sort of weird idea let me I
108:35 - might put a little thought experiment
108:37 - out there so something that I've been
108:40 - wanting to do a coding challenge for is
108:42 - the water ripples algorithm this is a
108:50 - water ripples algorithm that I used
108:52 - probably like 15 years ago in a project
108:54 - and it was used it was this wonderful
108:56 - webpage and I preferred to do this too
108:59 - it's hard for you to see so let me just
109:01 - and it has this interesting algorithm
109:05 - trying to think like would it be crazy
109:08 - to do this with tensors and it makes
109:11 - these nice like water ripple patterns I
109:13 - mean there's no need to do this
109:15 - potential oh yes and it's a little bit
109:16 - crazy probably to do that
109:18 - but anyway I was thinking of doing this
109:19 - as a coding challenge making this 2d
109:21 - water ripple based on this particular
109:24 - algorithm so so yeah so I didn't get I
109:31 - guess to match yeah I made two two
109:34 - videos edited videos will come out of
109:36 - this one is my overview thing and the
109:38 - other is the basic introduction to
109:40 - tensors and then I need to come back and
109:44 - do the rest of the stuff let me see if
109:45 - there's some questions people are asking
109:51 - [Music]
109:53 - I don't see any useful questions in the
109:58 - chat thank you Nikhil for watching
110:05 - helping answer a lot of questions and
110:06 - people were asking in the chat and
110:07 - answering my own questions and
110:09 - correcting all the things that I got
110:11 - wrong noob phobia asks how long did it
110:16 - take you to learn all the languages you
110:18 - know I started programming in really in
110:23 - 2001 so you could do the math from there
110:27 - a long time I guess I actually didn't
110:30 - start programming until I was 28 also
110:32 - which is I think important to mention
110:34 - because I get this question a lot like
110:36 - oh is it too late for me to start
110:37 - learning programming and no I don't
110:40 - think so
110:42 - good tensors be advantageous to use in
110:45 - non ML programming over matrices and
110:47 - vectors yes yes and no so I mean I the
110:53 - event I mean the concept of a tensor is
110:56 - the same thing as a vector or a matrix
110:57 - it's just more generalized and the the
111:02 - advantage you would get to using
111:03 - tensorflow j/s to have tensor operations
111:06 - in non quote unquote machine learning
111:08 - applications would be that you get the
111:10 - WebGL hardware acceleration so the real
111:12 - work that the creators and developers of
111:15 - tensorflow chass have done is
111:17 - magic of here's my arbitrary array of
111:19 - numbers make that fit into a WebGL
111:22 - texture because what gel just knows
111:24 - about image textures which are matrices
111:26 - and it does fast operations with those
111:28 - so how does this generic thing of
111:30 - numbers that I want to have this shape
111:31 - fit inside a WebGL texture that's what
111:34 - tensorflow digest is doing behind the
111:35 - scenes so that can make things really
111:36 - fast that issue is and we're gonna see
111:39 - this later is there is oh it is
111:41 - expensive computationally expensive to
111:43 - copy the data onto the graphics card so
111:45 - when I've done some experiment
111:48 - experiments where I'm kind of like doing
111:50 - things one at a time like I have this
111:51 - bit of data copied on the graphics cards
111:53 - in another bit of data copied on the
111:55 - graphics cards I'm running into
111:56 - performance issues whereas if I can get
111:58 - a whole batch of data and copy it all at
112:00 - once
112:01 - what the program's gonna run a lot
112:02 - faster so you have to think in terms of
112:04 - scale and batches and and how you're
112:06 - using it but yes there could be some
112:07 - advantages what is your math background
112:13 - dan amateur interested in math person
112:18 - that's my background I did actually
112:20 - technically major as an undergraduate
112:23 - sort of in mathematics my major was math
112:27 - and philosophy so I did take a lot of
112:28 - math courses many many years ago but I
112:31 - don't none of which do I remember my
112:34 - background is more recently in stuff
112:36 - that I've been trying to learn and
112:37 - reteach myself can you use Tesla Digest
112:41 - for decentralized ml in the browser
112:43 - probably not something that I know how
112:46 - to do Oh someone can teach me about
112:48 - decentralized ml what are the languages
112:51 - you learned okay so the languages that I
112:53 - know programming wise are well probably
112:59 - job lingo I started with the lingo
113:01 - programming languages Macromedia
113:02 - director check out John Henry Thompson's
113:04 - guest video on the coding train creator
113:06 - of the lingo programming language so
113:08 - into Java pretty pretty well from
113:10 - working with processing for many many
113:12 - years
113:12 - I know Java learned JavaScript just a
113:14 - few years ago
113:15 - to start working with p5.js and other
113:17 - web stuff that's I mean maybe I kind of
113:20 - like I can tiptoe around Python not and
113:24 - kind of get my way to the end zone if I
113:27 - have some good examples but I have to
113:28 - look every
113:29 - when I'm working if I have to Google
113:30 - will get the documentation I don't know
113:32 - any of the Python stuff inherently at
113:34 - all
113:35 - C++ I used to do a lot of C and C++ I
113:38 - forgot about that
113:39 - oh and I took a course once or I had to
113:41 - program in ada 95 alright everybody so I
113:46 - hope this was useful I hope these two
113:51 - chunks of video tutorials that I made
113:53 - will work if I can manage it if I can
113:57 - manage it I would love to come back this
113:59 - afternoon just to do a coding challenge
114:00 - that would the earliest that would be
114:02 - would be 3 p.m. Eastern Time and oh I
114:12 - should say one of the reason why I have
114:13 - to go is because I have an appointment
114:14 - to fulfill patreon rewards so I apology
114:19 - if anybody's watching there's a current
114:20 - patron of the coding train and I haven't
114:22 - sent you your book or stickers yet my
114:24 - apology apology is I don't have a good
114:26 - system I'm pretty pretty slow with that
114:27 - but a whole new batch of those is going
114:29 - out this afternoon that's it so schedule
114:37 - wise I will definitely be back next
114:39 - Friday let's look at a calendar I don't
114:42 - think I've like logged into my Google
114:44 - account so I don't want to um so let's
114:46 - look at a May April May 2002 this is
114:52 - going to incognito window right can I
114:58 - get with Google just a generic calendar
115:00 - no you have to sign in April 2000 18 all
115:08 - right so this is good oh wait May next
115:11 - Friday is me all right so looking at the
115:21 - calendar here I will be back May 4th
115:25 - this week is thisis week at ITP where I
115:28 - teach the 7th through the 11th there
115:31 - will be no live streams this week
115:33 - however if you go to ITP NYU edu slash
115:41 - thesis
115:43 - this is the schedule of thesis week and
115:46 - this whole thing is live streamed so I
115:49 - will tweet about it I'll try to make a
115:51 - post community post on YouTube but I
115:53 - encourage you to tune into ITP so thesis
115:55 - week there are some really exciting
115:57 - projects let run the gamut of so many
116:00 - different topics there's some machine
116:02 - learning ones that I definitely think
116:04 - those your interests in that topic will
116:05 - want to tune in for presentation
116:06 - schedule here's the presentation
116:08 - schedule so you can tune in during all
116:11 - these times to see all these theses so
116:13 - there will be I'm not live-streaming
116:14 - this on the coding train I wonder if I
116:16 - could like rebroadcast it somehow but so
116:22 - that's happening next week and then this
116:27 - is the ITP end of semesters show I
116:29 - usually try to do a live stream
116:30 - walkthrough interview things so that
116:32 - will happen on the 15th or the 16th and
116:34 - then the summer summer winds blows
116:39 - sweetly in from across the sea so then
116:45 - I'll be then I'm gonna really be diving
116:46 - into this content hopefully starting the
116:49 - week of the 21st of May through June
116:51 - July August I'm gonna be away quite a
116:53 - bit so August might be a month hiatus
116:55 - we'll see but that's kind of my friend
116:57 - alright so thank you everybody again for
117:02 - tuning in I
117:04 - localstorage anyway
117:12 - local storage I definitely need to do
117:14 - that hold on hold on this is where I'm
117:18 - gonna remember this stuff because this
117:23 - notes for next time oh yeah okay hold on
117:28 - let me just do this
117:30 - oh let me just add notes for next time
117:34 - local storage so these are things that I
117:37 - want to like cover next year that maybe
117:40 - don't have videos on okay so I made a
117:42 - note of that there alright um thank you
117:44 - everybody
117:45 - supposedly one of these days I'm gonna
117:46 - get one of these like altro videos to
117:48 - play here I'll just play my weird
117:49 - trailer thing as the outro video so to
117:52 - play the weird trailer thing as the
117:54 - outro video I'll be gone
117:57 - just follow the channel subscribe with
117:59 - the alarm bell will tell you if I'm
118:01 - going to come back this afternoon
118:01 - because I will set it up as an event
118:03 - with set the time for it or check my
118:06 - twitter twitter / twitter.com slash
118:08 - Schiffman i'll tweet if i'm going to be
118:10 - back this afternoon depends how today
118:13 - goes a lot to do
118:15 - okay there we go Mike
118:25 - [Music]
118:35 - [Music]
118:40 - [Music]
118:46 - [Music]
119:02 - creation
119:15 - [Music]
119:20 - [Music]
119:23 - you

Cleaned transcript:

good morning welcome here we go oh boy it's another Friday and that means it's time for the coding train now what's going on today here's the thing I have something to admit something to confess it the date is April 27 to Friday April 27th I as you may or may not know happen to have this fulltime job working Here I am in a closet because I have a building inside of New York City which is part of New York University Tisch School of the Arts ITP and soon to be ima next year as well and this is probably our busiest time and so this week has been a little bit crazy next week will be crazier still the week after crazier so the things are gonna be a little bit in flux here for the next three or four weeks I'm just checking the chat few people are actually seeing me but I'm excited about something which I have a plan for this summer which is not to break this elbow you've been watching my channel you might know that this elbow was broken last summer now it works pretty well I would say 80 90 90 some percent of what it used to be and so I actually took a twomonth hiatus last summer but I don't plan to take that hiatus this summer I spent I plan to take to spend June and July focusing on something called can you even hear that I don't know if you can hear that machine learning a JavaScript and primarily the primary they're the primary vehicle for this that I'm going to use is oh no no no don't look at that spoiler alert I have a little little stick planned that I didn't rehearse I probably should reverse that tensorflow gjs so what I'm going to try to do today I have a limited amount of time I have about an hour to and I'm gonna try to just sort of introduce a little bit about what tensorflow Jas is and my sense of it in this world as well as talk about this other library called ml5 which is a project being developed here at at NYU and ITP and Tisch School of the Arts have to remember to name everything and so on today I really want to set the stage for what I hope will be many tutorials and adventures throughout this summer okay so where to begin well the other thing is I'm kind of I'm wrapping up here this course called the nature of code spring 2018 I was kind of like trying to remember is there anything on here that I meant to do a coding challenge or tutorial about oh and I didn't put my cloaking device on this extra laptop let me see if I can find that oh here it is by the way so I have there's a little mystery we have to solve which is that there's been some activity in this room some things are in different places this should go here oh boy but this I need another piece of paper my my original cloaking device is missing but here is some more I'm very tired today by the way yesterday was national take your children to work day I had a really fun time learning about all the different things that happen here at Tish kids were here they were doing like teepees stuff I learned about making sound effects for the movies oh it was great get anything done or get do any of the preparation that I had hoped and imagined to do for this life so this is gonna be a little bit of a bust this morning I would like to do I was trying to come up with like could I at least do like some interesting short coding challenge oh I think I might have an idea for one so somebody should should give me a knock on the head virtually at about I shouldn't do this towards the end I was gonna say maybe like 1145 in an hour and I should maybe move stretches to a kind of a fun coding challenge alright so I was saying how things in this room by the way yes I was learning fully sound effects yesterday I was trying to remember what the name of it was fully I learned up that we had like a thing of dirt and there was like fake grass stuff and there was we made a raindrop sound effect with like a little like piece of leather and just doing this on it I don't really think like I want to go to the film school here that's really fun what else we learned it but some about photography there drama department we did some improv games that was fun a bunch of seven to twelve year olds doing improv games okay and oh wait hold up so I'm looking at Twitter here supposedly the people are tweeting that I'm here to talk about tensorflow DJ s which I am I should give a fair warning I have a very bad habit of wasting about 45 minutes at the beginning of my live streams before getting to any actual useful education I don't know if there is any actual useful educational content ever but if there would at least when I'm attempting to do so because I want it but so before I'm gonna try to get to this in a minute but before I begin so I was saying how like things are in different places here in this room it was the this I found here written on the whiteboard dan Shipman is a figment of our collective imagination also kittens so now we have a mystery who wrote this was this written this week is that really how my name is spelled where are the kittens are there kittens I would like to see the kittens are they small kittens cute kittens am I really a figment of your collective imagination to exist we will solve this mystery the mystery of who repetition though on the whiteboard in this room who has access to this room there's a door here the door opens people can come in people can go out but it is locked in order to enter the room you must have an NYU ID keycard that is authorized for the thing that you put the ID on so that clicks open yeah it didn't keep going you didn't get to the good part oh oh well oh the bad house good ideas come on bad okay do you want even know what that I that reference is so dated but it's still in my head look no idea what that what that was but wait why did that hold on a sec here I didn't go to the we didn't really uh what does it say they know hold on let me just put this back here my here my all right well I'm gonna workshop that you know I took this like theater class yesterday with a bunch of seven to twelve your I didn't actually take a class I sat in the corner and watched because I was a parent volunteer okay um so let me move on now and let's try to let's erase this note let's erase this note I'll read it to you one more time Dan Schiffman is a figment of our collective imagination also kittens alright okay here we go oh boy come back over here now I need to get set up with a few things here I want this website this website right what I need to actually look for here is let's go to this YouTube channel called the coding train and let's go down to it's not here neural networks and machine learning so this is what I've got so far and what I have always said in working on these video tutorials what's the idea here is that I want to talk about the idea of artificial intelligence in the broadest sense machine learning in the broadest sense deep learning in the well I think people are turning out for a second artificial intelligence and machine learning in the broadest sense then I want to create a set of tutorials where I implement simple examples to demonstrate particular topics in the 11 neuro evolution playlist should be in here as well and so that's what these playlists are doing so for example if if I come here into this playlist 10 for the don't whoa this is very confusing and I look at this playlist this is an entire set of video tutorials that I have made to discuss the idea of a neural network building something called a simple perceptron which is a model of a single neuron no network what it means to then take that perceptron and make a MOLLE multilayer perceptron with say Network then look at some of the matrix math that is behind the different algorithms that happen when data is fed forward through a network as well as when the error is sent backward through the network to adjust the network and then finally build a neural network library and ultimately then make some examples for example this I made an X or coding challenges in that neural network library I made this doodle classifier which is using the Google quickdraw data sets so these are the things that I have done to date and more recently if I come here to playlist number 11 boy what you will see here is I went off on a slight different direction for look at training a neural network with something called the genetic algorithm for the purpose of reinforcement style learning to Train and so I what example and actually there are five parts here so those have to get added to train an agent to play this flappy bird clone game thing okay so that's what I've done so far but I really wish I could have gotten organized yesterday look at this I'm logged in as myself so it has a playlist of liked videos fascinating let me go back one more time and so here's the thing it is time now for session 6 section 6 technically so chapter 11 should be here open what's really confusing about this let me try to I'm gonna try to map this out for myself I have two courses that I'm designing in my head now all these video tutorials that you know I want them to kind of stand alone and let people mix and match them as they please but in my head I have two courses I have nature of code and I have a course that I called it did a sort of version of this course at NYU called intelligence and learning okay so nature of code is a course where the first half I look at physics simulation and then to some extent also some like generative algorithms then this course moves to looking at neural networks oh I should put genetic algorithm this is a generative algorithms not to be confused with genetic algorithms and then after this the last topic of this course is now neuro evolution so this is playlist number nine for nature of code this is playlist number ten for nature code this is playlist number eleven for nature of code okay now my intelligence and learning course which doesn't really exist yet but I hope that maybe teach it next year and prepare a lot of and make a lot of video content for that over the summer it starts with this is kind of a we actually look so what it was like calling this so let me try to find it when I did it last year spring sixteen intelligence learning so this was a version of it that I did but it's it's really not okay so what did I algorithms and graphs so I started with also and I made a bunch of videos for these already this idea of algorithms graphs search and like kind of thinking about something called Big O notation I don't know if that's really if I do this course again if this fits exactly right but this is a sort of first part of that course so the reason why I put that there is with this idea of taking the broadest view of the idea of artificial intelligence what does it mean to create the illusion of intelligence or the actual practice and thoughtfulness of intelligence in a in computer software in programming with computation and so things like the a star path maze a star path finding algorithm to find the shortest path from bits start to end in a maze like graph structure is something that can fit into this so that was one then I did genetic okay okay okay so people are complaining in the chat that's fine uh maybe I should one of the things I actually did this is why people are complaining probably because I kind of promoted this a little bit I never really schedule or promote the live streams because I kind of I'm afraid for people actually tune in because I and I sort of said tensorflow dodgy a switch probably so I'm gonna get to that really soon what time is it 1100 okay so the other part of this course then let me just move on is genetic algorithms and neural networks so these this crosses over and now this is what leads me to believe to what I want to do today and so if I look back at here session one session to session three session four is somewhere sufficient five who knows this by the way this needs a little work I need to clean up these organizational structures what I want to do is all of this stuff about genetic algorithms neural networks in neuro evolution all of that used my own JavaScript library which were in these two files neural net and NDIS and matrix chess and so that what I want to do here is now well I want to look at actual oh boy that turned off yeah let me just write it here tensorflow das so the point of me doing a lot of this stuff with just my own javascript code well a lot of that for me was my own just like let me try to figure it out try stuff make some educational content have people look at it see what what gets made this should give me a foundation and basis with which to understand and start to work with a more sophisticated machine learning framework made by someone else and the machine learning framework library made by someone else that I am going to use it's tensorflow das so the very first thing I'm going to do is talk about what is tensorflow Jas the sort of core API this layers API and also this other library that I'm working on with some folks here at ITP called ml5 so I'm gonna erase this in a second and get started then what I would like to do I mean I don't know if I can get through all this today but I want to look at the basic oh you know just boils are alert the idea here is that ml 5 is the sort of highest level layer let's go to speak on top of the layers api and tetra fluro Jas which will provide some simple and easy to use examples to do some common machine learning tasks but I and so in some sense the reason for the ml 5 library is to and I'm gonna repeat myself in a second so I should probably stop talking about this is I know I want to make a lot of standalone videos and tutorials with just ml 5 but for me what I want to do is actually start with looking at some of the lower level stuff that's actually in tensorflow da Jas and build some examples so the two things that I want to do is I want to redo the XOR example which is a little bit of a trivial example but shows the sort of full story of a machine learning project where there's training data a model gets built and trained and then there is new data that comes in and the model makes guesses according to that new data we could visualize that and then I also want to do this that doodle classify are over again but using tensorflow das rather than my kind of goofy ridiculous toy javascript neural network implementation okay let me turn off notifications on my watch because they're bothering me let me come back over here and check the chat oh this turn this on and so we're going to get started so I'm gonna take a this is I'm gonna just want to leave this here so I can have this later to remember and now I am going to get started so first I need to erase this I needed to sorry than I if anybody watching was to do this part I needed to do that for my own sanity also Maxia who helps with video editing and managing the channel hopefully this might help you understand my thinking here so that we can reorganize some of these you know the websites and the different orders and playlists of different things okay okay it is 11 o'clock so seriously though this would be session six now I'm gonna consider this other place where you can find this stuff let's seal this show up here so we got some work to do on this website okay never mind if you by the way if you all haven't seen this emoji scavenger hunt project it's totally awesome I kind of want to play it right now but oh I'll do that a little aim here I'll do that the end okay so hold on there's actually a couple other things that I think would be useful to bring up here tensorflow dev summit tensorflow Jess YouTube I'm looking for the here we go this is what I'm looking for there we go okay okay and then we're gonna go do a little history lesson here okay what what what what this can't possibly be true oh I know what it is ah god I check this I was having some issues on this computer and I shut off WebGL which will really be a problem for this set of tutorials that I'm an attempt to do today where is that in in Chrome settings anybody know where are the worst of WebGL settings advanced system no wait isn't this all right in the right place hold on let's go to pro maybe I want preferences ah yeah no that's the right thing GL no somebody's gonna tell me I know GPU thank you I turns it off what I was ok GPU GPU where do I just where do I enable it no this is just the report hardware acceleration Hardwick van settings hardware acceleration okay advanced where's it van advanced advanced hardware Excel relaunch Chrome all right thank you everybody for your patience we need a picture of both Nikhil Daniel okay there we go we're back we're back everybody okay all right everyone flags that would have been another place thank you okay all right here we go everyone hello and welcome to another session of my street oddly organized perhaps hopefully somewhat organized set of videos about intelligence and learning so this where are we right now I am in the moment where I have finished a whole set of tutorials about neural networks and some basic machine learning types of things that one might do with the neural network comprehensive but a few demonstrations I built a little neural network library in JavaScript and went through some matrix math stuff so I'm ready finally at the time I've been saying this all along that I'm making this neural network library and kind of looking at how things work and trying to make some creative examples but later eventually someday I will use a more robust thoughtful welldesigned framework as the guts as the as the foundation from which I will build all of my examples and projects I'll use somebody else's machine learning code and so today is the day that I'm going to start talking about doing that and the foundation that I will be using for almost all of these videos is a project called tensorflow tasks so let's discuss for a moment where where did tensorflow jas come from so you might have heard of something called tensors weird to write this down ten sir flow now first of all you might even be asking yourself huh why is it even called tensor flow what is this thing called a tensor and now this is gonna be really important because when I start to actually look at the code for tensor flow gosh yes there's going to be stuff in there called tensors and the tensor is actually a mathematical thing it's a structure that holds numbers in it and it's really basically I mean I should look at the Wikipedia page for tensor probably and it'll give you a good I'll link to that in this video's description but we've been I've been referring to things as vectors well we have scalars which is like a single number like three we have this idea of vectors which is a list of numbers like 3 1 4 etc and we also have this idea of a matrix or matrices if I'm being consistent about singular or plural this idea of a matrix and a 2dimensional matrix might have a grid of numbers like 3 4 1 5 so a tensor is a structure a data structure essentially that really can store any ndimensional version of these types of things so this is the and because the building blocks of any machine learning algorithm are matrices of numbers this idea of tensorflow let's flow with the tensors insert animation of me flowing down the river of tensors well that happened in postproduction I seriously doubt it this is where the name tensor flow comes from so tensor flow is Google's open source machine learning library it is written you might be surprised to hear this because you might think ah tensor flow it's Python right well yes kind of sure there were people who are watching this who know more about me so if you're watching the recorded version of this check the video's description for called the correction I'll go look for all I'll try to met make any Corrections at the end but the tensor flow is actually a library written in C++ it is a lowlevel C++ library with a lot of functionality for doing machine learning now the reason why you might have thought to yourself oh isn't it Python well there simply is a sort of bindings for pythons so to speak a Python a wrapper so to speak for pi so python being a programming language that's primarily used not primarily use but it's very popular in the world of data science it makes sense if you're a data scientist and working with data and you want to do some stuff with machine learning that you would and you're already in Python you'd want to be able to access something like tensorflow so every most all in every example that you would see working with tensorflow is you're just kind of operating the lowlevel tensorflow stuff from Python in fact there are also Java bindings for tensorflow and probably other languages as well and in another youniverse if all this JavaScript stuff had never happened oh it's travel back in time and stopped JavaScript from happening maybe what would our life be like should we try that but I don't know a better or worse I would probably be investigating here right now talking about the Java bindings for tensorflow in an attempt to maybe go and use them with processing and actually this is something that I really I know that Gough read hater who is the creator of the Raspberry Pi arm version of processing has done some investigation of this and this is actually something I really would like to do but that aside this project tensorflow has been around for quite a while let me go look and find out how long it's been around and then I'll come back all right thank you apparently there was a super chat from Sam Graham taking a little break here let's look at first look let's look at tensor Wikipedia to see if I got that right geometric objects that describe linear relationship between geometric vector scalars and other tensors examples of dot product okay so I kind of got that right I kind of got that right then tensor flow let's let's look that up open source math library machine learning bubble developed by Google brain for internal Google Zeus was released on November 9th 2015 okay and let's say C++ where it where does it provides a Python ia API as well C++ but am i right about that that ultimately tensorflow itself is written in C++ even if you're using it in Python you're just kind of controlling the lowlevel math operations from Python is that right that's my understanding of it Haskell Java go rust boa and look at this March 30th 2018 okay all right I don't see anybody yelling at me Oh tensorflow documentation is oh wait okay first of all the keel is in the chat welcome thank you hello Nikhil oh boy oh no oh no I'm in trouble now Nikhil is one of them with danuel smoke off and many others that are part of the Google brain team and the big picture research group up in Cambridge at Google are the developers and creators of tensorflow digest which I'm going to get to in a minute anyway oh yes Alcott gives me a quote Google built the underlying tension flow software with the C++ programming language but I know I'm usually not so here's the thing I'm so unsure of myself with all of this machine learning stuff it's very different for me to do these tutorials like the nature of code physics simulation and steering behavior stuff like I had worked on that those examples and educational materials for like years I'm flying blind oh I was really just trying to look up the year sorry matcha for creating this edit 2015 okay so it was started in 2011 and was open sourced in 2015 okay okay Nikhil is giving me positive positive feedback I very much appreciate that okay thank you thank you all right okay I'll have to go look that up tensorflow actually started as a proprietary project at Google in 2011 and it was open source under the Apache License in 2015 so I don't know we can say open sourced in 2015 and it's a it's a project developed by the Google brain team who as I've learned recently I am NOT an official representative of Google in any way so don't get all this stuff wrong you could be up there doing a fine job over there I thought that can stay from the live street but I just now I'm not gonna turn red in a second I'm getting very embarrassed let me let me let's let's just do that little bit over again sorry for you live viewers I really try to not do this but since I had this like break moment anyway okay so I'm back I had to look that up tensorflow was open sourced in 2015 so tasteful actually is a project according to Wikipedia started in 2011 was a proprietary machine learning library used at Google for doing all sorts of stuff with neural networks and deep learning and more and then it was open sourced in 2015 under the Apache License so here's the thing last year I actually spent some time making some Python examples in tensorflow and I wanted them to talk to JavaScript so what I actually did is I wrote something called a flask server which is a Python flaps kiss kind of like flask is to Python as node is to JavaScript I'm sure that's wrong in many ways and then what I did is I had my p5 sketch talked to that flask server the flask server did Python stuff with tensorflow and then I could do machine learning tasks from within p5 and this is what I want to do I want to be able to demonstrate and make examples and show things about how in a beginnerfriendly programming library like p5 or just didn't native vanilla JavaScript or using three Jas or whatever JavaScript world you live in I lived in the p5 world most of the time I want to be able to try to do some tensorflow East stuff and so this was the way I was doing it last year in the nature of code intelligence and learning course that I attempted to teach over the summer I think it was last summer a project appeared a project appeared and it was called deep learned is now this is a my sense of this project is that this was a speculative project the idea behind deep learned digests is haha can we do this kind of stuff in JavaScript and if so how so one of the things that's special about doing machine learning in today's modern era with tensorflow is in addition to this whole landscape of all this stuff where these operations that are written in C++ actually get executed they you have this question of do they get executed on the cpu or do they get executed on the GPU and why should we care about this well the CPU the processing unit the computer's processing unit is the C stands for go out so is a little thing that chugs along and kind of does most of the work that your computer has to do some time some time in days of your video games and special effects and graphics needed more and more processing and computing power so graphics processing units were created graphics processing units were created and optimized to work with pictures images pixels what are images they are matrices of pixels remember though is talking about how matrices are important to tensors and deep learning all of the mathematical operations that happen in Network our matrix based operations multiply these matrices together add these matrices sum these matrices past this activation function over this matrix that sort of stuff so the fact that over years and years and years that graphics processing units got optimized heavily to work with twodimensional arrays of color information pixels it so happens that all this matrix stuff could be used with GPUs as well so this is really it's is why we deep loop the term deep learning from my point of view it's kind of in a way of like a rebrand of neural net machine learning with neural networks but now we live in an age of big data sets and really powerful GPUs and a lot of this modern research is coming from the fact that these older algorithms that we didn't think could do as much can do more now in the context of where we live now ok why am I saying this so how is this gonna work if we have a JavaScript implementation of tensorflow is the idea to just have another set of bindings so you're really just controlling C++ from JavaScript well that is certainly a possibility and I believe that exists or at least is in development there is a node dot J S package for working with tensor flow that actually connects directly to the C++ implementation it has a relationship to the attention flow just enough that I'm going to talk about here but that's not what I'm talking about here what the creators of deep learned is Nikhil and Daniel more information about them and the rest of the research teams that they work with in this video's description and want to miss credit anybody important they didn't actually write something to control native C++ GPU they actually just rewrote all the C++ algorithms loosely I don't know about all what's implemented so far not in JavaScript and isn't that gonna be really slow isn't that a terrible idea well first of all if you're me I like things from slow who cares I just want the stuff to run I want to play with it I want to learn about it I can always use something else to get it to run faster later but maybe in JavaScript alone it would run just way too slow there happens to be something in the world javascript called WebGL WebGL is the browser's interface to OpenGL for doing operations on the graphics card for drawing and making graphic stuff happened the browser so if the math operations of tensorflow in c++ can run on the GPU why can't the math operations inside of this thing called deep learn one via the GPU via WebGL so that's really the magic in my mind of what was accomplished with this original project called deep learned uh chance so let's go look at that website for deep learning as first second look here it is this is deep learn J's or don't go there why because deep learned digest has become tensorflow dot yes so this speculative project that I believe was started last summer 2017 while getting that slightly wrong looking in the chat clean Oh Nikhil was there central processing unit oh shoot a little correction here C is for central central processing unit Thank You Simon oh I've written stuff off the screen to hold on would I right off the screen oh it's just some question marks that's fine hold on looking in the chat here on March 30th less than a month ago deep learn Jack J s became adopt this speculative project of doing these machine learning stuff in JavaScript was adopted by the larger tensorflow project itself and there and and has become this vert this project called tensorflow j s woof so tensorflow j ass this is now the project oh we can write that over here by the way if you can't see what's written up there it's just some question marks sorry about that tensor flow is so we're gonna circle that we're gonna put some hearts on it and a few stars on this is now the framework that I am planning to use in my ended set of tutorials that you may or may not choose to and I may or may not choose to make coz eyes of right now I haven't made them yet but that's my plan there's some more stuff I want to say about this I wonder if I should move on to another video because I want to talk about the let me just mention it briefly here I don't know if you want to edit around this match yet but so there's two more things that I want to mention about this and I'm gonna get to a lot of this a lot more later but tensorflow Tijs isn't actually just once I mean it is one thing it's a project but there are two pieces of it oh yeah actually this is important for this video you might have also heard of something called Karis have you heard of Karis where can I put that let me put a Karis up here can you see that no you cannot let's try this again okay I probably should just point this camera a little bit higher since I tend to write alright alright what is there something I can erase here what I'm gonna do is I want to talk about Karis the layers API and ml5 all right I think I can erase some stuff let me just let this sit in a video for a second so they're gonna be a picture of it case I want to the question parts are very important yes okay where are we timewise here 11 oh my god so little time all right alright there's something else that's important as part of the picture here that I want to talk about and to do so I'm gonna just erase this area over here so okay so we have this terminology in programming high level versus low level and I actually saw a discussion about this going on in the chat there are low level programming languages there are high level programming languages one way to think about that is low level is actually you're manipulating the RAM and the data in the central processing unit like you just you're all the way in there and the deepest part of the computer moving the numbers around yourself versus high level is something like really high level is like the scratch programming environment for kids where I'm like moving puzzle pieces and blocks around to try to create an algorithm so that's one way of thinking about high level low level so it's kind of there could be this sort of a level of abstraction so tensorflow if I were to make I guess I should put low on the bottom tensorflow in terms of working with machine learning operations tensorflow is a lowlevel library to do the actual matrix math and gradient descent learning training algorithms all yourself written into the code yourself with tential it's common operations that are implemented for you but this is really lowlevel control of the algorithm itself you could invent new machine learning models by writing them in tensorflow yourself then in between that there previously was a proper still is sorry this project called Charis Charis Charis Charis I don't have pronounce it can't laughs I always think everything is French for some reason so and that's probably not even a French pronunciation of that word but that aside sorry sorry that you had to watch that caris was meant to be a higher level API built on top of tensorflow and in fact Kerris was actually originally designed to be a higher level API that could sit on top of a variety of other lowlevel machine learning frameworks so for example there's something called Theano is that what is called I think it's called Theano there's like pi/2 which is maybe well pi/2 is torch and then there's pie tortoises Python story so there's all these other lowerlevel machine learning frameworks that clearly am NOT an expert on we just say that again there's all these other machine lowerlevel machine learning frameworks I clearly have not an expert on but Karis the idea of the countess's you could kind of write your code make a machine learning thing and it could it could operate on top of any of these so Karis though however more recently became part of the tensorflow project itself and so Karis is actually intensive lower linked together so this is a higher level API that's written on that's built on top of tension flow and it exists as part as tensorflow Jas and it's so in tension flow digest there's no actual concept of Karis specifically but there is the core API and then what's called the layers API and the layers API is something that I'm gonna use much more in my video tutorials although I'm gonna start with a few that just look at the core stuff because it's kind of important to have a sense of what that is and how that works but layers so layers intensive logic is the equivalent of this thing called Kerris now a project that's being developed here at new york university with some collaborators from ITP and guests and researchers and students is a project called ml 5 the 5 in ml 5 is an omage homage that's French right to the 5 in p5 in the sense that I mean this is flawed for many reasons but in p5 you could think of as like a wrapper on top of canvas and Dom to jeju's like common creative coding functions to make drawing and making pictures and doing creative sketching projects a bit easier and friendlier in JavaScript ml 5 is yet another layer on top of well sorry I shouldn't put this it's it's only for JavaScript as a layer on top of tensorflow Jas to do some common to allow to kind of like even abstract the concepts even a bit further and you know I think one of the goals of ml5 is for it to be a library that high school class could use a kind of weekend workshop for artists could use these sort of context of people wanting to get a basic understanding and try some machine learning stuff out so anyway so this is all the stuff whether it was a long introduction to all these pieces this is all the stuff that I'm hoping to cover over the next several months here on this youtube channel called the coding tre it's really it's like embarrassing that I keep blowing this train whistle you know that's what I do and so that's the plan so you can kind of pick and choose ultimately you might be able see what's available for you at the time of watching this but ultimately you might want to skip ahead and look at some of these ml5 tutorials because you don't necessarily to do the ml5 examples you don't necessarily need to have a knowledge of the core api of tensorflow chasse or the layers api even but I'm gonna start even though I might but the goal for the ml5 Biggs librarian examples is to give people a starting place that you don't need to have gone through all the lowerlevel stuff for my own kind of sanity and figuring this stuff out also ml5 doesn't actually have a public release yet whatever that means but the goal is sort of like have a quoteunquote public release in June with more documentation examples and features my I'm gonna start very first thing I'm gonna do in the next video is just look at the core API in attentive flow yes and see like what some of the things you can make do with it what some of the functionality is and that kind of stuff all right how am i doing I think good I probably made a bunch of mistakes and missed a bunch of things so check the video's description cuz I'll write corrections and stuff in there and also I will in the next video if you continue on whatever the Google's YouTube machine learning algorithm tells you to watch we'll hopefully have some anything that needs to be corrected alright thanks for watching this I hope this was a helpful picture of my of all these pieces and my thinking as it relates to them look Wow okay goodbye all right um well this camera went off oh okay all right so I don't know I'm looking for questions now don't be so okay thank you thank you to what the web who said that I presented the information very well don't be so hard on yourself I appreciate that all right it's helpful to have the friendly feedback in the chat I appreciate it oh this computer the one that's invisible to you is about to die no that's not it for today I'm gonna keep going I have I have about another hour yes so maybe there's some new viewers I do a little bit I have a sort of weird workflow it's actually not that weird anywhere I've been noticing other people do something similar but which is that I do these live streams and the live streams are what used to be my own private recording sessions basically where I would like record a tutorial and then upload it later so now I just broadcast those live and then though the pieces of this that are and have some value will get edited down to something slightly shorter and uploaded and put into a playlist so that it could be used as part of a course or us just for you know for people when it does watch it just watch that that particular part so that so whatever all that the live stream will remain archives forever well as long as not forever I mean I'll be dead but if you're like really really let's say it's like the year 3000 and you're watching this maybe you could revive me somehow I don't know so I couldn't know that you're watching this somehow in the year 3000 with your maybe it's like actually like appearing you don't even have eyes anymore anyway I'm talking to a creature from the year 3000 that I had now on YouTube that makes no sense at all so the live stream would be archived and then also the edited videos will be in different playlists okay don't forget any semicolons today Ricardo asks will you teach web assembly in the future I guess I would have to so two options there are one I learned web assembly and then teach it option number two is I just attempt to like look at it during the live stream without knowing if it and they think about it cuz I don't really know whether sembly which is actually what I'm kind of doing with those machine learning stuff it was like honestly like I as much as I've been looking at tensorflow das and talking about it and I did try to make a project with it actually earlier this week so this isn't totally true I haven't really done the stuff that I'm about to talk about okay okay ah different Nikhil in the chat asks Dan will you go for dialogue flow dialogue flow is something I'm planning to cover next fall so I have a course that I teach called let's see if this will program from A to Z ooh look at this oh boy Oh electron that's another thing oh so many things anyway this is a course that I teach I'm gonna be teaching it again a tizzy f18 doesn't exist yet that'll be this coming fall and I'm planning to add dialogue flow as a module for that course so whenever I get to it I'm definitely gonna use it alright alright alright looking at the chat I'm getting messages from the future which is that maybe somebody traffic could travel back in time no no here look did you invent time travel and somehow you invent a time travel and you're watching this video travel back in time go to well maybe I shouldn't reveal where I am right now that sound like a wait you're smart you're from the future you know where I am now it's 1135 a.m. Eastern Daylight Time finally knock on the door if you have an issue with security downstairs tell them you're here to see you're from the future and you're here to see how do you pronounce you gotta get this very tricky to pronounce my name it's Daniel she's fun say you're here to see Dan Schiffman on and and they'll let you up you could figure out my phone number because you're from the future you could look it up in the archives alright so maybe somebody will knock at the door we'll see Wow suspense I could try my wait not this one okay never mind I was trying to get the serial music again okay um so let's see what do I want to do now I wanted to do a coding challenge today just to have the code maybe I could come back this afternoon maybe I'll so I'm gonna keep going with this oh I forgot to mention the whole point of this was I was gonna mention I had this video here for people to watch oh okay hold on alright this is the thing that I forgot I'll just do it in the beginning of the next video let me answer two quick questions that I see in the chat bridge fee asks how do you tell someone to learn Java scripts from scratch using what resources well I mean I don't want to toot my own train whistle but I do have a set of videos that are for total beginners to learn JavaScript and the p5 library acts as kind of the conduit for that maybe that's not for everybody but that certainly would be an option there's a book this is not really for beginners but a book that I really love is uh maybe this for beginners is eloquent JavaScript this is a wonderful book and I always used this as a reference to try to learn about how stuff in JavaScript actually works those would be two thoughts oh there's another question Jack goes too fast I was like there was a second question I was gonna answer but I don't remember what it was what was it we learn JavaScript and I can't remember okay okay so let's move on here and let's try to do a little tutorial about so I think I can get rid of all of this yeah so the next thing I'm going to do is just look at the core tensorflow jsapi so do you think one thing I want to do is let's find some better definitions what a tensor is and that was yeah let's so that'll be good for starting us out here let me get basic books p5 sketch going oh I'm in the wrong screen p5 tensor flow and Adam and oops what is this here the whiteboard totally needs a fresh coat of paint I know let's open this up in the Adam editor and let's run where am I let's run a little web server should really I don't know should just use the node one I should use the live server two but it's the reason the live server kind of makes me a little bit crazy and then okay so this is good actually I'm gonna I need a new template for the p5 manager really badly v5 CDN let's just do I mean I'm probably not gonna use p5 so I don't know even why I'm bothering with this very silly what I'm doing very very very silly one of these days I'm gonna just spend some time oh no so now this has to be 8080 and here we go yeah look at this okay alright so I think I'm just about ready here now okay so what do I need I want tensorflow touch yes and now five orange reference this okay alright alright somebody told me that the tensorflow what is a tensor the tensor flow documentation itself well look at this I would like to know this what is the difference between a matrix and a tensor Oh Oh general eyes matrix that's what I thought ah I wish I had said that but I think I sort of said that okay all right fine this is good this is helpful for me though this makes sense okay I'm good with I think I'm fine but I explained all right okay it's 1145 timer that thing I wanted to do yeah maybe I'll come back this afternoon if I can I have to leave well hold on let me just determine what time I have to leave okay Oh interesting you're interesting so I could okay okay all right that's fine okay sorry no this is sorry I'm like now I'm looking at my email which I was trying to check this where my schedule today so it's possible today between like three and four that I might be able to come back for a coding challenge let's see yes come in L you through the type of thing I don't I yes thank you Ricardo in the chat who is giving me helpful tips that I will never probably follow because I'm an old person with a calcified brain and I can't do anything but just type clear okay so we want to be here want to be here won't be here want to be here all right all right here we go so it's guilt Liz de asked is it possible to use GPU with tensorflow dodge ass not only is it possible it is impossible to not use the GPU tensorflow dodge yes as far as I know only works with the GPU so if you are working with a web browser does is that that does not support WebGL it will not it will not you will not be able to do it okay hello I'm back to for another video where I'm going to look at the core API of tensorflow dodge as and in my introductory video I totally forgot to mention and link to this particular announcement video machine learning in JavaScript from the tensorflow dev summit where Nikhil Thorat and Daniel smilk off the creators of tensorflow TAS talked about the project and you should watch that because it'll give you a lot more background and also that show interesting demos and other things that people are working on with it okay so I wanted to mention that now okay so if you didn't watch my previous video which is kind of an overview of the landscape of all the pieces around working with machine learning in JavaScript you could go watch that or you could just be right here because what I'm gonna do in this video is I'm just gonna go to this API reference and in this API reference what I want to do is talk about the basic building blocks of tensorflow jazz now here's the thing again I just want to mention I just want to mention that you don't have to be here right now anywhere else in the world than watching this video and ultimately I am going to be using something called the layers API which is part of tension flow digest which is a bit of a higher level abstraction then the stuff I'm going to start to look at in this video and I am also going to use a project called ml 5 which is a separate project being developed here at New York University which is built on top oh I guess oh boy this why what timeout timeout not time up we'll write that lower down ultimately I'm going to start using something called the layers API which is built on top which is part of tensorflow Jas so there's court enter flow Jas ultimately I'm going for projects that I'm gonna make I'm too useful the layers API which is a higher level abstraction and then even higher if I'm going inverse like down is higher level eventually I'm also going to use something called ml 5 which is a project being developed here at New York University which is a JavaScript library separate project from tensorflow GS but uses tensorflow J's behind the scenes to do some to show demonstrations of common machine learning tasks like image classification poetry generation and and things like that ok so but for me for my own sanity I would like to learn the basics of the core API of tensorflow yes and I think it's useful as foundational knowledge for moving along and looking at some of this other stuff but if you just want to get to like getting your webcam and trying to classify the images that are in your webcam you can you could find some videos that aren't made yet but they will be made maybe I don't know it's very confusing I'm gonna make them within they'll fly when we get it to that ok core API what is the core core core building block of everything in tensorflow it is something called a tensor and I talked about this in my previous video but just to recap we have this idea of a scalar which is a single number we have this idea of a vector which is a onedimensional list of numbers and we have this idea of a matrix which is really a two dimensional grid of numbers a tensor is a sort of more generic term that refers to any ndimensional thing of numbers and also the operations associated with those things like matrix multiple caishen and so this idea of this idea of a tensor being this building about it's a thing you can make and perform the common mathematical operation it's actually quite similar to in p5 if you watch my p5.js tutorials I use this create vector function so this create vector function is making a vector and actually three to two or three dimensional vector so it's always just like an XY inventory it's always just an X or Y and sometimes a Z and so then you can make these create vectors and you can get their magnitudes and normalize them two common mathematical operations the tensor is exactly the same thing so if we now go and look at the at the API here we can see here it is TF tensor so how do we even make a tensor let's just make a tensor so first of all I have some code I have some code not very little code I have a single file called sketched out JavaScript and you would think like okay let me just go and take this and let me just put it in here I'm gonna hit save then I'm gonna go back to the browser I'm gonna hit refresh and uncaught reference error TF is not defined so why is TF not defined well it's not defined because I need to import the tensor flow dot J's library so a way that I can do that I'm actually just going to Google I'm sure it's like in an obvious place but tensorflow J s CDN and then oh it's actually right here on the home page right here this is what I'm looking for I'm gonna grab this bit of code right here because what I want to do is reference the tensile voce has library file via a CD and a CDN is a content delivery network I could download the library include potential ojs javascript file in my project it's a little easier for me right now just to go into index.html you can see I'm actually referencing P 5 and P 5 Dom libraries to see the ends I don't know that I've been actually to use the P 5 library in this video but I can just add one more here and now I have tensorflow tachi s imported as part of my project and I'm gonna go over here I'm gonna hit refresh I look at that ha ha it's our first tensor we made a tensor we paid a tensor just take a break now for a minute I'll be right back okay yeah so people are mentioned require I mean maybe it's worth noting oh there is a cpu backend I'm told ok other laptop could use a small adjustment oh thank you oh yeah where's is it here there we go so better I think that's better but I can't really see it now ok so where are we now all right so I'm taking a break ok always what I'm going to do next a scalar 1d 2d ok ok shaped candy go with the shape now if I make a 2d 3d what I'm confused about here is or D buffer hold on a nested array of numbers flat array or type 2 right okay shape so what's interesting about this is I understand the idea of a shape and datatype well let me let me experiment for a second here I'm gonna come back to the video in a second but let's say if I make actually let me not make this a Const cuz I'm gonna wanna like alright actually I'm just do something ridiculous cuz I'm gonna want to like do this over and over again oh all right TF dot tensor so that's the whole thing so this we can see the datatype the shape is just to the size is 2 so I can okay hold on so if I make a tensor 2d see this is what I meant to do yesterday I was going to like look through all this stuff more detail tensor 2d requires the shape to be provided when values are a flat type oh so it has to have a shape because the values are not got it that makes sense so the shape could be because it's 2d it could be like 2 by 2 and then it would be this right so this would be a 2 by 2 tensor with four numbers right that makes sense and then we're seeing it like this that makes sense so what I don't understand is what if I want to have a because I could have like four of these now what work I see so if I wanted to have a 2 by a 2 D tensor but I wanted four of them so there could be multiple dimensions but ultimately the data itself is 2 by 2 is that sort of the idea so like for example if I want have two of these this would work and I've got yeah two to two by two tensors that makes sense and right because otherwise if I didn't give it if I just said it was two by two it would give me an error I see and then and then sorry I'm just talking myself through this before I go back into this like try to like make sure I kind of understand this oh oh nikhil is giving me feedback thank you Alka for pasting those in okay so I'm just and then I see so if I want it will be it could be inferred the shape can't be inferred right if it's flat but if I did this this would work no I would actually if I wanted to infer the shape completely I've got to actually do this just curious just trying to understand this will be okay I know I don't need to go into all this detail I just know that didn't work I guess you really need to you really need to pass the shape in the product of the entries in the shape must equal the number of values okay wait Nikhil is giving me actual information the product let's write that down the product and the number all right well that is our the project so I just was curious like when can it infer the shape it really can only infer the shape if you're making a single tensor that matches 2d 3d right r 1d I think okay all right so let's come back here and we're I forgot where I was leaving this off all right oh I need that thing where I copy/paste tensor TF tensor 2d all right yeah yeah okay so this yeah cool okay all right it can infer if you do this yeah yeah okay thank you all right shape is important I'm not I'm gonna not worry about the the the the okay so okay here we go let's let's keep moving now okay so I've made my first tensor now one of the things you'll notice if I go to the tensor flow documentation whoops is that in addition to just TF dot tensor and there's also by the way oh look at this I need to talk about shape and datatype but just for a moment I want to just look there also is this o TF scaler we're going to talk about that TF tensor 1d 2d okay so here's the thing this funk this idea of a tensor TF tensor is a generic concept that will work for any ndimensional tensor but if you're working with tensor flow Jas you want it to the extent possible use the functions that specify something about the shape so what do I mean by shape oh boy so here's something a really important concept a shape this shape refers to the dimensions of the tensor so for example we might this right here this has a shape that's 2 by 2 I could write this this now has a shape because you always say the rose first with matrix this is 2 rows by 3 columns 2 by 3 so one thing that's important when creating tensors is to also specify the shape like why does this even matter what we're just kind of in the lowlevel land here just to get a sense of how these things are the reason why this matters for example imagine if what I'm ultimately going to do is feed in image data into a neural network for some tasks like image classification well I want and what I have are lots of images maybe all those images are 28 by 28 pixels so my shape is going to be 28 comma 28 and let's say that I actually have a hundred of them I have a hundred 28 by 28 images this is exactly what I'm gonna have for the doodle classifier example that I'll make at some point then the shape of all of the data is 100 comma 28 28 because I have 28 by 28 images and I have a hundred of them so this is a really sort of important piece this idea of the shape is a really important piece of defining a tensor so let's take a look at that real quick so if I come back to but I have no plan for what up to it here really should play it these out so let's come back and let's go back to here and let's just do this in the console so I'm gonna say by the way one thing I didn't I mean I guess I could keep doing this in my code let's keep doing in my code so I have a record of it so um one thing I didn't mention by the way is why am I saying TF dot tensor and first of all I'm gonna write this in a different way I'm going to say constant nums let me call it data data equals let's write this out like this and then data dot print so let's look at this so one thing that's a key here that I'm using the the way of declaring a variable called Const for constant which means I can't reassign this object and so this is what you're gonna commonly see in the tensor flow to tutorials I could say let data but in a sense I'm using consequence I'm protecting myself from reassigning the variable data to something else later so I'm gonna say Const data then data dot print will go back here and we'll see there's the tensor now incidentally what if I say console dot log data so the tensor that print function is a helper function in there to let you sort of see just the information the data that's in the tensor but if I actually say console dot log data what you're actually gonna see is look at this whole thing so a tensor itself is actually this complex object it has functions associated with it look at this there's that shape the shape is just for right it's one dimensional and there's just four things in it its size is four meaning there's four right so this is really an and somewhere in here I probably could find the data if I looked hard enough rank one Strides object boy where is that data time out for a second right if you so if you pass a flat alright by the way this is great that Nikhil isn't the chat correcting everything so hopefully people are actually learning something from the Kiehl's comment I appreciate that and thank you Alka for pasting in the Kiehl's comments yeah I'm actually just curious here is can I find the data somewhere in here where would it be values no this is interesting is it just like pointing to like a memory address in WebGL or something like that this shows how little I know all right well I'm not going to worry about this ten start to strain will gives you that pretty print from print oh that's good to know okay okay okay what time is it twelve o'clock oh I'm doing terribly fine this is gonna take me a long time to get through all this stuff alright alright I'm gonna move on so so this is useful for us to be able to see those are a properties of it but most of the time we just want to look at the data so another thing that I could do here is I could say console dot log data to string so data dot print just takes the string version of the tensor and puts it into the console so if I do this hit refresh we can see there's that tensor as well okay now here's the thing what if what I want to do is this these are pixel values oh let's actually say these are pixel values like 0 0 127 255 and what I want is for these pixel values to be a 2 by 2 image to represent a 2 by 2 image so I want to create now I want to create the tensor with a 2 by 2 shape time out for a second my calendar is giving me alerts cuz I have meetings and things coming up it's very hard to make all this stuff happen ok ok so now let's look at what we see in the console now that I've defined a shape look at that we can see that it's basically a twodimensional array and what I could do is what's interesting about this let's say I had two of these images so now there's eight values and we'll make these like totally different numbers now what would happen if I try to turn eight numbers into a 2x2 a tensor a 2x2 tensor look at this I'm getting an error so this by the way this is an error you're probably gonna see throughout your life if you go down this road now not exactly this error but constructing tensor of shape for should match the length of values 8 so this means like hey you gave me eight values but you're trying to make a sense a tensor that only has four values I can't do that and this is actually quite a common error I was trying to work on a doodle classifier with tensorflow da Chasse earlier this week and I kept getting all these errors because I was trying to train my dataset what didn't match the size of what the machine learning model expected so we're getting a little baby steps into that this idea of if I'm preparing data as a big of I'm loading it from a file a spreadsheet and I have a big list of numbers I better have the right and a lot of numbers to put it in the right size tensor and so this can now be corrected in the code because I can say well actually what I want and it's awkward that all these numbers are just two but hopefully you're following me here is I want to have two by two and I want two of them so now if I add the shape is really and let's well I'm gonna give leave it as an exercise to you try to redo this but use like a three by five and so now I'm actually gonna do that myself I'm gonna hit refresh you can see there we go there's my essentially this is the shape now it's two by two by two and actually what's interesting here also is I really am thinking of these as two dimensional tensors so one thing away hold on no no stop roll that back for a second I'm gonna go in a slightly different order okay yes size means total number of elements I believe all right okay okay so let's do a couple more things here let's say console dot log data just so I can see now so what I have here is that the sort of pretty version of the data there it is those are all my numbers now shaped into these arrays now also here look at this I can see that the shape is two by two by two the size is eight meaning there's eight total numbers the type this is something I haven't talked about type is important I'm putting floatingpoint numbers in there so this that's the default type for example this could be 1 27 point 5 and if I'd run this again we're gonna see one twenty seven point five is in there however I could have changed the type to if we go look at the documentation int 32 so for example I could say you know what I want to be more I don't need to have floatingpoint numbers so I could change the data type to in 32 and that's just one more argument for me to add here in 32 now let me run this again and you could see it actually worked fine it didn't complain that I tried to give it a floatingpoint number but it just took off the decimal place and my assumption here is it's not going to round it like if I make this one twenty seven point nine 99 we're still gonna see 127 there it's always going to floor that value meaning taking take off the decimal place so we can see now in making it what's it why is this off there we can see now in making a tenser we have three important properties essentially we have the values these are the numbers that are going to go in the tensor we have the shape which is defining the dimensionality of the arrays of data basically and then we also have D type or data type which is saying what goes in that tensor and the only possibilities are floats intz or boolean z' so you can imagine just like if you know you only need integers you're gonna save some memory or some GP Younis by using integers instead of floats so this is what it means to make a tensor so I want to do two more things before I move on to the next video we'll start to look at some operations on mathematical operations on these tensors and and and also I need to talk about the difference being at so let's do a couple more things number one is let's let's um let's do something a little more interesting let's make a a tensor that had that is um five rows by three columns five by three so I need fifteen numbers so let's let's make an array well I'm gonna I'm gonna be a human who uses the constant at values is an array and I'm gonna just say I equals zero eyes less than fifteen i plus plus I'm going to make up some pretend data and I'm going to use P five random function I could say math dot random if I wasn't using P five between zero and 100 and then what I want to do is and then I'm gonna then I'm going to make a shape and I'm gonna say the shape is five by three then I'm going to say Const data equals TF tensor with the values and the shape so this is perhaps a bit more like how you might want to do it right for example I load this is me loading in a lot of data from a spreadsheet or another API or loading image files and converting them to pixels there's actually from pixels function intention flow digest that will just take pixel data and turn it into a tensor but so basically so so I have 15 numbers I set the shape and now I have this tensor and now we're gonna look at it down here let's take a look and see if this worked so now we can see now there's a little bit awkward to look at cuz of all but there we go look at this it is five by three five rows three columns so this is working this is good now what happens if I were to have 30 numbers I get that error but again this could be 2 by 5 by 3 and now you can see I have two chunks of 5 by 3 data so now at least we're able to see how this kind of shape stuff works and if I wanted to I could also add back in int 32 because maybe what I want are just integers and you can see it looks a little nicer there now here's the thing all this time I have been using just a generic tensor TF tensor but if we look at the API and this is the last thing that I wanted to point out here if we look at the API what you'll actually see is first of all it's scalar so this is rank of 0 meaning a single number rank 0 there's a idea of a ranking here tensorflow 1d is rank 1 that means a vector tensor flute atf tensor 2d is a matrix that means a matrix and 3 so even though you can just use TF tensor it's going to make your code more readable and you're gonna protect yourself for more errors if you use in here the actual if you actually specify the rank that you intend so for example if I just go back to the console for a second and I hit clear I could say scalar 4 so this now num dot print is just a tensor with a single number that's scalar now again I could have said num T equals TF tensor four and then I could say dumb t dot print this is exactly the same right TF tensor for or TF scalar but I've have possibly more legible readable code by saying TF dot scaler that's good right okay now let's think about what's going on here well this is really I really thinking of this as a 2 D tensor but I have a rank two but I have more than one of them will this work for me if I say tensor 2d let's take a look it does now if I were to say tensor 3d with this work yeah why is that bit but not if I went back to 15 right oh no I guess it doesn't wait hold on yeah okay well hold on let me think about this well I have to stop for a second this is where I oh wait hold on I got a look at the chat here yeah oh people are talking about in 32 versus float in 32 versus float I should have mentioned that let me let me mention that in the chat so wait I'm I don't know why I've really confused myself about 2d or 3d so let's look at the let's look at the documentation here for a second let's look at 3d example yes so this is really okay so really 4d yeah so really I'm really I'm like dude so that's my question here is that's my question here let me think about this for a second which is correct in this instance let me go back which is correct here I think this is really this is correct right because it's rank three I guess it just works anyway because I'm specifying the shape I see so if I didn't give it the shape but no it whoops oh did I oh whoops I lost the I think you found a bug sensor 2d the shape should have two values tensor 3d the shape should have three vows oh good I have woohoo I did something useful today should we file it as a github issue let's follow does a kid hub issue everybody even though I have to go in about ten minutes I haven't even finished making this one tutorial okay let's file this is a github issue let's go let's be good citizens everybody tensorflow J ass github let's go to issues and let's figure out this bug so hold on a sec we're on the server here so write this 30 30 30 30 30 30 30 all right hold on so okay so let's let's go back to putting the shape oh no three shape to be okay it requires shape to be provided that makes sense when the values are a flat array of course there's no way it could infer the shape if it's a flat array so that's fine so a shape shape let's do this so that's this is correct and this is the expected behavior this is incorrect this should give me an error yes okay oh they're already working on a fix all right so should I not file the issue is this good this is good it's good this is good to file an issue I know you're already working on a fix let's do it okay new issue oh my goodness okay 2d 2d were tensor 2d works with a shape of with a with a rank three shape okay so the tensor flow version is this one so by the way this if you're filing an issue this is wonderful the tensor flow J's project has a template for the issue so I want to say TF just version browser Chrome and then I'm going to go to my browser and we get the version version is chrome okay okay while making a video tutorial I found that TF tensor 2d does not throw an error if I pass it a rank three shape ie so let's go grab this code now and let's paste this in here let's clean it up a little bit let's do this because this code won't work without p5 so I'm going to change it to this so by the way a couple little github tidbits you haven't seen this these back ticks are for a code block and if you say what language the code blocks in it will syntax highlight it for you we can go to preview here so I put my TF jazz version version of Chrome I wrote a little explanation okay so I think this is probably pretty good and so I'm going to file this issue file this issue okay there we go all right this is fun times okay so where was I in this tutorial I'm gonna shoot let's let's I remember I was so this is where I was okay alright Sam the krail thank you for your super chats it is very kind of you I appreciate them advise that the YouTube has this feature it's very nice that people want to support while they're watching live stream I also have a patreon that you can join if you're so inclined and that will get you an invite to a slack channel where we'll discuss stuff what's commented out in my bug report shoot oh sorry let me thank you let me fix that okay great okay thank you Sam the Grail okay actually Sam yeah Sam the Grail okay alright let's go back to here alright everybody all right I got it now so I'm gonna come over here hours I talking about this I don't remember not yeah I'm sorry again so even though I have I know I know I remember I think okay so even though I am talking about I'm making these things as a generic tensor if I come back and look at the documentation if I come back and look at the documentation I will actually find oh we don't know I talked about that then I did the scalar thing okay sorry I did all this already then I went let me go back from where here yep this and okay this is where I was this is where I want to go from okay so now if I go back and look here even though I'm also just using the generic TF tensor this is really a rank three tensor a three dimensional tensor so to make my code more readable what I want to do is and and also to protect myself from errors what I want to do is come here and use TF tensor 3d so again if I know I'm just making a one dimensional vector I'm gonna want to use tensor dot 1d if I'm just making a single number TF dot scalar 1d so I'm just gonna change that here TF dot tensor 3d I am now going to go back to the sketch I'm gonna hit refresh and we can see there it is here is my tensor which is 5 2 by 5 by 3 okay so this I think this concludes this particular video tutorial where all I have done is show you what is it what is it answer how do I make a tensor what is shape and what is data type now that I have this building block finished in the next video I need to actually talk about something these tensors are some are known as spooky music immutable whoo spooky whatever I'm not gonna bother this music they're immutable I cannot change the value so there is something called TF variable and the distinction between TF tensor and TF variable is important and I probably want to look at reshaping tensors although as well as I want to look at the operations so what does it mean to multiply tensors add stuff to tensors square tensors what are the kinds of operations mathematical operations just like there are a whole bunch of mathematical operations with a p5 Ector what are some of the mathematical operations with a tense below GS tensor ok so that's what will be coming in the next couple videos alright everybody I did not get nearly through nearly as much as I had hoped to get through today but this was good I think I have a better sense now actually of what a tensor is and and how to create them I do oh and I want to look at one all these things well I think though mostly things random oh this is so useful random uniform I got to come up with this but the most that stuff is gonna come up zeros as I that's really so you there's all sorts of stuff here by the there's all I would encourage you I have to go in a minute so I'm gonna I'm definitely gonna be back next week doing a lot more of this but if I can make it I was gonna come back this afternoon I have this sort of weird idea let me I might put a little thought experiment out there so something that I've been wanting to do a coding challenge for is the water ripples algorithm this is a water ripples algorithm that I used probably like 15 years ago in a project and it was used it was this wonderful webpage and I preferred to do this too it's hard for you to see so let me just and it has this interesting algorithm trying to think like would it be crazy to do this with tensors and it makes these nice like water ripple patterns I mean there's no need to do this potential oh yes and it's a little bit crazy probably to do that but anyway I was thinking of doing this as a coding challenge making this 2d water ripple based on this particular algorithm so so yeah so I didn't get I guess to match yeah I made two two videos edited videos will come out of this one is my overview thing and the other is the basic introduction to tensors and then I need to come back and do the rest of the stuff let me see if there's some questions people are asking I don't see any useful questions in the chat thank you Nikhil for watching helping answer a lot of questions and people were asking in the chat and answering my own questions and correcting all the things that I got wrong noob phobia asks how long did it take you to learn all the languages you know I started programming in really in 2001 so you could do the math from there a long time I guess I actually didn't start programming until I was 28 also which is I think important to mention because I get this question a lot like oh is it too late for me to start learning programming and no I don't think so good tensors be advantageous to use in non ML programming over matrices and vectors yes yes and no so I mean I the event I mean the concept of a tensor is the same thing as a vector or a matrix it's just more generalized and the the advantage you would get to using tensorflow j/s to have tensor operations in non quote unquote machine learning applications would be that you get the WebGL hardware acceleration so the real work that the creators and developers of tensorflow chass have done is magic of here's my arbitrary array of numbers make that fit into a WebGL texture because what gel just knows about image textures which are matrices and it does fast operations with those so how does this generic thing of numbers that I want to have this shape fit inside a WebGL texture that's what tensorflow digest is doing behind the scenes so that can make things really fast that issue is and we're gonna see this later is there is oh it is expensive computationally expensive to copy the data onto the graphics card so when I've done some experiment experiments where I'm kind of like doing things one at a time like I have this bit of data copied on the graphics cards in another bit of data copied on the graphics cards I'm running into performance issues whereas if I can get a whole batch of data and copy it all at once what the program's gonna run a lot faster so you have to think in terms of scale and batches and and how you're using it but yes there could be some advantages what is your math background dan amateur interested in math person that's my background I did actually technically major as an undergraduate sort of in mathematics my major was math and philosophy so I did take a lot of math courses many many years ago but I don't none of which do I remember my background is more recently in stuff that I've been trying to learn and reteach myself can you use Tesla Digest for decentralized ml in the browser probably not something that I know how to do Oh someone can teach me about decentralized ml what are the languages you learned okay so the languages that I know programming wise are well probably job lingo I started with the lingo programming languages Macromedia director check out John Henry Thompson's guest video on the coding train creator of the lingo programming language so into Java pretty pretty well from working with processing for many many years I know Java learned JavaScript just a few years ago to start working with p5.js and other web stuff that's I mean maybe I kind of like I can tiptoe around Python not and kind of get my way to the end zone if I have some good examples but I have to look every when I'm working if I have to Google will get the documentation I don't know any of the Python stuff inherently at all C++ I used to do a lot of C and C++ I forgot about that oh and I took a course once or I had to program in ada 95 alright everybody so I hope this was useful I hope these two chunks of video tutorials that I made will work if I can manage it if I can manage it I would love to come back this afternoon just to do a coding challenge that would the earliest that would be would be 3 p.m. Eastern Time and oh I should say one of the reason why I have to go is because I have an appointment to fulfill patreon rewards so I apology if anybody's watching there's a current patron of the coding train and I haven't sent you your book or stickers yet my apology apology is I don't have a good system I'm pretty pretty slow with that but a whole new batch of those is going out this afternoon that's it so schedule wise I will definitely be back next Friday let's look at a calendar I don't think I've like logged into my Google account so I don't want to um so let's look at a May April May 2002 this is going to incognito window right can I get with Google just a generic calendar no you have to sign in April 2000 18 all right so this is good oh wait May next Friday is me all right so looking at the calendar here I will be back May 4th this week is thisis week at ITP where I teach the 7th through the 11th there will be no live streams this week however if you go to ITP NYU edu slash thesis this is the schedule of thesis week and this whole thing is live streamed so I will tweet about it I'll try to make a post community post on YouTube but I encourage you to tune into ITP so thesis week there are some really exciting projects let run the gamut of so many different topics there's some machine learning ones that I definitely think those your interests in that topic will want to tune in for presentation schedule here's the presentation schedule so you can tune in during all these times to see all these theses so there will be I'm not livestreaming this on the coding train I wonder if I could like rebroadcast it somehow but so that's happening next week and then this is the ITP end of semesters show I usually try to do a live stream walkthrough interview things so that will happen on the 15th or the 16th and then the summer summer winds blows sweetly in from across the sea so then I'll be then I'm gonna really be diving into this content hopefully starting the week of the 21st of May through June July August I'm gonna be away quite a bit so August might be a month hiatus we'll see but that's kind of my friend alright so thank you everybody again for tuning in I localstorage anyway local storage I definitely need to do that hold on hold on this is where I'm gonna remember this stuff because this notes for next time oh yeah okay hold on let me just do this oh let me just add notes for next time local storage so these are things that I want to like cover next year that maybe don't have videos on okay so I made a note of that there alright um thank you everybody supposedly one of these days I'm gonna get one of these like altro videos to play here I'll just play my weird trailer thing as the outro video so to play the weird trailer thing as the outro video I'll be gone just follow the channel subscribe with the alarm bell will tell you if I'm going to come back this afternoon because I will set it up as an event with set the time for it or check my twitter twitter / twitter.com slash Schiffman i'll tweet if i'm going to be back this afternoon depends how today goes a lot to do okay there we go Mike creation you
