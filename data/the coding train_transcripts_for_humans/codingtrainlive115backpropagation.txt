With timestamps:

00:00 - good morning it's Friday
00:04 - time for a usual coding Train livestream
00:08 - I seem a little bit subdued it's because
00:12 - I'm well I'm a little bit worried about
00:14 - today I today is the day that I'm once
00:20 - again this month of January where are we
00:23 - in January we'll pass the middle is I'm
00:32 - dedicating this month of January to
00:34 - attempting to work through I'm starting
00:38 - looking at the chat to work through
00:41 - building a neural simple neural network
00:44 - library from scratch in JavaScript I
00:46 - spent some time working on some matrix
00:48 - math stuff building a simple feet
00:50 - working through the feed-forward
00:51 - algorithm and today is the day to tackle
00:55 - a topic one particular technique for
00:58 - training the neural network for having
01:00 - the neural network learn and that
01:03 - technique is called it's not called
01:06 - quaternion
01:09 - but it is that it is something that does
01:12 - often caused me to run out of the room
01:14 - in fear and it's called back propagation
01:16 - now a couple things I want to mention
01:17 - number one is eventually I will be
01:22 - replacing a lot of the internals of this
01:25 - library that I'm building with a more
01:27 - sophisticated engine that's been highly
01:30 - optimized to do matrix math and also has
01:32 - a lot of libraries a lot of classes and
01:36 - objects and library functions for doing
01:38 - deep learning stuff called deep learning
01:40 - is so that's what that hopefully by the
01:42 - time we get into February that's where
01:44 - my focus will lie in looking at some
01:46 - more sophisticated models of deep
01:50 - learning systems to kind of do fun and
01:52 - creative stuff in the browser but I'm
01:58 - still I I want to use this time together
02:02 - in this live stream to kind of dive into
02:04 - the guts of how these systems work see
02:07 - what happens if we can not worry about
02:09 - optimizing or efficiency and just kind
02:12 - of
02:13 - build the pieces and parts and put them
02:14 - together get something to work so that
02:17 - we can start to understand how the
02:20 - algorithms work have a language a common
02:22 - language to speak to each other about
02:24 - about the pieces of these algorithms so
02:26 - we can ask the right questions later
02:28 - when we start to do projects and other
02:31 - types of things how's this sounding okay
02:36 - so I have done something somewhat
02:39 - shocking which is that I've prepared
02:41 - notes on that word note notes the plural
02:49 - of note meaning more than one note and
02:53 - not just more than one note or pages of
02:57 - notes page 1 page 2 page 3 page 4 page 5
03:10 - and then even some printouts of some
03:13 - code that I wrote at one point earlier
03:16 - in my life with some mistakes in it that
03:19 - some of the code for the back
03:20 - propagation algorithm
03:21 - oh there's even yet another page on the
03:24 - pad that I didn't rip off so another
03:26 - page of the notes I don't know how much
03:28 - these notes are gonna help me I want to
03:29 - say that once again I should thank and
03:31 - I'm gonna do this multiple times today
03:32 - but some of my a lot of my knowledge has
03:36 - come from this book make your own neural
03:38 - network you can check the coding train
03:40 - Amazon shop for a link to purchase this
03:42 - book and other books that I recommend
03:43 - are used in preparation for a lot of my
03:45 - videos and so this is extraordinarily
03:50 - helpful I also want to mention but no
03:57 - written order book this I'm gonna need
03:59 - to refer to this let's put it over here
04:00 - I also want to mention three blue one
04:03 - brown and I've been watching the in
04:07 - particular let me pull this up here
04:11 - three blue one brown
04:18 - I have been watching the playlist let's
04:24 - go to playlists well there's a couple
04:27 - here but I've been watching this
04:28 - playlist on neural networks and sexually
04:32 - I talked a little about how I'm doing
04:34 - this fundraising and training for a half
04:35 - marathon actually watched well I did my
04:39 - training run at the gym because the
04:40 - weather is terrible here in New York
04:43 - watch videos three and four like a
04:46 - chapter three independent like multiple
04:49 - times a treadmill on or like a little
04:51 - iphone so it was probably not the most
04:54 - optimal way to experiencing this video
04:56 - especially when I felt like I wanted to
04:57 - pause to like contemplate for a second
05:00 - and then like you know bouncing around
05:01 - fortunately no one was harmed in the
05:03 - watching of said videos on a treadmill
05:06 - but these videos are extraordinary
05:09 - they even more so than this what is a
05:13 - neural network a video this gives you a
05:15 - really nice intuitive understanding
05:18 - through diagrams and all sorts of stuff
05:20 - about the backpropagation algorithm in
05:23 - you will also I would highly recommend
05:25 - if you want to dig even deeper into this
05:27 - the well the essence of linear algebra
05:30 - series is perfect for a lot of the
05:31 - matrix math stuff but in particular this
05:34 - essence of calculus playlist and in
05:39 - particular I'm looking for something
05:43 - about the CHEO a chain product rule the
05:46 - chain and product rule which are
05:47 - necessary for the backpropagation so I'm
05:50 - really struggling as to what to do here
05:52 - because there's no way that I am going
05:56 - to cover in depth what's here in three
06:00 - blue one Browns channel there is
06:02 - probably no way that I'm going to cover
06:04 - with the same level of depth
06:06 - what's in to make your own neural
06:08 - network book and by the way I should
06:09 - also mention I was looking at some of
06:13 - this code yesterday it looks extremely
06:15 - helpful but I haven't I haven't had the
06:19 - chance to dive really deep into it but
06:22 - oh yeah this to the this is a free
06:26 - online book by michael nielsen that
06:30 - looks like three blue one
06:31 - Brown used as a reference and if we
06:34 - could go to and so how the
06:36 - backpropagation algorithm works oh boy
06:38 - how about I just do a dramatic reading
06:40 - of this page instead of trying to do it
06:43 - myself
06:44 - so this I actually should I wish I'd
06:45 - spent some more time with this page
06:46 - because I think this would be extremely
06:48 - helpful but I don't think that I'm going
06:51 - to go as in-depth as any of this content
06:55 - today we did that work could I actually
07:01 - like feed them through my ear so this is
07:03 - my plan this is my thinking as of right
07:05 - now play a lot of scary music so that I
07:10 - don't appear too serious on talking
07:13 - about back propagation this is my plan I
07:21 - am going to talk through how the
07:24 - algorithm works in generalities as it
07:27 - relates as it relates to this diagram
07:34 - that I went through in somewhat detail
07:36 - about the feed-forward algorithm so what
07:39 - does it mean to suddenly have an error
07:40 - here and then correct these weights
07:43 - which are tied to the error and then
07:45 - propagate backwards those errors to
07:48 - adjust these weights and in a many more
07:51 - layer system just keep propagating back
07:53 - and back and back and back so I want to
07:55 - talk about that JIT in generalities then
08:00 - I want to look at how the weights are
08:02 - know how the error is kind of like
08:04 - portioned out and how that can be
08:07 - expressed with matrix math because I'm
08:09 - going to need that from our library and
08:12 - then I'm going to refer back to my
08:15 - videos on gradient descent and ultra
08:21 - justing weights based on gradient
08:23 - descent and the three blue one Brown
08:25 - videos just present the formulas that
08:28 - are sort of at the end of those videos
08:31 - and implement those in code so I don't
08:33 - think I'm going to derive all of the
08:36 - pieces of the back propagation algorithm
08:38 - today or ever so definitely because I
08:42 - would like to I would like like to
08:44 - turn to that like put in some filler
08:45 - videos I do think that other people are
08:47 - better equipped to make that content
08:49 - than I am so I'm gonna try to stick to
08:52 - where hopefully I can be of most help
08:54 - which is kind of figuring out how to
08:56 - implement the formulas in code rather
08:58 - than prove or derive them but I it's
09:01 - important to at least have a discussion
09:02 - of the understanding of those formulas
09:04 - this is my thinking I'm just kind of
09:06 - talking it through and then I can
09:10 - hopefully do some coding challenges
09:11 - because once I have the library built I
09:13 - can say like hey let's train it on this
09:14 - let's try to use let's do a coding
09:16 - challenge use this library on this
09:18 - particular problem so that'll be
09:20 - flightless like difference in some of my
09:21 - coding challenges of the past where
09:23 - there was sort of no code maybe there's
09:25 - a p5 library of course but we're build
09:28 - these quick quick machine learning
09:30 - examples using the library and then if
09:33 - people want to refer back to how the
09:34 - library was built they can do that okay
09:37 - so that's my thinking what what does
09:39 - everybody think about this I feel like
09:46 - I'm a newscaster now with my notes
09:54 - [Music]
10:06 - oh that was all the spooky piano I have
10:16 - thought that was a much longer effect
10:19 - [Music]
10:50 - okay so how are you fall feeling about
10:53 - this how are we feeling about this
10:57 - Marin Marin how did i do today Marin in
11:01 - the chat says sounds like a solid plan
11:03 - look what could possibly go wrong
11:08 - what could possibly go wrong me trying
11:12 - to explain what what do you I've
11:13 - actually learned which is kind of
11:15 - interesting and I might as well plug
11:17 - this while I'm here is I'm participating
11:19 - in this conference put on by the
11:23 - processing foundation and school for
11:25 - poetic computation what can i maybe
11:29 - called if I do I meant to type in SF PC
11:32 - learning to teach so I'm doing a short
11:36 - presentation at this at this conference
11:38 - this is it no that's from last year last
11:41 - year yeah 2018 so one thing I that I'm
11:48 - discovering is when I make mistakes in
11:50 - code and have to figure out and debug
11:52 - them that seems to be a useful technique
11:57 - for education meaning people watching
12:00 - they're like oh okay I make those
12:01 - mistakes oh I'm seeing how you figure it
12:03 - out well this is helpful to me or this
12:04 - is interesting compelling and I'm
12:06 - learning something whereas when I spend
12:08 - an hour trying to explain some math
12:10 - formula but have the formula wrong the
12:11 - whole time that's not so helpful so it's
12:14 - not like oh yes I'm seeing your process
12:16 - of how you get it wrong for a while and
12:18 - then you realize that later and kind of
12:20 - have to do it so what I'm discovering is
12:21 - a lot of these these sort of
12:23 - mathematical or algorithmic discussions
12:25 - especially if they have like complex
12:26 - notation require something called I
12:28 - believe it's called preparation okay I
12:36 - have here in the form of
12:40 - notice so I'm a trying trying that but
12:52 - most likely I have to admit I wouldn't
12:54 - be surprised if I just end up redoing
12:55 - trying to do I need a couple tries with
12:58 - this so I probably should do those tries
13:00 - not live-streaming in front of a live
13:03 - audience on YouTube but I don't when
13:05 - else am I going to do it right now so I
13:10 - thank you for being guinea pigs and and
13:11 - this is actually what happened most
13:13 - recently if I go to my channel and go to
13:20 - yeah this particular video this
13:24 - particular tutorial was made on Tuesday
13:26 - of this week though if you look at the
13:29 - live stream archives there's a live
13:30 - stream from the last Friday oh we could
13:32 - go today when I went through this exact
13:34 - algorithm but I made so many mistakes it
13:36 - wasn't worth trying to edit that
13:37 - together in some way that made sense it
13:39 - just made more sense to just do it again
13:41 - so with this gradient descent stuff I'm
13:43 - just kind of warfare warning in advance
13:45 - I'm kind of in my mind thinking of today
13:47 - as let's just give this a try and see
13:52 - how it goes now the other thing I should
13:53 - mention is Simon okay I will Simon is
13:58 - telling me that if I watch his
14:02 - livestream I'll learn better how to
14:04 - pronounce mitad okay now the other thing
14:09 - I should mention is I was going through
14:10 - all this and looking at it looking at it
14:12 - and I kind of have the formulas and has
14:14 - some code I think for all of the weight
14:16 - adjustments that I need to do but of
14:19 - course I kind of forgot about the biased
14:20 - part so look at that yet so let's see if
14:24 - we could do put the back propagation
14:25 - formula stuff in for adjusting the
14:27 - weights and then if any of you who know
14:29 - what you're doing more than me which is
14:31 - probably a lot of you maybe you can help
14:35 - me figure out how to put the bias stuff
14:37 - in okay and in regards to the camera
14:41 - that goes to black I will just mention
14:43 - to the people are talking about in the
14:44 - chat I'm gonna fix that soon you know
14:47 - sometime in the next year whose
14:49 - I the what the solution to that is to
14:52 - put this firmware on the cameras called
14:55 - magic lantern so I just have to
14:57 - basically like you know it's kind of
15:00 - like should I start live streaming on
15:02 - time and have two hours or should I try
15:04 - to fix this camera thing for an hour
15:05 - only have an hour left and school the
15:06 - way it works out so I'm not optimizing
15:09 - for fixing it but I will I will okay now
15:16 - as you know in the coding train
15:17 - especially when attempting things like
15:20 - back propagation it's always good to
15:24 - stay hydrated
15:39 - hello Brazil
15:42 - it's amazing people in Brazil are
15:44 - watching alright now let me let me
15:59 - compile some resources here it's already
16:05 - getting warm in this room okay so I want
16:14 - [Music]
16:18 - this playlist available to me and I want
16:32 - I guess I should verify yeah I guess I
16:35 - should get one of these check marks on
16:36 - my channel huh I want I also want
16:44 - this playlist available to me so this is
16:56 - I want to reference neural networks
17:01 - essence of calculus neural networks and
17:08 - deep learning free book and the coding
17:13 - trade Amazon shop which has this book
17:18 - here that now my own videos the ones
17:25 - that I think this relates to our this
17:35 - particular video Hey look somehow my
17:39 - subtitles are a closed captioning on and
17:41 - they are on in German it looks like who
17:44 - wrote these I put discovered recently
17:45 - that I can so as it says who caption the
17:54 - videos here okay here could be another
18:00 - video from Suraj has a lot of videos
18:02 - about this topic as well that are
18:05 - terrific then I also want the perceptron
18:12 - [Music]
18:14 - which is basically you know talked about
18:23 - a bunch of stuff multiple time either
18:26 - pity hood because this is this kind of
18:29 - like a video about the feed-forward
18:31 - algorithm that's interesting fifteen
18:37 - minutes I forgot about this what's in
18:44 - these videos
18:47 - I think I made like duplicate content
18:52 - thought let's just kind of scan through
18:57 - this real quick
19:03 - oh I think this is me kind of like
19:05 - sketching out what I need own I stopped
19:12 - at the matrix where I need matrix math
19:16 - oh yeah so I did do this kind of twice
19:20 - there's the feed-forward algorithm in a
19:22 - previous video that's so interesting so
19:25 - okay that's fine so what I'm looking for
19:32 - though is this one
19:42 - and maybe somewhere in here the training
19:50 - function is where I want to kind of
19:54 - pause for a second because it's
19:57 - basically the same code where do I have
20:05 - a nice if someone could find me a time
20:07 - code in this video where I have a
20:10 - lifestyle of the Train function I guess
20:14 - I can actually just open up the code on
20:16 - my computer might be more efficient
20:23 - sorry everybody I'm just like trying to
20:26 - get organized here perceptron sketchbook
20:40 - sir anybody see a perceptron probably
20:44 - coding challenge well maybe I should go
20:48 - to I should really talk about the new
20:49 - website stuff too but it's too many
20:51 - things coding challenges perceptron huh
21:01 - didn't I do it as a coding challenge why
21:06 - is it not numbered that's a mistake hold
21:10 - on what video is that oh that's not a
21:16 - coding challenge it's actually like a
21:18 - tutorial somehow it's here though maybe
21:21 - let's see no oh this is different
21:33 - yeah this this is kind of relevant here
21:40 - alright so that I can reference if I
21:43 - need to okay part one I wonder why why
21:53 - what's that someone give me the a too
21:55 - long didn't read answer why why it's two
21:57 - million views why why all right
22:02 - oh I really wish that the climate
22:07 - control system in this room could be
22:08 - fixed really make things better for me
22:14 - 11:30 okay I got this we got this got
22:22 - this this this okay there's also like
22:32 - mathematics of gradient descent by the
22:38 - way just if I go click on videos that's
22:40 - nuts
22:41 - something is wrong with the universe is
22:45 - if my video shows up first that is a
22:47 - really a problem because I expect a lot
22:49 - of these other ones are much better but
22:50 - it prize to do with the fact that I've
22:52 - titled it that way math so if I just
22:54 - search for gradient descent mathematics
22:59 - yeah okay how about gradient descent
23:01 - algorithm just gradient descent there we
23:05 - go okay good good that's good oh I'm
23:07 - down there that's fine I'm happy with
23:09 - that look at these other ones first okay
23:16 - so this is actually this is actually the
23:20 - video that's most relevant all right
23:29 - okay Google knows it's me and so they're
23:32 - putting my videos up there cuz I watch
23:34 - them a lot that's the problem so but I'm
23:35 - not am I signed into Google hold on they
23:42 - probably even know they just know even
23:44 - if it's an incognito window I don't know
23:56 - I don't know Google
23:57 - I don't know Google Google just knows
24:06 - still does incognito window is like they
24:10 - just know right all right the Delta rule
24:17 - oh you should do a video on the Delta
24:20 - rule learning algorithm for single layer
24:22 - neural networks I think I did that
24:23 - already right though isn't that kind of
24:25 - like what my perceptron video is
24:27 - basically the Delta rule ah
24:34 - Tushar in the chat saying I have a
24:36 - question on your own the last video okay
24:37 - let me take a couple questions I'll do
24:40 - anything you think your procrastinate
24:42 - from trying to talk about gradient
24:45 - descent in a video so I'll take your
24:47 - question
25:07 - you guys still here like a weird okay
25:11 - hold on um do you guys hear our weird
25:14 - like echo like the timing of when you
25:17 - hear the sound that's being piped
25:19 - through the system versus the sound or
25:20 - my microphone is off like if I do this
25:23 - oh sure I wasn't looking at the chat I'm
25:34 - sure that question was asked I missed it
25:37 - which is a reason for me and maybe plug
25:39 - the patron for a second I do have a
25:42 - slack channel that I it'll be easier to
25:44 - keep an eye on and wear a nice friendly
25:47 - community of folks discuss different
25:48 - things that I'm doing the videos and
25:49 - provide some financial support for the
25:51 - channel so if you go to
25:54 - patreon.com/scishow de trein you'll get
25:55 - an invitation to the slack channel okay
26:01 - all right
26:02 - there is no echo all right all right all
26:04 - right all right let's we got is we've
26:11 - got to get this started with let's let's
26:15 - get rid of some let's let's let's catch
26:18 - up to where we are let's desktop
26:29 - let me get this going here
26:34 - look coops localhost there we go
26:42 - this is where we last left off with a
26:46 - feed-forward neural network that will
26:47 - give me a random output now we want to
26:50 - put the back propagation algorithm into
26:51 - it if I open up atom I should have my
26:59 - code let me open up this will be another
27:09 - useful reference that I should bring in
27:12 - here and I think now I've got everything
27:16 - I want to talk about did I make the joke
27:20 - last time oh I think I did it wasn't it
27:25 - wasn't very good
27:26 - it wasn't very good though sorry I you
27:28 - know one thing that I shouldn't that
27:29 - I've also learned to do is sometimes I'm
27:31 - responding the stuff in the chat but not
27:32 - saying what it is which is very
27:34 - confusing if you're watching and you're
27:35 - not even seeing this chat so I'll try to
27:37 - be better about that Taryn asks why
27:43 - don't you prefer visual studio code
27:46 - maybe I do I'm just not as used to using
27:49 - it I keep meaning to set it up but I
27:50 - kind of have all my settings and
27:52 - everything and Adam vaguely the way I
27:53 - like them although I really should turn
27:55 - off all the autocomplete stuff they
27:56 - might know how to do that alright I
28:05 - think I've got to get going
28:07 - and I think we've got to start this like
28:13 - it's got a notification the coding train
28:14 - is live maybe I should go watch that
28:16 - that's interesting
28:38 - let's read some random numbers this will
28:41 - this will get me a little warmed up here
28:42 - Oh getting too old for this YouTube
28:49 - thing forty six thousand seven hundred
28:57 - and thirty seven eighty eight thousand
29:00 - and twenty seven thirty five thousand
29:02 - four hundred and ninety seven twenty two
29:06 - thousand eight hundred and sixty three
29:08 - sixty five thousand and eleven twelve
29:11 - thousand two hundred twenty-eight sixty
29:13 - six thousand four hundred and thirty
29:15 - eleven thousand eight hundred and
29:17 - twenty-five 835 eighty one thousand
29:21 - eight hundred and thirty eight ninety
29:24 - four thousand four hundred and thirty
29:25 - one sixty three thousand nine hundred
29:27 - ninety five ninety one thousand one
29:30 - hundred and seventy one eighteen
29:32 - thousand two hundred and seventy three
29:35 - forty-seven thousand seven hundred and
29:37 - thirty seven hundred eighty four thirty
29:40 - nine thousand two hundred and eight
29:42 - thirty four thousand two hundred
29:44 - sixty-eight eighteen thousand ninety
29:47 - three seventeen thousand okay you know
29:54 - one thing that I should probably do
29:55 - really quickly in case in case you need
30:10 - to make a website where I read any
30:14 - possible random number will give you
30:16 - some footage for that real quick I'm
30:18 - kind of working on this on my own anyway
30:20 - but zero which I will be looking at the
30:25 - camera price you look at the camera
30:27 - zero one two three four five six seven
30:33 - eight nine 10 11 12 13 14 15 16 17 18 19
30:41 - 20 30 40 50
30:45 - see 70 80 90 100 1 200 300 400 500 600
30:56 - 700 800 900 thousand yes 1,000 2,000
31:04 - 3,000 4,000 5,000 6,000 7,000 8,000
31:09 - 9,000 10,000 I think you could put
31:13 - together though the one two three four
31:14 - five six seven eight nine and the word
31:15 - thousand thousand and you could put
31:18 - together twenty thousand yeah
31:20 - one hundred thousand so I think I think
31:22 - you have enough right because that you
31:23 - can you've got one hundred one hundred
31:25 - thousand nobody one hundred ten thousand
31:29 - one hundred and ten
31:31 - no you need the and one hundred and ten
31:32 - thousand one hundred and no but these
31:35 - don't go up that high
31:36 - they only go up tonight that ninety nine
31:38 - ninety nine so you can get even ninety
31:40 - nine thousand one hundred thirty-five
31:45 - yeah you're fine I think you have enough
31:49 - right you could probably make a bunch of
31:51 - video clips have a random where you
31:54 - picked and have me speak that random
31:55 - number right
31:56 - the and part means you want you want
31:59 - million all right fine million billion
32:03 - trillion all right whatever I missed
32:14 - I'll get you later
32:14 - am I being really quiet I didn't want
32:16 - the volume to be peaking too much okay
32:24 - glad that we worked it out oh oh I need
32:27 - my notes for sure like seriously I'll do
32:30 - anything I'll probably like agree to do
32:32 - you know that double pendulum simulation
32:34 - or something now he'll agree to do just
32:37 - about anything to avoid this
32:38 - backpropagation algorithm but we're
32:40 - gonna do it now all right all right Adam
32:43 - I need to command the settings and click
32:52 - packages search search for autocomplete
32:57 - um I guess I could so what do I want
33:04 - sometimes I like the autocomplete though
33:06 - which of these do I want to turn off
33:08 - I don't know now I'm afraid to disable
33:11 - it what if I've been using it all along
33:13 - and I didn't know that I've been using
33:16 - it well we'll leave then now we know
33:17 - where to look for this all right
33:24 - can anybody come up with any reason
33:27 - right now why shouldn't get started on
33:29 - the backpropagation thing okay oh yeah
33:39 - shoot nobody has a point what do you
33:43 - need two points for though is there's
33:53 - gonna be continuity errors point maybe
33:58 - you just need the audio though maybe
33:59 - you're just gonna use the audio which is
34:00 - fine all right I'll come back later at
34:07 - the end I will take your reading random
34:10 - numbers requests this stippling
34:16 - challenge re didn't I oh yeah the
34:17 - stippling challenge right all right all
34:24 - right here we go just need them house
34:33 - [Music]
34:38 - okay this cameras off
34:51 - I should yeah I know so many I'll do the
34:56 - reading I'll do that a sec I'll do some
34:58 - reading all the possible different
35:00 - numbers thing later I'll configure it in
35:02 - a way that's better
35:03 - just hold on let's let's let's let's
35:06 - actually do some let's try to get
35:08 - through this so my goal right now is
35:10 - let's see if I can get the
35:12 - backpropagation algorithm into the
35:14 - library by 12:30 which is like 45
35:20 - minutes or so and that gives me a half
35:22 - an hour slash an hour to try to at least
35:25 - solve maybe X or as a coding challenge
35:27 - that's my goal for yeah I need to like
35:30 - slow down here I'm gonna get to all this
35:31 - stuff eventually I'm just doing my best
35:33 - I got lots of videos you go back and
35:35 - watch those my hair is really really a
35:38 - mess today I wore a nice shirt for all
35:43 - of you I thought that might make the
35:44 - Grady the whole like backpropagation
35:46 - gradient descent thing a little bit less
35:48 - weird like it look like I know what I'm
35:50 - doing alright my glasses are very dirty
35:58 - really like I just I'm gonna start
36:11 - mmm eat me me me me me me me me me me me
36:13 - me me me me me me me okay here we are
36:24 - this video.i what I'm going to do with
36:27 - this video is I'm quite terrified at
36:29 - this moment I should say but I'm gonna
36:32 - start to talk about what it means to
36:35 - examine the output we fed in some input
36:39 - we did the feed-forward algorithm we got
36:41 - some output I want to know look at that
36:43 - output and I want to say what do I think
36:47 - about that output really isn't good
36:49 - output is a bad output is it right on
36:51 - it's a little bit off and I'm going to
36:53 - this is a technique known as supervised
36:56 - learning I the teacher I'm now gonna say
36:58 - this output is incorrect please adjust
37:01 - all your settings to make this output
37:04 - more correct and I have done this before
37:06 - I have done this in a video about linear
37:10 - regression and gradient descent I have
37:14 - done this in a video where I made a
37:18 - simple perceptron where I did this same
37:20 - type of learning algorithm in fact I
37:22 - have done this in genetic algorithm
37:24 - examples or I'm not using the same exact
37:26 - technique but a different technique to
37:28 - sort of teach a system to do something
37:30 - so this is what I want to do you can
37:32 - think of all of these weights that are
37:33 - inside the network as these little knobs
37:35 - little settings and I just want to like
37:38 - always adjust the settings and so the
37:40 - key there's gonna be key terms that are
37:42 - gonna come up here like error what's the
37:44 - difference between the output and the
37:46 - known correct answer that's the error
37:48 - the cost well over a large training data
37:52 - set what's the sort of cumulative error
37:54 - that's the sort of cost the current cost
37:57 - of the neural network and then this term
38:00 - that's called back propagation so if the
38:03 - error is the thing that tells me how to
38:05 - tune these weights the error is right
38:08 - here it's gonna tell me how to tune
38:09 - these weights how do I tune these
38:12 - weights they're not connected to the
38:14 - error they're connected to the thing
38:15 - that's connected to the error so I need
38:17 - to propagate backwards feed-forward is
38:20 - the process of moving all the data for
38:22 - through the network back propagation is
38:24 - the process of taking the error and
38:26 - basically like feeding backward to the
38:29 - error through the network now here's
38:31 - what we have here's where I have got to
38:33 - admit something this is probably I would
38:35 - say I'm trying to think of a topic that
38:37 - I've tackled in any my videos that's
38:39 - like harder this I can't think of one I
38:41 - don't know that I fully have a deep
38:43 - understanding of this I have implemented
38:45 - it before I spent a lot of time I
38:47 - prepared some notes I prepared some
38:50 - notes for the first time in my life
38:51 - which are right here that I'm going to
38:53 - use while I start to explain this but
38:55 - it's kind of unrealistic and I'm
38:57 - probably not the best person to go into
38:59 - all of the math so what I'm going to
39:01 - attempt to do is give a kind of just
39:04 - general overview of how the algorithm
39:06 - works
39:06 - look at how pieces of it actually work
39:09 - the math of it in particular as it
39:12 - relates to matrix matrices because I'm
39:14 - gonna need that understanding to
39:15 - implement it in code and then I'm gonna
39:19 - present some of the formulas to you that
39:21 - are the formulas for how you change the
39:23 - weights based on the error and then try
39:25 - to implement those formulas in code I'm
39:29 - sorry I put in a this is I I guess just
39:32 - probably be edited out but the permian
39:34 - should just stay in but I put in a order
39:36 - to try to fix the temperature in this
39:37 - room and I'm worried that something like
39:39 - someone's gonna just enter the room Fred
39:41 - fix the thermostat so I thought I heard
39:43 - like keys jangling I got worried so
39:45 - anyway okay so so this is my plan so
39:49 - probably take two or three videos but so
39:52 - my goal is really the implementation and
39:53 - I'm gonna provide to you a bunch of
39:56 - resources if you want to dive deeper
39:57 - into the math let me just mention those
40:00 - to you so number one make your own
40:04 - neural network by Tariq Rasheed this is
40:06 - a book that I was reading on the subway
40:09 - this morning you can actually get it on
40:11 - the kit on your Kindle app for a very
40:12 - inexpensive amount you can also find it
40:16 - here on an a lot books I Remmick
40:18 - recommend on my coding train amazon.com
40:21 - sheet slash shops mask coding training
40:23 - the three blue one brown video series
40:27 - what is a neural network what is back
40:28 - propagation really doing back
40:30 - propagation calculus you could pause
40:32 - right now and go watch these videos
40:34 - which will give you a
40:35 - deeper set of knowledge about the math
40:37 - that's that I'm going to use as well as
40:40 - this the essence of calculus right one
40:43 - of the things that's used in the math is
40:46 - the chain rule and the product rule so
40:48 - this is this this particular video might
40:49 - be useful to you as well as this
40:52 - particular online book which I found
40:54 - through the three blue one Brown videos
40:56 - neural networks and deep learning and
40:58 - I'd be remiss if I didn't also mention
41:00 - Suraj's YouTube channel that I've
41:02 - mentioned a lot on this video has a lot
41:04 - of different a lot of a lot of my videos
41:06 - a lot of different videos on similar
41:08 - content especially if you're interested
41:09 - in Python and using the tensorflow
41:11 - library and that kind of stuff okay so
41:14 - that's that now let me start I think I
41:18 - think we're ready to start timeout for a
41:20 - second alright so that was my brief
41:36 - little introduction and the next thing
41:39 - I'm going to do is I need to do some
41:42 - erasing so let's take a oh please do
41:49 - this no no I definitely no applauses I
41:51 - just want to have a freeze-frame of this
41:54 - white board because I'm probably gonna
41:55 - erase it what do I want to keep I think
42:03 - in the end I think I just want to erase
42:06 - this I think I'm gonna erase this and
42:11 - start over okay
42:18 - so if you talk amongst yourselves I'm
42:23 - gonna erase the whiteboard this is just
42:26 - water in case you're wondering helps it
42:29 - erase a little faster this is whiteboard
42:34 - paint so the whiteboard is actually
42:36 - painted on to the wall which is kind of
42:38 - wonderful but there's a bit of a problem
42:40 - number one is it's very Glary so I can't
42:43 - it's very hard for me to figure out how
42:45 - to like this room and properly because
42:47 - if there's a lot of glare on the
42:49 - whiteboard and then also it's very hard
42:52 - to clean it's just so you can see here
42:54 - can you see how dirty it is I nothing
42:56 - would make me happier in life than to
42:58 - have a perfectly pristine white
43:01 - whiteboard that's totally clean this is
43:06 - a start so if I need to refer back to
43:18 - that notation
43:33 - okay let me check our various chats as
43:38 - they're going and alright I don't see
43:42 - anything like any emergency messages we
43:54 - get my notes we right-click my notes I
43:58 - don't have a place for night he's like a
44:00 - little podium for notes let me just look
44:03 - at them for a second here okay this is
44:04 - where I'm starting okay okay okay God
44:08 - all right
44:14 - I've erased the whiteboard and I am now
44:16 - ready to start talking about the
44:19 - backpropagation algorithm so let's
44:23 - assume right now that this is my output
44:27 - neuron and just for the sake of
44:34 - simplicity at this moment let's just
44:36 - pretend this is one of the hidden
44:38 - neurons but let's just pretend that
44:42 - there's just one of them
44:43 - so there is some wait there's a also a
44:48 - bias but we'll come and come back to the
44:50 - bias at the very end so I'm gonna kind
44:51 - of do everything without the bias and
44:53 - then come back to the bias there's some
44:54 - weight which is connecting this hidden
44:56 - neuron to the output neuron now the
45:00 - input of the output of this neuron
45:03 - multiplied by the way to center here
45:05 - pasture the activation function and we
45:07 - get some sort of answer let's say that
45:09 - the answer that we get is point seven so
45:14 - this that can be referred to as the
45:15 - guess the output but I'm gonna call that
45:18 - I'm gonna call it the guess am i
45:21 - standing on the white board I'm
45:22 - dangerously close I erased might just
45:25 - pause for a second here let me let me
45:28 - put a little mark up here
45:29 - so really output is the very top all
45:35 - right point seven is the guest
45:42 - now in the road to I again one more time
45:50 - wait wait wait I'm seeing crazy stuff in
45:52 - the chat no all right that's fine
45:55 - that's nonsense that if something
45:57 - anything to do with me sometimes I think
45:58 - if the chat is being blasted with emojis
46:00 - it's like get my to get my attention but
46:03 - I'm now I'm only gonna soon that's ever
46:04 - the case if it's the slack Channel point
46:08 - seven is the guess now in the case of
46:13 - supervised learning where I have a
46:15 - prepared data set where I have sort of
46:18 - like known answers so I'm going to train
46:20 - the network to have these weights so
46:21 - that later on I can put in unknown data
46:24 - to get good results I would have some
46:27 - sort of answer and that answer could all
46:38 - that goes wrong today though I'm gonna
46:39 - be really happy I'm gonna have some sort
46:45 - of answer look I really like pushing the
46:53 - boundaries I don't know why I should
46:56 - just lower everything a little bit
47:00 - sorry machi I'm making this like
47:03 - annoying to edit for like no reason
47:14 - I would have some sort of answer known
47:17 - answer so I'm going to write that here
47:19 - and I'm gonna say the known answer is
47:21 - one
47:22 - I wanted this neuron with this
47:24 - particular input that came in I wanted
47:28 - to see the answer of one so this means
47:31 - now I also have an error and the error
47:37 - is calculated with a simple formula what
47:40 - is the desired output the answer - the
47:43 - guess what's the difference between
47:45 - those two things so we can see is that
47:47 - the error is 0.3 so in my simple
47:51 - perceptron and in other videos that I've
47:53 - made I've then taken this error and used
47:57 - it as a way to basically nudge-nudge
48:00 - that weights
48:02 - I wanted a 1 I only got point 7 I can
48:04 - make the white a little bit higher to
48:06 - get more stuff right I want more and
48:08 - look more stuff right I want that weight
48:10 - higher maybe the bias needs to be
48:12 - addressed but 0.7 I just need to
48:13 - increase that way to increase the weight
48:15 - in the direction of the error that's how
48:17 - this works now here's the next piece of
48:22 - this let's say however that instead of
48:32 - just one weight coming in here I have
48:39 - two weights because there are two hidden
48:41 - neurons H 1 and H 2 now we have a
48:47 - problem we have this error which I know
48:51 - I need to nudge weight 1 and weight -
48:53 - but which one's really kind of
48:54 - responsible for the air there's a lot of
48:56 - blame placement going on here normally I
48:58 - would think like let's try not to blame
49:00 - anybody but this is the problem and I
49:02 - could just say like I don't know half of
49:04 - an error increase them both increase
49:06 - they're both the same amount but there's
49:08 - a key aspect of the way that the
49:11 - learning process works with gradient
49:13 - descent and back propagation is we
49:15 - really need to figure out who's
49:16 - responsible for the error so let me take
49:19 - the scenario these weights could
49:21 - actually be weights where this weight is
49:23 - zero point two and this weight is there
49:27 - point one well you could be made set we
49:32 - could now make the argument that this
49:33 - connection is more responsible for the
49:37 - error because it has a higher weight in
49:39 - fact it's two-thirds responsible right
49:42 - this weight is double of this weight so
49:44 - in fact when we do these ninjas we take
49:47 - this error to nudge this one but we'll
49:49 - nudge it only by 33% and then we'll take
49:53 - this error and I know this is going to
49:54 - go out of your view but it's coming all
49:56 - the way back here and we'll judge that
49:58 - by an up let's see can you see this here
50:01 - 67% actually I just do it back here
50:05 - 67% so this is a key aspect of the bout
50:16 - and this is I've basically done this
50:17 - before so this is where we look for this
50:19 - Delta weight we adjust the weight based
50:22 - on that error and the inputs passing
50:25 - through okay so now little bit of pause
50:30 - for a second how's this going I'm
50:36 - getting good tips on how to fix my
50:38 - whiteboard issue this camera went off so
50:41 - I might as well fix that while I'm here
50:53 - all right no one's saying anything like
50:56 - is horribly wrong so I'm gonna keep
50:59 - going let me check my notes what was the
51:05 - next thing so step three is okay step
51:11 - three is adding the layer here okay but
51:20 - so so maybe this makes sense to you and
51:23 - if it does good here's the tricky aspect
51:27 - this is why there's something this is
51:28 - why this video is essentially about back
51:30 - propagation this is not the diagram of
51:34 - the neural network that I created in the
51:36 - previous videos in fact this hidden
51:38 - layer is
51:39 - connected to the inputs and all of those
51:44 - have weights and I I spent a lot of time
51:51 - worrying about the indices let's get
51:54 - those indices correct so this if this is
51:57 - input 1 and this is input 2 we usually
52:01 - call this sorry I usually call those X
52:04 - and this by the way the output we can
52:06 - refer to as Y it's usually what's often
52:09 - referred to as Y so if these are this is
52:11 - input 1 X 1 input 2 X 2 this weight is
52:15 - the weight from 1 to 1 this weight is
52:19 - the weight from 1 to 2 now that might
52:23 - look backwards to you it's the weight
52:26 - from 1 to 2 or kids connected between
52:29 - hidden to and in what but the reason why
52:31 - I'm numbering it that way is this is
52:33 - that it's going to be in the second row
52:35 - of our matrix this weight is the second
52:37 - row of our matrix and this is a weight
52:41 - from 2 to 1 so I write 1 2 and this is a
52:45 - weight from 2 to 2 so I write 2 2 so
52:50 - here's the thing if this error which is
52:54 - happening right here is used to tune
52:56 - these two weights proportionally based
52:58 - on their weights well how do I connect
53:02 - this error back to these weights how do
53:04 - they get tuned well it what I need just
53:07 - this is a what was Inc this to realize
53:10 - is this is like a little section of what
53:12 - could possibly be a much larger neural
53:14 - network right there could be many more
53:16 - layers this way and many more layers
53:18 - that way so these weights are just tuned
53:20 - based on whatever this neurons error is
53:23 - right here is at the end so I can
53:25 - actually calculate the error directly
53:26 - but here they're not connected directly
53:28 - to this error that sorry here they're
53:30 - not connected they're connected to these
53:31 - so what is the error coming out of here
53:34 - if I just had that if I had like hidden
53:38 - efore error error hidden 1 what is that
53:43 - equal if I and I knew if I knew what
53:47 - this was right if I knew the error
53:48 - coming out of here error hidden 1 then I
53:52 - could adjust these what
53:53 - because the air coming out of here
53:54 - adjust these weights this error just
53:56 - these weights and this error e to error
53:59 - hidden - if I knew what that is then I
54:03 - could adjust the weights coming in to
54:05 - that so this is the idea of back
54:07 - propagation there's an error here it
54:10 - goes to here how many no but if I could
54:11 - calculate these errors then I could
54:13 - continue to go back here and then if
54:15 - there were more I would just calculate
54:16 - these errors and keep going this way so
54:17 - this is the real question how do I
54:19 - calculate that hidden air how do I
54:22 - calculate the error of a neuron anywhere
54:25 - within the network that's not
54:26 - necessarily directly connected to the
54:28 - output where that error is just a simple
54:30 - target - out guess and it pause for a
54:34 - second okay I think this is all still
54:43 - making sense I'm gonna look at my notes
54:47 - I am now finished with my first page of
54:51 - notes thank you very much maybe I'm not
54:56 - going to need a second try I'll never
54:57 - need that page of notes again I look
54:59 - forward to the time where I forgotten
55:01 - how all this works again because it's
55:02 - like years later but I have this nice
55:04 - video that explains it I can go back to
55:05 - watch it all right all right okay
55:20 - so the way that I want to do this yeah
55:31 - yeah yeah all right I'm thinking about
55:35 - this sorry I'm
55:49 - Oh II won okay yeah yeah all right all
55:54 - right okay
55:58 - all right I'm gonna to figure this out
56:01 - I'm gonna pull just the section of this
56:03 - diagram out over here
56:04 - I'm gonna need to progressively over
56:06 - time make this diagram more and more
56:08 - complicated but right now I'm going to
56:09 - simplify for a second again so what I
56:12 - want to do is I just want to take
56:14 - actually it's sort of here already but I
56:17 - just want I don't want to pull it over
56:18 - here so this is the output and these are
56:21 - the two hiddens
56:23 - this is the weight of this is the weight
56:27 - of 0.2 and this is the weight of 0.1 and
56:29 - the error here is equal to 0.3 so what I
56:36 - want to and this is the hidden layer
56:38 - what remember what I'm trying to you
56:41 - know h1 and h2 what I'm trying to
56:43 - calculate is the error of hidden one and
56:50 - the error of hidden - okay so the way
56:56 - that I do that is by taking a taking
57:00 - this error and getting a portion of it
57:02 - what portion should I get and I sort of
57:05 - said this already two-thirds and
57:08 - one-third so if this is weight one and
57:12 - this is weight - right there's a very
57:14 - simple scenario this is almost like back
57:16 - to that perceptron again then what I
57:18 - want to do is say the error of hidden
57:20 - one is weight 1 divided by weight 1 plus
57:24 - weight 2 times the error of the output e
57:28 - output the error of hidden one is a
57:33 - portion of that outputs error the error
57:36 - of hidden two is weight 2 divided by
57:41 - weight 1 plus weight 2 times that output
57:46 - error again we sort of said this already
57:48 - a realized I'm cut but this is fine it
57:50 - doesn't hurt to try to do this twice so
57:52 - these would actually be the errors here
57:55 - these are now the errors and I can just
57:57 - use the same gradient descent algorithm
57:59 - to to
58:00 - these weights but this actually is kind
58:03 - of a rare circumstance where you know
58:06 - everything's just connected to one thing
58:08 - so what I want to do now is I'm gonna
58:12 - make this diagram a little bit more
58:14 - complex and I'm gonna take the case of
58:21 - having more than one output neuron see
58:29 - how this works I'm gonna add another so
58:33 - let's say there's two outputs which
58:35 - means there's error one and error two
58:39 - right maybe this is trying to recognize
58:42 - a true we could actually recognize like
58:44 - true or false
58:46 - or something to recognize a zero or a
58:47 - one
58:48 - so in that case maybe the desired answer
58:51 - is you know it's is one and zero but we
58:54 - got we got point seven and point three
58:59 - is the outputs so here this error
59:02 - I made this to let's make this point for
59:04 - this error would be negative zero point
59:07 - four right and now what I need to do is
59:17 - have more connections here and once
59:22 - again this is and I'm this is for this
59:27 - matrix I've got kind of the same
59:28 - notation 1 1 2 1 1 2 2 2 ok now look at
59:37 - this now error 1 is one thing what's the
59:42 - error coming out of hidden error 1 sorry
59:45 - hidden error one is base so pause for
59:53 - second I got it in my head but I'm just
59:55 - gonna take a break shouldn't the air be
59:56 - squared
59:57 - yes yeah when I get to the yes right now
60:04 - I'm not worried about that it was asked
60:06 - in the chat room the air be squared when
60:08 - I calculate this sort of cost the total
60:10 - cost we're gonna square it yes you do
60:13 - square the error but and this
60:14 - case I'm just trying to look at how this
60:16 - gets divided up so if this is our
60:24 - diagram now with two outputs so there
60:28 - are two errors known errors that we can
60:30 - calculate in our supervised learning
60:31 - system we need to figure out still
60:34 - figure out hidden error one hidden air
60:35 - or two well this still stands right this
60:38 - is still a part of the error coming out
60:41 - of here it's the part that it's the part
60:43 - of that it's responsible for it's how
60:45 - much is it responsible for 0.3 and so in
60:48 - that case this should be air output 1
60:51 - and I'm actually just going to if it's
60:54 - an error on the output I'm just gonna
60:56 - say e 1 so this is and this now is this
61:02 - weight let me let me let this weights
61:07 - portion and and this weights Portland
61:13 - portion right whoa sorry oh shoot try to
61:23 - click undo in my head I was just like
61:25 - undo I'm gonna look at my notes here I
61:31 - want to make sure I'm using being
61:33 - consistent about my 1 1 and 1/2 yeah I
61:36 - am that's what I that's what I've been
61:38 - using 1 1 and 1/2 right yeah okay sorry
61:45 - let me do this again this error coming
61:50 - out of hidden neuron 1 is the same as it
61:54 - was before it's still the portion of the
61:58 - error based on this weight in this way
61:59 - how much of like how much are we
62:01 - contributing to that point 3 air well
62:03 - this this one has a certain percentage
62:06 - in this one has a certain percentage and
62:07 - that is weight 1 1 divided by 1 1 plus
62:11 - 1/2 times that error now here's the
62:14 - thing that's just how much it's
62:17 - contributing to air 1 how much is it
62:19 - contributing to air - we've got to
62:21 - calculate that as well and that is the
62:25 - portion of
62:27 - these two and I think you must make sure
62:29 - I have a lot of space so how much of
62:31 - weight 2 1 / weight to 1 plus weight to
62:36 - 2 times air 2 so if their multiplicity
62:42 - total cumulative error of hidden neuron
62:45 - 1 it's that can it's it's its portion of
62:49 - the connection from it to error 1 sum
62:52 - with all the other connections to air 1
62:54 - and the portion of its connection to air
62:57 - 2 some with all of the connections to
63:00 - air 2 and I'm kind of it's that
63:03 - connected to air - it's connected to the
63:05 - output but the output is producing that
63:06 - error so this is it's some right now I
63:09 - could do the same thing for this error
63:13 - it's it's portion that it contributes to
63:16 - this error which is now wait 1 2 right
63:22 - you can see that these are the same
63:23 - right this is its portion weight its
63:26 - that's where it's connected this is its
63:27 - portion that's where it's connected
63:29 - right it's it's its percentage portion
63:32 - of the total weights it's all connected
63:35 - to output 1 and then how is it connected
63:41 - to output - wait - - its portion of the
63:46 - total of air - of the error for output -
63:53 - so this is now how those errors are
63:57 - computed so again if the mathematics of
64:00 - gradient descent tells us how to take an
64:03 - error to nudge weights then we
64:08 - calculated this error now we can we can
64:11 - take the error is coming out of the
64:12 - output these are our formulas for
64:13 - calculating the errors coming out of the
64:15 - hidden and those errors are the things
64:19 - that could then with gradient descent
64:20 - tell us how to nudge these weights then
64:23 - if we had more layers we could calculate
64:25 - these errors and keep going back that is
64:27 - back propagation that is how the hidden
64:31 - errors are calculated so this is the end
64:34 - of this sort of part one of just sort of
64:36 - like the basic idea of back propagation
64:37 - I'm going to check to see if there any
64:40 - question
64:40 - which I will answer at the beginning of
64:42 - the next video I'm gonna check to see if
64:49 - they're already questions or Corrections
64:51 - which I will mention at the beginning of
64:53 - the next video where I will also
64:56 - implement at least just this much in the
64:59 - code alright alright I'm not seeing any
65:11 - like thing that I got horribly wrong I
65:14 - actually think that might be fine
65:17 - it did prepare but the longer the more
65:20 - we get through today the less
65:21 - preparation I have that's the problem
65:23 - like I was preparing but then I kind of
65:25 - like ran out of time like I gave myself
65:26 - like an hour this poor to prepare only
65:28 - reading what how but back propagation is
65:31 - and I did watch those videos yesterday
65:33 - from 3 Brouillette brown but it's kind
65:34 - of you know I went to sleep everything
65:37 - i'lli'lli'll I this way while I sleep in
65:39 - all the knowledge it just leaks out if
65:40 - you look at the pillow in the morning
65:41 - just like all these like math symbols
65:43 - there it's amazing because just like put
65:44 - that back in so what I want to do oh I
65:55 - forgot I forgot actually there was more
65:59 - to this what I meant to do is like
66:01 - actually I meant to turn this into
66:04 - matrix math but that's fine I'll do that
66:07 - in the next video ok this is going great
66:19 - I never heard such feedback before
66:22 - amazing ok just wait to the power I get
66:27 - like here's a formula I forget it let's
66:30 - just implement it okay all right so
66:39 - all right all right okay I'm gonna keep
66:53 - going
66:55 - all right so in the previous video I
66:59 - went over how to calculate the error in
67:02 - a supervised learning system right the
67:05 - error is just the known answer - the
67:07 - system's guess then what I did is talk
67:09 - about well how can I take that error and
67:12 - move it backwards through the system
67:14 - feedback words instead of feeding the
67:16 - data forward to the network feed the
67:17 - error backwards and the way that I can
67:19 - do that is by looking at like JIT each
67:21 - of these contribute to that error let's
67:23 - kind of like make this error like a
67:25 - portion of that error and these were the
67:26 - formulas that I went through right look
67:29 - at this this neuron hidden neuron is
67:31 - connected to this output this is the
67:33 - weight how much is this way two
67:35 - percentage of all the other neurons that
67:37 - are connected it's just two in this case
67:39 - so it's just the sum of these two but
67:41 - there could be many more so what I want
67:43 - to do in the code now is actually add
67:45 - this training function where I send in
67:49 - input data to the network with a known
67:51 - answer and have the training function
67:54 - calculate the error so calculate this
67:57 - basically I want to calculate this
68:00 - vector e1 e2 and I sent vector but this
68:05 - matrix one column matrix that's what I
68:08 - want to calculate that's going to be a
68:09 - really easy part so let's do that first
68:12 - alright so I actually I say in a
68:14 - previous video I sort of set up this
68:16 - function I think right this is this is
68:18 - the feed-forward function where I just
68:20 - take the inputs and I feed them forwards
68:22 - and return the output here is a function
68:25 - where I take the inputs and get an
68:27 - answer now ultimately I should just be
68:31 - able to use the feed-forward function
68:33 - right
68:33 - I might have to tweak this but on the
68:37 - one hand I should just be able to say I
68:41 - should just be I'll say let output equal
68:44 - this dot feed-forward inputs no reason
68:50 - why I should be able to do that and
68:53 - and now what I need to do is calculate
68:54 - the error so the error is Error error
68:59 - equals the target so answer maybe I
69:02 - should call this target targets I'll
69:05 - call it target it might be plural right
69:06 - there these are the target output that I
69:09 - want targets
69:10 - - outputs so let's call this outputs
69:14 - again the first example that I'm gonna
69:16 - build is gonna have just one output but
69:18 - I want the system to work for as many
69:20 - outputs as there are so in theory I
69:23 - should be able say let error equal math
69:27 - dot subtract targets comma outputs now
69:33 - this is where we get a little bit into
69:34 - the weeds here if I'm not math matrix I
69:38 - didn't even know if I've implemented
69:40 - this function in my matrix library so
69:42 - that's what I need to go and check but
69:43 - this is where we're kind of like oh my
69:45 - goodness if we're only we're using numpy
69:46 - we just use - and it just works or maybe
69:49 - there's some other highly optimized
69:51 - JavaScript matrix math library that will
69:53 - do this for us I'm gonna get to all that
69:54 - yes deep learning is is coming but I'm
69:58 - right now I just want to do this in my
70:01 - code now here there's a little bit of
70:03 - awkwardness here this is an array so
70:08 - this comes out as an array I actually
70:12 - want that to be a matrix so it's a
70:15 - little bit awkward but I'm gonna say
70:16 - outputs equal outputs dot Oh matrix from
70:21 - array outputs so this is me convert
70:29 - convert array to matrix object and then
70:34 - the targets I probably the users gonna
70:38 - send that in as an array so I'm also
70:39 - going to say targets equals matrix from
70:42 - array targets so this is a bit of
70:47 - awkwardness because of the way that I
70:48 - develop my library that probably someday
70:50 - in the future I want to like refactor or
70:52 - rethink but I need to have those things
70:54 - be matrix
70:54 - objects for me it will do this matrix
70:56 - subtraction now let's check the let's
71:02 - check the matrix library I
71:05 - believe there's no subtract function
71:10 - there is an add function which adds to
71:14 - the alright so this is so silly the way
71:17 - that I'm doing this because I probably
71:18 - in a real world if I were really big
71:21 - thoughtful I would make a comprehensive
71:22 - matrix library that has every
71:24 - possibility that I'm just gonna put in
71:26 - there what I need so what I need is a
71:29 - static subtract function and I'm gonna
71:37 - have another I'm gonna call it other for
71:41 - the other matrix
71:42 - oh no no matrix a and matrix B so this
71:45 - should return a new matrix a minus B so
71:51 - a couple things one is if the rows and
71:56 - columns aren't the same I mean I want to
71:58 - do this element wise right basically if
72:00 - you've forgotten what's going on here
72:02 - basically I have I have a matrix like
72:04 - this which has these guesses 0.7 0.4 and
72:08 - I want to and I have the known answers 1
72:12 - 0 and I want to take this matrix minus
72:15 - this matrix to give me one that has 0.3
72:18 - negative point 4 so let me add a
72:22 - function that does that it's too bad I
72:24 - didn't have it from before and I should
72:27 - do some error checking here like check
72:28 - if the rows and columns are the same
72:30 - I'll put that in later I'll put that in
72:32 - later got to kind of move along here
72:33 - some that'll be in there maybe when you
72:35 - look at the code someday but I'm just
72:36 - gonna say I'm gonna do a first I need to
72:39 - make a result which is a new matrix
72:45 - which is a new matrix that has the same
72:48 - number of rows and columns as either of
72:51 - them I think it call it rows and columns
72:54 - right so I need to make a new matrix as
72:55 - the same amount and then I'm gonna use
72:57 - my very subtle silly loop function that
73:00 - I use over and over again to loop over
73:02 - all the elements and just say the result
73:05 - dot data index I which is the row index
73:12 - J equals a data index I
73:18 - A - B data index I index J right so this
73:25 - is me just going through and subtracting
73:27 - everything from one to the other and
73:29 - then I can return this result all right
73:35 - so I had this static subtract function
73:37 - is it silly to have a subtract function
73:39 - when I use of an odd function
73:41 - that then you know a die could be used
73:43 - for subtract by just saying like
73:44 - multiply the whole matrix by negative
73:46 - one but whatever I'm just doing I'm
73:47 - doing it as I'm doing it will refactor
73:49 - later all right so now let's think about
73:53 - this I'm going to comment this out and
73:59 - I'm going to say these are my inputs now
74:03 - I'm gonna have my targets and in this
74:06 - case my neural network is two to one so
74:10 - I'm gonna keep that my target is just
74:12 - one I want to get one now what I'm going
74:15 - to do is I want to say neural network
74:18 - instead of feed-forward I want to say
74:20 - train with these inputs and these
74:25 - targets so let's just run the code and
74:28 - see if I get any errors then I'll debug
74:30 - the actual output so let's go to the
74:33 - browser where I've kind of got this code
74:36 - runs output is not defined sketch dot J
74:39 - S line 11 oh there is no output now hey
74:47 - no errors thank you I'm done I forget
74:50 - about the thermal Network stuff
74:52 - oh yeah I am more to do I'm afraid to
74:54 - keep going but I'm gonna keep going
74:54 - let's just look in the training function
74:57 - and console.log the inputs the targets
75:06 - oh let's just console.log the air
75:12 - actually I just do arrow dot print sorry
75:17 - so I can say outputs dot print because I
75:22 - have this print function and targets
75:26 - targets dot print and error dot print so
75:30 - I just want to look I just want to
75:31 - examine all
75:32 - these things are I got an error oh error
75:37 - dot print all right so okay something
75:41 - went wrong so this is what did I this is
75:46 - the output this is the target this
75:49 - should be the difference so what went
75:51 - wrong here within my subtract matrix dot
75:53 - subtract target's
75:55 - comma outputs let's look at this
75:58 - function something must be wrong here
76:13 - anybody sick of looking zeni buddy see
76:15 - it I J columns rows columns in that data
76:27 - Petros ADA columns return results this
76:32 - dot rows oh yes thank you
76:38 - alright here's the mistake this dot rows
76:41 - this columns that makes no sense
76:43 - I make I'm writing a static function
76:45 - that's not called on an instance of a
76:47 - matrix object I want to look through
76:48 - everything I've got to loop through
76:49 - everything in result or a and B they
76:51 - should all have the same number of rows
76:52 - and columns thank you to Rubin in the
76:57 - chat who pointed that out to me okay
76:58 - result dot Rose result columns all right
77:01 - now let's run this again great this
77:04 - looks right this is what the neural
77:05 - network produced this is what it this is
77:09 - the known output and this is the this is
77:13 - the error and just to just to take this
77:15 - one step further if I were to have two
77:19 - outputs I don't know I'm not and and
77:21 - have a second target this is matching
77:24 - what I've drawn over here we could see
77:28 - these are the guest outputs this is the
77:31 - target and these are the errors so we
77:33 - have now written into our code all of
77:36 - the pieces we need to get these two
77:40 - errors get the air the output errors the
77:44 - next step we need to do is calculate the
77:47 - hidden errors I need to calculate the
77:49 - hidden pairs
78:07 - so looking at the code
78:10 - I need another step here I need to now
78:14 - say let hidden errors equal and figure
78:20 - that out
78:20 - so what goes here what goes here this is
78:23 - the and I suppose if I'm being
78:26 - consistent I should say errors there
78:27 - right errors are the out pen and and if
78:30 - I'm being really consistent I should say
78:32 - output errors now what I need to do is
78:35 - calculate the hidden errors okay so
78:40 - let's go back to here and I want to do
78:42 - this with matrix math so this looks like
78:45 - hey that could be some matrix math going
78:48 - on here right this looks like a weighted
78:50 - sum or something dot product D like
78:52 - looking thing and here's the trick this
78:57 - looks good right but what's all this
78:59 - like fractions stuff well if you look at
79:02 - this this is the same as this this is
79:05 - the same as this these are really
79:07 - normalizing dividing these weights by
79:10 - the sum of all the weights is a way of
79:12 - normalizing everything so they all add
79:14 - up to 100% in the end we're gonna kind
79:17 - of like multiply everything by this
79:18 - learning rate constant anyway so we
79:20 - could say like make a big step or make a
79:22 - small step so that normalizing of it
79:24 - kind of doesn't matter I mean it's sort
79:26 - of important but we can also ignore it
79:27 - and that's kind of a trick here we're
79:29 - gonna just take out the we know we want
79:31 - the amount of the air to be proportional
79:33 - but the fact that we're multiplying by
79:36 - its weight it's going to be proportional
79:38 - we don't have to divide it by the Sun so
79:40 - we can actually take the bottom out and
79:42 - I'm gonna say wait one one here I'm
79:46 - gonna say wait one two here I'm gonna
79:50 - say wait to two here and I'm gonna say
79:55 - wait to one here and look at this by
79:59 - golly doesn't that look like some matrix
80:02 - math right that's got to be the result
80:04 - of some matrix product now I need more
80:07 - space on the whiteboard so how can I do
80:10 - is kind of condense this a little bit
80:11 - well one one times e 1 plus weight of 2
80:15 - 1 times e to make this take up less
80:18 - space
80:19 - and then wait one two times e1 plus wait
80:26 - what was that - two times e2 now
80:31 - conveniently I have this matrix here and
80:36 - what matrix would I put here to get this
80:39 - right
80:40 - if I want the matrix product of these
80:41 - two matrices I need to put a row in here
80:44 - that's right the row this row W 1 1 W 2
80:51 - 1 the weighted sum the dot product of
80:54 - this row and this column is exactly this
80:56 - now let's take this one 1 2 and wait - -
81:02 - right
81:02 - take this dot product with here boom
81:04 - we've got this this matrix product the
81:08 - matrix product between these two
81:10 - matrices is that hidden errors coming
81:12 - out of the hidden layer this is really
81:17 - exciting but there's something really
81:19 - strange here
81:20 - it's like stare at this now all along I
81:22 - keep getting my indices wrong right
81:24 - I've been getting my histories wrong in
81:25 - these tutorials I had to do the
81:26 - tutorials over again and this might look
81:28 - wrong right because should not be wrong
81:31 - column row column row column row column
81:34 - this is Row one how eyes are - there
81:37 - looks like I got it backwards well the
81:38 - fact is I did get the backwards I got
81:40 - that backwards on purpose this is
81:41 - actually this weight matrix transposed
81:44 - so these weights are stored in a matrix
81:47 - already in my code that matrix looks
81:50 - like this weight 1 1 wait 1 2 wait 2 1
81:55 - wait 2 2 transpose this matrix to this
82:02 - and take the dot product with the heirs
82:04 - and boom I've got the hidden errors
82:08 - coming out ok let's do that
82:14 - this is working because it's square but
82:16 - am I gonna have an issue if it's not
82:18 - square I didn't notice this when I was
82:20 - working this out or anyway let's go put
82:24 - that into our code
82:27 - oops yes w superscript T okay all right
82:47 - the good news is
82:49 - oops right the good news is I believe in
82:54 - the matrix library I already wrote a
82:56 - function called transpose here's I
82:59 - should be consistent this function
83:01 - transpose returns a new matrix that's
83:04 - the transposition of the previous one
83:06 - and so I should actually make this
83:08 - static and it should require to receive
83:11 - a it should receive some matrix object
83:15 - and so it should be this so I just
83:21 - changed this to be a static function so
83:23 - that what I can do is I can say here
83:27 - back in the library what do I need to
83:29 - transpose I need to transpose the
83:31 - weights that are going from hidden to
83:37 - output that's the weights H of so I have
83:42 - this dot weights H oh so let hidden um
83:53 - weights H o transpose T for transpose
84:03 - hmm I could also just change the let who
84:07 - T these are the way naming I could use
84:10 - some work on the naming but these are
84:12 - the weights from hidden to output
84:13 - transposed equals matrix dot transpose
84:18 - this dot weights hidden output so the
84:23 - hidden errors now should be matrix
84:28 - multiply output way what did I say hey
84:35 - if we look over here matrix multiply
84:38 - that transpose
84:39 - matrix and those errors that transposed
84:47 - matrix and the output errors so this is
84:53 - calculate the hidden layer errors now if
84:58 - I were writing a proper library that
85:01 - could support multiple layers there'd be
85:02 - some kind of loop going on here because
85:04 - I keep doing this from layer to layer to
85:06 - layer but since I just have this two
85:08 - layer Network I'm just gonna do the
85:10 - output layers output errors and the
85:12 - hidden errors so that I can sort of get
85:14 - this back propagation thing going in
85:16 - just one step
85:18 - I'm getting a nice comment from the chat
85:21 - I was worrying about the dimensions of
85:23 - my matrices and Kate week Minh writes
85:25 - the dimensions work out when going
85:27 - backwards the transpose right of course
85:30 - because I was worried about the
85:32 - dimensions not working because you know
85:34 - the rows and columns have to match
85:36 - properly when you're doing matrix
85:38 - multiplication but since I'm now going
85:41 - backwards it actually makes sense that
85:42 - the matrices have to be transposed you
85:44 - can pause and think about that for a
85:45 - second but that does make sense now ok
85:48 - so I think I've come oh and I was also
85:51 - but you know if you're looking at this
85:53 - notation in like a text book or
85:55 - something
85:55 - you'll often see if I have like if you
85:57 - have a weight matrix that's W and maybe
86:00 - it's like W IJ for rows and columns
86:02 - you'll often see T in the superscript as
86:06 - and you can't see that write it over
86:12 - already over here right you know if this
86:13 - is the weight matrix W and you have
86:16 - columns and rows then you'll often see a
86:19 - superscript of T and that refers to this
86:22 - this matrix trans transposed and so
86:27 - maybe this would be you know weight
86:31 - h.o.t or something I don't know
86:33 - so my notation is as you all know is
86:35 - kind of poor but I try to do my best to
86:37 - explain it hopefully it will match up
86:39 - with other other notation that you see
86:41 - in that sort of thing okay so this is
86:43 - now done in terms of back propagation
86:46 - I've compute
86:47 - the error I've computed the I've
86:50 - propagated that error backwards to
86:52 - compute the hidden layers error and now
86:55 - I just need to add the part where I
86:57 - adjust all of the weights these weights
87:00 - based on this error and these weights
87:02 - based on this error and then we just
87:04 - move on and then we're done so I'm gonna
87:07 - do that in the next video I should warn
87:09 - you that the math for doing that is this
87:12 - which I'll discuss in generalities the
87:15 - math of gradient descent for finding
87:19 - these Delta weights is pretty
87:22 - complicated I have two videos that I've
87:25 - made where I kind of do something
87:26 - similar which I will reference as well
87:28 - as all of those three blue one Brown
87:30 - videos that go through the math in
87:31 - detail I'll probably start the next
87:33 - video just by presenting the formula and
87:35 - then implementing kind of talking
87:37 - through the pieces of the formula and
87:38 - then implementing that in code okay
87:45 - [Music]
87:48 - right people are telling me I don't need
87:52 - the IJ yeah yeah I don't need the IJ but
87:55 - anyway
87:57 - lower case T upper case T so now what I
88:00 - need to do is I need to where am I in my
88:07 - notes what time is it oh it's twelve
88:10 - thirty five remember items can be done
88:12 - with backpropagation by 12:30 that
88:13 - didn't happen that's okay though
88:23 - even though it's search done alright
88:31 - alright so I think does everybody feel
88:37 - ok let's get a snapshot of the
88:39 - whiteboard as is I think I'm gonna erase
88:40 - this now because what what error do I
88:43 - have result equals new matrix this
88:47 - columns this dot row huh where where
88:50 - where where where do I need to change
88:56 - that sorry I'm seeing something in the
89:03 - chat that I got columns and rows
89:05 - incorrect somewhere I think I have that
89:09 - here in the static transpose no this is
89:18 - correct
89:19 - because but you might think this is
89:21 - incorrect but I have written my matrix
89:23 - library in such a way that the rows and
89:25 - columns come in first so if you're
89:27 - transposing it you send the columns in
89:29 - first and the rows second so I think
89:31 - that's correct
89:36 - I'll let and see if anybody I'm gonna
89:39 - give it a minute
89:39 - so what I need to do now is find the
89:45 - best notation for writing out the
89:48 - formula and I kinda this is where I kind
89:51 - of got stuck in my preparation for today
89:53 - although I have to say I can't use this
89:58 - on static ah oh I see thank you here got
90:07 - it
90:10 - um what's the best way I'll just I'll
90:16 - just do this at the beginning of the
90:17 - next video shoot as I didn't try to I
90:24 - would have gotten this error if I tried
90:25 - to run the code let's let's add
90:28 - something to the end yeah it's fine I'm
90:30 - just at the beginning yeah yeah I got I
90:33 - got it everybody I'm just trying to
90:35 - figure out where to know I haven't
90:37 - updated the wait Simon is asking have
90:39 - you update the weights no there's a
90:40 - whole big step here I mean kind of like
90:42 - I just did the sort of like beginning
90:46 - part so I the whole big step so I guess
90:52 - I'll just cop to this error at the
90:55 - beginning of the next video so what I'm
90:59 - trying to do I'm gonna erase the
91:00 - whiteboard I don't think this has any
91:03 - value anymore or at least this whole
91:07 - bottom part doesn't I can leave this
91:09 - diagram here maybe for reference so let
91:13 - me try doing that
91:31 - and let me try to find a form a way of
91:39 - noting the formula for updating the
91:42 - weights this time I need to try twice
91:44 - I'm not gonna so this is my this is my
91:55 - thinking and I'm going to kind of use my
91:56 - notes here a little bit I'm going to
92:00 - need help with everybody and well that's
92:05 - just the mouse oh it's like there's a
92:06 - black mark on the whiteboard so I I will
92:11 - correct this by the way at the beginning
92:13 - of the next video me just make sure the
92:17 - whiteboard is dry so my thinking is the
92:28 - any given weight equals its its weight
92:40 - plus a change in weight right and the so
92:46 - the thing we're trying to calculate is
92:48 - this another way of and the way that
92:52 - we're going to calculate this is by
92:54 - thinking about the way the the hat when
93:00 - we want to understand like two we want
93:02 - to adjust the weights we want to adjust
93:06 - the weights so you know we want to think
93:12 - about this as the change in error based
93:16 - on the change in weights so how does the
93:22 - right if I know how the error changes
93:25 - based on the how the I'm just sort of
93:29 - thinking this through based on how the
93:31 - weight changes right this is Nicole
93:33 - gradient descent thing then basically
93:36 - I'm saying Delta I want to change
93:40 - take the weight and minimize the error
93:44 - so adjust it based on the negative
93:52 - direction how the error changes based on
93:53 - the week and this error is something
93:56 - different it's actually the like
93:58 - cumulative cost so it's that's the kind
94:00 - of squares of all the errors let's see
94:02 - if I'm getting this about right and so
94:05 - that question becomes how do I know how
94:11 - the error changes based on the weights
94:13 - and that is the formula that I've kind
94:19 - of derived previously is the error times
94:29 - the so in this case right this is the
94:34 - output the derivative of the output
94:39 - based on the weight Oh talking this
94:44 - through I'm gonna come back and explain
94:46 - it let's see if everything I'm I'm gonna
94:51 - do this again I'm just kind of like
94:53 - talking this through into my head and so
94:56 - now what I need to do is figure this out
94:58 - and what this is is output is sigmoid
95:05 - right of the weighted sum how did I
95:08 - write this down
95:09 - I just wrote weighted sum of my notes we
95:11 - can think of the weighted sum of all the
95:13 - connections coming in but I could just
95:15 - call that like X for a second and so and
95:21 - the derivative of sigmoid is a known
95:24 - thing the derivative of sigmoid x equals
95:29 - sigmoid of x times 1 minus Sigma of X
95:37 - and it looks like a 6 and so and so
95:45 - basically if I put this here I kind of
95:52 - have the I kind of
95:57 - the formula for any given single weight
96:03 - let's look at this shot so so this I
96:09 - think is right that's where my notes end
96:15 - let me look at the code that I did so
96:18 - that's really like the gradient the good
96:22 - thing of this is would it be correct to
96:23 - refer to this kind of as the gradient
96:26 - and since I need to do that for
96:29 - everything how can I know take this with
96:31 - matrix math I know that what I do in my
96:34 - code is I I have I take the outputs
96:52 - transposed I need more room on the white
97:01 - board
97:12 - let me get this far so I think of what I
97:23 - want to do is try to get this far for a
97:26 - single weight then I'll erase and try to
97:30 - get further with turn this into matrix
97:35 - matrices anybody have any Corrections or
97:40 - notes or thoughts you want to add to
97:43 - what I haven't erased this and kind of
97:44 - go through it but actually like try to
97:46 - explain it a bit more as I do that I
97:50 - mean I could just include the final
97:52 - formula at the top
97:55 - maybe I should maybe I should write that
97:57 - down I don't see output equals matrix I
98:04 - don't see anybody talking about right
98:07 - now Jack who's never just talking
98:09 - amongst themselves all right so I think
98:16 - right we just we just need to figure it
98:18 - out for any one of these given weights
98:21 - how to adjust it based on the error and
98:28 - we've calculated all the error stuff
98:29 - already okay
98:41 - just looking at this all right I'm gonna
98:48 - yeah start with a single weight all
98:51 - right I'm looking in the chassis some
98:55 - people are typing in the slack patron
98:57 - group I might not get to to be honest
99:00 - the challenge today I think I can I
99:06 - think I can
99:17 - alright alright alright I'm just gonna
99:23 - move forward I'm gonna erase all this
99:25 - I'm gonna try to explain it again I
99:29 - forgot where I was I finished I just
99:32 - finished a video right okay
99:55 - all right oh the yes the error function
100:03 - meaning the the cost of all of the sum
100:08 - the sum squared of all the errors a
100:10 - squaring yeah yeah I do I do need to
100:13 - talk about that yes yes thank you okay I
100:17 - will I definitely will mention that all
100:27 - right so in this video I'm going to
100:28 - start to talk through learning learning
100:31 - with gradient descent we've looked at
100:33 - how to back propagate the error that's
100:36 - coming from the output back through the
100:38 - network now we need to look at how do we
100:40 - turn all the dial set but before I get
100:41 - to that I did make a sort of major
100:43 - mistake in the previous video where I
100:45 - was making this transpose function and
100:47 - turning it into static so the word this
100:50 - now means nothing so I need to make the
100:52 - resulting transpose matrix out of the
100:55 - one that's being passed in so just
100:57 - wanted to correct that right here the
100:58 - beginning of this video quickly okay now
101:01 - so there's a few things we need to talk
101:04 - about so first I want to talk about what
101:06 - is this thing called gradient descent
101:08 - now let me once again as I always do
101:12 - reference that other materials will go
101:14 - to this in more depth I made a video
101:16 - called the mathematics of gradient
101:17 - descent it don't really recommend that
101:19 - one is my top choice but you could go
101:20 - watch that and it'll also reference the
101:23 - one that do recommend which is the three
101:24 - blue one Brown video about gradient
101:26 - descent and there's a nice Siraj video
101:28 - about gradient descent to suit to I'll
101:29 - link to both of those in this video's
101:31 - description but the basic idea is that
101:33 - we need to have come up with a cost
101:36 - function what is the cost function well
101:40 - this is an error this is like an error
101:42 - vector right point three and negative
101:44 - point four what we want to do is we
101:46 - actually want to look at the cumulative
101:48 - error over time of many training
101:50 - examples so if we say like here's a
101:52 - hundred pieces of data with known
101:54 - labelled answers what's your cumulative
101:57 - error over all of them and the way that
101:59 - that's typically calculated is by the
102:02 - sum which is often
102:05 - used this nice uppercase Sigma is that
102:10 - uppercase the right thing to say with
102:11 - critters the sum the weight the sum of
102:15 - all of the you know targets minus output
102:19 - squared so this is often referred to as
102:22 - a cost function and you could graph this
102:25 - function like you could imagine it it's
102:27 - like a function like y equals x squared
102:28 - and if I had a function like y equals x
102:31 - squared it would look at this and if I
102:33 - wanted to minimize that function I would
102:37 - have to start moving I would want to
102:39 - like look at how do I go down how do I
102:41 - go down towards zero how do I walk down
102:44 - gradient descent and how do I know the
102:47 - direction of the slope of this function
102:49 - at any given point with a concept known
102:52 - as the derivative so again I would refer
102:55 - you to some videos from three blue one
102:58 - brown that go through the calculus what
103:00 - is a derivative what is great to send in
103:02 - more detail but this is the basic idea
103:04 - I want to push the weights I want to
103:07 - know what happens right when I change
103:11 - the weight how does that affect the
103:12 - error the change in the error based on
103:16 - the change in weights
103:17 - right I want to minimize minimize the
103:20 - error
103:20 - I want change the weight in the
103:22 - direction that changes the air this is
103:23 - kind of the notation for that the change
103:25 - in air L through the change of weights
103:26 - and you know the kinds of derivatives is
103:28 - more sophisticated than that but that's
103:29 - a way of thinking of it right now
103:31 - there's something else I want to mention
103:33 - about this ah yes so here's the thing
103:35 - this is a nice little like graph of y
103:38 - equal x squared and sort of notation
103:40 - over here but the thing that's really
103:41 - crazy about this is we aren't living
103:44 - into this would work if we have a single
103:46 - output this would kind of is what it
103:48 - would look like if there were two
103:49 - outputs then we might have this like
103:52 - Bowl perhaps I think if there were three
103:54 - outputs oh I've lost I can't think
103:57 - we're gonna get into crazy and
103:58 - dimensional spaces then you can't really
104:01 - imagine so that's one thing that's
104:03 - really confusing about this is these
104:04 - outputs could be many many so we really
104:06 - have to in a way tackle them one time
104:08 - like let's well how would it work with
104:11 - for just one and
104:12 - in essence we're gonna do the same thing
104:15 - to all of them by pushing them through
104:16 - some matrix math so that's ultimately
104:19 - what I'm gonna do I'm gonna explain a
104:21 - little bit further but eventually again
104:24 - I'm gonna be glossing over some details
104:25 - and mostly focus on trying to implement
104:28 - this in the code all right let me pause
104:31 - for a second I'm not seeing anyone
104:39 - yelling at me that I said anything
104:41 - horribly wrong so that's the thing that
104:43 - I wanted the error function thank you
104:45 - okay we come for reminding me about that
104:47 - that's what I wanted to start so where
105:00 - is the actual eraser now that we've
105:05 - talked about this or cost function how
105:07 - we're trying to minimize that function
105:08 - using gradient descent walking along the
105:11 - graph of that function towards the
105:12 - bottom let's talk about how we actually
105:15 - do that what's the thing we're actually
105:17 - trying to calculate well what we want to
105:19 - do as we've look at all this training
105:22 - data is we want to say how do I want to
105:25 - change a weight I want any given weight
105:27 - this W for any given weight just one
105:30 - weight in the system
105:31 - I want to the process that I'm gonna run
105:33 - in my code is to change that weight by
105:37 - some Delta weight that change in weight
105:39 - so here we are I should be able to just
105:43 - say if I could just figure out the
105:44 - formula for Delta weight I'm done all
105:50 - right well I did say that
105:53 - another sorry I did just a moment ago
105:58 - talked about the cost function and the
106:01 - idea of how does the error change when I
106:04 - adjust the weights well this is really
106:07 - another way of saying Delta weight if I
106:10 - said Delta weight is you know how does
106:13 - the error change great when I change the
106:15 - weight if it gets bigger when I change
106:17 - the weight I wanted to get I want the
106:18 - error to get smaller so I could kind of
106:20 - say Delta weight is negative how the how
106:23 - the
106:23 - Changez when I changed the weight so
106:26 - this is just another way of kind of
106:27 - writing the same thing people who know
106:30 - more about mathematics might be getting
106:32 - upset with me right now I will look
106:34 - forward to your comments all right so
106:36 - now let's think about this how do I
106:38 - calculate this so how do I know how the
106:42 - error changes when I change the weight
106:46 - well this is something that I have gone
106:48 - through in my gradient descent video
106:50 - what I can do here is actually just say
106:55 - looking at my notes oh I forgot about
106:58 - the learning rate sorry sorry sorry
107:05 - sorry
107:13 - I should add one more thing in here
107:15 - which is that this Delta weight can also
107:18 - be a factor
107:19 - I can also factor in this thing called a
107:20 - learning rate I'm gonna kind of fold
107:22 - this in at the end but the learning rate
107:24 - is just like I this is how much I should
107:26 - this is the direction that I need to
107:27 - move you know should just look like a
107:29 - tape it or let's try to move like a lot
107:31 - and so you've seen this again in the
107:33 - simple perceptron example this idea of a
107:35 - learning rate so that should get folded
107:36 - in here as well so basically what I'm
107:39 - doing here is I'm saying this is a
107:41 - formula that I've derived in other
107:42 - places this should equal I'm looking at
107:46 - my notes over here just a few pi get
107:48 - this right
107:49 - negative the error itself times how the
107:56 - output changes when I change a weight so
107:58 - I'm gonna call that do DW how does the
108:03 - output change when I change the weight
108:05 - well what is the output that's something
108:08 - we know we've been working on that right
108:10 - the output of any given neuron is the
108:11 - weighted sum of everything coming in
108:13 - pass through this sigmoid function and
108:15 - again sigmoid is kind of a historical
108:17 - activation function that's not used as
108:19 - much right now and later you know we can
108:21 - swap in other activation functions
108:22 - understand but they're one of the
108:24 - reasons why it's useful to use sigmoid
108:25 - is it makes doing this calculation kind
108:27 - of easy so if the output the function
108:31 - that's the output is equal to Sigma E
108:34 - and that's the Greek that's the
108:35 - lowercase Greek letters
108:37 - I'm trying to write this out maybe we
108:39 - should just write the word sigmoid I'm
108:41 - gonna write in this sig si G sigmoid of
108:45 - that whole weighted sum of everything
108:47 - let's just call that X for a second then
108:50 - if this is the output function how that
108:54 - output changes when I change a weight is
108:56 - the derivative of that function write
108:59 - out any given weight write that this is
109:01 - the weighted sum how they changed these
109:02 - weights what is the slope of that
109:04 - function in this crazy n-dimensional
109:05 - space well it just so happens if we go
109:09 - back to Wikipedia sigmoid derivative
109:22 - lets derivative where is it on here it
109:27 - looks so it's so nice man that on here
109:29 - fail a little failed moment here let's
109:33 - find a different place I don't want to
109:38 - derive it just want to look at it yeah
109:44 - here it is that's a nice little oh wait
109:49 - hold on I'm getting some good questions
109:55 - maybe add a factor of 1/2 to make the
109:58 - derivative the error
109:59 - oh yeah yeah so wait where do I add that
110:15 - you mean added in here because this
110:18 - thing is the derivative air is actually
110:21 - there's R squared or something
110:24 - is that what you meant K week Minh I
110:26 - always just flop that off cuz it's just
110:28 - another constant in the system I think
110:31 - you should do a Eric in the chat rightz
110:34 - I think you should do a past propagating
110:36 - errors back through the whole network
110:37 - then update the weights it doesn't
110:39 - matter much now but if you did have more
110:41 - layers who would be able to update all
110:43 - the weights in parallel language
110:45 - permitting I think that's kind of what I
110:47 - did already I'm trying to do but I see
110:50 - what you're saying right because I've
110:53 - already I already did this step of
111:00 - passing the errors back and so now I
111:02 - just want to do update these weights and
111:05 - update these weights we already have
111:07 - these errors so update so I need to fill
111:09 - in the updates here and the updates here
111:11 - that's what I'm trying to do okay maybe
111:16 - I missed it in the stream Y is Delta
111:18 - wait in the first formula using the
111:19 - uppercase Delta but in the second
111:21 - formula upper case Delta wait equals to
111:23 - negative lower case Delta e over lower
111:25 - case Delta W I mean it's really the same
111:34 - [Music]
111:38 - yeah my notation is kind of bad but I
111:41 - may have said I really sort of mean this
111:43 - as shorthand this in a way in the cost
111:49 - function yeah yeah and the cost function
111:55 - okay okay
112:07 - so I'm just reading the chat for a
112:09 - second right I was looking for a nice
112:13 - derivative that of sigmoid
112:17 - [Music]
112:45 - [Music]
112:53 - anybody if anybody has a suggestion for
112:55 - a nice webpage that's gonna I mean I
112:58 - guess I should just use this one it's
112:59 - just like this is a little janky but
113:15 - yeah it's also it's driving it now I
113:25 - know what this thing my drift is just
113:27 - wanted to like find a webpage that
113:28 - confirms it for me maybe math world
113:31 - sigmoid derivative yeah there we go
113:49 - all right I'm just gonna yeah Reuben I
113:52 - did sort of mention that I believe
113:54 - during a bunch of times and I will
113:56 - continue to mention that all right
113:58 - WolframAlpha well from alpha what is the
114:07 - derivative of sigmoid and I cannot spell
114:10 - derivative
114:22 - all these web pages are making me sad I
114:25 - mean that's a nice way of putting it but
114:26 - okay I'm just gonna I'm just gonna I'm
114:29 - gonna I don't need it I don't need a
114:31 - diagram gosh darn it come back over here
114:50 - yeah I will
115:19 - yeah yeah no I'm definitely not doing
115:21 - the derivation I just wanted to put the
115:23 - confirm that I was correct so we could
115:28 - one of them Here I am now on a webpage
115:31 - which shows the derivative a sigmoid and
115:33 - this is what's really nice about it the
115:35 - derivative a sigmoid is actually sigmoid
115:37 - times 1 minus Sigma so that's what makes
115:39 - kind of working this out once I start to
115:41 - plug the math into my actual source code
115:43 - kind of easy so if I come back over here
115:46 - I can say this is times sigmoid of the
115:55 - output - oh sorry
115:56 - times 1 minus Sigma of the output I
116:00 - don't know why I put so this is
116:02 - basically what I'm looking at here if I
116:05 - want to change any given weight I change
116:08 - it based on the derivative of sigmoid
116:12 - multiplied by the error and this is kind
116:16 - of this is the same formula that's
116:17 - appeared into several other of my
116:19 - previous videos and one that you can
116:21 - kind of dive deeper in with some of the
116:23 - links that I've included in this video's
116:24 - description the big question now is how
116:28 - do this is just I'm really just talking
116:29 - about the case of changing one weight
116:31 - how do I turn this into matrix math
116:38 - alright alright so now again this is
116:52 - where my notes have ended and I need a
116:56 - way of writing this with matrix math and
117:01 - the way that I'm gonna do that anybody
117:11 - who wants to help me with this I would
117:16 - gladly take your help
117:19 - oh I forgot about the 1/2 thing I will
117:24 - I'll bring that up I'll bring that up
117:26 - next
117:37 - let me see is there a notation that's
117:39 - used in okay this is the notation that
117:47 - is used in the tariq Rashid book so let
117:56 - me try to write that out dare I erase
118:00 - this top I think that's what I have to
118:03 - erase so let's look at the this notation
118:25 - Delta wait for and he uses JK I've been
118:32 - using I J so I'm gonna keep that I know
118:34 - that's JK is much better equals we have
118:39 - the learning rate I've talked about
118:41 - which times the error vector which is J
118:52 - right because it's a it's like one I'm
119:02 - iterating over the rows so it should be
119:04 - so confusing it's gonna say the error
119:09 - the error vector times sigmoid of the
119:18 - output vector sigmoid of the output
119:24 - vector this is the same thing that I did
119:27 - that I've got right here the same
119:31 - notation I'm just confirming that I'm
119:33 - right times 1 minus Sigma of the output
119:39 - question is if I'm thinking of this
119:43 - as rows columns and things are being
119:46 - transposed and as this is it what's the
119:57 - what's they in disk index it should go
119:59 - under E and under oh that's a question
120:02 - oh my goodness wrong camera wrong camera
120:04 - wrong camera wrong camera so long so
120:07 - long with the wrong camera and I wrote
120:10 - up let me do this again sorry everybody
120:12 - this is good I need multiple tries at
120:14 - this anyway I'm writing off the
120:19 - whiteboard so I'm looking at the
120:23 - notation that's in the tariq Rashid book
120:26 - and this is what's written
120:28 - JK equals the learning rate I'm changing
120:34 - some learning rate times e sub K times
120:41 - sigmoid of o sub K times 1 minus Sigma Y
120:48 - of K and again I'm going up to high
121:01 - times
121:04 - sorry sigmoid make sure you can see this
121:08 - of o sub K times sigmoid sorry times 1
121:17 - minus Sigma of okay so this is the
121:25 - formula as written in oh and then this
121:31 - whole thing right this is
121:43 - why what did I miss I missed this whole
121:46 - thing now and I'm running out of space
121:54 - so I'll figure this out is the mate way
121:56 - and the matrix product right which my
121:58 - symbol for that is like a train the
122:00 - matrix product with the transposed
122:05 - output J so this is what's written into
122:13 - the tarik Rashid book the what I would
122:15 - like to do is try to match this with
122:17 - what I've done so far so in theory
122:21 - except I think ah
122:24 - Tariq one of the things that I noticed
122:26 - is the trick Rashid book I'm pretty sure
122:28 - does columns rows that's why I'm getting
122:30 - confused
122:31 - so my notation has been rows columns and
122:35 - this is a single column vector okay and
122:40 - then this is transposed transposed so it
122:47 - would be J there we go so this would
122:52 - match there and so
123:05 - right that's the bias derivative if I
123:08 - don't multiply it would you multiply it
123:10 - with multiply it with the activations as
123:14 - well so what's this step so why is it
123:27 - that when I explain this why is it that
123:37 - I went through this Delta wait and I
123:43 - didn't include I forgot to actually
123:47 - multiply it by the output here because
123:52 - oh is that because of the negative 1/2
123:54 - thing and then I'm taking the 1/2 thing
123:55 - and I'm taking that out but with the
123:57 - bias it's just 1 and so that goes goes
123:59 - away this is where I this is where I
124:04 - this is comfortable for me and then I'm
124:08 - going to be able to turn this into code
124:10 - but I'm kind of confused as to I mean I
124:14 - get that this needs to be here but I'm
124:16 - trying to get what's a nice way of
124:18 - explaining how I missed putting this
124:22 - doing is the matrix product with these
124:24 - would be with oh but it always is
124:30 - multiplied by that well how come I
124:31 - missed it down here was that just my own
124:34 - mistake I'm gonna go check out some
124:39 - things that are being typed
124:49 - yeah we believe in the chat rightz why
124:53 - am i watching this I'm just learning to
124:54 - make the ellipse in j/s yeah go back to
124:57 - that don't get out while you can
125:03 - all right I'm just gonna I Dan if you
125:07 - already calculated this sigmoid in the
125:08 - outputs you can just do x times 1 minus
125:12 - X target is the derivative in the last
125:17 - layer predicted minus target there's a
125:22 - lot of interesting things being
125:23 - discussed in the chat all right I'm just
125:26 - going to and hold it I'm looking at this
125:40 - camera went off you need to do some more
125:45 - reading and preparing I mean I have to
125:47 - just be done for today and I will
125:59 - eris in the weights in now but I'm just
126:03 - I wish I had that gradient descent
126:07 - formula let's see here I think in this
126:19 - video I could find the part of this
126:27 - video right I'm doing it here basically
126:33 - going through exactly what I'm trying to
126:35 - go through now
126:53 - yeah
127:07 - oh no I'm just so close to finding the
127:09 - right point in this video I think
127:11 - actually this might help if I kind of
127:13 - match it up with the diagram in this
127:14 - video
127:28 - yes you are watching a livestream of a
127:32 - person looking back in a video tutorial
127:35 - that they previously made to try to find
127:37 - something to help understand the video
127:39 - tutorial that they're trying to make
127:41 - right now that is one day what that is
127:44 - exactly what you were doing and you
127:46 - should all just oh here it is down here
127:49 - please step away step away from the I
128:10 - did all this
128:12 - it went through this you know another
128:21 - way for me to do this is actually now
128:23 - that I'm realizing this is just alright
128:27 - alright so what I'm going to right
128:44 - err times X this is this is the
128:52 - equivalent of X this is obviously the
128:55 - error and the new thing here is the fact
129:01 - that I have sigmoid as the activation
129:03 - function right so in this the difference
129:07 - is in here
129:09 - there was no sigmoid function so the
129:13 - derivative was much simpler and doesn't
129:17 - show up right right can I can I make a
129:26 - argument can I can I can I stop where I
129:29 - am right now and connect this here with
129:39 - this video where I derive this formula
129:44 - for the change in weights the derivative
129:50 - is 1 in your video yes I think that's
129:55 - right all right so this is what I'm
129:59 - going to do
130:13 - I seek a week Mon is typing so I must
130:22 - wait K week one usually has something
130:28 - very wise to say
130:42 - shouldn't that be shouldn't that be the
130:49 - hidden layer activations we x yes the
131:02 - hidden layer activations I mean it
131:05 - depends where I am I got rid of my
131:07 - diagram right now I'm not doing the it
131:12 - would be multiplying by the hidden layer
131:14 - activations this is just like what I'm
131:18 - doing right now is generically for any
131:20 - connection and then I'm going to do the
131:23 - same for the output and the same for
131:25 - that the hidden layer but the output
131:27 - uses the error vector from the output
131:31 - and the hidden layers is the error
131:33 - vector the hidden errors so yeah all
131:36 - right two times target minus W index I
131:44 - times yeah exactly exactly exactly yeah
131:49 - I just erased that I know on purpose all
131:53 - right
132:00 - I'm gonna go in back into the video
132:02 - tutorial and we'll see what happens
132:04 - I just took a little break staring at
132:09 - this I'm having this moment it's like
132:10 - haven't I gone through all of this
132:13 - before and then I realized ah I mean I
132:16 - kind of referenced before I have it so I
132:18 - went back and looked so what I want to
132:20 - do right now is like this is where I
132:21 - sort of ended here in trying to sort of
132:23 - look at how the error changes based on
132:25 - how the when the weights change that
132:27 - I've talked about this in my mathematics
132:29 - of gradient descent and in this
132:32 - mathematics of gradient descent video
132:34 - there was no sigmoid function so
132:39 - ultimately what I have here is and this
132:41 - is sort of 18 minutes into this video
132:42 - which boils down to the chain and by the
132:46 - way one thing that I missed over here is
132:48 - is the the 2 or the 1/2 so nice when you
132:54 - take the derivative of something squared
132:56 - you get a 2 and so you divide by 2 or
132:59 - multiply 2 so I just sort of like take
133:01 - those out but because it's just a
133:03 - constant I'm gonna have a learning rate
133:04 - later anyway so but that's why you see
133:07 - the 2 over here but what you see here is
133:10 - change right the change in the weights
133:14 - this is M and B for a single and this is
133:16 - really for a single weight y equals MX
133:18 - plus B the bias I mean not to that yet
133:20 - is the error times the x times the
133:26 - learning rate and that's ultimately what
133:27 - I have here but-but-but-but solet's so I
133:32 - have this book so let me write this let
133:35 - me now write this out with meit sort of
133:37 - more matrix notation this is a
133:39 - make-your-own neural network and I'm
133:40 - going to basically use the exact and I
133:43 - highly recommend kind of reading this
133:44 - chapter along I'm going to use the
133:45 - basically the exact same you know what I
133:47 - want to calculate is the changing
133:49 - weights for every weight in the matrix
133:52 - of connections every row and every
133:55 - column and what I want that to be I got
133:58 - to be a little bit lower here this is
134:01 - based on what I kind of worked out here
134:02 - well I have a learning rate multiplied
134:06 - by the errors and the error vector right
134:09 - the error vector is a single column
134:11 - vector so I'm looking at all the
134:12 - different rooms
134:15 - time's the actual output oh uh sorry
134:25 - hold on oh if I come back to here this
134:27 - is basically sorry so let's just looking
134:32 - at this right now let's go back to my
134:34 - previous video that's basically the same
134:37 - thing as I've got right here I've got
134:38 - the change in weight is the error there
134:41 - times the learning rate but there's an X
134:44 - in there so the X what's the X over here
134:48 - well the X is actually that output so
134:51 - I'm gonna have to do the matrix product
134:52 - which I my symbol for that is a trade
134:55 - but I'm gonna use uses a dot right now
134:56 - with the actual output itself but the
135:00 - thing that's missing in this particular
135:03 - scenario is I didn't have this sigmoid
135:05 - function that's how I need to look which
135:07 - way do I go which way do I adjust based
135:09 - on those activations are coming from the
135:10 - sigmoid function so I need the
135:12 - derivative of the sigmoid function here
135:14 - so I also need to say sigmoid of the
135:19 - output that output is a single column
135:23 - vector times 1 minus Sigma of the output
135:34 - which is that one column vector and now
135:38 - I take the matrix product with the
135:41 - actual output itself now here's the
135:43 - thing this is something we've actually
135:45 - calculated already in my code I've died
135:48 - calculated the outputs I did that while
135:52 - I was feeding forward through the
135:53 - network so this is actually something
135:55 - that's already calculated and I just
135:57 - need to put it in here so this now if I
135:59 - can just take this Delta weights then I
136:02 - could say for every weight I J adjust it
136:07 - by its corresponding Delta weight I J so
136:12 - this is now the formula that I need to
136:15 - implement in my code I need to implement
136:19 - this in my code to calculate all of the
136:23 - Delta weights how do I change all of
136:25 - those weights for
136:26 - every single weight whether it's and
136:30 - this error and this output this could be
136:32 - the final output and the final error or
136:35 - this could be the hidden output and the
136:37 - hidden error same formulas gonna work
136:39 - and now of course I'll do the bias as
136:41 - well but here now I think I could put
136:44 - this into my code yes oh okay
136:55 - so and in fact I'm doing something
136:58 - really weird here so this is really I
137:00 - could simplify this even further right
137:03 - because the activation function is
137:05 - inherent it's already been calculated so
137:07 - I really should say just the output
137:10 - vector times 1 minus that output vector
137:19 - and this is going to have to be
137:22 - transposed so that the because of the
137:26 - way the matrix products worked out but
137:28 - let's let's get to that when I get into
137:29 - the code okay I should go back I think I
137:35 - should go I'm gonna maybe I should try
137:36 - explaining this again why is it trance
137:40 - [Music]
137:42 - this is definitely gonna require a
137:44 - second shot let me go into the code and
137:47 - if I have to redo this next week I will
137:51 - so I have to like watch this back and
137:54 - then figure out where my explanations
137:55 - went awry
137:56 - I can already take could already hear my
137:58 - ta telling me the last oh should be H I
138:02 - think oh yes yes thank you ah that's it
138:09 - that's what I've got wrong let me maybe
138:11 - this is salvageable yeah this is H thank
138:20 - you that's definitely H
138:27 - yeah why does it say Oh in the Oh Joe in
138:32 - the in the book here that's interesting
138:35 - okay let me try this whole explanation
138:42 - again I don't remember where it comes
138:56 - from differentiating the inner function
138:58 - which depends on the previous layer I
138:59 - either hidden okay
139:04 - some Oh squirty chair okay do I dare let
139:14 - me start over
139:15 - like almost to the last bit you just put
139:18 - this in the code and then whoa whoa oh
139:21 - I'm in the wrong I thought I was in the
139:22 - whiteboard because this screen has a
139:25 - like a shot of a whiteboard on it and
139:27 - we're almost done like I'm almost in the
139:29 - last bit where I could just put the code
139:31 - in oh it's 1:30 I have to go shoot let
139:37 - me let me see let me I'll come back I
139:39 - don't have time through this conference
139:41 - oh let's see if I can let me give me
139:46 - like let's just let me try this
139:47 - explanation one more time this is
139:49 - definitely have to come back and and
139:50 - continue this another time like next
139:52 - week but I'm so close to the end because
139:58 - if I once I just write that get that
140:00 - formula written correctly out there I
140:01 - could just put it in the code because
140:05 - basically yeah skip the conference time
140:14 - for it yeah I know I need to do a coding
140:16 - challenge I can't I'm not gonna have a
140:17 - coding challenge today because of back
140:19 - propagation
140:20 - back propagation
140:30 - just thinking for a second where I spend
140:33 - so much time like the momentum is
140:36 - totally died okay I'm gonna I'm gonna go
140:47 - back to where I referenced that previous
140:49 - video no that was fine
140:54 - so I reference that previous video I
140:56 - came over here
140:58 - bla bla bla bla bla and then I went back
141:05 - there okay alright so what I want to do
141:11 - now so where am I
141:14 - I had this formula I'm realized it's
141:17 - similar to my previous video oh no I'm
141:20 - just repeating myself but which is over
141:22 - here now what I need to do is take that
141:25 - formula that I've gone through and and
141:27 - let me actually just take it and write
141:30 - it directly from this book basically
141:33 - it's you know what I'm gonna do I'm
141:36 - gonna let's just implement the code
141:43 - shoot give me one more chance here
142:02 - so I'm reading the jack I believe there
142:04 - are people here still watching so after
142:07 - I reference this video so let's try to
142:12 - take this now and turn this into a
142:15 - matrix math basically remember what I
142:18 - want to do is I need to know the changer
142:20 - weight for every weight that's in our
142:23 - weight matrix and so this is exactly the
142:27 - same formula that I did in the gradient
142:29 - descent video it's basically what's down
142:30 - here but let's kind of rewrite it with
142:32 - some notation and thank you again to
142:34 - this book that I'm kind of also pulling
142:36 - from so if I want to know the change in
142:39 - weight for every weight in our row
142:43 - column matrix row I column J this equals
142:49 - referring back over here ok well first
142:51 - we need the learning rate times the
142:54 - error right the error now the error is a
142:58 - single column vector right so it's just
143:02 - rows now this could be the error of
143:06 - again these are the change in weights
143:08 - for any weight matrix anywhere in the
143:09 - network so this error could be the final
143:11 - output error or could be the hidden
143:13 - error and then I need to use the sigmoid
143:19 - function so the derivative of the
143:22 - sigmoid function that's the piece that's
143:23 - missing over here right oh shoot
143:32 - oh sorry and then I need to multiply the
143:40 - actual output itself from the previous
143:44 - what the previous layer whatever those I
143:47 - need to multiply by the inputs itself
143:49 - that's why that's why X is over here so
143:51 - let's just let's make the case what I'm
143:53 - doing I'm doing the weight adjustments
143:55 - for the final layer we're gonna so I
143:58 - need to multiply this by that and
144:03 - actually that's going to be the matrix
144:04 - product which I don't have a symbol for
144:06 - a good thing you can't see is I'm trying
144:08 - right or I need to buy those hidden
144:10 - outputs so I need to what's coming out
144:12 - of the hidden layer that's X so I'm
144:15 - gonna put that here and so now but I
144:19 - need to also figure out what direction
144:21 - to go I need the derivative I didn't
144:23 - have the sigmoid function in this video
144:26 - so it's not there in the formula but now
144:28 - what I need is this and guess what I
144:32 - already have this this is just the
144:35 - output itself right
144:37 - the so the derivative of this that's the
144:42 - output of the final output is the the
144:46 - sigmoid of the weighted sum of
144:48 - everything and so now the derivative is
144:51 - is o index I that single column vector
144:56 - output x times 1 minus o so this is
145:03 - actually the entire formula I need it's
145:05 - terrible terrible terrible me who has
145:09 - written this too high on the whiteboard
145:11 - I am hereby fired from ever making a
145:15 - youtube video again I'm gonna I'm gonna
145:17 - rewrite it a little bit lower because
145:19 - we've really got this now we've really
145:21 - got this Delta weight formula I need to
145:24 - this is just I should just show you this
145:27 - right I need to take the learning rate
145:31 - multiplied by the errors which I said
145:36 - was J but oh sorry I multiplied by the
145:41 - output multiplied by one
145:46 - - the output and this whole thing now
145:51 - gets the matrix product of what was the
145:54 - X in this situation if this is the final
145:57 - output of the network then this becomes
146:01 - the hidden and this is the matrix
146:04 - product this is so this is the Delta
146:09 - wait look at this
146:10 - I love this now this is the Delta wait
146:13 - for hidden to output the Delta wait for
146:19 - input to hidden then equals the learning
146:24 - rate times the hidden errors times the
146:30 - hidden output times 1 minus they hit an
146:36 - output and that whole dot product with
146:41 - the input itself determine to call X so
146:47 - this is how this is how I adjust the
146:51 - weights this is why I need all my
146:53 - goodness this is why I spent all those
146:54 - other videos on just figure out what
146:56 - this hidden error is this is really the
146:58 - magic of back propagation right here so
147:00 - these are the formulas you know are
147:02 - these formulas didn't even there they
147:04 - gonna make sense to you especially with
147:05 - this massive video that I've put
147:07 - together probably not but we kind of
147:10 - have a ride to them but the thing but I
147:13 - haven't do these formulas make total
147:19 - complete sense to you well hopefully
147:21 - again the idea is to like look at this
147:23 - and kind of get an intuitive
147:25 - understanding right we know we need to
147:27 - adjust the weights based on the error
147:33 - the hidden error is kind of like a way
147:36 - of getting the portion of that that
147:38 - final error we know we need to adjust
147:40 - them in the direction of looking at the
147:50 - atom as well do these formulas really
147:56 - make any sense to you probably not a
147:59 - if you want to kind of unpack them
148:01 - further I think you could pause now and
148:02 - go back and kind of look it just watch
148:05 - my mathematical grading descent video
148:06 - look at the three blue one Brown videos
148:08 - but the pieces of this should really
148:10 - make sense right we know we want to
148:11 - change all the weights one of adjust
148:13 - those dials to get a better output that
148:14 - output is based on the error that error
148:17 - is back propagated to a hidden error if
148:19 - we're going back weights in the matrix
148:21 - the derivative of the sigmoid function
148:23 - is important because it tells us the
148:24 - direction of which style things are
148:26 - being activated and then of course we
148:29 - need the what's try to come before
148:31 - hidden comes before the final output and
148:35 - the input comes before the hidden this
148:39 - is exactly this is exactly what I've
148:41 - done in in the previous videos of what I
148:45 - had when we were looking at y equals MX
148:49 - plus B so we can connect that back to
148:52 - the gradient descent video okay so I
148:54 - think it's time I'm going to stop right
148:55 - now and I'm going to from here implement
149:01 - this in code alright so probably what I
149:08 - need to do that probably what I need to
149:11 - do is redo the explanation because I
149:15 - think I've really botched that but I
149:17 - think if I I think I'm gonna I think
149:21 - this is right enough that I can put this
149:23 - in the code and maybe we've actually got
149:25 - our neural network code I don't know
149:31 - what bag provocation means so I'm gonna
149:36 - I'm gonna have to return to this I've
149:37 - run out of time today but let's see if I
149:39 - can at least put this in my code I'm not
149:41 - even gonna do this as a video tutorial
149:42 - I'm gonna just come back to this last
149:44 - piece if so I'm gonna I think I need to
149:48 - do redo the videos I've realized it
149:50 - today redo the videos where I explain
149:53 - these won't be the final published ones
149:55 - the Delta weights how that's calculated
149:59 - but let's just see if this kind of
150:01 - matches up really quickly so if I'm
150:04 - doing this one here so here's the thing
150:07 - what I let me save this as
150:11 - old and then so I know where I last left
150:19 - it so a couple things I need I need D I
150:24 - need a function called D sigmoid and
150:26 - that is if I have let y equal sigmoid of
150:38 - X now I return Y times 1 minus y so this
150:43 - I'm going to need is the the derivative
150:46 - of sigmoid function I think this is
150:51 - going to like mess the problem is I want
150:54 - to use this stuff so I think instead of
150:57 - doing feed-forward what I can actually
151:02 - do here is do this and then I was
151:13 - calling these outputs yeah this is the
151:18 - autocomplete that I guess I don't want
151:21 - and then I don't need this anymore so
151:24 - targets though I do need so what I just
151:27 - did is put the whole feet forward in
151:29 - here thing because I need to and then I
151:31 - need to calculate Delta
151:34 - the idea is to calculate Delta weights
151:37 - and so how do I do that what are the
151:42 - pieces that I need the pieces I need is
151:46 - are the air the air this this is the so
151:48 - I have this and now I need to multiply
151:54 - it by this and dark these that's an
152:01 - element-wise multiplication right
152:08 - so I actually want to say right this is
152:15 - map through sigmoid already right there
152:17 - so I want to say like I'll just call it
152:20 - D outputs for right now equals so I need
152:26 - to make a copy of it so I need to like
152:33 - did I have a mistake here yeah no that's
152:38 - fine I'm gonna come back I'm gonna come
152:45 - back to this next week I just I kind of
152:47 - wanted to put the code in here so the
152:49 - the derivative of the outputs if I can
152:57 - take suffice ik function called 1 minus
153:10 - yeah actually the outputs already have
153:14 - so the thing is I need a fake
153:17 - I need a like already sigmoid function
153:23 - which just does this right because it
153:31 - it's D sigmoid if I'm looking at the
153:35 - stuff that's already been sigmoid let's
153:37 - just put this up here again this is
153:39 - gonna be a mess I just want to put the
153:40 - code in here and then I'm gonna have a
153:42 - like some time to think about it I'll
153:43 - come back and do another live stream
153:45 - where I finish this oops so so I want to
153:51 - say matrix dot map the outputs right I
153:58 - want to because they've already been
154:02 - sigmoid map already sigmoid outputs
154:07 - already sigmoid so I'm gonna need a map
154:14 - function that's static to make a new
154:18 - vector
154:19 - where's map static map matrix and let
154:29 - result equal a new matrix matrix dot
154:34 - rows matrix Kyle's sorry I'm gonna do a
154:38 - lot of stuff without really explaining
154:39 - it right now fully just to try to
154:51 - function value and then return results
154:55 - so now I have this so I have this now I
155:06 - need this right and I have that but
155:10 - those get multiplied by each other just
155:12 - element wise so can I say D outputs
155:18 - that's the hata mard Hadamard how do you
155:24 - say that again Hadamard Hadamard product
155:28 - I'm gonna write a Hadamard function with
155:31 - the output errors right so this is the
155:38 - hata mart and so if i put that into the
155:40 - library tonight actually oh there is a
155:43 - mask a l'heure product that's the
155:46 - Hadamard so that should be called
155:47 - probably Hadamard but whatever it's
155:50 - called multiplied it's already in the
155:51 - library just cuz I have to go I'm kind
155:54 - of like race through this I can multiply
155:58 - by the learning rate there also should
156:04 - be how come this my library doesn't
156:07 - check to see if it's a just a single
156:11 - number
156:11 - oh no that's only the scalar product
156:14 - that multiply function is only doing the
156:16 - scalar product got it got it got it oh I
156:18 - had it previously doing that so I can
156:19 - adjust this adjust this to do Hadamard I
156:23 - need to do that I see I see so if n is
156:26 - an instance of a matrix now I should do
156:30 - the Hadamard product otherwise just do
156:33 - the scalar product which is different
156:37 - than the matrix product which also is
156:39 - called multiplies and the naming is
156:40 - terrible here but
156:50 - so that would be this so now okay so
156:53 - that works multiplied by the learning
156:56 - rate so I'm just gonna say I'm just
156:58 - gonna have that be a constant here right
156:59 - now I'm not a constant cuz let let
157:03 - learning rate equal 0.1 or something
157:04 - okay so this well this went off so I did
157:13 - this component this component at this
157:15 - point now I need to do the matrix
157:16 - product with but that has to be
157:19 - transposed so now what I need to do is
157:23 - say let this actually transpose no no
157:31 - where am i I'm here hmm
157:35 - yeah target on outputs now I need to
157:38 - just take this here I need to take these
157:42 - but hidden T equal matrix dot transpose
157:49 - hidden hidden T right right
158:04 - this is the matrix transposed and then a
158:08 - delta so this is really what I should
158:16 - really say is like sigmoid no ya d
158:21 - outputs yeah sorry sorry so now let
158:26 - Delta weights equal matrix dot multiply
158:31 - D outputs hidden transpose Delta weights
158:41 - dot print
158:47 - then
158:53 - all right Simon is asking why practice
158:57 - in a livestream I don't have a good
158:58 - reason just because I'm already
158:59 - live-streaming I need to practice and
159:01 - maybe this will help okay
159:05 - sorry for everybody sort of joining in
159:07 - the middle I've run out of time and I
159:09 - have to go so it's just trying to like
159:10 - see if I was in the right path I need to
159:14 - double back and redo the video
159:18 - explanation I need to watch my gradient
159:20 - descent video to connect it better and
159:22 - then kind of just present these formulas
159:26 - I think that makes more sense I can make
159:28 - this much shorter I just wanted to see
159:30 - if this would actually work in the code
159:32 - I don't have time to test it out and
159:36 - then so now we would say this dot
159:42 - weights which is hidden to output hidden
159:53 - to output dot add add Delta weights
160:06 - so this is change hidden to output
160:13 - weights gradient descent okay now
160:23 - alright so I think I'm going to finish
160:25 - up here I just want it so what I'm gonna
160:27 - do better than no practice exactly so I
160:33 - don't know if I've gotten this right I
160:35 - need to stop freeze frame here for
160:38 - myself sorry that this live stream is
160:41 - kind of ending abruptly and I didn't
160:42 - even do a coding challenge these are the
160:49 - formulas that I've arrived at for how to
160:51 - change the weights between hittin and
160:54 - output and how to change the weights
160:55 - between input and hit it I need to come
160:58 - back and redo my explanation I think
161:01 - everything looks fine all the way up
161:04 - until it's all the way up until the sort
161:09 - of I got to the point where I started
161:10 - started with this line right here so
161:14 - that is where I'm gonna return the next
161:16 - time I come back hopefully sometime next
161:18 - early next week to continue this live
161:20 - stream that's not finished I'm gonna
161:22 - start from here refer I'm gonna try to
161:25 - derive not derive derive is the wrong
161:28 - I'm definitely not going to derive I'm
161:29 - gonna start from here present these
161:32 - formulas kind of understand them as they
161:34 - relate to my previous gradient descent
161:36 - videos then I am going to then I am
161:42 - going to put that into the code here to
161:46 - see if I got it right so but I what I
161:55 - need to do I need to finish this code
161:57 - and then test it so I'm just gonna I'm
161:59 - gonna freeze frame this I will publish
162:00 - this somewhere let's publish this
162:02 - somewhere right now when the archive for
162:08 - this live stream gets posted which is
162:11 - typically sometime this weekend I will
162:16 - include a link to just like a Google
162:18 - Drive folder with
162:20 - code in progress so if anybody wants to
162:23 - like try to keep working on it or like
162:25 - check what I've got so far and refer to
162:28 - it you'll be able to I'm not going to
162:30 - put this code up in github exactly I
162:34 - mean I will eventually it aesthetically
162:37 - this is the same as what I already have
162:40 - in my this should be basically I'm
162:45 - redoing I think there are mistakes in
162:48 - here this exact function neural network
162:51 - prototype train this is a version that I
162:53 - made last spring and you can kind of see
162:55 - there's some similar stuff but I've kind
162:58 - of started over from scratch so this is
163:02 - the end for today it's 150 I I
163:04 - definitely have to be out of here I'm
163:08 - sorry that I didn't get as far as I
163:12 - wanted to the next steps are I knew this
163:16 - would be hard the next steps are redo my
163:22 - gradient descent video I'm just maybe
163:26 - I'll put this into some notes up here
163:33 - next to do redo gradient descent video
163:41 - about Delta wait formulas connect to
163:49 - mathematics of gradient descent the
163:53 - video ok then implement gradient descent
163:58 - in library somebody to talk about
164:05 - different activation functions then do X
164:11 - or coding challenge and then M NIST
164:14 - coding challenge alright so this is my
164:18 - this is my what I got to do next
164:20 - thank you everybody for being here today
164:25 - while I'm this is really it's I don't
164:27 - know I was gonna say this is kind of
164:28 - different than what I usually do but to
164:29 - be honest it's
164:30 - what I always do which is I'm trying to
164:32 - learn something yeah it's different in
164:34 - the sense that I do sometimes do videos
164:37 - about stuff that I I guess I know a
164:39 - little better or that a little simpler
164:40 - that I've been working with for 10 years
164:42 - or something this is like I don't really
164:45 - know what I'm doing so really trying to
164:47 - learn and get a sense of the mathematics
164:50 - of building a simple neural network and
164:52 - how to implement that with my own code
164:55 - so that I can later work from a higher
164:57 - level and feel comfortable with this or
164:59 - language of all this stuff so this is
165:02 - all I had time for today I would say
165:05 - that I'd come back and finish this later
165:06 - this afternoon but that's I know for a
165:08 - fact that it's not going to happen
165:09 - because I so that uh if anyone wants to
165:13 - join the patreon to discuss this board
165:15 - the slack group you're welcome to maybe
165:17 - you don't want to after watching this
165:19 - live stream but I'm gonna be busy for
165:22 - the rest of today and tomorrow with this
165:24 - processing foundation and SF PC learn to
165:29 - teach where I'm going to talk about how
165:31 - much of a disaster my video stuff is
165:33 - okay thanks sorry I don't have time to
165:37 - really answer questions and there will
165:41 - be some new videos continuing this stuff
165:44 - that get published and the coding
165:45 - challenges will come as well any last
165:51 - words
165:51 - I got a few donations from the chat
165:53 - that's very nice let me see if I can
165:55 - where do I check that here no this is
165:58 - where I send a super Joe can I see it in
166:00 - my live dashboard so I can thank the
166:03 - people by the way it's I look it's
166:07 - wonderful that people donate through the
166:09 - YouTube super chat my preference is the
166:13 - as a way of supporting the channel is
166:15 - through patreon but obviously not
166:18 - everyone can do that so that's wonderful
166:20 - - where do where can I find them I don't
166:28 - it doesn't there's nowhere where I can
166:31 - see to find I know I can see them later
166:33 - once I finish and there's a place in my
166:36 - creator studio we can go back and see
166:38 - them but I'd love to be able to thank
166:39 - the people right now
166:43 - [Music]
166:49 - if anybody that's by the way those are
166:55 - you saying nice things to me in the chat
166:57 - but really appreciate it it actually
167:01 - like it does mean a lot it does help me
167:04 - I mean you shouldn't say if you don't
167:05 - mean it but if you did actually learn
167:07 - something if you think this isn't so
167:08 - terrible you say something nice about it
167:09 - that does help and I absolutely welcome
167:11 - I've gotten lots of wonderful
167:13 - constructive criticism on ways I can
167:15 - make the videos better and that kind of
167:17 - thing too so Socrates says I donated $5
167:21 - Thank You Socrates and yeah okay so this
167:29 - is part one of this like this is part
167:32 - one of this week's episode it will be
167:34 - continued next week hopefully before
167:36 - next Friday but it might just be that
167:38 - next Friday I do the continuation of
167:40 - this we shall see I actually have a
167:42 - makeup class I have to teach next Friday
167:44 - the semester is starting so I'll figure
167:48 - it out I am I'm in it to win it I did it
167:51 - to lose it you have my random number
167:57 - video and blah blah blah all right thank
168:03 - you everybody for being with me today
168:06 - send me your feedback in the comments
168:09 - when this live stream gets archived at
168:11 - Schiffman at twitter the live stream
168:12 - will be archived within an hour I just
168:14 - leave it unlisted till effect till I'm
168:16 - Matt to help stance has a chance to like
168:17 - put in the description of the links to
168:18 - all the things so yeah I made it to
168:21 - learn it so I will come back later I'll
168:26 - be back next time definitely next Friday
168:28 - if not and next Friday will be a late
168:31 - one because I have a makeup class that
168:33 - I'm teaching here at NYU that goes until
168:36 - three o'clock in the afternoon about so
168:38 - I'm probably gonna be doing like a 4
168:39 - p.m. or 5 p.m. Eastern Time live stream
168:42 - which is very late I know for all of you
168:44 - international depending on where you are
168:45 - obviously but international viewers all
168:47 - right thank you Socrates Etna Bruna Gino
168:50 - Zak and Kay weak bond
168:54 - I like to say your name with emphasis
168:57 - thank in it to win it hash tagging it to
169:01 - win it
169:03 - wilderness Dan this will be longer well
169:06 - the nernst and will be back alright
169:09 - alright it's so hard to know like right
169:13 - that the thing is like could I just try
169:15 - just write this formula up on the board
169:17 - and implemented the code and be like go
169:19 - watch those other videos that explain it
169:20 - more Shai explained it I'm kind of like
169:22 - being in the middle here which i think
169:24 - is a bad place to be but the best place
169:27 - to be I think is like I'm really gonna
169:28 - explain everything and it's gonna make
169:30 - sense or I'm gonna just present to you
169:33 - the these formulas that I'm going to
169:34 - implement and here are resources for you
169:36 - to dive deeper into those formulas kind
169:38 - of in the middle so I I'll come back and
169:40 - sort of sort that out all right this
169:45 - live stream is ending in three two one
169:52 - goodbye and good luck have a wonderful
169:54 - weekend give a friend a hug all that
169:57 - sort of stuff goodbye

Cleaned transcript:

good morning it's Friday time for a usual coding Train livestream I seem a little bit subdued it's because I'm well I'm a little bit worried about today I today is the day that I'm once again this month of January where are we in January we'll pass the middle is I'm dedicating this month of January to attempting to work through I'm starting looking at the chat to work through building a neural simple neural network library from scratch in JavaScript I spent some time working on some matrix math stuff building a simple feet working through the feedforward algorithm and today is the day to tackle a topic one particular technique for training the neural network for having the neural network learn and that technique is called it's not called quaternion but it is that it is something that does often caused me to run out of the room in fear and it's called back propagation now a couple things I want to mention number one is eventually I will be replacing a lot of the internals of this library that I'm building with a more sophisticated engine that's been highly optimized to do matrix math and also has a lot of libraries a lot of classes and objects and library functions for doing deep learning stuff called deep learning is so that's what that hopefully by the time we get into February that's where my focus will lie in looking at some more sophisticated models of deep learning systems to kind of do fun and creative stuff in the browser but I'm still I I want to use this time together in this live stream to kind of dive into the guts of how these systems work see what happens if we can not worry about optimizing or efficiency and just kind of build the pieces and parts and put them together get something to work so that we can start to understand how the algorithms work have a language a common language to speak to each other about about the pieces of these algorithms so we can ask the right questions later when we start to do projects and other types of things how's this sounding okay so I have done something somewhat shocking which is that I've prepared notes on that word note notes the plural of note meaning more than one note and not just more than one note or pages of notes page 1 page 2 page 3 page 4 page 5 and then even some printouts of some code that I wrote at one point earlier in my life with some mistakes in it that some of the code for the back propagation algorithm oh there's even yet another page on the pad that I didn't rip off so another page of the notes I don't know how much these notes are gonna help me I want to say that once again I should thank and I'm gonna do this multiple times today but some of my a lot of my knowledge has come from this book make your own neural network you can check the coding train Amazon shop for a link to purchase this book and other books that I recommend are used in preparation for a lot of my videos and so this is extraordinarily helpful I also want to mention but no written order book this I'm gonna need to refer to this let's put it over here I also want to mention three blue one brown and I've been watching the in particular let me pull this up here three blue one brown I have been watching the playlist let's go to playlists well there's a couple here but I've been watching this playlist on neural networks and sexually I talked a little about how I'm doing this fundraising and training for a half marathon actually watched well I did my training run at the gym because the weather is terrible here in New York watch videos three and four like a chapter three independent like multiple times a treadmill on or like a little iphone so it was probably not the most optimal way to experiencing this video especially when I felt like I wanted to pause to like contemplate for a second and then like you know bouncing around fortunately no one was harmed in the watching of said videos on a treadmill but these videos are extraordinary they even more so than this what is a neural network a video this gives you a really nice intuitive understanding through diagrams and all sorts of stuff about the backpropagation algorithm in you will also I would highly recommend if you want to dig even deeper into this the well the essence of linear algebra series is perfect for a lot of the matrix math stuff but in particular this essence of calculus playlist and in particular I'm looking for something about the CHEO a chain product rule the chain and product rule which are necessary for the backpropagation so I'm really struggling as to what to do here because there's no way that I am going to cover in depth what's here in three blue one Browns channel there is probably no way that I'm going to cover with the same level of depth what's in to make your own neural network book and by the way I should also mention I was looking at some of this code yesterday it looks extremely helpful but I haven't I haven't had the chance to dive really deep into it but oh yeah this to the this is a free online book by michael nielsen that looks like three blue one Brown used as a reference and if we could go to and so how the backpropagation algorithm works oh boy how about I just do a dramatic reading of this page instead of trying to do it myself so this I actually should I wish I'd spent some more time with this page because I think this would be extremely helpful but I don't think that I'm going to go as indepth as any of this content today we did that work could I actually like feed them through my ear so this is my plan this is my thinking as of right now play a lot of scary music so that I don't appear too serious on talking about back propagation this is my plan I am going to talk through how the algorithm works in generalities as it relates as it relates to this diagram that I went through in somewhat detail about the feedforward algorithm so what does it mean to suddenly have an error here and then correct these weights which are tied to the error and then propagate backwards those errors to adjust these weights and in a many more layer system just keep propagating back and back and back and back so I want to talk about that JIT in generalities then I want to look at how the weights are know how the error is kind of like portioned out and how that can be expressed with matrix math because I'm going to need that from our library and then I'm going to refer back to my videos on gradient descent and ultra justing weights based on gradient descent and the three blue one Brown videos just present the formulas that are sort of at the end of those videos and implement those in code so I don't think I'm going to derive all of the pieces of the back propagation algorithm today or ever so definitely because I would like to I would like like to turn to that like put in some filler videos I do think that other people are better equipped to make that content than I am so I'm gonna try to stick to where hopefully I can be of most help which is kind of figuring out how to implement the formulas in code rather than prove or derive them but I it's important to at least have a discussion of the understanding of those formulas this is my thinking I'm just kind of talking it through and then I can hopefully do some coding challenges because once I have the library built I can say like hey let's train it on this let's try to use let's do a coding challenge use this library on this particular problem so that'll be flightless like difference in some of my coding challenges of the past where there was sort of no code maybe there's a p5 library of course but we're build these quick quick machine learning examples using the library and then if people want to refer back to how the library was built they can do that okay so that's my thinking what what does everybody think about this I feel like I'm a newscaster now with my notes oh that was all the spooky piano I have thought that was a much longer effect okay so how are you fall feeling about this how are we feeling about this Marin Marin how did i do today Marin in the chat says sounds like a solid plan look what could possibly go wrong what could possibly go wrong me trying to explain what what do you I've actually learned which is kind of interesting and I might as well plug this while I'm here is I'm participating in this conference put on by the processing foundation and school for poetic computation what can i maybe called if I do I meant to type in SF PC learning to teach so I'm doing a short presentation at this at this conference this is it no that's from last year last year yeah 2018 so one thing I that I'm discovering is when I make mistakes in code and have to figure out and debug them that seems to be a useful technique for education meaning people watching they're like oh okay I make those mistakes oh I'm seeing how you figure it out well this is helpful to me or this is interesting compelling and I'm learning something whereas when I spend an hour trying to explain some math formula but have the formula wrong the whole time that's not so helpful so it's not like oh yes I'm seeing your process of how you get it wrong for a while and then you realize that later and kind of have to do it so what I'm discovering is a lot of these these sort of mathematical or algorithmic discussions especially if they have like complex notation require something called I believe it's called preparation okay I have here in the form of notice so I'm a trying trying that but most likely I have to admit I wouldn't be surprised if I just end up redoing trying to do I need a couple tries with this so I probably should do those tries not livestreaming in front of a live audience on YouTube but I don't when else am I going to do it right now so I thank you for being guinea pigs and and this is actually what happened most recently if I go to my channel and go to yeah this particular video this particular tutorial was made on Tuesday of this week though if you look at the live stream archives there's a live stream from the last Friday oh we could go today when I went through this exact algorithm but I made so many mistakes it wasn't worth trying to edit that together in some way that made sense it just made more sense to just do it again so with this gradient descent stuff I'm just kind of warfare warning in advance I'm kind of in my mind thinking of today as let's just give this a try and see how it goes now the other thing I should mention is Simon okay I will Simon is telling me that if I watch his livestream I'll learn better how to pronounce mitad okay now the other thing I should mention is I was going through all this and looking at it looking at it and I kind of have the formulas and has some code I think for all of the weight adjustments that I need to do but of course I kind of forgot about the biased part so look at that yet so let's see if we could do put the back propagation formula stuff in for adjusting the weights and then if any of you who know what you're doing more than me which is probably a lot of you maybe you can help me figure out how to put the bias stuff in okay and in regards to the camera that goes to black I will just mention to the people are talking about in the chat I'm gonna fix that soon you know sometime in the next year whose I the what the solution to that is to put this firmware on the cameras called magic lantern so I just have to basically like you know it's kind of like should I start live streaming on time and have two hours or should I try to fix this camera thing for an hour only have an hour left and school the way it works out so I'm not optimizing for fixing it but I will I will okay now as you know in the coding train especially when attempting things like back propagation it's always good to stay hydrated hello Brazil it's amazing people in Brazil are watching alright now let me let me compile some resources here it's already getting warm in this room okay so I want this playlist available to me and I want I guess I should verify yeah I guess I should get one of these check marks on my channel huh I want I also want this playlist available to me so this is I want to reference neural networks essence of calculus neural networks and deep learning free book and the coding trade Amazon shop which has this book here that now my own videos the ones that I think this relates to our this particular video Hey look somehow my subtitles are a closed captioning on and they are on in German it looks like who wrote these I put discovered recently that I can so as it says who caption the videos here okay here could be another video from Suraj has a lot of videos about this topic as well that are terrific then I also want the perceptron which is basically you know talked about a bunch of stuff multiple time either pity hood because this is this kind of like a video about the feedforward algorithm that's interesting fifteen minutes I forgot about this what's in these videos I think I made like duplicate content thought let's just kind of scan through this real quick oh I think this is me kind of like sketching out what I need own I stopped at the matrix where I need matrix math oh yeah so I did do this kind of twice there's the feedforward algorithm in a previous video that's so interesting so okay that's fine so what I'm looking for though is this one and maybe somewhere in here the training function is where I want to kind of pause for a second because it's basically the same code where do I have a nice if someone could find me a time code in this video where I have a lifestyle of the Train function I guess I can actually just open up the code on my computer might be more efficient sorry everybody I'm just like trying to get organized here perceptron sketchbook sir anybody see a perceptron probably coding challenge well maybe I should go to I should really talk about the new website stuff too but it's too many things coding challenges perceptron huh didn't I do it as a coding challenge why is it not numbered that's a mistake hold on what video is that oh that's not a coding challenge it's actually like a tutorial somehow it's here though maybe let's see no oh this is different yeah this this is kind of relevant here alright so that I can reference if I need to okay part one I wonder why why what's that someone give me the a too long didn't read answer why why it's two million views why why all right oh I really wish that the climate control system in this room could be fixed really make things better for me 1130 okay I got this we got this got this this this okay there's also like mathematics of gradient descent by the way just if I go click on videos that's nuts something is wrong with the universe is if my video shows up first that is a really a problem because I expect a lot of these other ones are much better but it prize to do with the fact that I've titled it that way math so if I just search for gradient descent mathematics yeah okay how about gradient descent algorithm just gradient descent there we go okay good good that's good oh I'm down there that's fine I'm happy with that look at these other ones first okay so this is actually this is actually the video that's most relevant all right okay Google knows it's me and so they're putting my videos up there cuz I watch them a lot that's the problem so but I'm not am I signed into Google hold on they probably even know they just know even if it's an incognito window I don't know I don't know Google I don't know Google Google just knows still does incognito window is like they just know right all right the Delta rule oh you should do a video on the Delta rule learning algorithm for single layer neural networks I think I did that already right though isn't that kind of like what my perceptron video is basically the Delta rule ah Tushar in the chat saying I have a question on your own the last video okay let me take a couple questions I'll do anything you think your procrastinate from trying to talk about gradient descent in a video so I'll take your question you guys still here like a weird okay hold on um do you guys hear our weird like echo like the timing of when you hear the sound that's being piped through the system versus the sound or my microphone is off like if I do this oh sure I wasn't looking at the chat I'm sure that question was asked I missed it which is a reason for me and maybe plug the patron for a second I do have a slack channel that I it'll be easier to keep an eye on and wear a nice friendly community of folks discuss different things that I'm doing the videos and provide some financial support for the channel so if you go to patreon.com/scishow de trein you'll get an invitation to the slack channel okay all right there is no echo all right all right all right all right let's we got is we've got to get this started with let's let's get rid of some let's let's let's catch up to where we are let's desktop let me get this going here look coops localhost there we go this is where we last left off with a feedforward neural network that will give me a random output now we want to put the back propagation algorithm into it if I open up atom I should have my code let me open up this will be another useful reference that I should bring in here and I think now I've got everything I want to talk about did I make the joke last time oh I think I did it wasn't it wasn't very good it wasn't very good though sorry I you know one thing that I shouldn't that I've also learned to do is sometimes I'm responding the stuff in the chat but not saying what it is which is very confusing if you're watching and you're not even seeing this chat so I'll try to be better about that Taryn asks why don't you prefer visual studio code maybe I do I'm just not as used to using it I keep meaning to set it up but I kind of have all my settings and everything and Adam vaguely the way I like them although I really should turn off all the autocomplete stuff they might know how to do that alright I think I've got to get going and I think we've got to start this like it's got a notification the coding train is live maybe I should go watch that that's interesting let's read some random numbers this will this will get me a little warmed up here Oh getting too old for this YouTube thing forty six thousand seven hundred and thirty seven eighty eight thousand and twenty seven thirty five thousand four hundred and ninety seven twenty two thousand eight hundred and sixty three sixty five thousand and eleven twelve thousand two hundred twentyeight sixty six thousand four hundred and thirty eleven thousand eight hundred and twentyfive 835 eighty one thousand eight hundred and thirty eight ninety four thousand four hundred and thirty one sixty three thousand nine hundred ninety five ninety one thousand one hundred and seventy one eighteen thousand two hundred and seventy three fortyseven thousand seven hundred and thirty seven hundred eighty four thirty nine thousand two hundred and eight thirty four thousand two hundred sixtyeight eighteen thousand ninety three seventeen thousand okay you know one thing that I should probably do really quickly in case in case you need to make a website where I read any possible random number will give you some footage for that real quick I'm kind of working on this on my own anyway but zero which I will be looking at the camera price you look at the camera zero one two three four five six seven eight nine 10 11 12 13 14 15 16 17 18 19 20 30 40 50 see 70 80 90 100 1 200 300 400 500 600 700 800 900 thousand yes 1,000 2,000 3,000 4,000 5,000 6,000 7,000 8,000 9,000 10,000 I think you could put together though the one two three four five six seven eight nine and the word thousand thousand and you could put together twenty thousand yeah one hundred thousand so I think I think you have enough right because that you can you've got one hundred one hundred thousand nobody one hundred ten thousand one hundred and ten no you need the and one hundred and ten thousand one hundred and no but these don't go up that high they only go up tonight that ninety nine ninety nine so you can get even ninety nine thousand one hundred thirtyfive yeah you're fine I think you have enough right you could probably make a bunch of video clips have a random where you picked and have me speak that random number right the and part means you want you want million all right fine million billion trillion all right whatever I missed I'll get you later am I being really quiet I didn't want the volume to be peaking too much okay glad that we worked it out oh oh I need my notes for sure like seriously I'll do anything I'll probably like agree to do you know that double pendulum simulation or something now he'll agree to do just about anything to avoid this backpropagation algorithm but we're gonna do it now all right all right Adam I need to command the settings and click packages search search for autocomplete um I guess I could so what do I want sometimes I like the autocomplete though which of these do I want to turn off I don't know now I'm afraid to disable it what if I've been using it all along and I didn't know that I've been using it well we'll leave then now we know where to look for this all right can anybody come up with any reason right now why shouldn't get started on the backpropagation thing okay oh yeah shoot nobody has a point what do you need two points for though is there's gonna be continuity errors point maybe you just need the audio though maybe you're just gonna use the audio which is fine all right I'll come back later at the end I will take your reading random numbers requests this stippling challenge re didn't I oh yeah the stippling challenge right all right all right here we go just need them house okay this cameras off I should yeah I know so many I'll do the reading I'll do that a sec I'll do some reading all the possible different numbers thing later I'll configure it in a way that's better just hold on let's let's let's let's actually do some let's try to get through this so my goal right now is let's see if I can get the backpropagation algorithm into the library by 1230 which is like 45 minutes or so and that gives me a half an hour slash an hour to try to at least solve maybe X or as a coding challenge that's my goal for yeah I need to like slow down here I'm gonna get to all this stuff eventually I'm just doing my best I got lots of videos you go back and watch those my hair is really really a mess today I wore a nice shirt for all of you I thought that might make the Grady the whole like backpropagation gradient descent thing a little bit less weird like it look like I know what I'm doing alright my glasses are very dirty really like I just I'm gonna start mmm eat me me me me me me me me me me me me me me me me me me okay here we are this video.i what I'm going to do with this video is I'm quite terrified at this moment I should say but I'm gonna start to talk about what it means to examine the output we fed in some input we did the feedforward algorithm we got some output I want to know look at that output and I want to say what do I think about that output really isn't good output is a bad output is it right on it's a little bit off and I'm going to this is a technique known as supervised learning I the teacher I'm now gonna say this output is incorrect please adjust all your settings to make this output more correct and I have done this before I have done this in a video about linear regression and gradient descent I have done this in a video where I made a simple perceptron where I did this same type of learning algorithm in fact I have done this in genetic algorithm examples or I'm not using the same exact technique but a different technique to sort of teach a system to do something so this is what I want to do you can think of all of these weights that are inside the network as these little knobs little settings and I just want to like always adjust the settings and so the key there's gonna be key terms that are gonna come up here like error what's the difference between the output and the known correct answer that's the error the cost well over a large training data set what's the sort of cumulative error that's the sort of cost the current cost of the neural network and then this term that's called back propagation so if the error is the thing that tells me how to tune these weights the error is right here it's gonna tell me how to tune these weights how do I tune these weights they're not connected to the error they're connected to the thing that's connected to the error so I need to propagate backwards feedforward is the process of moving all the data for through the network back propagation is the process of taking the error and basically like feeding backward to the error through the network now here's what we have here's where I have got to admit something this is probably I would say I'm trying to think of a topic that I've tackled in any my videos that's like harder this I can't think of one I don't know that I fully have a deep understanding of this I have implemented it before I spent a lot of time I prepared some notes I prepared some notes for the first time in my life which are right here that I'm going to use while I start to explain this but it's kind of unrealistic and I'm probably not the best person to go into all of the math so what I'm going to attempt to do is give a kind of just general overview of how the algorithm works look at how pieces of it actually work the math of it in particular as it relates to matrix matrices because I'm gonna need that understanding to implement it in code and then I'm gonna present some of the formulas to you that are the formulas for how you change the weights based on the error and then try to implement those formulas in code I'm sorry I put in a this is I I guess just probably be edited out but the permian should just stay in but I put in a order to try to fix the temperature in this room and I'm worried that something like someone's gonna just enter the room Fred fix the thermostat so I thought I heard like keys jangling I got worried so anyway okay so so this is my plan so probably take two or three videos but so my goal is really the implementation and I'm gonna provide to you a bunch of resources if you want to dive deeper into the math let me just mention those to you so number one make your own neural network by Tariq Rasheed this is a book that I was reading on the subway this morning you can actually get it on the kit on your Kindle app for a very inexpensive amount you can also find it here on an a lot books I Remmick recommend on my coding train amazon.com sheet slash shops mask coding training the three blue one brown video series what is a neural network what is back propagation really doing back propagation calculus you could pause right now and go watch these videos which will give you a deeper set of knowledge about the math that's that I'm going to use as well as this the essence of calculus right one of the things that's used in the math is the chain rule and the product rule so this is this this particular video might be useful to you as well as this particular online book which I found through the three blue one Brown videos neural networks and deep learning and I'd be remiss if I didn't also mention Suraj's YouTube channel that I've mentioned a lot on this video has a lot of different a lot of a lot of my videos a lot of different videos on similar content especially if you're interested in Python and using the tensorflow library and that kind of stuff okay so that's that now let me start I think I think we're ready to start timeout for a second alright so that was my brief little introduction and the next thing I'm going to do is I need to do some erasing so let's take a oh please do this no no I definitely no applauses I just want to have a freezeframe of this white board because I'm probably gonna erase it what do I want to keep I think in the end I think I just want to erase this I think I'm gonna erase this and start over okay so if you talk amongst yourselves I'm gonna erase the whiteboard this is just water in case you're wondering helps it erase a little faster this is whiteboard paint so the whiteboard is actually painted on to the wall which is kind of wonderful but there's a bit of a problem number one is it's very Glary so I can't it's very hard for me to figure out how to like this room and properly because if there's a lot of glare on the whiteboard and then also it's very hard to clean it's just so you can see here can you see how dirty it is I nothing would make me happier in life than to have a perfectly pristine white whiteboard that's totally clean this is a start so if I need to refer back to that notation okay let me check our various chats as they're going and alright I don't see anything like any emergency messages we get my notes we rightclick my notes I don't have a place for night he's like a little podium for notes let me just look at them for a second here okay this is where I'm starting okay okay okay God all right I've erased the whiteboard and I am now ready to start talking about the backpropagation algorithm so let's assume right now that this is my output neuron and just for the sake of simplicity at this moment let's just pretend this is one of the hidden neurons but let's just pretend that there's just one of them so there is some wait there's a also a bias but we'll come and come back to the bias at the very end so I'm gonna kind of do everything without the bias and then come back to the bias there's some weight which is connecting this hidden neuron to the output neuron now the input of the output of this neuron multiplied by the way to center here pasture the activation function and we get some sort of answer let's say that the answer that we get is point seven so this that can be referred to as the guess the output but I'm gonna call that I'm gonna call it the guess am i standing on the white board I'm dangerously close I erased might just pause for a second here let me let me put a little mark up here so really output is the very top all right point seven is the guest now in the road to I again one more time wait wait wait I'm seeing crazy stuff in the chat no all right that's fine that's nonsense that if something anything to do with me sometimes I think if the chat is being blasted with emojis it's like get my to get my attention but I'm now I'm only gonna soon that's ever the case if it's the slack Channel point seven is the guess now in the case of supervised learning where I have a prepared data set where I have sort of like known answers so I'm going to train the network to have these weights so that later on I can put in unknown data to get good results I would have some sort of answer and that answer could all that goes wrong today though I'm gonna be really happy I'm gonna have some sort of answer look I really like pushing the boundaries I don't know why I should just lower everything a little bit sorry machi I'm making this like annoying to edit for like no reason I would have some sort of answer known answer so I'm going to write that here and I'm gonna say the known answer is one I wanted this neuron with this particular input that came in I wanted to see the answer of one so this means now I also have an error and the error is calculated with a simple formula what is the desired output the answer the guess what's the difference between those two things so we can see is that the error is 0.3 so in my simple perceptron and in other videos that I've made I've then taken this error and used it as a way to basically nudgenudge that weights I wanted a 1 I only got point 7 I can make the white a little bit higher to get more stuff right I want more and look more stuff right I want that weight higher maybe the bias needs to be addressed but 0.7 I just need to increase that way to increase the weight in the direction of the error that's how this works now here's the next piece of this let's say however that instead of just one weight coming in here I have two weights because there are two hidden neurons H 1 and H 2 now we have a problem we have this error which I know I need to nudge weight 1 and weight but which one's really kind of responsible for the air there's a lot of blame placement going on here normally I would think like let's try not to blame anybody but this is the problem and I could just say like I don't know half of an error increase them both increase they're both the same amount but there's a key aspect of the way that the learning process works with gradient descent and back propagation is we really need to figure out who's responsible for the error so let me take the scenario these weights could actually be weights where this weight is zero point two and this weight is there point one well you could be made set we could now make the argument that this connection is more responsible for the error because it has a higher weight in fact it's twothirds responsible right this weight is double of this weight so in fact when we do these ninjas we take this error to nudge this one but we'll nudge it only by 33% and then we'll take this error and I know this is going to go out of your view but it's coming all the way back here and we'll judge that by an up let's see can you see this here 67% actually I just do it back here 67% so this is a key aspect of the bout and this is I've basically done this before so this is where we look for this Delta weight we adjust the weight based on that error and the inputs passing through okay so now little bit of pause for a second how's this going I'm getting good tips on how to fix my whiteboard issue this camera went off so I might as well fix that while I'm here all right no one's saying anything like is horribly wrong so I'm gonna keep going let me check my notes what was the next thing so step three is okay step three is adding the layer here okay but so so maybe this makes sense to you and if it does good here's the tricky aspect this is why there's something this is why this video is essentially about back propagation this is not the diagram of the neural network that I created in the previous videos in fact this hidden layer is connected to the inputs and all of those have weights and I I spent a lot of time worrying about the indices let's get those indices correct so this if this is input 1 and this is input 2 we usually call this sorry I usually call those X and this by the way the output we can refer to as Y it's usually what's often referred to as Y so if these are this is input 1 X 1 input 2 X 2 this weight is the weight from 1 to 1 this weight is the weight from 1 to 2 now that might look backwards to you it's the weight from 1 to 2 or kids connected between hidden to and in what but the reason why I'm numbering it that way is this is that it's going to be in the second row of our matrix this weight is the second row of our matrix and this is a weight from 2 to 1 so I write 1 2 and this is a weight from 2 to 2 so I write 2 2 so here's the thing if this error which is happening right here is used to tune these two weights proportionally based on their weights well how do I connect this error back to these weights how do they get tuned well it what I need just this is a what was Inc this to realize is this is like a little section of what could possibly be a much larger neural network right there could be many more layers this way and many more layers that way so these weights are just tuned based on whatever this neurons error is right here is at the end so I can actually calculate the error directly but here they're not connected directly to this error that sorry here they're not connected they're connected to these so what is the error coming out of here if I just had that if I had like hidden efore error error hidden 1 what is that equal if I and I knew if I knew what this was right if I knew the error coming out of here error hidden 1 then I could adjust these what because the air coming out of here adjust these weights this error just these weights and this error e to error hidden if I knew what that is then I could adjust the weights coming in to that so this is the idea of back propagation there's an error here it goes to here how many no but if I could calculate these errors then I could continue to go back here and then if there were more I would just calculate these errors and keep going this way so this is the real question how do I calculate that hidden air how do I calculate the error of a neuron anywhere within the network that's not necessarily directly connected to the output where that error is just a simple target out guess and it pause for a second okay I think this is all still making sense I'm gonna look at my notes I am now finished with my first page of notes thank you very much maybe I'm not going to need a second try I'll never need that page of notes again I look forward to the time where I forgotten how all this works again because it's like years later but I have this nice video that explains it I can go back to watch it all right all right okay so the way that I want to do this yeah yeah yeah all right I'm thinking about this sorry I'm Oh II won okay yeah yeah all right all right okay all right I'm gonna to figure this out I'm gonna pull just the section of this diagram out over here I'm gonna need to progressively over time make this diagram more and more complicated but right now I'm going to simplify for a second again so what I want to do is I just want to take actually it's sort of here already but I just want I don't want to pull it over here so this is the output and these are the two hiddens this is the weight of this is the weight of 0.2 and this is the weight of 0.1 and the error here is equal to 0.3 so what I want to and this is the hidden layer what remember what I'm trying to you know h1 and h2 what I'm trying to calculate is the error of hidden one and the error of hidden okay so the way that I do that is by taking a taking this error and getting a portion of it what portion should I get and I sort of said this already twothirds and onethird so if this is weight one and this is weight right there's a very simple scenario this is almost like back to that perceptron again then what I want to do is say the error of hidden one is weight 1 divided by weight 1 plus weight 2 times the error of the output e output the error of hidden one is a portion of that outputs error the error of hidden two is weight 2 divided by weight 1 plus weight 2 times that output error again we sort of said this already a realized I'm cut but this is fine it doesn't hurt to try to do this twice so these would actually be the errors here these are now the errors and I can just use the same gradient descent algorithm to to these weights but this actually is kind of a rare circumstance where you know everything's just connected to one thing so what I want to do now is I'm gonna make this diagram a little bit more complex and I'm gonna take the case of having more than one output neuron see how this works I'm gonna add another so let's say there's two outputs which means there's error one and error two right maybe this is trying to recognize a true we could actually recognize like true or false or something to recognize a zero or a one so in that case maybe the desired answer is you know it's is one and zero but we got we got point seven and point three is the outputs so here this error I made this to let's make this point for this error would be negative zero point four right and now what I need to do is have more connections here and once again this is and I'm this is for this matrix I've got kind of the same notation 1 1 2 1 1 2 2 2 ok now look at this now error 1 is one thing what's the error coming out of hidden error 1 sorry hidden error one is base so pause for second I got it in my head but I'm just gonna take a break shouldn't the air be squared yes yeah when I get to the yes right now I'm not worried about that it was asked in the chat room the air be squared when I calculate this sort of cost the total cost we're gonna square it yes you do square the error but and this case I'm just trying to look at how this gets divided up so if this is our diagram now with two outputs so there are two errors known errors that we can calculate in our supervised learning system we need to figure out still figure out hidden error one hidden air or two well this still stands right this is still a part of the error coming out of here it's the part that it's the part of that it's responsible for it's how much is it responsible for 0.3 and so in that case this should be air output 1 and I'm actually just going to if it's an error on the output I'm just gonna say e 1 so this is and this now is this weight let me let me let this weights portion and and this weights Portland portion right whoa sorry oh shoot try to click undo in my head I was just like undo I'm gonna look at my notes here I want to make sure I'm using being consistent about my 1 1 and 1/2 yeah I am that's what I that's what I've been using 1 1 and 1/2 right yeah okay sorry let me do this again this error coming out of hidden neuron 1 is the same as it was before it's still the portion of the error based on this weight in this way how much of like how much are we contributing to that point 3 air well this this one has a certain percentage in this one has a certain percentage and that is weight 1 1 divided by 1 1 plus 1/2 times that error now here's the thing that's just how much it's contributing to air 1 how much is it contributing to air we've got to calculate that as well and that is the portion of these two and I think you must make sure I have a lot of space so how much of weight 2 1 / weight to 1 plus weight to 2 times air 2 so if their multiplicity total cumulative error of hidden neuron 1 it's that can it's it's its portion of the connection from it to error 1 sum with all the other connections to air 1 and the portion of its connection to air 2 some with all of the connections to air 2 and I'm kind of it's that connected to air it's connected to the output but the output is producing that error so this is it's some right now I could do the same thing for this error it's it's portion that it contributes to this error which is now wait 1 2 right you can see that these are the same right this is its portion weight its that's where it's connected this is its portion that's where it's connected right it's it's its percentage portion of the total weights it's all connected to output 1 and then how is it connected to output wait its portion of the total of air of the error for output so this is now how those errors are computed so again if the mathematics of gradient descent tells us how to take an error to nudge weights then we calculated this error now we can we can take the error is coming out of the output these are our formulas for calculating the errors coming out of the hidden and those errors are the things that could then with gradient descent tell us how to nudge these weights then if we had more layers we could calculate these errors and keep going back that is back propagation that is how the hidden errors are calculated so this is the end of this sort of part one of just sort of like the basic idea of back propagation I'm going to check to see if there any question which I will answer at the beginning of the next video I'm gonna check to see if they're already questions or Corrections which I will mention at the beginning of the next video where I will also implement at least just this much in the code alright alright I'm not seeing any like thing that I got horribly wrong I actually think that might be fine it did prepare but the longer the more we get through today the less preparation I have that's the problem like I was preparing but then I kind of like ran out of time like I gave myself like an hour this poor to prepare only reading what how but back propagation is and I did watch those videos yesterday from 3 Brouillette brown but it's kind of you know I went to sleep everything i'lli'lli'll I this way while I sleep in all the knowledge it just leaks out if you look at the pillow in the morning just like all these like math symbols there it's amazing because just like put that back in so what I want to do oh I forgot I forgot actually there was more to this what I meant to do is like actually I meant to turn this into matrix math but that's fine I'll do that in the next video ok this is going great I never heard such feedback before amazing ok just wait to the power I get like here's a formula I forget it let's just implement it okay all right so all right all right okay I'm gonna keep going all right so in the previous video I went over how to calculate the error in a supervised learning system right the error is just the known answer the system's guess then what I did is talk about well how can I take that error and move it backwards through the system feedback words instead of feeding the data forward to the network feed the error backwards and the way that I can do that is by looking at like JIT each of these contribute to that error let's kind of like make this error like a portion of that error and these were the formulas that I went through right look at this this neuron hidden neuron is connected to this output this is the weight how much is this way two percentage of all the other neurons that are connected it's just two in this case so it's just the sum of these two but there could be many more so what I want to do in the code now is actually add this training function where I send in input data to the network with a known answer and have the training function calculate the error so calculate this basically I want to calculate this vector e1 e2 and I sent vector but this matrix one column matrix that's what I want to calculate that's going to be a really easy part so let's do that first alright so I actually I say in a previous video I sort of set up this function I think right this is this is the feedforward function where I just take the inputs and I feed them forwards and return the output here is a function where I take the inputs and get an answer now ultimately I should just be able to use the feedforward function right I might have to tweak this but on the one hand I should just be able to say I should just be I'll say let output equal this dot feedforward inputs no reason why I should be able to do that and and now what I need to do is calculate the error so the error is Error error equals the target so answer maybe I should call this target targets I'll call it target it might be plural right there these are the target output that I want targets outputs so let's call this outputs again the first example that I'm gonna build is gonna have just one output but I want the system to work for as many outputs as there are so in theory I should be able say let error equal math dot subtract targets comma outputs now this is where we get a little bit into the weeds here if I'm not math matrix I didn't even know if I've implemented this function in my matrix library so that's what I need to go and check but this is where we're kind of like oh my goodness if we're only we're using numpy we just use and it just works or maybe there's some other highly optimized JavaScript matrix math library that will do this for us I'm gonna get to all that yes deep learning is is coming but I'm right now I just want to do this in my code now here there's a little bit of awkwardness here this is an array so this comes out as an array I actually want that to be a matrix so it's a little bit awkward but I'm gonna say outputs equal outputs dot Oh matrix from array outputs so this is me convert convert array to matrix object and then the targets I probably the users gonna send that in as an array so I'm also going to say targets equals matrix from array targets so this is a bit of awkwardness because of the way that I develop my library that probably someday in the future I want to like refactor or rethink but I need to have those things be matrix objects for me it will do this matrix subtraction now let's check the let's check the matrix library I believe there's no subtract function there is an add function which adds to the alright so this is so silly the way that I'm doing this because I probably in a real world if I were really big thoughtful I would make a comprehensive matrix library that has every possibility that I'm just gonna put in there what I need so what I need is a static subtract function and I'm gonna have another I'm gonna call it other for the other matrix oh no no matrix a and matrix B so this should return a new matrix a minus B so a couple things one is if the rows and columns aren't the same I mean I want to do this element wise right basically if you've forgotten what's going on here basically I have I have a matrix like this which has these guesses 0.7 0.4 and I want to and I have the known answers 1 0 and I want to take this matrix minus this matrix to give me one that has 0.3 negative point 4 so let me add a function that does that it's too bad I didn't have it from before and I should do some error checking here like check if the rows and columns are the same I'll put that in later I'll put that in later got to kind of move along here some that'll be in there maybe when you look at the code someday but I'm just gonna say I'm gonna do a first I need to make a result which is a new matrix which is a new matrix that has the same number of rows and columns as either of them I think it call it rows and columns right so I need to make a new matrix as the same amount and then I'm gonna use my very subtle silly loop function that I use over and over again to loop over all the elements and just say the result dot data index I which is the row index J equals a data index I A B data index I index J right so this is me just going through and subtracting everything from one to the other and then I can return this result all right so I had this static subtract function is it silly to have a subtract function when I use of an odd function that then you know a die could be used for subtract by just saying like multiply the whole matrix by negative one but whatever I'm just doing I'm doing it as I'm doing it will refactor later all right so now let's think about this I'm going to comment this out and I'm going to say these are my inputs now I'm gonna have my targets and in this case my neural network is two to one so I'm gonna keep that my target is just one I want to get one now what I'm going to do is I want to say neural network instead of feedforward I want to say train with these inputs and these targets so let's just run the code and see if I get any errors then I'll debug the actual output so let's go to the browser where I've kind of got this code runs output is not defined sketch dot J S line 11 oh there is no output now hey no errors thank you I'm done I forget about the thermal Network stuff oh yeah I am more to do I'm afraid to keep going but I'm gonna keep going let's just look in the training function and console.log the inputs the targets oh let's just console.log the air actually I just do arrow dot print sorry so I can say outputs dot print because I have this print function and targets targets dot print and error dot print so I just want to look I just want to examine all these things are I got an error oh error dot print all right so okay something went wrong so this is what did I this is the output this is the target this should be the difference so what went wrong here within my subtract matrix dot subtract target's comma outputs let's look at this function something must be wrong here anybody sick of looking zeni buddy see it I J columns rows columns in that data Petros ADA columns return results this dot rows oh yes thank you alright here's the mistake this dot rows this columns that makes no sense I make I'm writing a static function that's not called on an instance of a matrix object I want to look through everything I've got to loop through everything in result or a and B they should all have the same number of rows and columns thank you to Rubin in the chat who pointed that out to me okay result dot Rose result columns all right now let's run this again great this looks right this is what the neural network produced this is what it this is the known output and this is the this is the error and just to just to take this one step further if I were to have two outputs I don't know I'm not and and have a second target this is matching what I've drawn over here we could see these are the guest outputs this is the target and these are the errors so we have now written into our code all of the pieces we need to get these two errors get the air the output errors the next step we need to do is calculate the hidden errors I need to calculate the hidden pairs so looking at the code I need another step here I need to now say let hidden errors equal and figure that out so what goes here what goes here this is the and I suppose if I'm being consistent I should say errors there right errors are the out pen and and if I'm being really consistent I should say output errors now what I need to do is calculate the hidden errors okay so let's go back to here and I want to do this with matrix math so this looks like hey that could be some matrix math going on here right this looks like a weighted sum or something dot product D like looking thing and here's the trick this looks good right but what's all this like fractions stuff well if you look at this this is the same as this this is the same as this these are really normalizing dividing these weights by the sum of all the weights is a way of normalizing everything so they all add up to 100% in the end we're gonna kind of like multiply everything by this learning rate constant anyway so we could say like make a big step or make a small step so that normalizing of it kind of doesn't matter I mean it's sort of important but we can also ignore it and that's kind of a trick here we're gonna just take out the we know we want the amount of the air to be proportional but the fact that we're multiplying by its weight it's going to be proportional we don't have to divide it by the Sun so we can actually take the bottom out and I'm gonna say wait one one here I'm gonna say wait one two here I'm gonna say wait to two here and I'm gonna say wait to one here and look at this by golly doesn't that look like some matrix math right that's got to be the result of some matrix product now I need more space on the whiteboard so how can I do is kind of condense this a little bit well one one times e 1 plus weight of 2 1 times e to make this take up less space and then wait one two times e1 plus wait what was that two times e2 now conveniently I have this matrix here and what matrix would I put here to get this right if I want the matrix product of these two matrices I need to put a row in here that's right the row this row W 1 1 W 2 1 the weighted sum the dot product of this row and this column is exactly this now let's take this one 1 2 and wait right take this dot product with here boom we've got this this matrix product the matrix product between these two matrices is that hidden errors coming out of the hidden layer this is really exciting but there's something really strange here it's like stare at this now all along I keep getting my indices wrong right I've been getting my histories wrong in these tutorials I had to do the tutorials over again and this might look wrong right because should not be wrong column row column row column row column this is Row one how eyes are there looks like I got it backwards well the fact is I did get the backwards I got that backwards on purpose this is actually this weight matrix transposed so these weights are stored in a matrix already in my code that matrix looks like this weight 1 1 wait 1 2 wait 2 1 wait 2 2 transpose this matrix to this and take the dot product with the heirs and boom I've got the hidden errors coming out ok let's do that this is working because it's square but am I gonna have an issue if it's not square I didn't notice this when I was working this out or anyway let's go put that into our code oops yes w superscript T okay all right the good news is oops right the good news is I believe in the matrix library I already wrote a function called transpose here's I should be consistent this function transpose returns a new matrix that's the transposition of the previous one and so I should actually make this static and it should require to receive a it should receive some matrix object and so it should be this so I just changed this to be a static function so that what I can do is I can say here back in the library what do I need to transpose I need to transpose the weights that are going from hidden to output that's the weights H of so I have this dot weights H oh so let hidden um weights H o transpose T for transpose hmm I could also just change the let who T these are the way naming I could use some work on the naming but these are the weights from hidden to output transposed equals matrix dot transpose this dot weights hidden output so the hidden errors now should be matrix multiply output way what did I say hey if we look over here matrix multiply that transpose matrix and those errors that transposed matrix and the output errors so this is calculate the hidden layer errors now if I were writing a proper library that could support multiple layers there'd be some kind of loop going on here because I keep doing this from layer to layer to layer but since I just have this two layer Network I'm just gonna do the output layers output errors and the hidden errors so that I can sort of get this back propagation thing going in just one step I'm getting a nice comment from the chat I was worrying about the dimensions of my matrices and Kate week Minh writes the dimensions work out when going backwards the transpose right of course because I was worried about the dimensions not working because you know the rows and columns have to match properly when you're doing matrix multiplication but since I'm now going backwards it actually makes sense that the matrices have to be transposed you can pause and think about that for a second but that does make sense now ok so I think I've come oh and I was also but you know if you're looking at this notation in like a text book or something you'll often see if I have like if you have a weight matrix that's W and maybe it's like W IJ for rows and columns you'll often see T in the superscript as and you can't see that write it over already over here right you know if this is the weight matrix W and you have columns and rows then you'll often see a superscript of T and that refers to this this matrix trans transposed and so maybe this would be you know weight h.o.t or something I don't know so my notation is as you all know is kind of poor but I try to do my best to explain it hopefully it will match up with other other notation that you see in that sort of thing okay so this is now done in terms of back propagation I've compute the error I've computed the I've propagated that error backwards to compute the hidden layers error and now I just need to add the part where I adjust all of the weights these weights based on this error and these weights based on this error and then we just move on and then we're done so I'm gonna do that in the next video I should warn you that the math for doing that is this which I'll discuss in generalities the math of gradient descent for finding these Delta weights is pretty complicated I have two videos that I've made where I kind of do something similar which I will reference as well as all of those three blue one Brown videos that go through the math in detail I'll probably start the next video just by presenting the formula and then implementing kind of talking through the pieces of the formula and then implementing that in code okay right people are telling me I don't need the IJ yeah yeah I don't need the IJ but anyway lower case T upper case T so now what I need to do is I need to where am I in my notes what time is it oh it's twelve thirty five remember items can be done with backpropagation by 1230 that didn't happen that's okay though even though it's search done alright alright so I think does everybody feel ok let's get a snapshot of the whiteboard as is I think I'm gonna erase this now because what what error do I have result equals new matrix this columns this dot row huh where where where where where do I need to change that sorry I'm seeing something in the chat that I got columns and rows incorrect somewhere I think I have that here in the static transpose no this is correct because but you might think this is incorrect but I have written my matrix library in such a way that the rows and columns come in first so if you're transposing it you send the columns in first and the rows second so I think that's correct I'll let and see if anybody I'm gonna give it a minute so what I need to do now is find the best notation for writing out the formula and I kinda this is where I kind of got stuck in my preparation for today although I have to say I can't use this on static ah oh I see thank you here got it um what's the best way I'll just I'll just do this at the beginning of the next video shoot as I didn't try to I would have gotten this error if I tried to run the code let's let's add something to the end yeah it's fine I'm just at the beginning yeah yeah I got I got it everybody I'm just trying to figure out where to know I haven't updated the wait Simon is asking have you update the weights no there's a whole big step here I mean kind of like I just did the sort of like beginning part so I the whole big step so I guess I'll just cop to this error at the beginning of the next video so what I'm trying to do I'm gonna erase the whiteboard I don't think this has any value anymore or at least this whole bottom part doesn't I can leave this diagram here maybe for reference so let me try doing that and let me try to find a form a way of noting the formula for updating the weights this time I need to try twice I'm not gonna so this is my this is my thinking and I'm going to kind of use my notes here a little bit I'm going to need help with everybody and well that's just the mouse oh it's like there's a black mark on the whiteboard so I I will correct this by the way at the beginning of the next video me just make sure the whiteboard is dry so my thinking is the any given weight equals its its weight plus a change in weight right and the so the thing we're trying to calculate is this another way of and the way that we're going to calculate this is by thinking about the way the the hat when we want to understand like two we want to adjust the weights we want to adjust the weights so you know we want to think about this as the change in error based on the change in weights so how does the right if I know how the error changes based on the how the I'm just sort of thinking this through based on how the weight changes right this is Nicole gradient descent thing then basically I'm saying Delta I want to change take the weight and minimize the error so adjust it based on the negative direction how the error changes based on the week and this error is something different it's actually the like cumulative cost so it's that's the kind of squares of all the errors let's see if I'm getting this about right and so that question becomes how do I know how the error changes based on the weights and that is the formula that I've kind of derived previously is the error times the so in this case right this is the output the derivative of the output based on the weight Oh talking this through I'm gonna come back and explain it let's see if everything I'm I'm gonna do this again I'm just kind of like talking this through into my head and so now what I need to do is figure this out and what this is is output is sigmoid right of the weighted sum how did I write this down I just wrote weighted sum of my notes we can think of the weighted sum of all the connections coming in but I could just call that like X for a second and so and the derivative of sigmoid is a known thing the derivative of sigmoid x equals sigmoid of x times 1 minus Sigma of X and it looks like a 6 and so and so basically if I put this here I kind of have the I kind of the formula for any given single weight let's look at this shot so so this I think is right that's where my notes end let me look at the code that I did so that's really like the gradient the good thing of this is would it be correct to refer to this kind of as the gradient and since I need to do that for everything how can I know take this with matrix math I know that what I do in my code is I I have I take the outputs transposed I need more room on the white board let me get this far so I think of what I want to do is try to get this far for a single weight then I'll erase and try to get further with turn this into matrix matrices anybody have any Corrections or notes or thoughts you want to add to what I haven't erased this and kind of go through it but actually like try to explain it a bit more as I do that I mean I could just include the final formula at the top maybe I should maybe I should write that down I don't see output equals matrix I don't see anybody talking about right now Jack who's never just talking amongst themselves all right so I think right we just we just need to figure it out for any one of these given weights how to adjust it based on the error and we've calculated all the error stuff already okay just looking at this all right I'm gonna yeah start with a single weight all right I'm looking in the chassis some people are typing in the slack patron group I might not get to to be honest the challenge today I think I can I think I can alright alright alright I'm just gonna move forward I'm gonna erase all this I'm gonna try to explain it again I forgot where I was I finished I just finished a video right okay all right oh the yes the error function meaning the the cost of all of the sum the sum squared of all the errors a squaring yeah yeah I do I do need to talk about that yes yes thank you okay I will I definitely will mention that all right so in this video I'm going to start to talk through learning learning with gradient descent we've looked at how to back propagate the error that's coming from the output back through the network now we need to look at how do we turn all the dial set but before I get to that I did make a sort of major mistake in the previous video where I was making this transpose function and turning it into static so the word this now means nothing so I need to make the resulting transpose matrix out of the one that's being passed in so just wanted to correct that right here the beginning of this video quickly okay now so there's a few things we need to talk about so first I want to talk about what is this thing called gradient descent now let me once again as I always do reference that other materials will go to this in more depth I made a video called the mathematics of gradient descent it don't really recommend that one is my top choice but you could go watch that and it'll also reference the one that do recommend which is the three blue one Brown video about gradient descent and there's a nice Siraj video about gradient descent to suit to I'll link to both of those in this video's description but the basic idea is that we need to have come up with a cost function what is the cost function well this is an error this is like an error vector right point three and negative point four what we want to do is we actually want to look at the cumulative error over time of many training examples so if we say like here's a hundred pieces of data with known labelled answers what's your cumulative error over all of them and the way that that's typically calculated is by the sum which is often used this nice uppercase Sigma is that uppercase the right thing to say with critters the sum the weight the sum of all of the you know targets minus output squared so this is often referred to as a cost function and you could graph this function like you could imagine it it's like a function like y equals x squared and if I had a function like y equals x squared it would look at this and if I wanted to minimize that function I would have to start moving I would want to like look at how do I go down how do I go down towards zero how do I walk down gradient descent and how do I know the direction of the slope of this function at any given point with a concept known as the derivative so again I would refer you to some videos from three blue one brown that go through the calculus what is a derivative what is great to send in more detail but this is the basic idea I want to push the weights I want to know what happens right when I change the weight how does that affect the error the change in the error based on the change in weights right I want to minimize minimize the error I want change the weight in the direction that changes the air this is kind of the notation for that the change in air L through the change of weights and you know the kinds of derivatives is more sophisticated than that but that's a way of thinking of it right now there's something else I want to mention about this ah yes so here's the thing this is a nice little like graph of y equal x squared and sort of notation over here but the thing that's really crazy about this is we aren't living into this would work if we have a single output this would kind of is what it would look like if there were two outputs then we might have this like Bowl perhaps I think if there were three outputs oh I've lost I can't think we're gonna get into crazy and dimensional spaces then you can't really imagine so that's one thing that's really confusing about this is these outputs could be many many so we really have to in a way tackle them one time like let's well how would it work with for just one and in essence we're gonna do the same thing to all of them by pushing them through some matrix math so that's ultimately what I'm gonna do I'm gonna explain a little bit further but eventually again I'm gonna be glossing over some details and mostly focus on trying to implement this in the code all right let me pause for a second I'm not seeing anyone yelling at me that I said anything horribly wrong so that's the thing that I wanted the error function thank you okay we come for reminding me about that that's what I wanted to start so where is the actual eraser now that we've talked about this or cost function how we're trying to minimize that function using gradient descent walking along the graph of that function towards the bottom let's talk about how we actually do that what's the thing we're actually trying to calculate well what we want to do as we've look at all this training data is we want to say how do I want to change a weight I want any given weight this W for any given weight just one weight in the system I want to the process that I'm gonna run in my code is to change that weight by some Delta weight that change in weight so here we are I should be able to just say if I could just figure out the formula for Delta weight I'm done all right well I did say that another sorry I did just a moment ago talked about the cost function and the idea of how does the error change when I adjust the weights well this is really another way of saying Delta weight if I said Delta weight is you know how does the error change great when I change the weight if it gets bigger when I change the weight I wanted to get I want the error to get smaller so I could kind of say Delta weight is negative how the how the Changez when I changed the weight so this is just another way of kind of writing the same thing people who know more about mathematics might be getting upset with me right now I will look forward to your comments all right so now let's think about this how do I calculate this so how do I know how the error changes when I change the weight well this is something that I have gone through in my gradient descent video what I can do here is actually just say looking at my notes oh I forgot about the learning rate sorry sorry sorry sorry I should add one more thing in here which is that this Delta weight can also be a factor I can also factor in this thing called a learning rate I'm gonna kind of fold this in at the end but the learning rate is just like I this is how much I should this is the direction that I need to move you know should just look like a tape it or let's try to move like a lot and so you've seen this again in the simple perceptron example this idea of a learning rate so that should get folded in here as well so basically what I'm doing here is I'm saying this is a formula that I've derived in other places this should equal I'm looking at my notes over here just a few pi get this right negative the error itself times how the output changes when I change a weight so I'm gonna call that do DW how does the output change when I change the weight well what is the output that's something we know we've been working on that right the output of any given neuron is the weighted sum of everything coming in pass through this sigmoid function and again sigmoid is kind of a historical activation function that's not used as much right now and later you know we can swap in other activation functions understand but they're one of the reasons why it's useful to use sigmoid is it makes doing this calculation kind of easy so if the output the function that's the output is equal to Sigma E and that's the Greek that's the lowercase Greek letters I'm trying to write this out maybe we should just write the word sigmoid I'm gonna write in this sig si G sigmoid of that whole weighted sum of everything let's just call that X for a second then if this is the output function how that output changes when I change a weight is the derivative of that function write out any given weight write that this is the weighted sum how they changed these weights what is the slope of that function in this crazy ndimensional space well it just so happens if we go back to Wikipedia sigmoid derivative lets derivative where is it on here it looks so it's so nice man that on here fail a little failed moment here let's find a different place I don't want to derive it just want to look at it yeah here it is that's a nice little oh wait hold on I'm getting some good questions maybe add a factor of 1/2 to make the derivative the error oh yeah yeah so wait where do I add that you mean added in here because this thing is the derivative air is actually there's R squared or something is that what you meant K week Minh I always just flop that off cuz it's just another constant in the system I think you should do a Eric in the chat rightz I think you should do a past propagating errors back through the whole network then update the weights it doesn't matter much now but if you did have more layers who would be able to update all the weights in parallel language permitting I think that's kind of what I did already I'm trying to do but I see what you're saying right because I've already I already did this step of passing the errors back and so now I just want to do update these weights and update these weights we already have these errors so update so I need to fill in the updates here and the updates here that's what I'm trying to do okay maybe I missed it in the stream Y is Delta wait in the first formula using the uppercase Delta but in the second formula upper case Delta wait equals to negative lower case Delta e over lower case Delta W I mean it's really the same yeah my notation is kind of bad but I may have said I really sort of mean this as shorthand this in a way in the cost function yeah yeah and the cost function okay okay so I'm just reading the chat for a second right I was looking for a nice derivative that of sigmoid anybody if anybody has a suggestion for a nice webpage that's gonna I mean I guess I should just use this one it's just like this is a little janky but yeah it's also it's driving it now I know what this thing my drift is just wanted to like find a webpage that confirms it for me maybe math world sigmoid derivative yeah there we go all right I'm just gonna yeah Reuben I did sort of mention that I believe during a bunch of times and I will continue to mention that all right WolframAlpha well from alpha what is the derivative of sigmoid and I cannot spell derivative all these web pages are making me sad I mean that's a nice way of putting it but okay I'm just gonna I'm just gonna I'm gonna I don't need it I don't need a diagram gosh darn it come back over here yeah I will yeah yeah no I'm definitely not doing the derivation I just wanted to put the confirm that I was correct so we could one of them Here I am now on a webpage which shows the derivative a sigmoid and this is what's really nice about it the derivative a sigmoid is actually sigmoid times 1 minus Sigma so that's what makes kind of working this out once I start to plug the math into my actual source code kind of easy so if I come back over here I can say this is times sigmoid of the output oh sorry times 1 minus Sigma of the output I don't know why I put so this is basically what I'm looking at here if I want to change any given weight I change it based on the derivative of sigmoid multiplied by the error and this is kind of this is the same formula that's appeared into several other of my previous videos and one that you can kind of dive deeper in with some of the links that I've included in this video's description the big question now is how do this is just I'm really just talking about the case of changing one weight how do I turn this into matrix math alright alright so now again this is where my notes have ended and I need a way of writing this with matrix math and the way that I'm gonna do that anybody who wants to help me with this I would gladly take your help oh I forgot about the 1/2 thing I will I'll bring that up I'll bring that up next let me see is there a notation that's used in okay this is the notation that is used in the tariq Rashid book so let me try to write that out dare I erase this top I think that's what I have to erase so let's look at the this notation Delta wait for and he uses JK I've been using I J so I'm gonna keep that I know that's JK is much better equals we have the learning rate I've talked about which times the error vector which is J right because it's a it's like one I'm iterating over the rows so it should be so confusing it's gonna say the error the error vector times sigmoid of the output vector sigmoid of the output vector this is the same thing that I did that I've got right here the same notation I'm just confirming that I'm right times 1 minus Sigma of the output question is if I'm thinking of this as rows columns and things are being transposed and as this is it what's the what's they in disk index it should go under E and under oh that's a question oh my goodness wrong camera wrong camera wrong camera wrong camera so long so long with the wrong camera and I wrote up let me do this again sorry everybody this is good I need multiple tries at this anyway I'm writing off the whiteboard so I'm looking at the notation that's in the tariq Rashid book and this is what's written JK equals the learning rate I'm changing some learning rate times e sub K times sigmoid of o sub K times 1 minus Sigma Y of K and again I'm going up to high times sorry sigmoid make sure you can see this of o sub K times sigmoid sorry times 1 minus Sigma of okay so this is the formula as written in oh and then this whole thing right this is why what did I miss I missed this whole thing now and I'm running out of space so I'll figure this out is the mate way and the matrix product right which my symbol for that is like a train the matrix product with the transposed output J so this is what's written into the tarik Rashid book the what I would like to do is try to match this with what I've done so far so in theory except I think ah Tariq one of the things that I noticed is the trick Rashid book I'm pretty sure does columns rows that's why I'm getting confused so my notation has been rows columns and this is a single column vector okay and then this is transposed transposed so it would be J there we go so this would match there and so right that's the bias derivative if I don't multiply it would you multiply it with multiply it with the activations as well so what's this step so why is it that when I explain this why is it that I went through this Delta wait and I didn't include I forgot to actually multiply it by the output here because oh is that because of the negative 1/2 thing and then I'm taking the 1/2 thing and I'm taking that out but with the bias it's just 1 and so that goes goes away this is where I this is where I this is comfortable for me and then I'm going to be able to turn this into code but I'm kind of confused as to I mean I get that this needs to be here but I'm trying to get what's a nice way of explaining how I missed putting this doing is the matrix product with these would be with oh but it always is multiplied by that well how come I missed it down here was that just my own mistake I'm gonna go check out some things that are being typed yeah we believe in the chat rightz why am i watching this I'm just learning to make the ellipse in j/s yeah go back to that don't get out while you can all right I'm just gonna I Dan if you already calculated this sigmoid in the outputs you can just do x times 1 minus X target is the derivative in the last layer predicted minus target there's a lot of interesting things being discussed in the chat all right I'm just going to and hold it I'm looking at this camera went off you need to do some more reading and preparing I mean I have to just be done for today and I will eris in the weights in now but I'm just I wish I had that gradient descent formula let's see here I think in this video I could find the part of this video right I'm doing it here basically going through exactly what I'm trying to go through now yeah oh no I'm just so close to finding the right point in this video I think actually this might help if I kind of match it up with the diagram in this video yes you are watching a livestream of a person looking back in a video tutorial that they previously made to try to find something to help understand the video tutorial that they're trying to make right now that is one day what that is exactly what you were doing and you should all just oh here it is down here please step away step away from the I did all this it went through this you know another way for me to do this is actually now that I'm realizing this is just alright alright so what I'm going to right err times X this is this is the equivalent of X this is obviously the error and the new thing here is the fact that I have sigmoid as the activation function right so in this the difference is in here there was no sigmoid function so the derivative was much simpler and doesn't show up right right can I can I make a argument can I can I can I stop where I am right now and connect this here with this video where I derive this formula for the change in weights the derivative is 1 in your video yes I think that's right all right so this is what I'm going to do I seek a week Mon is typing so I must wait K week one usually has something very wise to say shouldn't that be shouldn't that be the hidden layer activations we x yes the hidden layer activations I mean it depends where I am I got rid of my diagram right now I'm not doing the it would be multiplying by the hidden layer activations this is just like what I'm doing right now is generically for any connection and then I'm going to do the same for the output and the same for that the hidden layer but the output uses the error vector from the output and the hidden layers is the error vector the hidden errors so yeah all right two times target minus W index I times yeah exactly exactly exactly yeah I just erased that I know on purpose all right I'm gonna go in back into the video tutorial and we'll see what happens I just took a little break staring at this I'm having this moment it's like haven't I gone through all of this before and then I realized ah I mean I kind of referenced before I have it so I went back and looked so what I want to do right now is like this is where I sort of ended here in trying to sort of look at how the error changes based on how the when the weights change that I've talked about this in my mathematics of gradient descent and in this mathematics of gradient descent video there was no sigmoid function so ultimately what I have here is and this is sort of 18 minutes into this video which boils down to the chain and by the way one thing that I missed over here is is the the 2 or the 1/2 so nice when you take the derivative of something squared you get a 2 and so you divide by 2 or multiply 2 so I just sort of like take those out but because it's just a constant I'm gonna have a learning rate later anyway so but that's why you see the 2 over here but what you see here is change right the change in the weights this is M and B for a single and this is really for a single weight y equals MX plus B the bias I mean not to that yet is the error times the x times the learning rate and that's ultimately what I have here butbutbutbut solet's so I have this book so let me write this let me now write this out with meit sort of more matrix notation this is a makeyourown neural network and I'm going to basically use the exact and I highly recommend kind of reading this chapter along I'm going to use the basically the exact same you know what I want to calculate is the changing weights for every weight in the matrix of connections every row and every column and what I want that to be I got to be a little bit lower here this is based on what I kind of worked out here well I have a learning rate multiplied by the errors and the error vector right the error vector is a single column vector so I'm looking at all the different rooms time's the actual output oh uh sorry hold on oh if I come back to here this is basically sorry so let's just looking at this right now let's go back to my previous video that's basically the same thing as I've got right here I've got the change in weight is the error there times the learning rate but there's an X in there so the X what's the X over here well the X is actually that output so I'm gonna have to do the matrix product which I my symbol for that is a trade but I'm gonna use uses a dot right now with the actual output itself but the thing that's missing in this particular scenario is I didn't have this sigmoid function that's how I need to look which way do I go which way do I adjust based on those activations are coming from the sigmoid function so I need the derivative of the sigmoid function here so I also need to say sigmoid of the output that output is a single column vector times 1 minus Sigma of the output which is that one column vector and now I take the matrix product with the actual output itself now here's the thing this is something we've actually calculated already in my code I've died calculated the outputs I did that while I was feeding forward through the network so this is actually something that's already calculated and I just need to put it in here so this now if I can just take this Delta weights then I could say for every weight I J adjust it by its corresponding Delta weight I J so this is now the formula that I need to implement in my code I need to implement this in my code to calculate all of the Delta weights how do I change all of those weights for every single weight whether it's and this error and this output this could be the final output and the final error or this could be the hidden output and the hidden error same formulas gonna work and now of course I'll do the bias as well but here now I think I could put this into my code yes oh okay so and in fact I'm doing something really weird here so this is really I could simplify this even further right because the activation function is inherent it's already been calculated so I really should say just the output vector times 1 minus that output vector and this is going to have to be transposed so that the because of the way the matrix products worked out but let's let's get to that when I get into the code okay I should go back I think I should go I'm gonna maybe I should try explaining this again why is it trance this is definitely gonna require a second shot let me go into the code and if I have to redo this next week I will so I have to like watch this back and then figure out where my explanations went awry I can already take could already hear my ta telling me the last oh should be H I think oh yes yes thank you ah that's it that's what I've got wrong let me maybe this is salvageable yeah this is H thank you that's definitely H yeah why does it say Oh in the Oh Joe in the in the book here that's interesting okay let me try this whole explanation again I don't remember where it comes from differentiating the inner function which depends on the previous layer I either hidden okay some Oh squirty chair okay do I dare let me start over like almost to the last bit you just put this in the code and then whoa whoa oh I'm in the wrong I thought I was in the whiteboard because this screen has a like a shot of a whiteboard on it and we're almost done like I'm almost in the last bit where I could just put the code in oh it's 130 I have to go shoot let me let me see let me I'll come back I don't have time through this conference oh let's see if I can let me give me like let's just let me try this explanation one more time this is definitely have to come back and and continue this another time like next week but I'm so close to the end because if I once I just write that get that formula written correctly out there I could just put it in the code because basically yeah skip the conference time for it yeah I know I need to do a coding challenge I can't I'm not gonna have a coding challenge today because of back propagation back propagation just thinking for a second where I spend so much time like the momentum is totally died okay I'm gonna I'm gonna go back to where I referenced that previous video no that was fine so I reference that previous video I came over here bla bla bla bla bla and then I went back there okay alright so what I want to do now so where am I I had this formula I'm realized it's similar to my previous video oh no I'm just repeating myself but which is over here now what I need to do is take that formula that I've gone through and and let me actually just take it and write it directly from this book basically it's you know what I'm gonna do I'm gonna let's just implement the code shoot give me one more chance here so I'm reading the jack I believe there are people here still watching so after I reference this video so let's try to take this now and turn this into a matrix math basically remember what I want to do is I need to know the changer weight for every weight that's in our weight matrix and so this is exactly the same formula that I did in the gradient descent video it's basically what's down here but let's kind of rewrite it with some notation and thank you again to this book that I'm kind of also pulling from so if I want to know the change in weight for every weight in our row column matrix row I column J this equals referring back over here ok well first we need the learning rate times the error right the error now the error is a single column vector right so it's just rows now this could be the error of again these are the change in weights for any weight matrix anywhere in the network so this error could be the final output error or could be the hidden error and then I need to use the sigmoid function so the derivative of the sigmoid function that's the piece that's missing over here right oh shoot oh sorry and then I need to multiply the actual output itself from the previous what the previous layer whatever those I need to multiply by the inputs itself that's why that's why X is over here so let's just let's make the case what I'm doing I'm doing the weight adjustments for the final layer we're gonna so I need to multiply this by that and actually that's going to be the matrix product which I don't have a symbol for a good thing you can't see is I'm trying right or I need to buy those hidden outputs so I need to what's coming out of the hidden layer that's X so I'm gonna put that here and so now but I need to also figure out what direction to go I need the derivative I didn't have the sigmoid function in this video so it's not there in the formula but now what I need is this and guess what I already have this this is just the output itself right the so the derivative of this that's the output of the final output is the the sigmoid of the weighted sum of everything and so now the derivative is is o index I that single column vector output x times 1 minus o so this is actually the entire formula I need it's terrible terrible terrible me who has written this too high on the whiteboard I am hereby fired from ever making a youtube video again I'm gonna I'm gonna rewrite it a little bit lower because we've really got this now we've really got this Delta weight formula I need to this is just I should just show you this right I need to take the learning rate multiplied by the errors which I said was J but oh sorry I multiplied by the output multiplied by one the output and this whole thing now gets the matrix product of what was the X in this situation if this is the final output of the network then this becomes the hidden and this is the matrix product this is so this is the Delta wait look at this I love this now this is the Delta wait for hidden to output the Delta wait for input to hidden then equals the learning rate times the hidden errors times the hidden output times 1 minus they hit an output and that whole dot product with the input itself determine to call X so this is how this is how I adjust the weights this is why I need all my goodness this is why I spent all those other videos on just figure out what this hidden error is this is really the magic of back propagation right here so these are the formulas you know are these formulas didn't even there they gonna make sense to you especially with this massive video that I've put together probably not but we kind of have a ride to them but the thing but I haven't do these formulas make total complete sense to you well hopefully again the idea is to like look at this and kind of get an intuitive understanding right we know we need to adjust the weights based on the error the hidden error is kind of like a way of getting the portion of that that final error we know we need to adjust them in the direction of looking at the atom as well do these formulas really make any sense to you probably not a if you want to kind of unpack them further I think you could pause now and go back and kind of look it just watch my mathematical grading descent video look at the three blue one Brown videos but the pieces of this should really make sense right we know we want to change all the weights one of adjust those dials to get a better output that output is based on the error that error is back propagated to a hidden error if we're going back weights in the matrix the derivative of the sigmoid function is important because it tells us the direction of which style things are being activated and then of course we need the what's try to come before hidden comes before the final output and the input comes before the hidden this is exactly this is exactly what I've done in in the previous videos of what I had when we were looking at y equals MX plus B so we can connect that back to the gradient descent video okay so I think it's time I'm going to stop right now and I'm going to from here implement this in code alright so probably what I need to do that probably what I need to do is redo the explanation because I think I've really botched that but I think if I I think I'm gonna I think this is right enough that I can put this in the code and maybe we've actually got our neural network code I don't know what bag provocation means so I'm gonna I'm gonna have to return to this I've run out of time today but let's see if I can at least put this in my code I'm not even gonna do this as a video tutorial I'm gonna just come back to this last piece if so I'm gonna I think I need to do redo the videos I've realized it today redo the videos where I explain these won't be the final published ones the Delta weights how that's calculated but let's just see if this kind of matches up really quickly so if I'm doing this one here so here's the thing what I let me save this as old and then so I know where I last left it so a couple things I need I need D I need a function called D sigmoid and that is if I have let y equal sigmoid of X now I return Y times 1 minus y so this I'm going to need is the the derivative of sigmoid function I think this is going to like mess the problem is I want to use this stuff so I think instead of doing feedforward what I can actually do here is do this and then I was calling these outputs yeah this is the autocomplete that I guess I don't want and then I don't need this anymore so targets though I do need so what I just did is put the whole feet forward in here thing because I need to and then I need to calculate Delta the idea is to calculate Delta weights and so how do I do that what are the pieces that I need the pieces I need is are the air the air this this is the so I have this and now I need to multiply it by this and dark these that's an elementwise multiplication right so I actually want to say right this is map through sigmoid already right there so I want to say like I'll just call it D outputs for right now equals so I need to make a copy of it so I need to like did I have a mistake here yeah no that's fine I'm gonna come back I'm gonna come back to this next week I just I kind of wanted to put the code in here so the the derivative of the outputs if I can take suffice ik function called 1 minus yeah actually the outputs already have so the thing is I need a fake I need a like already sigmoid function which just does this right because it it's D sigmoid if I'm looking at the stuff that's already been sigmoid let's just put this up here again this is gonna be a mess I just want to put the code in here and then I'm gonna have a like some time to think about it I'll come back and do another live stream where I finish this oops so so I want to say matrix dot map the outputs right I want to because they've already been sigmoid map already sigmoid outputs already sigmoid so I'm gonna need a map function that's static to make a new vector where's map static map matrix and let result equal a new matrix matrix dot rows matrix Kyle's sorry I'm gonna do a lot of stuff without really explaining it right now fully just to try to function value and then return results so now I have this so I have this now I need this right and I have that but those get multiplied by each other just element wise so can I say D outputs that's the hata mard Hadamard how do you say that again Hadamard Hadamard product I'm gonna write a Hadamard function with the output errors right so this is the hata mart and so if i put that into the library tonight actually oh there is a mask a l'heure product that's the Hadamard so that should be called probably Hadamard but whatever it's called multiplied it's already in the library just cuz I have to go I'm kind of like race through this I can multiply by the learning rate there also should be how come this my library doesn't check to see if it's a just a single number oh no that's only the scalar product that multiply function is only doing the scalar product got it got it got it oh I had it previously doing that so I can adjust this adjust this to do Hadamard I need to do that I see I see so if n is an instance of a matrix now I should do the Hadamard product otherwise just do the scalar product which is different than the matrix product which also is called multiplies and the naming is terrible here but so that would be this so now okay so that works multiplied by the learning rate so I'm just gonna say I'm just gonna have that be a constant here right now I'm not a constant cuz let let learning rate equal 0.1 or something okay so this well this went off so I did this component this component at this point now I need to do the matrix product with but that has to be transposed so now what I need to do is say let this actually transpose no no where am i I'm here hmm yeah target on outputs now I need to just take this here I need to take these but hidden T equal matrix dot transpose hidden hidden T right right this is the matrix transposed and then a delta so this is really what I should really say is like sigmoid no ya d outputs yeah sorry sorry so now let Delta weights equal matrix dot multiply D outputs hidden transpose Delta weights dot print then all right Simon is asking why practice in a livestream I don't have a good reason just because I'm already livestreaming I need to practice and maybe this will help okay sorry for everybody sort of joining in the middle I've run out of time and I have to go so it's just trying to like see if I was in the right path I need to double back and redo the video explanation I need to watch my gradient descent video to connect it better and then kind of just present these formulas I think that makes more sense I can make this much shorter I just wanted to see if this would actually work in the code I don't have time to test it out and then so now we would say this dot weights which is hidden to output hidden to output dot add add Delta weights so this is change hidden to output weights gradient descent okay now alright so I think I'm going to finish up here I just want it so what I'm gonna do better than no practice exactly so I don't know if I've gotten this right I need to stop freeze frame here for myself sorry that this live stream is kind of ending abruptly and I didn't even do a coding challenge these are the formulas that I've arrived at for how to change the weights between hittin and output and how to change the weights between input and hit it I need to come back and redo my explanation I think everything looks fine all the way up until it's all the way up until the sort of I got to the point where I started started with this line right here so that is where I'm gonna return the next time I come back hopefully sometime next early next week to continue this live stream that's not finished I'm gonna start from here refer I'm gonna try to derive not derive derive is the wrong I'm definitely not going to derive I'm gonna start from here present these formulas kind of understand them as they relate to my previous gradient descent videos then I am going to then I am going to put that into the code here to see if I got it right so but I what I need to do I need to finish this code and then test it so I'm just gonna I'm gonna freeze frame this I will publish this somewhere let's publish this somewhere right now when the archive for this live stream gets posted which is typically sometime this weekend I will include a link to just like a Google Drive folder with code in progress so if anybody wants to like try to keep working on it or like check what I've got so far and refer to it you'll be able to I'm not going to put this code up in github exactly I mean I will eventually it aesthetically this is the same as what I already have in my this should be basically I'm redoing I think there are mistakes in here this exact function neural network prototype train this is a version that I made last spring and you can kind of see there's some similar stuff but I've kind of started over from scratch so this is the end for today it's 150 I I definitely have to be out of here I'm sorry that I didn't get as far as I wanted to the next steps are I knew this would be hard the next steps are redo my gradient descent video I'm just maybe I'll put this into some notes up here next to do redo gradient descent video about Delta wait formulas connect to mathematics of gradient descent the video ok then implement gradient descent in library somebody to talk about different activation functions then do X or coding challenge and then M NIST coding challenge alright so this is my this is my what I got to do next thank you everybody for being here today while I'm this is really it's I don't know I was gonna say this is kind of different than what I usually do but to be honest it's what I always do which is I'm trying to learn something yeah it's different in the sense that I do sometimes do videos about stuff that I I guess I know a little better or that a little simpler that I've been working with for 10 years or something this is like I don't really know what I'm doing so really trying to learn and get a sense of the mathematics of building a simple neural network and how to implement that with my own code so that I can later work from a higher level and feel comfortable with this or language of all this stuff so this is all I had time for today I would say that I'd come back and finish this later this afternoon but that's I know for a fact that it's not going to happen because I so that uh if anyone wants to join the patreon to discuss this board the slack group you're welcome to maybe you don't want to after watching this live stream but I'm gonna be busy for the rest of today and tomorrow with this processing foundation and SF PC learn to teach where I'm going to talk about how much of a disaster my video stuff is okay thanks sorry I don't have time to really answer questions and there will be some new videos continuing this stuff that get published and the coding challenges will come as well any last words I got a few donations from the chat that's very nice let me see if I can where do I check that here no this is where I send a super Joe can I see it in my live dashboard so I can thank the people by the way it's I look it's wonderful that people donate through the YouTube super chat my preference is the as a way of supporting the channel is through patreon but obviously not everyone can do that so that's wonderful where do where can I find them I don't it doesn't there's nowhere where I can see to find I know I can see them later once I finish and there's a place in my creator studio we can go back and see them but I'd love to be able to thank the people right now if anybody that's by the way those are you saying nice things to me in the chat but really appreciate it it actually like it does mean a lot it does help me I mean you shouldn't say if you don't mean it but if you did actually learn something if you think this isn't so terrible you say something nice about it that does help and I absolutely welcome I've gotten lots of wonderful constructive criticism on ways I can make the videos better and that kind of thing too so Socrates says I donated $5 Thank You Socrates and yeah okay so this is part one of this like this is part one of this week's episode it will be continued next week hopefully before next Friday but it might just be that next Friday I do the continuation of this we shall see I actually have a makeup class I have to teach next Friday the semester is starting so I'll figure it out I am I'm in it to win it I did it to lose it you have my random number video and blah blah blah all right thank you everybody for being with me today send me your feedback in the comments when this live stream gets archived at Schiffman at twitter the live stream will be archived within an hour I just leave it unlisted till effect till I'm Matt to help stance has a chance to like put in the description of the links to all the things so yeah I made it to learn it so I will come back later I'll be back next time definitely next Friday if not and next Friday will be a late one because I have a makeup class that I'm teaching here at NYU that goes until three o'clock in the afternoon about so I'm probably gonna be doing like a 4 p.m. or 5 p.m. Eastern Time live stream which is very late I know for all of you international depending on where you are obviously but international viewers all right thank you Socrates Etna Bruna Gino Zak and Kay weak bond I like to say your name with emphasis thank in it to win it hash tagging it to win it wilderness Dan this will be longer well the nernst and will be back alright alright it's so hard to know like right that the thing is like could I just try just write this formula up on the board and implemented the code and be like go watch those other videos that explain it more Shai explained it I'm kind of like being in the middle here which i think is a bad place to be but the best place to be I think is like I'm really gonna explain everything and it's gonna make sense or I'm gonna just present to you the these formulas that I'm going to implement and here are resources for you to dive deeper into those formulas kind of in the middle so I I'll come back and sort of sort that out all right this live stream is ending in three two one goodbye and good luck have a wonderful weekend give a friend a hug all that sort of stuff goodbye
