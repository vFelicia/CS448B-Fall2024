With timestamps:

00:00 - hello I am back in this video I am
00:03 - finally going to start to build the
00:05 - neural network architecture to make this
00:07 - color classifier I am going to take this
00:11 - data over here which is a long array of
00:14 - many many RGB values normalized to
00:16 - arrange the ER to one which matches with
00:18 - all of these one hot encoded labels and
00:21 - if you don't know what I'm talking about
00:22 - then you might want to go back and watch
00:24 - the first seven yes that's right seven
00:26 - parts of this tutorial series so it's
00:30 - getting very very long but this I think
00:31 - is I'm really getting to the good stuff
00:34 - I don't know maybe it was good stuff
00:35 - before maybe this is bad stuff I don't
00:36 - really know but this I'm really excited
00:38 - I'm excited because now what I'm gonna
00:40 - do and I'm gonna use tension flow yes
00:42 - but I'm going to create the neural
00:44 - network architecture so let's just
00:45 - remind ourselves what we have we have a
00:49 - data set most of the first seven videos
00:52 - of the series was all just about
00:53 - collecting and cleaning that data set
00:55 - and that data set is many many RGB
01:00 - values I think I have like 5000 which is
01:04 - actually is kind of very small for a
01:06 - data set but it's fine for this
01:07 - particular demonstration I have 5,000
01:10 - RGB values each one is labeled with
01:12 - something like blueish or reddish or
01:14 - purplish these were crowd-sourced
01:17 - but those got converted to one hot
01:20 - encoded vectors meaning if there are
01:23 - nine if there are nine labels
01:30 - well let's see then I have a vector that
01:34 - looks like this 1 2 3 4 5 6 7 s 10 9 9
01:38 - and maybe this one refers to purplish if
01:41 - this particular element of this array of
01:45 - numbers has a 1 in it it is that that
01:48 - and that one is for a particular label
01:50 - this one sort of ok so that's what I
01:52 - have so what I I know that I need to
01:54 - have some kind of neural network and the
01:57 - inputs has have a shape of 3 there are 3
02:02 - inputs are G B the outputs have a shape
02:09 - of 9 1 2 3 4 5
02:12 - five six seven eight nine this is the
02:15 - output layer this has a shape of nine
02:20 - inputs of the shape of three outputs
02:22 - have a shape of nine because the goal of
02:25 - this is by what once this whole thing is
02:27 - trained and finished if I send in some
02:29 - RGB values what I'm gonna get is a bunch
02:32 - of numbers all between 0 and 1 and I'm
02:34 - gonna find the one that's the highest
02:36 - and and those numbers are gonna be the
02:38 - probability of this particular data
02:40 - point being a particularly Bowl and I'm
02:42 - gonna find the one that's highest in
02:44 - front of sign at that label who
02:45 - classification we're doing
02:46 - classification so now what goes in
02:49 - between all this now this is a big
02:52 - question and many different scenarios
02:54 - might call for multiple layers different
02:56 - kinds of layers there's something called
02:58 - a convolutional layer which I'll get to
03:01 - but I'm gonna do something really simple
03:03 - I'm gonna have a basic dense layer which
03:07 - is kind of the standard building block
03:10 - of neural network systems and I'm gonna
03:13 - give it some number of nodes so for the
03:15 - sake of our even right now let's pretend
03:17 - that I just gave it four nodes and a
03:18 - dense layer this output is also going to
03:20 - be a dense layer dense layer means fully
03:23 - connected meaning that every input is
03:26 - connected to every node and then every
03:32 - node in the hidden layer this dense
03:34 - layer is connected to every output now
03:36 - I'm going to let your imagination draw
03:39 - the rest of all these connections but so
03:41 - this is what I want to architect so
03:42 - let's now go and architect this now I'm
03:45 - going to do this using tensorflow dot
03:47 - yes and the layers API if you don't know
03:49 - about the layers API you're gonna watch
03:51 - my three or four part series about the
03:53 - layers API tutorial but I'm bookin I
03:54 - sort of talk you through it while we're
03:55 - doing in here so you don't necessarily
03:56 - have to watch that okay so if I come
03:58 - back again this is what I built so far I
04:01 - have all of the training data in tensors
04:04 - and you can see the shape of it I have
04:07 - 5643
04:08 - RGB values and 5643 labels nine with
04:12 - nine possibilities okay so the first
04:15 - thing that I want to do is and I'm gonna
04:16 - do some goofy stuff with some global
04:18 - variables that I might not know that you
04:21 - know just to make my life kind of easier
04:23 - I'm gonna create a variable called
04:26 - and my model which I'm going to create
04:29 - in setup at the end after I've prepared
04:31 - all the data I'm going to say model
04:33 - equals T f dot sequential TF dot
04:40 - sequential so that now that's that's me
04:43 - creating a sequential neural network
04:46 - model it's sequential because it's a
04:47 - feed-forward the layers go in this order
04:50 - so now what I need to do is create some
04:53 - layers so the first thing I want to do
04:55 - is make the hidden let's make the output
04:57 - layer now let's make that we should do
04:58 - it in order we have to do it in order
04:59 - and I make the hidden layer hidden
05:02 - equals TF layers dense and then I put
05:10 - some configuration stuff so I'd make a
05:12 - layer by calling TF dot layers and then
05:16 - I specify the kind of layer this is
05:17 - gonna be a dense layer and then I can
05:19 - pass an object in as an argument and
05:21 - that's where I can configure things like
05:24 - input so I don't remember any of this
05:28 - let's go look it up so let's go to the
05:32 - documentation let's go to TF TF layers
05:38 - and let's go to dense where do we see
05:42 - that sorry I'm looking around for it and
05:46 - it's right there in front of my face
05:48 - under basic so I'm gonna make a TF
05:49 - layers dense I'm gonna click on that and
05:52 - now I'm gonna see these are all of the
05:57 - things that I can pass into the
05:58 - configuration so I need to specify the
06:01 - number of units the number of units is
06:03 - like the number of nodes and I made up
06:05 - four right here
06:06 - maybe let's try 16 maybe we want to have
06:08 - some more than four whatever we can make
06:10 - up anything we want
06:12 - so I'm going to now say units 16 one
06:17 - thing that I know I need is an
06:18 - activation function again I can't cover
06:20 - everything in this video I have other
06:22 - videos where I've talked about what an
06:23 - activation function is and how it works
06:25 - but the idea is the activation function
06:27 - is the function that takes all the sum
06:31 - of all of the things passing through the
06:34 - network being multiplied by the weights
06:35 - and
06:36 - squashes them into some range and so
06:39 - there probably is a really useful
06:42 - interesting discussion about we could
06:43 - have about what would be the best
06:45 - activation function to use right here
06:48 - right now maybe later
06:49 - hello try some different ones but just
06:51 - for simplicity I'm gonna use I'm gonna
06:53 - make a bad decision and just use sigmoid
06:57 - this sort of like historically original
06:58 - activation function of neural networks I
07:02 - want to use the activation function
07:03 - sigmoid let's see what else do I want
07:05 - input dimensions so this is something
07:08 - that I definitely need to do here
07:10 - because remember this this this these
07:12 - inputs this is not actually a lair this
07:15 - is a two-layer Network it looks like
07:16 - there's three but I'm just drawing it
07:18 - with three things and the inputs being
07:20 - but that's not a lair but I do need to
07:21 - specify that three things are coming in
07:24 - so I need to come here and say the input
07:29 - dimensions input dimensions is three
07:34 - because I have an RGB value this should
07:35 - do me just fine for right now so then I
07:39 - want to also create the output layer
07:45 - output TF that's gonna be dense that's
07:48 - going to have nine units because there
07:50 - are nine labels again that's completely
07:52 - arbitrary that's just how I happen to
07:53 - prepare my data set now I don't need the
07:57 - input dimensions because the input
08:00 - dimensions can be inferred by the
08:02 - previous one the input dimensions to the
08:04 - output or the number of units of the
08:05 - hidden so I don't need that but I do
08:06 - need to specify an activation function
08:08 - and guess what I am going to use a
08:13 - different activation function softmax so
08:17 - I'm just gonna type that in right now I
08:18 - will come back and explain what softmax
08:20 - is in a separate video which I think
08:23 - will be the next video of this series
08:24 - I'm just gonna push this a little bit
08:26 - further now I'm gonna say model dot add
08:31 - the hidden and then model dot add the
08:36 - output so this is now me this is now the
08:40 - code for exactly what I diagrammed right
08:45 - here three inputs into a hidden layer
08:49 - with
08:50 - number of units with some activation
08:51 - function into an output layer with some
08:54 - number of units and an activation
08:55 - function okay so we have now built the
08:59 - model here's the thing the next thing
09:01 - that I need to do and I'm gonna do this
09:02 - in the next video what I need to do is
09:05 - create an optimizer so let's just put
09:07 - this in comments create an optimizer and
09:10 - I need an optimization function which
09:12 - typically in the past I've used mean
09:14 - squared error but I'm gonna use
09:17 - something called categorical cross Troup
09:23 - I don't know no no but it sounds really
09:26 - scary but it's not and I can't its I
09:29 - also can't spell it so I'm gonna create
09:32 - the optimizer and then I'm going to
09:35 - compile the model and then I'm going to
09:37 - train the model these are the next step
09:39 - so they need to do this is the
09:41 - architecture for the model people
09:43 - telling me I have an error oh yeah I
09:44 - have something extra extra comma here
09:48 - but so this one we do the next video and
09:51 - so what I need to do in the next video
09:52 - this is like just a few lines of code
09:54 - but I need to I mean I could just add
09:57 - them but I would like to try to
09:59 - understand a bit more about what why am
10:01 - i have softmax here instead of sigmoid
10:03 - or you or any of the other activation
10:06 - functions and why I might choose
10:09 - categorical cross-entropy instead of
10:11 - mean squared error which is if you have
10:13 - happened to watch my ex or tensorflow
10:16 - TAS coding challenge or some of my other
10:18 - layers tutorials I I always just use
10:22 - mean squared hair so that's what's
10:23 - coming the next video I'm going to
10:25 - create the optimizer I'm gonna compile
10:27 - the model and I'm going to talk about
10:28 - softmax and categorical rules and true
10:33 - oh wait wait wait
10:36 - let's actually run this and see if
10:37 - there's a syntax errors no okay and if I
10:42 - just say if I if I look in the console
10:45 - here at model we can see there it is
10:47 - this is the object and it's got all this
10:50 - stuff in it alright see you in the next
10:51 - video
10:53 - [Music]
11:00 - you

Cleaned transcript:

hello I am back in this video I am finally going to start to build the neural network architecture to make this color classifier I am going to take this data over here which is a long array of many many RGB values normalized to arrange the ER to one which matches with all of these one hot encoded labels and if you don't know what I'm talking about then you might want to go back and watch the first seven yes that's right seven parts of this tutorial series so it's getting very very long but this I think is I'm really getting to the good stuff I don't know maybe it was good stuff before maybe this is bad stuff I don't really know but this I'm really excited I'm excited because now what I'm gonna do and I'm gonna use tension flow yes but I'm going to create the neural network architecture so let's just remind ourselves what we have we have a data set most of the first seven videos of the series was all just about collecting and cleaning that data set and that data set is many many RGB values I think I have like 5000 which is actually is kind of very small for a data set but it's fine for this particular demonstration I have 5,000 RGB values each one is labeled with something like blueish or reddish or purplish these were crowdsourced but those got converted to one hot encoded vectors meaning if there are nine if there are nine labels well let's see then I have a vector that looks like this 1 2 3 4 5 6 7 s 10 9 9 and maybe this one refers to purplish if this particular element of this array of numbers has a 1 in it it is that that and that one is for a particular label this one sort of ok so that's what I have so what I I know that I need to have some kind of neural network and the inputs has have a shape of 3 there are 3 inputs are G B the outputs have a shape of 9 1 2 3 4 5 five six seven eight nine this is the output layer this has a shape of nine inputs of the shape of three outputs have a shape of nine because the goal of this is by what once this whole thing is trained and finished if I send in some RGB values what I'm gonna get is a bunch of numbers all between 0 and 1 and I'm gonna find the one that's the highest and and those numbers are gonna be the probability of this particular data point being a particularly Bowl and I'm gonna find the one that's highest in front of sign at that label who classification we're doing classification so now what goes in between all this now this is a big question and many different scenarios might call for multiple layers different kinds of layers there's something called a convolutional layer which I'll get to but I'm gonna do something really simple I'm gonna have a basic dense layer which is kind of the standard building block of neural network systems and I'm gonna give it some number of nodes so for the sake of our even right now let's pretend that I just gave it four nodes and a dense layer this output is also going to be a dense layer dense layer means fully connected meaning that every input is connected to every node and then every node in the hidden layer this dense layer is connected to every output now I'm going to let your imagination draw the rest of all these connections but so this is what I want to architect so let's now go and architect this now I'm going to do this using tensorflow dot yes and the layers API if you don't know about the layers API you're gonna watch my three or four part series about the layers API tutorial but I'm bookin I sort of talk you through it while we're doing in here so you don't necessarily have to watch that okay so if I come back again this is what I built so far I have all of the training data in tensors and you can see the shape of it I have 5643 RGB values and 5643 labels nine with nine possibilities okay so the first thing that I want to do is and I'm gonna do some goofy stuff with some global variables that I might not know that you know just to make my life kind of easier I'm gonna create a variable called and my model which I'm going to create in setup at the end after I've prepared all the data I'm going to say model equals T f dot sequential TF dot sequential so that now that's that's me creating a sequential neural network model it's sequential because it's a feedforward the layers go in this order so now what I need to do is create some layers so the first thing I want to do is make the hidden let's make the output layer now let's make that we should do it in order we have to do it in order and I make the hidden layer hidden equals TF layers dense and then I put some configuration stuff so I'd make a layer by calling TF dot layers and then I specify the kind of layer this is gonna be a dense layer and then I can pass an object in as an argument and that's where I can configure things like input so I don't remember any of this let's go look it up so let's go to the documentation let's go to TF TF layers and let's go to dense where do we see that sorry I'm looking around for it and it's right there in front of my face under basic so I'm gonna make a TF layers dense I'm gonna click on that and now I'm gonna see these are all of the things that I can pass into the configuration so I need to specify the number of units the number of units is like the number of nodes and I made up four right here maybe let's try 16 maybe we want to have some more than four whatever we can make up anything we want so I'm going to now say units 16 one thing that I know I need is an activation function again I can't cover everything in this video I have other videos where I've talked about what an activation function is and how it works but the idea is the activation function is the function that takes all the sum of all of the things passing through the network being multiplied by the weights and squashes them into some range and so there probably is a really useful interesting discussion about we could have about what would be the best activation function to use right here right now maybe later hello try some different ones but just for simplicity I'm gonna use I'm gonna make a bad decision and just use sigmoid this sort of like historically original activation function of neural networks I want to use the activation function sigmoid let's see what else do I want input dimensions so this is something that I definitely need to do here because remember this this this these inputs this is not actually a lair this is a twolayer Network it looks like there's three but I'm just drawing it with three things and the inputs being but that's not a lair but I do need to specify that three things are coming in so I need to come here and say the input dimensions input dimensions is three because I have an RGB value this should do me just fine for right now so then I want to also create the output layer output TF that's gonna be dense that's going to have nine units because there are nine labels again that's completely arbitrary that's just how I happen to prepare my data set now I don't need the input dimensions because the input dimensions can be inferred by the previous one the input dimensions to the output or the number of units of the hidden so I don't need that but I do need to specify an activation function and guess what I am going to use a different activation function softmax so I'm just gonna type that in right now I will come back and explain what softmax is in a separate video which I think will be the next video of this series I'm just gonna push this a little bit further now I'm gonna say model dot add the hidden and then model dot add the output so this is now me this is now the code for exactly what I diagrammed right here three inputs into a hidden layer with number of units with some activation function into an output layer with some number of units and an activation function okay so we have now built the model here's the thing the next thing that I need to do and I'm gonna do this in the next video what I need to do is create an optimizer so let's just put this in comments create an optimizer and I need an optimization function which typically in the past I've used mean squared error but I'm gonna use something called categorical cross Troup I don't know no no but it sounds really scary but it's not and I can't its I also can't spell it so I'm gonna create the optimizer and then I'm going to compile the model and then I'm going to train the model these are the next step so they need to do this is the architecture for the model people telling me I have an error oh yeah I have something extra extra comma here but so this one we do the next video and so what I need to do in the next video this is like just a few lines of code but I need to I mean I could just add them but I would like to try to understand a bit more about what why am i have softmax here instead of sigmoid or you or any of the other activation functions and why I might choose categorical crossentropy instead of mean squared error which is if you have happened to watch my ex or tensorflow TAS coding challenge or some of my other layers tutorials I I always just use mean squared hair so that's what's coming the next video I'm going to create the optimizer I'm gonna compile the model and I'm going to talk about softmax and categorical rules and true oh wait wait wait let's actually run this and see if there's a syntax errors no okay and if I just say if I if I look in the console here at model we can see there it is this is the object and it's got all this stuff in it alright see you in the next video you
