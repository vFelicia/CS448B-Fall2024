With timestamps:

00:00 - welcome back I'm going to actually write
00:03 - some code in this video um not that much
00:06 - so what I'm doing now welcome I made a
00:09 - few introductory videos covered some
00:12 - background uh about uh neural networks
00:15 - and why they exist and where I'm trying
00:16 - to go with this and in this video I'm
00:19 - going to actually begin to write the
00:21 - code for a simple JavaScript neural
00:24 - network library now I've actually
00:25 - already done this it exists here at this
00:28 - repository uh GitHub sh man/
00:31 - neural-network P5 I'm designing this
00:33 - library to be used with a set of p5js
00:36 - examples with a uh Library a JavaScript
00:39 - library called P5 although ultimately
00:41 - this Library stands alone on itself by
00:44 - itself you don't have to use it with
00:45 - just P5 so before I can write the code
00:49 - let me come over here to the Whiteboard
00:51 - and this is where I last left off
00:53 - talking about how the general structure
00:57 - of a neural network library Works uh of
00:59 - a neur Network system
01:01 - works and so what I need to do
01:04 - here is when in the code I
01:10 - create a neural network I want to create
01:15 - three
01:17 - things I want to create an input
01:22 - layer I want to create a hidden
01:27 - layer and I want to create
01:30 - an output
01:31 - layer so when I create a
01:35 - new the way I want to design this
01:38 - libraries I want to say new neural
01:40 - network and I want to give it can you
01:43 - see this I think you can I want to give
01:44 - it three
01:45 - arguments the number of input neurons
01:49 - let's just use the word neurons the
01:52 - number of hidden neurons and the number
01:54 - of output neurons so I'm doing something
01:57 - which I typically don't do which is
01:59 - usually like to have a specific problem
02:01 - that I'm trying to solve and like write
02:03 - the code for that problem and in here
02:05 - the problem that I want to solve is I
02:06 - want to make a generic kind of useful
02:09 - library that could be used in a bunch of
02:11 - different contexts so I don't know what
02:13 - those numbers are going to be uh I don't
02:16 - know what the data is I'm just kind of
02:17 - working on the skeleton the structure of
02:19 - the library before I start to apply it
02:21 - to things so let's just make up some
02:23 - numbers let's say there's going to be
02:28 - three input neur
02:30 - neur four hidden neurons and two output
02:36 - neurons what this means now is in a feed
02:39 - forward neural network there are three
02:43 - inputs we could imagine again I'm using
02:45 - the kind of classic example of guessing
02:48 - the price of a house this could be
02:51 - number of bedrooms number of bathrooms
02:54 - square footage so those are like three
02:56 - parameters of a house these will connect
02:59 - to one 2 3 four hidden neurons so this
03:04 - is the input
03:05 - layer this is the hidden
03:08 - layer and then I'm kind of running out
03:10 - of space here there will be two
03:13 - outputs and then this is the output
03:17 - layer so this is the configuration the
03:21 - idea of a up and so what I'm building
03:24 - here is what's known as this is a
03:25 - multi-layered perceptron these are
03:28 - individual perceptron units essentially
03:30 - that are have multiple layers and it
03:33 - also is an another important term that I
03:36 - want to add here is I want to create a
03:39 - fully connected
03:42 - Network and now there are variations to
03:44 - this that we might see in future
03:46 - examples but the idea of a fully
03:47 - connected network is that every input is
03:51 - connected to every hidden every hidden
03:54 - is connected to every output but I so I
03:56 - can draw those connections and it's not
03:58 - so many that I you know if I were doing
04:00 - some kind of post production I would
04:03 - speed this up but I'm going to just draw
04:06 - this web of all these connections so
04:08 - every input is connected to every hidden
04:11 - and every
04:13 - hidden is connected to every
04:16 - output whoa oops ah ah I messed this up
04:21 - but I'll get it eventually there we go
04:25 - right so you can see that every every
04:27 - node is connected to every node in the
04:29 - next layer so the idea is that those
04:32 - three inputs come
04:34 - in the data feeds forward and those two
04:37 - outputs come out so this is the
04:39 - structure now we have to get into a lot
04:41 - of details here well how do I keep track
04:43 - of all of these connections how do I
04:45 - actually do the loops to like do all the
04:47 - sums of everything and how do I read the
04:49 - outputs I'm going to get to all that but
04:51 - this is the overall structure so let's
04:53 - go back to the
04:55 - code and now let's actually try to like
04:58 - write a little bit of this Library very
04:59 - very
05:01 - little so where am I going here okay so
05:04 - this is my code there's nothing yet I'm
05:05 - going to create a new file and I'm going
05:08 - to call this nn. JS so this is now going
05:12 - to be my so here's the thing ultimately
05:16 - I want this to be like a proper
05:20 - JavaScript library but ultimately what
05:22 - is a JavaScript library but a file with
05:24 - some JavaScript in it so I might later
05:26 - as this gets more sophisticated optimize
05:29 - it and use some sort of like build
05:31 - process or break it up into multiple
05:33 - files but right now I just want to kind
05:34 - of like get the pieces going so I am
05:37 - going to I'm also going to use es5
05:40 - syntax this is the trajectory that I've
05:43 - been on soon in future videos I'm going
05:46 - to start adopting some es6 syntax But
05:49 - ultimately maybe this Library I'll do a
05:51 - followup and come back and kind of I'm
05:52 - going to do a lot of things maybe not in
05:54 - the most optimal or efficient way but
05:56 - hopefully in the most easy to understand
05:57 - and follow way so I want to create a
06:00 - Constructor
06:02 - function called neural
06:05 - network okay and I should also mention
06:07 - again while we're here that I built this
06:10 - Library already and when I built it I
06:12 - based just about everything out of this
06:13 - book called make your own neural network
06:15 - by trq rashed and so while I'm doing
06:18 - this now kind of a bit more on the Fly
06:19 - I'm sure everything that's in my brain
06:21 - ultimately came from here and probably
06:23 - some other sources too okay
06:26 - so what do I want to do the core thing
06:29 - that I want to do is I want to create
06:31 - the N neural network with a certain
06:33 - number of input nodes number Hidden
06:35 - number of output so I'm going to add
06:38 - those as arguments here I'm going to say
06:42 - um number of input number of hidden
06:48 - number of output I'm going to create a
06:50 - neural network with three arguments and
06:52 - then I'm going to say uh input nodes I
06:56 - think I'm going to be long-winded about
06:58 - this equals number of
07:00 - input and so I'm going to create three
07:03 - uh hidden nodes is this argument and uh
07:10 - output nodes is this argument is that an
07:14 - O yes it is okay so this is we've
07:17 - actually written some code the idea
07:19 - being that what I want to do is say
07:22 - things
07:23 - like VAR
07:27 - brain and brain is a new new
07:31 - neural network that has three inputs
07:34 - with three hidden and one output right
07:36 - this is the idea so I need to figure out
07:39 - what shape and shap using the word shape
07:41 - very specifically does the data come in
07:45 - that's how many input nodes I want what
07:50 - shape is the output that I want am I
07:52 - looking for a single output am I trying
07:54 - to look for a range of outputs that's
07:56 - how many outputs I want then how many
07:58 - hidden neur so I want well that's kind
08:00 - of an open question well maybe I want as
08:03 - many as I could possibly fit in well the
08:06 - program running reasonably fast but it
08:08 - sort of depends on the complexity of the
08:09 - problem and we'll come back to that
08:11 - later and I should also note that I this
08:14 - is a simp oversimplification of how
08:16 - neural network architectures can be this
08:19 - is by definition a thre layer Network
08:21 - and this library is only going to allow
08:23 - for a three- layer Network an input a
08:26 - single hidden and a sing and an output
08:28 - but as something might think about for
08:30 - the future how would you write the code
08:31 - to have multiple hidden layers because a
08:34 - lot of neural network-based Learning
08:36 - Systems need multiple hidden layers to
08:38 - be able to perform optimally but for now
08:41 - I'm going to keep things very simple
08:43 - okay what is the next
08:46 - step written very we did write some code
08:50 - thankfully wrote some code now we got to
08:52 - stop again the next step is the
08:55 - feed
08:57 - forward process
09:01 - the way that the feed forward process
09:04 - works is that we
09:07 - receive these
09:10 - inputs oh there's so much to do so many
09:13 - pieces to this puzzle I'm excited to get
09:15 - through it all though so let's just say
09:18 - for example we're looking at this hidden
09:20 - neuron do you remember from the
09:22 - perceptron videos maybe you didn't watch
09:24 - those so let's talk about it the idea is
09:28 - that we need to do something called a
09:31 - weighted sum so let's pretend this is
09:35 - the house prediction thing and this was
09:37 - the number of bedrooms three this is the
09:39 - number of bathrooms you know this is the
09:41 - number the square
09:43 - feet so each one of these connections
09:47 - right the data is going to flow in the
09:49 - data comes in here the number three
09:51 - comes in here and then look at this
09:53 - there's four outgoing
09:56 - connections each one of those
09:58 - connections has a weight to it now
10:01 - ultimately the whole point of doing this
10:03 - learning neural network based Learning
10:05 - System is we want to tweak those weights
10:08 - we want to train the brain train the
10:10 - neural network to have optimal weights
10:13 - to get good results results that makes
10:15 - sense and that training process is
10:17 - something that I'm going to get to I
10:19 - don't know how many videos down the road
10:20 - from now but not too far away these
10:23 - weights will H typically to start one
10:26 - way of thinking about them is they're
10:27 - going to just have random values between
10:29 - -1 and one and there's a wide variety of
10:32 - techniques and strategies for
10:34 - initializing random weights or not just
10:37 - random to a neural network but for right
10:39 - now a good way for us to get started
10:40 - they all have random
10:42 - weights so even though I'm looking at
10:45 - each one of these flowing out a slightly
10:47 - better way for me to look at this with
10:49 - you is actually just look at all the
10:51 - connections flowing in so this
10:53 - particular hidden neuron has three
10:56 - connections flowing in a three
10:59 - and the input values are three 2 and
11:01 - 1,00 each one of those has a weight so
11:04 - let's pretend this is like 0.5 U let's
11:07 - say this is like uh
11:10 - .5 and this particular weight is one so
11:14 - I'm making using very very simple
11:15 - numbers the idea is that each hidden
11:18 - neuron does something called a weighted
11:22 - sum so it takes the input multiplied by
11:25 - the weight and adds that to the other
11:28 - input multiplied by the weight and adds
11:30 - that to the other input multiplied by
11:32 - the weight so we could actually do this
11:33 - 3 * .5 is
11:36 - 1.5 plus 2 * .5 is -1 plus 1,00 * 1 is +
11:45 - 1,000 so this value now is a
11:50 - [Music]
11:51 - 100.5 now we can see there's a huge flaw
11:54 - here which is that the fact that square
11:57 - footage is kind of a big number and
12:00 - number of bedrooms and number of
12:02 - bathrooms are small numbers means this
12:04 - kind of way of summing it is going to
12:05 - produce some like odd results this the
12:08 - square footage is going to be weighted
12:09 - so heavily just by the fact that it's
12:11 - bigger numbers so a lot of time in
12:13 - working with a machine learning or
12:15 - neural network based system we need to
12:16 - do some type of cleaning or normalizing
12:19 - of the data and we might do something
12:20 - where we you know we sample this down so
12:24 - they you know we actually do the number
12:25 - of bedrooms between 0 and five as a
12:27 - value between 0 and one and number of
12:29 - bathroom is always a value between 0 1
12:30 - and square footage this would actually
12:32 - turn into 0.1 like because the range is
12:33 - between 0 and 10,000 square feet or
12:35 - something so we would do some kind of
12:37 - normalization of these values but this
12:39 - is again further down the road when we
12:40 - start to apply the library in an actual
12:43 - project once this weighted sum is
12:46 - complete the result of that weighted sum
12:50 - gets sent out through the outgoing
12:53 - connections but it gets passed through
12:55 - an activation
12:57 - function so I'm going to come back to
12:59 - the activation function this is
13:01 - something we did with the perceptron and
13:02 - that's going to be a separate video
13:04 - where we look at different activation
13:05 - functions and how they work right now I
13:08 - want to focus on this weighted
13:11 - sum so I could keep going here I could
13:14 - create some type of array of I could
13:19 - create an object that's like each one of
13:21 - these nodes or neurons is an object then
13:23 - I could iterate over I could have
13:25 - connection objects so there's a bunch of
13:27 - different approaches I could take
13:29 - but the classic and standard approach is
13:33 - actually to look at storing all of these
13:37 - weighted Connections in something called
13:39 - a
13:42 - matrix which is really just like a
13:44 - spreadsheet a grid of numbers looking at
13:48 - the inputs as an array and doing some
13:51 - type of math that basically takes take
13:54 - that array of inputs multiply it by that
13:57 - Matrix
13:59 - of weights and
14:01 - generate the outputs of this hidden
14:05 - layer so this is so give me a second
14:09 - here I'm going to erase I'm G to I'm GNA
14:11 - I'm going to make the case for this with
14:12 - a simpler scenario so let's look at this
14:15 - diagram which just has fewer connections
14:17 - it's going to be easier for us to unpack
14:19 - so we can think of these inputs as x0
14:22 - and X1 let's not even worry about the
14:25 - output right now these are the inputs
14:28 - this is the hidden l layer right hidden
14:30 - layer so let's think about this and and
14:34 - actually let me change these numbers to
14:36 - X1 and X2 you know sometimes I like to
14:39 - count from zero sometimes I like to
14:40 - count from one I don't know why but I I
14:42 - I feel like in this case let's let's
14:44 - call it one and two so this is really
14:46 - like hidden one hidden two so each one
14:50 - of these connections right each one of
14:52 - these weights you could say here this is
14:55 - a weight that goes from one to one this
14:59 - is a weight that goes from one to two
15:03 - this is a weight that goes
15:07 - from 2 to one and this is a weight right
15:11 - here that goes from two to
15:14 - two so notice how there are two inputs
15:19 - two hidden neurons four
15:22 - weights in other
15:24 - words the weights and I'm going to draw
15:27 - I'm going to kind of use start to use
15:29 - Matrix notation a little bit the weights
15:32 - can be expressed like this 1
15:35 - 1 1
15:38 - 2 2 1 2 two okay so this is a way of
15:45 - expressing the weights and a way of
15:47 - expressing the
15:49 - inputs I could write it like this X1
15:53 - X2 okay so I'm making the case that I
15:58 - have two inputs and I have four weights
16:01 - and I could write it out like a matrix
16:04 - of numbers a 2X two
16:07 - Matrix and this is essentially a 2X one
16:11 - Matrix whenever I'm going to get more
16:13 - into matrices in the next video or am I
16:17 - in that video already I don't even
16:18 - remember I don't know where I am in my
16:21 - world but uh typically when we talk
16:24 - about a matrix a grid of numbers we
16:26 - reference it rows by columns 2 by two 2
16:33 - by one okay so let me just show you
16:36 - something remember this we need a
16:38 - weighted sum here this weighted sum is
16:46 - X1
16:48 - times weight 1
16:51 - one plus
16:56 - X2 times
16:59 - weight 2
17:02 - one okay that's the weighted sum for
17:05 - this neuron or node the weighted sum for
17:08 - this neuron or node is
17:11 - X1 times the weight from 1 to
17:15 - 2 and X2 times the weight of 2 to
17:21 - 2 plus X2 * the weight of 2: two
17:29 - it so
17:31 - happens I could take these two results I
17:34 - could call this
17:36 - like uh H1 and call this
17:40 - H2 and I could say let me actually say I
17:44 - could I could basically say this times
17:48 - this equals H1
17:53 - H2 so this is the actual math the way
17:55 - that we described it look at both inputs
17:58 - coming in multiplied by their weights
18:00 - and summed look at both inputs coming in
18:03 - multipli by their weights and summed
18:06 - these are the the it written out but it
18:09 - so just happens that this exact math
18:12 - writing it like this and producing this
18:15 - outcome is exactly the math that is part
18:19 - of a field of study called linear
18:22 - algebra linear algebra involves
18:25 - manipulating vectors and matrices a
18:28 - vector being being a one-dimensional
18:30 - list of values a matrix being a
18:33 - two-dimensional list of values the
18:35 - inputs are always onedimensional the
18:38 - outputs are always onedimensional the
18:40 - weights are always can always be
18:43 - expressed as twood dimensionals it's
18:45 - every input connected to every hidden
18:48 - you can think of it very much like
18:50 - pixels every row and every column so
18:53 - this is where I need to stop and what I
18:56 - want to do is do a few videos that cover
19:00 - this notation and math with a bit more
19:03 - detail writing a little JavaScript
19:05 - simple JavaScript Matrix
19:08 - library and ultimately once we done that
19:11 - we can come back here and see how if we
19:13 - have that Library written we can then
19:16 - use it to do the math between the inputs
19:18 - and the hidden and the hidd to the
19:21 - output and ultimately later we're also
19:23 - going to go backwards through the
19:24 - network to tweak values and and train it
19:27 - and that's we're also going to use the
19:28 - same Matrix math so this is why we need
19:32 - or why we don't need because we could
19:34 - kind of do it without it but why it's
19:36 - useful to work with this idea of linear
19:38 - algebra and I should note once again
19:41 - that if we were doing this in something
19:43 - like python using a library like
19:45 - something called nump we would get all
19:47 - of this stuff for free and there are
19:49 - JavaScript Matrix libraries and like but
19:51 - I'm going to kind of unpack some of this
19:53 - and and write a lot of the code from
19:54 - scratch just to have a sense of how it's
19:55 - working so hopefully as you were
19:57 - watching the video you saw a little
19:59 - annotation um this is actually incorrect
20:02 - I mean everything about this math is
20:03 - correct this matches this right the
20:06 - weighted sum is X1 * weight one from 1
20:11 - to one X2 * weight from 2 to one but
20:14 - actually the notation that I the way I
20:16 - wrote this Matrix as we go as I go into
20:18 - the next video where I actually look at
20:20 - how the Matrix math works this really
20:22 - should be written as one two and this
20:25 - should really be written as 21 the
20:27 - reason why that is is is this should be
20:30 - X1 * w11 + X2 * w21 which is written
20:36 - right here so that Matrix math that I'm
20:38 - going to go in more detail in the next
20:39 - video we take this row and multiply it
20:43 - by this column and this row and multiply
20:45 - it by this column and you can see that's
20:47 - what these two things are okay so thanks
20:50 - for bearing with me I there's a lot of
20:52 - little pieces but I am going to get back
20:54 - into the code so in the next video I'm
20:56 - not very confident about the order I'm
20:58 - doing on this in but it's just the way
20:59 - that I'm going to choose to build it and
21:01 - so the ne again I'm saying this again
21:03 - the next video I'm going to look at the
21:05 - Matrix math again and then write a
21:07 - generic library that does that math and
21:10 - then come back and put it back into the
21:12 - neural network itself okay so see you in
21:14 - the next video thanks
21:20 - [Music]

Cleaned transcript:

welcome back I'm going to actually write some code in this video um not that much so what I'm doing now welcome I made a few introductory videos covered some background uh about uh neural networks and why they exist and where I'm trying to go with this and in this video I'm going to actually begin to write the code for a simple JavaScript neural network library now I've actually already done this it exists here at this repository uh GitHub sh man/ neuralnetwork P5 I'm designing this library to be used with a set of p5js examples with a uh Library a JavaScript library called P5 although ultimately this Library stands alone on itself by itself you don't have to use it with just P5 so before I can write the code let me come over here to the Whiteboard and this is where I last left off talking about how the general structure of a neural network library Works uh of a neur Network system works and so what I need to do here is when in the code I create a neural network I want to create three things I want to create an input layer I want to create a hidden layer and I want to create an output layer so when I create a new the way I want to design this libraries I want to say new neural network and I want to give it can you see this I think you can I want to give it three arguments the number of input neurons let's just use the word neurons the number of hidden neurons and the number of output neurons so I'm doing something which I typically don't do which is usually like to have a specific problem that I'm trying to solve and like write the code for that problem and in here the problem that I want to solve is I want to make a generic kind of useful library that could be used in a bunch of different contexts so I don't know what those numbers are going to be uh I don't know what the data is I'm just kind of working on the skeleton the structure of the library before I start to apply it to things so let's just make up some numbers let's say there's going to be three input neur neur four hidden neurons and two output neurons what this means now is in a feed forward neural network there are three inputs we could imagine again I'm using the kind of classic example of guessing the price of a house this could be number of bedrooms number of bathrooms square footage so those are like three parameters of a house these will connect to one 2 3 four hidden neurons so this is the input layer this is the hidden layer and then I'm kind of running out of space here there will be two outputs and then this is the output layer so this is the configuration the idea of a up and so what I'm building here is what's known as this is a multilayered perceptron these are individual perceptron units essentially that are have multiple layers and it also is an another important term that I want to add here is I want to create a fully connected Network and now there are variations to this that we might see in future examples but the idea of a fully connected network is that every input is connected to every hidden every hidden is connected to every output but I so I can draw those connections and it's not so many that I you know if I were doing some kind of post production I would speed this up but I'm going to just draw this web of all these connections so every input is connected to every hidden and every hidden is connected to every output whoa oops ah ah I messed this up but I'll get it eventually there we go right so you can see that every every node is connected to every node in the next layer so the idea is that those three inputs come in the data feeds forward and those two outputs come out so this is the structure now we have to get into a lot of details here well how do I keep track of all of these connections how do I actually do the loops to like do all the sums of everything and how do I read the outputs I'm going to get to all that but this is the overall structure so let's go back to the code and now let's actually try to like write a little bit of this Library very very little so where am I going here okay so this is my code there's nothing yet I'm going to create a new file and I'm going to call this nn. JS so this is now going to be my so here's the thing ultimately I want this to be like a proper JavaScript library but ultimately what is a JavaScript library but a file with some JavaScript in it so I might later as this gets more sophisticated optimize it and use some sort of like build process or break it up into multiple files but right now I just want to kind of like get the pieces going so I am going to I'm also going to use es5 syntax this is the trajectory that I've been on soon in future videos I'm going to start adopting some es6 syntax But ultimately maybe this Library I'll do a followup and come back and kind of I'm going to do a lot of things maybe not in the most optimal or efficient way but hopefully in the most easy to understand and follow way so I want to create a Constructor function called neural network okay and I should also mention again while we're here that I built this Library already and when I built it I based just about everything out of this book called make your own neural network by trq rashed and so while I'm doing this now kind of a bit more on the Fly I'm sure everything that's in my brain ultimately came from here and probably some other sources too okay so what do I want to do the core thing that I want to do is I want to create the N neural network with a certain number of input nodes number Hidden number of output so I'm going to add those as arguments here I'm going to say um number of input number of hidden number of output I'm going to create a neural network with three arguments and then I'm going to say uh input nodes I think I'm going to be longwinded about this equals number of input and so I'm going to create three uh hidden nodes is this argument and uh output nodes is this argument is that an O yes it is okay so this is we've actually written some code the idea being that what I want to do is say things like VAR brain and brain is a new new neural network that has three inputs with three hidden and one output right this is the idea so I need to figure out what shape and shap using the word shape very specifically does the data come in that's how many input nodes I want what shape is the output that I want am I looking for a single output am I trying to look for a range of outputs that's how many outputs I want then how many hidden neur so I want well that's kind of an open question well maybe I want as many as I could possibly fit in well the program running reasonably fast but it sort of depends on the complexity of the problem and we'll come back to that later and I should also note that I this is a simp oversimplification of how neural network architectures can be this is by definition a thre layer Network and this library is only going to allow for a three layer Network an input a single hidden and a sing and an output but as something might think about for the future how would you write the code to have multiple hidden layers because a lot of neural networkbased Learning Systems need multiple hidden layers to be able to perform optimally but for now I'm going to keep things very simple okay what is the next step written very we did write some code thankfully wrote some code now we got to stop again the next step is the feed forward process the way that the feed forward process works is that we receive these inputs oh there's so much to do so many pieces to this puzzle I'm excited to get through it all though so let's just say for example we're looking at this hidden neuron do you remember from the perceptron videos maybe you didn't watch those so let's talk about it the idea is that we need to do something called a weighted sum so let's pretend this is the house prediction thing and this was the number of bedrooms three this is the number of bathrooms you know this is the number the square feet so each one of these connections right the data is going to flow in the data comes in here the number three comes in here and then look at this there's four outgoing connections each one of those connections has a weight to it now ultimately the whole point of doing this learning neural network based Learning System is we want to tweak those weights we want to train the brain train the neural network to have optimal weights to get good results results that makes sense and that training process is something that I'm going to get to I don't know how many videos down the road from now but not too far away these weights will H typically to start one way of thinking about them is they're going to just have random values between 1 and one and there's a wide variety of techniques and strategies for initializing random weights or not just random to a neural network but for right now a good way for us to get started they all have random weights so even though I'm looking at each one of these flowing out a slightly better way for me to look at this with you is actually just look at all the connections flowing in so this particular hidden neuron has three connections flowing in a three and the input values are three 2 and 1,00 each one of those has a weight so let's pretend this is like 0.5 U let's say this is like uh .5 and this particular weight is one so I'm making using very very simple numbers the idea is that each hidden neuron does something called a weighted sum so it takes the input multiplied by the weight and adds that to the other input multiplied by the weight and adds that to the other input multiplied by the weight so we could actually do this 3 * .5 is 1.5 plus 2 * .5 is 1 plus 1,00 * 1 is + 1,000 so this value now is a 100.5 now we can see there's a huge flaw here which is that the fact that square footage is kind of a big number and number of bedrooms and number of bathrooms are small numbers means this kind of way of summing it is going to produce some like odd results this the square footage is going to be weighted so heavily just by the fact that it's bigger numbers so a lot of time in working with a machine learning or neural network based system we need to do some type of cleaning or normalizing of the data and we might do something where we you know we sample this down so they you know we actually do the number of bedrooms between 0 and five as a value between 0 and one and number of bathroom is always a value between 0 1 and square footage this would actually turn into 0.1 like because the range is between 0 and 10,000 square feet or something so we would do some kind of normalization of these values but this is again further down the road when we start to apply the library in an actual project once this weighted sum is complete the result of that weighted sum gets sent out through the outgoing connections but it gets passed through an activation function so I'm going to come back to the activation function this is something we did with the perceptron and that's going to be a separate video where we look at different activation functions and how they work right now I want to focus on this weighted sum so I could keep going here I could create some type of array of I could create an object that's like each one of these nodes or neurons is an object then I could iterate over I could have connection objects so there's a bunch of different approaches I could take but the classic and standard approach is actually to look at storing all of these weighted Connections in something called a matrix which is really just like a spreadsheet a grid of numbers looking at the inputs as an array and doing some type of math that basically takes take that array of inputs multiply it by that Matrix of weights and generate the outputs of this hidden layer so this is so give me a second here I'm going to erase I'm G to I'm GNA I'm going to make the case for this with a simpler scenario so let's look at this diagram which just has fewer connections it's going to be easier for us to unpack so we can think of these inputs as x0 and X1 let's not even worry about the output right now these are the inputs this is the hidden l layer right hidden layer so let's think about this and and actually let me change these numbers to X1 and X2 you know sometimes I like to count from zero sometimes I like to count from one I don't know why but I I I feel like in this case let's let's call it one and two so this is really like hidden one hidden two so each one of these connections right each one of these weights you could say here this is a weight that goes from one to one this is a weight that goes from one to two this is a weight that goes from 2 to one and this is a weight right here that goes from two to two so notice how there are two inputs two hidden neurons four weights in other words the weights and I'm going to draw I'm going to kind of use start to use Matrix notation a little bit the weights can be expressed like this 1 1 1 2 2 1 2 two okay so this is a way of expressing the weights and a way of expressing the inputs I could write it like this X1 X2 okay so I'm making the case that I have two inputs and I have four weights and I could write it out like a matrix of numbers a 2X two Matrix and this is essentially a 2X one Matrix whenever I'm going to get more into matrices in the next video or am I in that video already I don't even remember I don't know where I am in my world but uh typically when we talk about a matrix a grid of numbers we reference it rows by columns 2 by two 2 by one okay so let me just show you something remember this we need a weighted sum here this weighted sum is X1 times weight 1 one plus X2 times weight 2 one okay that's the weighted sum for this neuron or node the weighted sum for this neuron or node is X1 times the weight from 1 to 2 and X2 times the weight of 2 to 2 plus X2 * the weight of 2 two it so happens I could take these two results I could call this like uh H1 and call this H2 and I could say let me actually say I could I could basically say this times this equals H1 H2 so this is the actual math the way that we described it look at both inputs coming in multiplied by their weights and summed look at both inputs coming in multipli by their weights and summed these are the the it written out but it so just happens that this exact math writing it like this and producing this outcome is exactly the math that is part of a field of study called linear algebra linear algebra involves manipulating vectors and matrices a vector being being a onedimensional list of values a matrix being a twodimensional list of values the inputs are always onedimensional the outputs are always onedimensional the weights are always can always be expressed as twood dimensionals it's every input connected to every hidden you can think of it very much like pixels every row and every column so this is where I need to stop and what I want to do is do a few videos that cover this notation and math with a bit more detail writing a little JavaScript simple JavaScript Matrix library and ultimately once we done that we can come back here and see how if we have that Library written we can then use it to do the math between the inputs and the hidden and the hidd to the output and ultimately later we're also going to go backwards through the network to tweak values and and train it and that's we're also going to use the same Matrix math so this is why we need or why we don't need because we could kind of do it without it but why it's useful to work with this idea of linear algebra and I should note once again that if we were doing this in something like python using a library like something called nump we would get all of this stuff for free and there are JavaScript Matrix libraries and like but I'm going to kind of unpack some of this and and write a lot of the code from scratch just to have a sense of how it's working so hopefully as you were watching the video you saw a little annotation um this is actually incorrect I mean everything about this math is correct this matches this right the weighted sum is X1 * weight one from 1 to one X2 * weight from 2 to one but actually the notation that I the way I wrote this Matrix as we go as I go into the next video where I actually look at how the Matrix math works this really should be written as one two and this should really be written as 21 the reason why that is is is this should be X1 * w11 + X2 * w21 which is written right here so that Matrix math that I'm going to go in more detail in the next video we take this row and multiply it by this column and this row and multiply it by this column and you can see that's what these two things are okay so thanks for bearing with me I there's a lot of little pieces but I am going to get back into the code so in the next video I'm not very confident about the order I'm doing on this in but it's just the way that I'm going to choose to build it and so the ne again I'm saying this again the next video I'm going to look at the Matrix math again and then write a generic library that does that math and then come back and put it back into the neural network itself okay so see you in the next video thanks
