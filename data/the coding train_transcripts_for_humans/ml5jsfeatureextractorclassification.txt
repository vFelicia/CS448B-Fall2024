With timestamps:

00:00 - OK, it is time now.
00:02 - I am going to build a image
classifier using my own images.
00:07 - I am going to teach
this example to not
00:11 - say acoustic guitar or electric
guitar, but to say ukulele.
00:13 - I am going to teach this
example not to say syringe,
00:18 - but just say train whistle.
00:19 - OK, this is what
we're going to do--
00:21 - and the process that I'm
going to use is transfer .
00:23 - Learning
00:24 - I described this process
in the previous video.
00:27 - You can go back and
watch that if you want,
00:28 - or you can just keep
following with me.
00:30 - I'm going to write
the code for this.
00:32 - One thing that I
want to mention is--
00:33 - you might be wondering,
oh, why is it
00:35 - called a feature extractor?
00:39 - How does this stuff happen?
00:40 - So I want to first
mention it, and thank
00:41 - Gene Kogan for making the image
classification and regression
00:45 - transfer learning with
MobileNet examples.
00:47 - You can find these sort of
native TensorFlow.js versions
00:51 - of what I'm doing right
now here at the ml4a--
00:54 - machine learning for artists--
00:55 - website.
00:56 - It's a wonderful website
with tons of resources
00:58 - about machine learning.
00:59 - And also it's an interesting
whole discussion here,
01:02 - in terms of how should
the ml5 API work,
01:04 - or what should things be called.
01:06 - So if you're curious about how
open-source projects choose
01:09 - their names of things,
I might reference
01:12 - this particular thread.
01:14 - But the important
piece for us is
01:17 - to be at the ml5 website on the
featureExtractor documentation
01:22 - page.
01:23 - I going to need to make
heavy use of this page
01:25 - to look up what the names of
all the functions I need to type
01:27 - are in.
01:28 - And then, of course, I
should mentioned, if I just
01:30 - go down here under
Examples, Classifier
01:32 - with Feature Extractor,
this is basically
01:34 - what I'm going to build.
01:36 - The point of this video is
I'm going to build it up,
01:40 - but you could just look at this
example instead, if you want.
01:43 - But anyway, all right.
01:44 - So let's go back to the
featureExtractor documentation
01:48 - page.
01:49 - What I've got here
is the code that I
01:52 - wrote from a previous
video, a couple videos ago,
01:55 - using the MobileNet
model to classify images
01:57 - from the webcam.
01:59 - OK, so if I go to the
code, the main thing
02:02 - that I need to change
is I no longer want
02:04 - to make an ml5 imageClassifier.
02:06 - I want to make an
ml5.featureExtractor.
02:13 - And the difference
here, also, is
02:15 - I'm not going to
reference the video yet.
02:18 - So I'm just going to make
a featureExtractor built
02:21 - on top of MobileNet.
02:22 - And this callback
modelReady means
02:24 - the model has been loaded,
the MobileNet model
02:26 - is downloaded from wherever
it needed to download it from,
02:29 - and it's there and ready to go.
02:32 - So this should say
model is ready.
02:34 - I'm going to get rid of
this predict function.
02:36 - So now, if we just refresh
this-- a lot of stuff's
02:38 - going to break here--
but I see model is ready,
02:41 - and the video appears.
02:42 - Now, there's no labels
anymore, because I got rid
02:43 - of the imageClassifier--
02:44 - I have a featureExtractor now.
02:46 - But I want an
imageClassifier, so what I do
02:49 - is I'm going to add a variable.
02:52 - I'm going to write a
variable called classifier.
02:54 - So the variable mobilenet
is now referring
02:57 - to the featureExtractor,
and the classifier
03:00 - equals mobilenet,
the featureExtractor,
03:04 - dot classification.
03:06 - So I want to make--
03:09 - classification.
03:10 - I want to make a
classification object
03:14 - from the featureExtractor,
and I need to give that--
03:16 - I want to say, and I want to
use images from the video.
03:19 - And again, now, if I were doing
this with a database of JPEGs
03:22 - that I'm loading,
or something else,
03:24 - I would do it in
a different way.
03:25 - But I'm going to use the
video in this example.
03:27 - So I'm going to say
video, and then I'm
03:28 - going to say videoReady, because
I want to have a callback also
03:31 - to know that the video is ready.
03:34 - I don't really
need that callback,
03:35 - but it's sort of useful.
03:37 - And I write that up here--
03:39 - videoReady.
03:40 - And I'm going to say
a video is ready.
03:42 - So let's refresh this again, and
we should see my image pop up,
03:47 - model's ready, video's ready.
03:48 - OK, now we are ready to
train our own labels.
03:55 - We have the featureExtractor,
we have the classifier.
03:57 - We can give it
images of a ukulele--
04:00 - and I don't know if I
wrote this on the board
04:03 - just now, or earlier,
or when, but I used
04:04 - to have ukulele spelled wrong.
04:06 - It's spelled U-K-U-L-E-L-E.
I don't know why,
04:10 - but I feel like it's very
important to say right now.
04:13 - OK, so how do I do that?
04:16 - Well, I could look up in
the featureExtractor page,
04:20 - there's a function
called addImage.
04:23 - And it's right here.
04:26 - I know some of this API
from having practiced
04:28 - this a little bit,
but this is what
04:29 - I'm looking for-- addImage.
04:31 - addImage and a label.
04:32 - So I can say--
04:34 - oh, but when do I
want to add an image?
04:36 - I want to make a button that,
every time I press the button,
04:41 - I'm saying that's
a ukulele image.
04:43 - So let's first create ukeButton,
and then I'm going to say,
04:47 - in setup, ukeButton equals
createButton ukulele--
04:57 - but if it's uke, it's just
U-K-E. Boy, it's confusing.
05:00 - Now, here's the thing--
05:02 - I am creating this
example in a very basic,
05:06 - what I hope is
beginner-friendly way.
05:08 - There are so many ways you can
build an interface, and style
05:12 - your page, and handle events.
05:14 - I'm just going to use
simple p5 functions that
05:17 - place a button on the page,
and a simple callback function
05:21 - for when I press the button.
05:24 - So then I'm going to say
[? ukePressed ?] mousePressed.
05:27 - And I could put the
function name here
05:29 - and write the function
somewhere else, which
05:30 - I do in some videos, to be kind
of-- but I'm just going to put
05:33 - an anonymous function in here.
05:35 - The idea of this
anonymous function
05:37 - is, when I press the
button, this function
05:40 - should be executed.
05:40 - So the code I'm
writing in here will
05:42 - happen when I press the button.
05:44 - And what do I want to do there?
05:45 - I want to say
classifier.addImage,
05:49 - and I want to give it a label--
05:51 - ukulele.
05:53 - I hope that's spelled right.
05:55 - So now, whenever I press
the button, it's a ukulele.
05:58 - Let's add oh train whistle
one while we're at it.
06:03 - Let's say
whistleButton, and let's
06:07 - do exactly the same thing,
but with whistleButton.
06:15 - whistleButton, createButton--
I'm going to have the button
06:20 - say whistle.
06:20 - I cannot spell anything.
06:23 - whistleButton, and then
addImage("whistle").
06:26 - So you can see here
I have two buttons.
06:28 - Ukulele adds an image
assigned the label ukulele,
06:33 - whistle adds an image
assigned the label whistle.
06:36 - Now, there's not really much
point to me running this code.
06:39 - I should just make
sure I have no errors.
06:40 - We can see the
buttons are there now.
06:42 - The image comes up, but
nothing's going to happen.
06:45 - I can keep pressing
this ukulele button,
06:46 - I can keep pressing the whistle
button-- nothing's happening,
06:49 - because I'm not giving
myself any feedback.
06:53 - So right now, I just have
to hope it's working.
06:55 - But the thing that
I need to do next
06:59 - is actually apply
a training step.
07:01 - Now, one thing that
was interesting--
07:03 - I showed in the previous
video the Teachable Machine,
07:08 - this project I'm basically
making exactly the same thing.
07:12 - Train green, train
purple, train orange.
07:14 - My buttons are train
ukulele, train whistle.
07:17 - But it just started
to work immediately.
07:19 - That's because there's a
slightly different algorithm
07:21 - at play here.
07:22 - The algorithm that
I'm using in ml5
07:27 - requires an additional
step, a training step.
07:30 - I'm going to write
this here-- training.
07:33 - So basically, the process is
add a bunch of images-- say,
07:37 - hey, this is a ukulele, this
is ukulele, this is a ukulele,
07:40 - this is a whistle, this is a
whistle, this is a ukulele,
07:42 - this is a whistle,
this is a ukulele.
07:43 - Once I've added all those
images, then I explicitly say,
07:47 - I'm done with all
of these images.
07:49 - I'm going to let the model train
itself using those the features
07:54 - that it extracted
of those images,
07:56 - and map those features
basically to these labels.
07:58 - And by map, I really mean--
there's another machine
08:02 - learning model right here.
08:03 - It's actually like a
neural network model.
08:08 - And so that mapping between
the features and the new labels
08:10 - happens with another
neural network here.
08:12 - But ml5 and TensorFlow.js are
handling all of that for us.
08:17 - OK, now, let me
come back over here
08:20 - and add the training steps.
08:22 - So I certainly need
one more button--
08:25 - train choo-choo button.
08:30 - So I'm going to add that button.
08:33 - createButton trainButton--
I'm going to say train--
08:40 - trainButton-- oh, OK, now here--
now, what am I doing in here?
08:44 - OK, what I'm doing
in here is I'm
08:46 - going to say classifier.train.
08:51 - So I'm going to say, hey,
classifier, train yourself.
08:54 - Now, I could just
leave it at this
08:56 - and kind of say,
hey, great, I'm done.
08:58 - But something that
I should do here
09:01 - is I can actually put a
callback inside of here.
09:04 - Now, this is getting
pretty awkward.
09:06 - I'm going to allow it-- and you
know what, actually, though?
09:08 - I'm going to make a
separate function just for--
09:10 - because I might want to
do a bunch of things.
09:14 - I'm going to call
it whileTraining.
09:17 - So I just want to put this
in a separate function
09:20 - so I can look at it on its own.
09:22 - What did I call
it? whileTraining--
09:25 - so this function
whileTraining is a function
09:28 - that's kind of kind run over and
over again during the training
09:31 - process, and it's going to
report back to me information
09:34 - about the training process.
09:35 - And the information it's
going to report to me
09:37 - is something called loss.
09:40 - Loss.
09:40 - Let's just actually
run this, and then I'm
09:44 - going to explain what loss is.
09:45 - Let's just see if I can
get this to work so far.
09:48 - So I'm going to refresh.
09:50 - My model is ready,
my video is ready.
09:52 - I have a ukulele.
09:54 - I can press the ukulele,
ukulele, ukulele.
09:55 - I'm going to show it
a lot of ukuleles.
09:57 - Here's a lot of
examples of ukuleles.
10:00 - And now, I'm going to
hold up my train whistle.
10:02 - Here's a lot of examples
of train whistles.
10:05 - Now, I'm going to
hit the button train.
10:08 - And yes, we see this
number here in the console,
10:12 - it's training and training
a training while training
10:15 - console.log this value, loss.
10:19 - What is the loss?
10:22 - So loss is a really
important term
10:25 - in machine learning,
data science.
10:28 - It's often also called cost.
10:31 - And by this, I mean loss
function or cost function.
10:34 - And what the loss function our
cost function is calculating
10:37 - is the error.
10:38 - So what do I mean by error?
10:40 - So basically, if I say
to the machine learning
10:44 - model that's training,
hey, here's a ukulele--
10:49 - this is a ukulele,
but just pretend
10:50 - you don't know what
it is for a second.
10:52 - What do you think it is?
10:53 - And if it comes back and
says, I think it's a ukulele,
10:55 - guess what?
10:56 - The error is 0.
10:58 - If it comes back and says, I
think it's a train whistle,
11:01 - then the error is not
0, it's something else.
11:04 - But something that I haven't
really mentioned, because we're
11:06 - kind of living above
the fray here--
11:09 - we're talking in
terms of labels.
11:11 - The machine learning
model underneath the hood
11:13 - is just working with numbers.
11:15 - The labels are only a thing
for us, the human being,
11:18 - to use at the end
of the process.
11:20 - So when it's actually
calculating an error,
11:22 - even if it guesses
it's a ukulele,
11:24 - it's calculating error based
on the numeric probability
11:27 - that it guessed.
11:28 - So the machine learning
model might think, OK,
11:31 - that's 80% likely
to be a ukulele.
11:35 - So that might be
the right answer,
11:37 - but we know the right answer
is it's 100% a ukulele.
11:39 - So the error is
actually the difference
11:41 - between mean 100%
and 80%, or 0.2.
11:43 - So this idea of
the aggregate error
11:46 - of all of the training
images over time--
11:48 - ml5 and TensorFlow.js is
calculating that for you
11:52 - during that training process.
11:53 - And we're seeing that here--
11:55 - [CLOCK TICKING]
11:57 - There's a clock tick tocking,
because I have a clock sound
12:00 - effect, for some unknown reason.
12:01 - We can see that that loss
is getting very, very low.
12:03 - So you want the error to be low.
12:05 - The error started kind of
high-- it was like 6.92,
12:10 - and then it got lower and lower
lower, as it was training.
12:12 - And one thing you'll notice
is, eventually, it's stopped.
12:15 - Now, it's an arbitrary decision
when do you stop training.
12:18 - Ml5 has default settings.
12:20 - It's going to train for
a while until the loss
12:22 - of a certain amount,
or something like that.
12:23 - But you can see
when it's finished.
12:25 - It gives you the low as--
12:26 - the loss as no.
12:28 - The low.
12:28 - The loss as null.
12:30 - So I can actually say here, if
loss equals null, console.log--
12:41 - I'm going to say
training complete,
12:46 - else I can console.log the loss.
12:49 - And then guess what?
12:50 - When the training is complete,
what do I want to do?
12:53 - I want to classify.
12:54 - So I'm going to say
classifier.classify(gotResults).
13:01 - And I already have the
gotResults function.
13:06 - I'm getting results,
just like I got results
13:08 - from the raw MobileNet
model, but now I'm
13:11 - getting the results from my
new transfer learn trained,
13:16 - custom trained model.
13:17 - So I say
classifier.classify(gotResults).
13:21 - I get the results,
if there's an error.
13:23 - Otherwise, I can give the
class name to the label
13:25 - to get drawn on the screen.
13:26 - And then what I
want to do-- again,
13:28 - I want to say
classifier.classify.
13:30 - So before, it was
called predict.
13:33 - Now, it's called classify.
13:35 - I'm not sure why.
13:36 - Maybe, at some point,
check that code
13:38 - that goes along with this
video to see if that's changed.
13:40 - But that's what it is right now.
13:41 - OK, so I think we're actually
done with this example,
13:44 - sort of.
13:46 - Let's try.
13:49 - OK, model is ready,
video is ready.
13:51 - Oh, boy.
13:52 - What's the chance that
this is going to work?
13:54 - Stop and guess.
13:55 - I give it like 10
to 1 this works.
13:59 - OK, I'm going to step
out of the frame,
14:01 - and I'm going to train it with
a bunch of images of a ukulele.
14:06 - I'm going to move
the ukulele around
14:07 - to give it a lot of
different examples.
14:09 - Further back-- and
again, I should probably
14:14 - add something that shows me
how many training examples,
14:16 - because I probably
want to give around
14:18 - the same number of training
examples for the whistle
14:20 - as with the ukulele.
14:21 - I'm going to train this.
14:23 - Whistle, whistle,
whistle, whistle, whistle.
14:25 - Now, I'm going to hit train.
14:28 - When it's done, we should
start seeing labels.
14:30 - Training complete.
14:31 - I don't see labels.
14:35 - No labels, that's so sad.
14:37 - What did I do wrong?
14:38 - OK, so something's wrong.
14:40 - I got to figure this out.
14:41 - I got to debug this.
14:42 - All right, I really should
put console.dot(results)
14:45 - in, because I don't even know if
this function is being called.
14:49 - Let's make sure dot classify is
the right name of the function.
14:53 - Classify, callback-- yeah.
14:58 - Oh, no, no-- yeah, callback.
15:02 - And then the callback is a
function, otherwise blah,
15:05 - blah, blah.
15:06 - So I think this is right.
15:09 - Everything looks right to me.
15:11 - Am I just not drawing
the label properly?
15:13 - No, there it is, label--
15:18 - This is me not paying
close attention.
15:21 - This is interesting, and
this is now a question
15:24 - for the ml5 library itself.
15:27 - But the thing that comes
in the gotResults function
15:31 - is actually just the
label that it guesses.
15:34 - So there's not like the class
name and the probability,
15:36 - all that stuff that
came with MobileNet.
15:38 - It's not giving us
all that information,
15:40 - which probably makes sense.
15:41 - So actually, all I want to do
is say label equals results.
15:48 - So that was a little
bit of a digression.
15:50 - I had to figure out
what I did wrong there.
15:52 - Now, this should work.
15:55 - OK, ready?
15:56 - Let's try it with the ukulele.
16:01 - Giving it a lot of
examples of a ukulele.
16:06 - Giving it that a lot of
examples of a whistle.
16:12 - Now, I'm going to train it.
16:13 - I'm going to do
my training dance.
16:17 - And training is complete.
16:21 - Here's a ukulele.
16:24 - It is a ukulele.
16:29 - It is a whistle.
16:33 - It is a ukulele.
16:36 - It is a whistle.
16:39 - Yay, OK.
16:42 - So this actually works.
16:43 - Here's the thing--
16:43 - I want to show you something
more interesting here.
16:47 - It's not more interesting--
16:48 - I'm going to show you
something different.
16:50 - I'm going to leave all the
variable names the same,
16:53 - but I might change
this to smile,
16:55 - I'm going to change it to happy,
I'm going change this to sad.
17:02 - OK, so I'm going
to run this again,
17:06 - and I'm going to do this.
17:10 - And we get lots of
pictures of me smiling.
17:19 - Then I'm going to hit train,
let it train for a little bit.
17:24 - It's finished.
17:38 - How long should I do that for?
17:39 - This is pretty cool.
17:40 - Here's the thing--
it doesn't just
17:42 - have to be image recognition.
17:43 - I can training with
different facial expressions.
17:47 - What MobileNet is really
good at, if you recall,
17:50 - is taking an image
and boiling it
17:52 - down to a smaller
list of numbers
17:54 - that kind of define the
essence of that image.
17:56 - So it doesn't matter
what the content is,
17:58 - it can figure out the essence
of me smiling, the essence of me
18:01 - frowning.
18:02 - It is important,
though, to remember
18:04 - it's not really learning smiling
or frowning, it's kind of--
18:08 - and if I were to
turn this this way,
18:11 - you can see some of the
room here-- and I go here,
18:19 - it's not working so well,
because it learned all
18:23 - that stuff with the background.
18:25 - So there's a lot
of nuance to this,
18:27 - and it's really
important to remember.
18:29 - It's not some magic.
18:31 - The computer doesn't
have suddenly
18:32 - some understanding of emotions.
18:35 - It's really just looking
at, I've got a new image,
18:37 - and I'm comparing it to a
bunch of images I got before.
18:41 - What's it most similar like?
18:42 - Let's see if it--
18:43 - now that I moved the camera
around-- is it working.
18:50 - So here's the thing--
18:53 - go.
18:54 - Go forth.
18:55 - Add a third category.
18:57 - Make a project.
18:58 - Make a game where
I'm playing Pong--
19:00 - actually, this is a project I
should reference by Alejandro.
19:03 - Let me go to ml5js.org.
19:06 - Alejandro, under
experiments, made
19:09 - a project called Pong
ML, which basically
19:12 - is the same exact technique to
train various hand gestures,
19:17 - and then you could play
Pong with hand gestures.
19:19 - So there's so many possibilities
of what you could do just
19:22 - from being able to train
starting with the MobileNet
19:26 - model, and applying
transfer learning.
19:30 - OK, are you with me?
19:31 - Did this makes sense?
19:32 - Make something.
19:33 - Go and try this.
19:34 - Make your own version of it.
19:35 - Really think about
the interface.
19:37 - And one more thing--
19:39 - something you're really
going to want to do--
19:40 - and you've probably
noticed this--
19:42 - every time I
restarted the sketch,
19:44 - I had to retrain
with images again,
19:46 - which is fun for the sort of
real-time interactive nature
19:49 - of this.
19:50 - But if you're actually going
to do this for a while,
19:53 - you're building an
installation or something,
19:55 - you've trained it
with a bunch of images
19:56 - and you want to, I don't know,
save the model that you've
20:00 - done, save all that work,
you're going to want to save
20:03 - and be able to reload the result
of that transfer learning.
20:06 - At present, at the
time of this recording,
20:08 - there isn't a way to do that
easily with the ml5 library.
20:11 - It's certainly
technically possible.
20:13 - There is a GitHub issue.
20:14 - Currently, it is
ml5-library/issues/174 with
20:18 - a discussion about this feature.
20:19 - Certainly, once
this feature exists,
20:22 - I will come back
and make a video
20:24 - to show you how to do that.
20:25 - OK, thanks for
watching this video.
20:27 - Oh, I'm going to
do one more video.
20:29 - I'm going to something
interesting--
20:31 - hopefully.
20:31 - I'm going to do one more
video with something
20:33 - very similar to what
I did just here,
20:35 - but I'm going to do something
called a regression.
20:37 - So I'm going to define what
a regression is, and show you
20:39 - how I can do transfer learning
with the MobileNet model
20:42 - to perform a regression.
20:43 - Why would I want to do that?
20:44 - I don't know.
20:45 - I guess we'll have to watch.
20:46 - Basically a slider.
20:47 - If I want to be able to control
something with my hand moving
20:49 - back and forth as
a slider, that's
20:50 - something that I can
use a regression for.
20:52 - All right, see you there.
20:54 - [MUSIC PLAYING]

Cleaned transcript:

OK, it is time now. I am going to build a image classifier using my own images. I am going to teach this example to not say acoustic guitar or electric guitar, but to say ukulele. I am going to teach this example not to say syringe, but just say train whistle. OK, this is what we're going to do and the process that I'm going to use is transfer . Learning I described this process in the previous video. You can go back and watch that if you want, or you can just keep following with me. I'm going to write the code for this. One thing that I want to mention is you might be wondering, oh, why is it called a feature extractor? How does this stuff happen? So I want to first mention it, and thank Gene Kogan for making the image classification and regression transfer learning with MobileNet examples. You can find these sort of native TensorFlow.js versions of what I'm doing right now here at the ml4a machine learning for artists website. It's a wonderful website with tons of resources about machine learning. And also it's an interesting whole discussion here, in terms of how should the ml5 API work, or what should things be called. So if you're curious about how opensource projects choose their names of things, I might reference this particular thread. But the important piece for us is to be at the ml5 website on the featureExtractor documentation page. I going to need to make heavy use of this page to look up what the names of all the functions I need to type are in. And then, of course, I should mentioned, if I just go down here under Examples, Classifier with Feature Extractor, this is basically what I'm going to build. The point of this video is I'm going to build it up, but you could just look at this example instead, if you want. But anyway, all right. So let's go back to the featureExtractor documentation page. What I've got here is the code that I wrote from a previous video, a couple videos ago, using the MobileNet model to classify images from the webcam. OK, so if I go to the code, the main thing that I need to change is I no longer want to make an ml5 imageClassifier. I want to make an ml5.featureExtractor. And the difference here, also, is I'm not going to reference the video yet. So I'm just going to make a featureExtractor built on top of MobileNet. And this callback modelReady means the model has been loaded, the MobileNet model is downloaded from wherever it needed to download it from, and it's there and ready to go. So this should say model is ready. I'm going to get rid of this predict function. So now, if we just refresh this a lot of stuff's going to break here but I see model is ready, and the video appears. Now, there's no labels anymore, because I got rid of the imageClassifier I have a featureExtractor now. But I want an imageClassifier, so what I do is I'm going to add a variable. I'm going to write a variable called classifier. So the variable mobilenet is now referring to the featureExtractor, and the classifier equals mobilenet, the featureExtractor, dot classification. So I want to make classification. I want to make a classification object from the featureExtractor, and I need to give that I want to say, and I want to use images from the video. And again, now, if I were doing this with a database of JPEGs that I'm loading, or something else, I would do it in a different way. But I'm going to use the video in this example. So I'm going to say video, and then I'm going to say videoReady, because I want to have a callback also to know that the video is ready. I don't really need that callback, but it's sort of useful. And I write that up here videoReady. And I'm going to say a video is ready. So let's refresh this again, and we should see my image pop up, model's ready, video's ready. OK, now we are ready to train our own labels. We have the featureExtractor, we have the classifier. We can give it images of a ukulele and I don't know if I wrote this on the board just now, or earlier, or when, but I used to have ukulele spelled wrong. It's spelled UKULELE. I don't know why, but I feel like it's very important to say right now. OK, so how do I do that? Well, I could look up in the featureExtractor page, there's a function called addImage. And it's right here. I know some of this API from having practiced this a little bit, but this is what I'm looking for addImage. addImage and a label. So I can say oh, but when do I want to add an image? I want to make a button that, every time I press the button, I'm saying that's a ukulele image. So let's first create ukeButton, and then I'm going to say, in setup, ukeButton equals createButton ukulele but if it's uke, it's just UKE. Boy, it's confusing. Now, here's the thing I am creating this example in a very basic, what I hope is beginnerfriendly way. There are so many ways you can build an interface, and style your page, and handle events. I'm just going to use simple p5 functions that place a button on the page, and a simple callback function for when I press the button. So then I'm going to say [? ukePressed ?] mousePressed. And I could put the function name here and write the function somewhere else, which I do in some videos, to be kind of but I'm just going to put an anonymous function in here. The idea of this anonymous function is, when I press the button, this function should be executed. So the code I'm writing in here will happen when I press the button. And what do I want to do there? I want to say classifier.addImage, and I want to give it a label ukulele. I hope that's spelled right. So now, whenever I press the button, it's a ukulele. Let's add oh train whistle one while we're at it. Let's say whistleButton, and let's do exactly the same thing, but with whistleButton. whistleButton, createButton I'm going to have the button say whistle. I cannot spell anything. whistleButton, and then addImage("whistle"). So you can see here I have two buttons. Ukulele adds an image assigned the label ukulele, whistle adds an image assigned the label whistle. Now, there's not really much point to me running this code. I should just make sure I have no errors. We can see the buttons are there now. The image comes up, but nothing's going to happen. I can keep pressing this ukulele button, I can keep pressing the whistle button nothing's happening, because I'm not giving myself any feedback. So right now, I just have to hope it's working. But the thing that I need to do next is actually apply a training step. Now, one thing that was interesting I showed in the previous video the Teachable Machine, this project I'm basically making exactly the same thing. Train green, train purple, train orange. My buttons are train ukulele, train whistle. But it just started to work immediately. That's because there's a slightly different algorithm at play here. The algorithm that I'm using in ml5 requires an additional step, a training step. I'm going to write this here training. So basically, the process is add a bunch of images say, hey, this is a ukulele, this is ukulele, this is a ukulele, this is a whistle, this is a whistle, this is a ukulele, this is a whistle, this is a ukulele. Once I've added all those images, then I explicitly say, I'm done with all of these images. I'm going to let the model train itself using those the features that it extracted of those images, and map those features basically to these labels. And by map, I really mean there's another machine learning model right here. It's actually like a neural network model. And so that mapping between the features and the new labels happens with another neural network here. But ml5 and TensorFlow.js are handling all of that for us. OK, now, let me come back over here and add the training steps. So I certainly need one more button train choochoo button. So I'm going to add that button. createButton trainButton I'm going to say train trainButton oh, OK, now here now, what am I doing in here? OK, what I'm doing in here is I'm going to say classifier.train. So I'm going to say, hey, classifier, train yourself. Now, I could just leave it at this and kind of say, hey, great, I'm done. But something that I should do here is I can actually put a callback inside of here. Now, this is getting pretty awkward. I'm going to allow it and you know what, actually, though? I'm going to make a separate function just for because I might want to do a bunch of things. I'm going to call it whileTraining. So I just want to put this in a separate function so I can look at it on its own. What did I call it? whileTraining so this function whileTraining is a function that's kind of kind run over and over again during the training process, and it's going to report back to me information about the training process. And the information it's going to report to me is something called loss. Loss. Let's just actually run this, and then I'm going to explain what loss is. Let's just see if I can get this to work so far. So I'm going to refresh. My model is ready, my video is ready. I have a ukulele. I can press the ukulele, ukulele, ukulele. I'm going to show it a lot of ukuleles. Here's a lot of examples of ukuleles. And now, I'm going to hold up my train whistle. Here's a lot of examples of train whistles. Now, I'm going to hit the button train. And yes, we see this number here in the console, it's training and training a training while training console.log this value, loss. What is the loss? So loss is a really important term in machine learning, data science. It's often also called cost. And by this, I mean loss function or cost function. And what the loss function our cost function is calculating is the error. So what do I mean by error? So basically, if I say to the machine learning model that's training, hey, here's a ukulele this is a ukulele, but just pretend you don't know what it is for a second. What do you think it is? And if it comes back and says, I think it's a ukulele, guess what? The error is 0. If it comes back and says, I think it's a train whistle, then the error is not 0, it's something else. But something that I haven't really mentioned, because we're kind of living above the fray here we're talking in terms of labels. The machine learning model underneath the hood is just working with numbers. The labels are only a thing for us, the human being, to use at the end of the process. So when it's actually calculating an error, even if it guesses it's a ukulele, it's calculating error based on the numeric probability that it guessed. So the machine learning model might think, OK, that's 80% likely to be a ukulele. So that might be the right answer, but we know the right answer is it's 100% a ukulele. So the error is actually the difference between mean 100% and 80%, or 0.2. So this idea of the aggregate error of all of the training images over time ml5 and TensorFlow.js is calculating that for you during that training process. And we're seeing that here [CLOCK TICKING] There's a clock tick tocking, because I have a clock sound effect, for some unknown reason. We can see that that loss is getting very, very low. So you want the error to be low. The error started kind of high it was like 6.92, and then it got lower and lower lower, as it was training. And one thing you'll notice is, eventually, it's stopped. Now, it's an arbitrary decision when do you stop training. Ml5 has default settings. It's going to train for a while until the loss of a certain amount, or something like that. But you can see when it's finished. It gives you the low as the loss as no. The low. The loss as null. So I can actually say here, if loss equals null, console.log I'm going to say training complete, else I can console.log the loss. And then guess what? When the training is complete, what do I want to do? I want to classify. So I'm going to say classifier.classify(gotResults). And I already have the gotResults function. I'm getting results, just like I got results from the raw MobileNet model, but now I'm getting the results from my new transfer learn trained, custom trained model. So I say classifier.classify(gotResults). I get the results, if there's an error. Otherwise, I can give the class name to the label to get drawn on the screen. And then what I want to do again, I want to say classifier.classify. So before, it was called predict. Now, it's called classify. I'm not sure why. Maybe, at some point, check that code that goes along with this video to see if that's changed. But that's what it is right now. OK, so I think we're actually done with this example, sort of. Let's try. OK, model is ready, video is ready. Oh, boy. What's the chance that this is going to work? Stop and guess. I give it like 10 to 1 this works. OK, I'm going to step out of the frame, and I'm going to train it with a bunch of images of a ukulele. I'm going to move the ukulele around to give it a lot of different examples. Further back and again, I should probably add something that shows me how many training examples, because I probably want to give around the same number of training examples for the whistle as with the ukulele. I'm going to train this. Whistle, whistle, whistle, whistle, whistle. Now, I'm going to hit train. When it's done, we should start seeing labels. Training complete. I don't see labels. No labels, that's so sad. What did I do wrong? OK, so something's wrong. I got to figure this out. I got to debug this. All right, I really should put console.dot(results) in, because I don't even know if this function is being called. Let's make sure dot classify is the right name of the function. Classify, callback yeah. Oh, no, no yeah, callback. And then the callback is a function, otherwise blah, blah, blah. So I think this is right. Everything looks right to me. Am I just not drawing the label properly? No, there it is, label This is me not paying close attention. This is interesting, and this is now a question for the ml5 library itself. But the thing that comes in the gotResults function is actually just the label that it guesses. So there's not like the class name and the probability, all that stuff that came with MobileNet. It's not giving us all that information, which probably makes sense. So actually, all I want to do is say label equals results. So that was a little bit of a digression. I had to figure out what I did wrong there. Now, this should work. OK, ready? Let's try it with the ukulele. Giving it a lot of examples of a ukulele. Giving it that a lot of examples of a whistle. Now, I'm going to train it. I'm going to do my training dance. And training is complete. Here's a ukulele. It is a ukulele. It is a whistle. It is a ukulele. It is a whistle. Yay, OK. So this actually works. Here's the thing I want to show you something more interesting here. It's not more interesting I'm going to show you something different. I'm going to leave all the variable names the same, but I might change this to smile, I'm going to change it to happy, I'm going change this to sad. OK, so I'm going to run this again, and I'm going to do this. And we get lots of pictures of me smiling. Then I'm going to hit train, let it train for a little bit. It's finished. How long should I do that for? This is pretty cool. Here's the thing it doesn't just have to be image recognition. I can training with different facial expressions. What MobileNet is really good at, if you recall, is taking an image and boiling it down to a smaller list of numbers that kind of define the essence of that image. So it doesn't matter what the content is, it can figure out the essence of me smiling, the essence of me frowning. It is important, though, to remember it's not really learning smiling or frowning, it's kind of and if I were to turn this this way, you can see some of the room here and I go here, it's not working so well, because it learned all that stuff with the background. So there's a lot of nuance to this, and it's really important to remember. It's not some magic. The computer doesn't have suddenly some understanding of emotions. It's really just looking at, I've got a new image, and I'm comparing it to a bunch of images I got before. What's it most similar like? Let's see if it now that I moved the camera around is it working. So here's the thing go. Go forth. Add a third category. Make a project. Make a game where I'm playing Pong actually, this is a project I should reference by Alejandro. Let me go to ml5js.org. Alejandro, under experiments, made a project called Pong ML, which basically is the same exact technique to train various hand gestures, and then you could play Pong with hand gestures. So there's so many possibilities of what you could do just from being able to train starting with the MobileNet model, and applying transfer learning. OK, are you with me? Did this makes sense? Make something. Go and try this. Make your own version of it. Really think about the interface. And one more thing something you're really going to want to do and you've probably noticed this every time I restarted the sketch, I had to retrain with images again, which is fun for the sort of realtime interactive nature of this. But if you're actually going to do this for a while, you're building an installation or something, you've trained it with a bunch of images and you want to, I don't know, save the model that you've done, save all that work, you're going to want to save and be able to reload the result of that transfer learning. At present, at the time of this recording, there isn't a way to do that easily with the ml5 library. It's certainly technically possible. There is a GitHub issue. Currently, it is ml5library/issues/174 with a discussion about this feature. Certainly, once this feature exists, I will come back and make a video to show you how to do that. OK, thanks for watching this video. Oh, I'm going to do one more video. I'm going to something interesting hopefully. I'm going to do one more video with something very similar to what I did just here, but I'm going to do something called a regression. So I'm going to define what a regression is, and show you how I can do transfer learning with the MobileNet model to perform a regression. Why would I want to do that? I don't know. I guess we'll have to watch. Basically a slider. If I want to be able to control something with my hand moving back and forth as a slider, that's something that I can use a regression for. All right, see you there. [MUSIC PLAYING]
