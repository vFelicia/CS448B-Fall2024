With timestamps:

00:00 - alright I'm back in part 471 of building
00:04 - a color classifier now what am I gonna
00:06 - do here in the previous video I created
00:09 - the architecture of my model a hidden
00:12 - layer and output layer a subsea tension
00:15 - fluid is sequential model to dense
00:18 - layers activation functions units etc
00:20 - now at the end of the last video the
00:22 - next thing I need to do is define an
00:24 - optimization function and then compile
00:28 - the model well I really botched that is
00:30 - what there's three things I need to do
00:31 - optimization function loss function and
00:34 - compile the model and so I kind of
00:36 - conflated optimization and loss I'm
00:38 - optimizing against the loss but the
00:41 - optimizer that I want to make is I can
00:46 - use Const I guess here I get a very
00:48 - inconsistent about winning using
00:50 - converses let maybe I'll go back and
00:51 - clean up that code at some point I'm
00:53 - gonna say I can get it from TF train
00:58 - stochastic gradient descent and I can
01:01 - create a learning rate which I'm going
01:02 - to say is like 0.2 so this so one thing
01:06 - to do is create an optimization function
01:07 - right there are different options and we
01:09 - can try other options stochastic
01:11 - gradient descent is the one that I
01:12 - basically used in almost all of my
01:14 - examples and covered in detail in my how
01:16 - to build a neural network from scratch
01:17 - series and the idea of graded descent is
01:21 - walking along trying to go down the
01:24 - graph of the loss function to minimize
01:26 - that loss so what is the loss function
01:30 - that I want well if I'm gonna say model
01:33 - dot compile I believe this is a whoops
01:37 - this is a function that I'm going to
01:39 - write with a configuration option and
01:44 - one of the things when I compile the
01:46 - model I need to specify up to optimizer
01:50 - optimizer now this is very awkward that
01:52 - I just called this up here but that's
01:57 - fine and then the other thing that I
01:59 - just specified is a loss function mean
02:01 - squared error so this is typically what
02:06 - I have done in previous examples if you
02:09 - look at my
02:10 - EXOR coding challenge but this is now
02:13 - going to change and the reason is
02:15 - because I am using an activation
02:17 - function called softmax so let's talk
02:19 - about what softmax is softmax question
02:26 - mark ok so remember the output that we
02:31 - want from the neural network is a
02:33 - probability distribution right what's an
02:36 - example of what an output might look
02:39 - like it might look like this there's
02:40 - nine values 0.1 0.1 0.2 0 zero zero zero
02:47 - point seven zero zero right
02:50 - oh-ho my math is off zero point six
02:53 - right these all add up to a hundred
02:56 - percent this is the idea we're what this
03:00 - is saying is this particular RGB color
03:03 - has a 60% chance of being you know
03:06 - blueish if that's the particularly ball
03:08 - that matches with zero one two three
03:10 - four five index number six a 10% chance
03:13 - of being reddish a 10% chance of being
03:16 - purplish and a 2% chance of being
03:17 - greenish this is what we want now the
03:20 - training data is encoded like this and
03:23 - maybe we can actually look at it right
03:27 - next to it maybe this is what the
03:28 - training data looks like 0 0 1 0 0 0 0 0
03:32 - 0 a 1 hot encoded vector because
03:36 - actually the correct label for that
03:38 - color is greenish so I need a loss
03:42 - function sorry good cat across entropy
03:50 - and soft backs are linked together
03:51 - they're used together so that's why I
03:53 - just can't remember which one I'm
03:55 - explaining but I need a loss function to
03:58 - give me the error between this
04:00 - probability distribution and this
04:02 - probability distribution but I need my
04:04 - neural network to generate a probability
04:06 - distribution in the first place
04:08 - activation function as you might recall
04:11 - is something that squashes any number
04:13 - into some range it's one way of thinking
04:15 - about it the sigmoid function if we
04:17 - to graph that sigmoid function it looks
04:20 - like a boy can never do this something
04:23 - like this oh boy that's a terrible graph
04:25 - of it you can look it up on Wikipedia
04:27 - something more like this right and this
04:29 - the top is one the bottom is zero so any
04:32 - number given to sigmoid results in a
04:33 - number between zero and one softmax is
04:36 - an activation function that not only
04:38 - squashes the values that are coming in
04:42 - to these outputs between zero and one
04:43 - but guarantees that they all add up to
04:46 - one
04:46 - now you might say to yourself that's
04:48 - easy that's very easy to do we do this
04:51 - all the time with normalizing data I
04:53 - could just find I could just take all of
04:56 - the outputs add them all up and then
04:58 - divide each one by the sum of the total
05:00 - right because let's say somewhere I have
05:03 - these numbers two two one five right I
05:10 - can add all these up and they're going
05:14 - to add up well look at that they added
05:15 - up to ten let me divide by ten I have
05:18 - point to 0.2 0.1 0.5 so this we could do
05:24 - this sort of like divided by the sum as
05:26 - our activation function in but that's
05:28 - but but this is not going to give us an
05:30 - AK or an accurate probability
05:32 - distribution that we want for this
05:34 - scenario and softmax is another way of
05:36 - doing the same thing with more that that
05:38 - sort of expands the difference this one
05:41 - makes this one much more likely expands
05:43 - the difference between these different
05:45 - values so the way that softmax works is
05:48 - we actually do the following
05:51 - you know that I gotta find it aracely's
05:54 - know that natural number e for natural
05:58 - log 2.7 something I think well what if I
06:02 - said and took E squared e squared e to
06:06 - the 1 power e to the 5th power what if I
06:09 - took all of these what if I took all of
06:14 - these and then added them all up and
06:19 - made that I'll call that the e sum and
06:22 - then just took each one of these values
06:24 - and / e summed that is softmax
06:28 - nutshell you'd only thing I'm gonna do
06:29 - I'm gonna have a like a tan tangent
06:30 - video that you can go and watch now or
06:32 - I'm actually gonna write the code for
06:34 - the softmax function
06:35 - I think we'll explain it better only
06:36 - it's worth doing that in this video but
06:38 - I'm gonna I'm going to do that in a
06:40 - separate video so look for that in the
06:41 - delete it look for a link to that in
06:43 - this video's description
06:47 - just to like just to be sure that I'm
06:51 - right about this we can now go here and
06:53 - this makes it look like oh my god this
06:55 - is like the craziest scariest thing in
06:57 - the world but you can see it right here
07:00 - the softmax function for a vector of
07:04 - values Z means take every value e to
07:08 - that Z index J power divided by the sum
07:15 - of all of those values and so that and
07:17 - you can see here the probability theory
07:19 - the output of the softmax function can
07:21 - be used to represent a categorical
07:23 - distribution a probability distribution
07:25 - over K different possible outcomes
07:28 - alright so again in a separate video I'm
07:31 - gonna write the code for softmax and
07:33 - actually it's right there intensive load
07:34 - KS also as functions for doing it and
07:37 - I'm gonna compare what those outcomes
07:39 - look like versus just summing and
07:41 - dividing but I'm gonna move on and say
07:43 - so if I've established that softmax is
07:48 - what I'm using as the activation
07:50 - function for the last layer the output
07:53 - layer the question then becomes what
07:56 - loss function should I use how do I
07:59 - calculate the error between the node the
08:02 - target outputs with the training data
08:05 - and what the what the model generated
08:07 - during the training process so again
08:08 - mean squared error would work here but I
08:11 - am gonna change that
08:15 - sounds like two categorical
08:18 - cross-entropy why am i using that so
08:21 - first of all what is entropy entropy is
08:26 - a term that refers to like the chaos
08:28 - associated with the system's you can
08:30 - think about probability distribution is
08:32 - like being very chaotic or more or less
08:34 - chaotic so what the cross entropy
08:37 - function is a loss function designed to
08:39 - compare to probability distributions and
08:42 - look at how much chaos there is in
08:44 - between then the cross entropy between
08:46 - them and the math of it is you know mean
08:50 - squared error is like subtract take this
08:52 - one - this one and then do like the
08:55 - square root square it then do the square
08:57 - root or make it up to the square root
08:59 - and add them all together mean squared
09:00 - error I've talked about that you can
09:02 - look it up it's a pretty simple
09:03 - mathematical function cross entropy if
09:07 - we look at it we get we could build that
09:09 - I could build this in a separate video
09:10 - which might be worth doing as well is
09:13 - really just the if if I have two
09:15 - probability distributions P and Q I'm
09:17 - looking at the mine negative the sum of
09:20 - one probability distribution times the
09:22 - log of the other probability
09:24 - distribution so again you can research
09:26 - what cross entropy how the math behind
09:29 - it works more in more detail and maybe
09:31 - I'll do a video about that for those who
09:32 - are interested but at the moment the
09:35 - important thing to do where am i over
09:38 - here the important thing to realize is
09:39 - that softmax is an activation function
09:41 - for generating a probability
09:43 - distribution and cross entropy is a loss
09:47 - function that works well for comparing
09:49 - two probability distributions so for a
09:51 - classification problem those are the two
09:54 - things we want to use so we've done that
09:57 - oh I think I'm done with this video let
10:01 - me just uh let me just kind of like run
10:03 - this code oh wait we got it unknown loss
10:05 - ah okay I think this is lowercase e okay
10:10 - there we go so so now we're done what is
10:14 - the next step what am I gonna do in the
10:16 - next video it is now time for me to call
10:20 - model dot fit model dot fit is actually
10:25 - the function I will call
10:27 - with the exes and the Y's that I've
10:29 - prepared in a previous video to train
10:31 - the model right I really only got two
10:33 - steps left and I'm sure there's gonna be
10:35 - lots of other stuff that are forgetting
10:36 - about right now I want to train the
10:37 - model then I want to use the model to
10:41 - give me a label for a new color that the
10:43 - user is going to specify okay so in the
10:45 - next video I'm going to actually add
10:47 - model dot fit see you then
10:51 - [Music]
10:59 - you

Cleaned transcript:

alright I'm back in part 471 of building a color classifier now what am I gonna do here in the previous video I created the architecture of my model a hidden layer and output layer a subsea tension fluid is sequential model to dense layers activation functions units etc now at the end of the last video the next thing I need to do is define an optimization function and then compile the model well I really botched that is what there's three things I need to do optimization function loss function and compile the model and so I kind of conflated optimization and loss I'm optimizing against the loss but the optimizer that I want to make is I can use Const I guess here I get a very inconsistent about winning using converses let maybe I'll go back and clean up that code at some point I'm gonna say I can get it from TF train stochastic gradient descent and I can create a learning rate which I'm going to say is like 0.2 so this so one thing to do is create an optimization function right there are different options and we can try other options stochastic gradient descent is the one that I basically used in almost all of my examples and covered in detail in my how to build a neural network from scratch series and the idea of graded descent is walking along trying to go down the graph of the loss function to minimize that loss so what is the loss function that I want well if I'm gonna say model dot compile I believe this is a whoops this is a function that I'm going to write with a configuration option and one of the things when I compile the model I need to specify up to optimizer optimizer now this is very awkward that I just called this up here but that's fine and then the other thing that I just specified is a loss function mean squared error so this is typically what I have done in previous examples if you look at my EXOR coding challenge but this is now going to change and the reason is because I am using an activation function called softmax so let's talk about what softmax is softmax question mark ok so remember the output that we want from the neural network is a probability distribution right what's an example of what an output might look like it might look like this there's nine values 0.1 0.1 0.2 0 zero zero zero point seven zero zero right ohho my math is off zero point six right these all add up to a hundred percent this is the idea we're what this is saying is this particular RGB color has a 60% chance of being you know blueish if that's the particularly ball that matches with zero one two three four five index number six a 10% chance of being reddish a 10% chance of being purplish and a 2% chance of being greenish this is what we want now the training data is encoded like this and maybe we can actually look at it right next to it maybe this is what the training data looks like 0 0 1 0 0 0 0 0 0 a 1 hot encoded vector because actually the correct label for that color is greenish so I need a loss function sorry good cat across entropy and soft backs are linked together they're used together so that's why I just can't remember which one I'm explaining but I need a loss function to give me the error between this probability distribution and this probability distribution but I need my neural network to generate a probability distribution in the first place activation function as you might recall is something that squashes any number into some range it's one way of thinking about it the sigmoid function if we to graph that sigmoid function it looks like a boy can never do this something like this oh boy that's a terrible graph of it you can look it up on Wikipedia something more like this right and this the top is one the bottom is zero so any number given to sigmoid results in a number between zero and one softmax is an activation function that not only squashes the values that are coming in to these outputs between zero and one but guarantees that they all add up to one now you might say to yourself that's easy that's very easy to do we do this all the time with normalizing data I could just find I could just take all of the outputs add them all up and then divide each one by the sum of the total right because let's say somewhere I have these numbers two two one five right I can add all these up and they're going to add up well look at that they added up to ten let me divide by ten I have point to 0.2 0.1 0.5 so this we could do this sort of like divided by the sum as our activation function in but that's but but this is not going to give us an AK or an accurate probability distribution that we want for this scenario and softmax is another way of doing the same thing with more that that sort of expands the difference this one makes this one much more likely expands the difference between these different values so the way that softmax works is we actually do the following you know that I gotta find it aracely's know that natural number e for natural log 2.7 something I think well what if I said and took E squared e squared e to the 1 power e to the 5th power what if I took all of these what if I took all of these and then added them all up and made that I'll call that the e sum and then just took each one of these values and / e summed that is softmax nutshell you'd only thing I'm gonna do I'm gonna have a like a tan tangent video that you can go and watch now or I'm actually gonna write the code for the softmax function I think we'll explain it better only it's worth doing that in this video but I'm gonna I'm going to do that in a separate video so look for that in the delete it look for a link to that in this video's description just to like just to be sure that I'm right about this we can now go here and this makes it look like oh my god this is like the craziest scariest thing in the world but you can see it right here the softmax function for a vector of values Z means take every value e to that Z index J power divided by the sum of all of those values and so that and you can see here the probability theory the output of the softmax function can be used to represent a categorical distribution a probability distribution over K different possible outcomes alright so again in a separate video I'm gonna write the code for softmax and actually it's right there intensive load KS also as functions for doing it and I'm gonna compare what those outcomes look like versus just summing and dividing but I'm gonna move on and say so if I've established that softmax is what I'm using as the activation function for the last layer the output layer the question then becomes what loss function should I use how do I calculate the error between the node the target outputs with the training data and what the what the model generated during the training process so again mean squared error would work here but I am gonna change that sounds like two categorical crossentropy why am i using that so first of all what is entropy entropy is a term that refers to like the chaos associated with the system's you can think about probability distribution is like being very chaotic or more or less chaotic so what the cross entropy function is a loss function designed to compare to probability distributions and look at how much chaos there is in between then the cross entropy between them and the math of it is you know mean squared error is like subtract take this one this one and then do like the square root square it then do the square root or make it up to the square root and add them all together mean squared error I've talked about that you can look it up it's a pretty simple mathematical function cross entropy if we look at it we get we could build that I could build this in a separate video which might be worth doing as well is really just the if if I have two probability distributions P and Q I'm looking at the mine negative the sum of one probability distribution times the log of the other probability distribution so again you can research what cross entropy how the math behind it works more in more detail and maybe I'll do a video about that for those who are interested but at the moment the important thing to do where am i over here the important thing to realize is that softmax is an activation function for generating a probability distribution and cross entropy is a loss function that works well for comparing two probability distributions so for a classification problem those are the two things we want to use so we've done that oh I think I'm done with this video let me just uh let me just kind of like run this code oh wait we got it unknown loss ah okay I think this is lowercase e okay there we go so so now we're done what is the next step what am I gonna do in the next video it is now time for me to call model dot fit model dot fit is actually the function I will call with the exes and the Y's that I've prepared in a previous video to train the model right I really only got two steps left and I'm sure there's gonna be lots of other stuff that are forgetting about right now I want to train the model then I want to use the model to give me a label for a new color that the user is going to specify okay so in the next video I'm going to actually add model dot fit see you then you
