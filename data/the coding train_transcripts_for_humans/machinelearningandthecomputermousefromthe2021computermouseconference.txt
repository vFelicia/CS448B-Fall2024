With timestamps:

00:06 - [Music]
00:24 - sorry where'd you go
00:31 - [Music]
00:38 - [Music]
00:52 - hello computer mouse conference
00:54 - attendees
00:55 - maybe some viewers from the coding train
00:57 - i don't know
00:58 - why are you watching this video i'm
01:00 - gonna try something new
01:01 - i would like to invite the computer
01:04 - mouse to be with me out here
01:05 - in the garden with my desktop i just
01:08 - realized i forgot my train whistle
01:10 - but let's take a trip you and me and
01:12 - explore what it means to learn
01:14 - with the mouse so where do i want to
01:16 - begin
01:17 - 40 years ago a man named douglas
01:20 - engelbart created what he called
01:21 - the xy position indicator for a display
01:24 - system
01:25 - or more commonly as we know it today the
01:28 - computer
01:29 - mouse he presented it in what is
01:31 - famously known as
01:32 - the mother of all demos and so much of
01:34 - what we take for granted today
01:36 - every time we interface with technology
01:38 - can be traced back to this original demo
01:40 - perhaps a little bit less known was that
01:42 - a few weeks before engelbart's mouse was
01:44 - demoed to the public
01:45 - the german company telefunken presented
01:48 - the first
01:49 - official device that displaced a cursor
01:52 - on a monitor this invention was called a
01:54 - roll hugo
01:55 - i'm not sure if i'm pronouncing that
01:57 - correctly but that directly translates
01:59 - to rolling ball and it is just the
02:02 - cutest
02:03 - most delightful rolling ball i have ever
02:05 - seen over the decades the mouse has
02:07 - changed fundamentally in terms of its
02:09 - design
02:09 - in terms of how we the human beings here
02:12 - in the world
02:13 - use the mouse and interface with our
02:15 - computers and here i find myself today
02:17 - at an actual conference all about the
02:20 - computer mouse
02:21 - itself thank you so much mn ashley for
02:24 - inviting me i'm really thrilled
02:25 - to try to do something a little bit out
02:27 - of my comfort zone here
02:29 - i have a bunch of demos to show but
02:31 - ultimately
02:32 - i want to take the question presented
02:36 - by the computer mouse conference itself
02:37 - the fundamental question of computer
02:39 - mouse scholarship
02:40 - and explore it what is it exactly that
02:42 - the mouse sees
02:44 - and how does that shape what humans
02:49 - come on see come on
02:53 - [Music]
02:56 - come on come on
03:08 - [Music]
03:11 - i want to begin this journey with the
03:14 - image
03:14 - of a mouse itself you might have seen in
03:17 - the news or on social media lots of
03:18 - examples of this blank does not exist
03:21 - or perhaps this ai dreams about cats
03:24 - and it'll haunt your nightmares and you
03:26 - might be wondering should you be worried
03:28 - i mean i'm a little bit scared i have
03:30 - of nightmares i don't want to have
03:31 - nightmares this blank does not exist
03:33 - typically refers to the concept of
03:35 - synthetic media and
03:37 - there are lots of different kinds of
03:38 - techniques to generate imagery
03:40 - but what is grabbing all the headlines
03:42 - today what you're
03:43 - often hearing about is something called
03:46 - a gan
03:46 - or generative adversarial network a gan
03:50 - is a system of
03:50 - two neural networks one is known as the
03:53 - generator the other is the discriminator
03:55 - the generator is just making images
03:57 - generate images images images images
03:59 - maybe it's trying
04:00 - to create images of mice
04:03 - the discriminator's job is to look at an
04:05 - image made by the generator and compare
04:07 - it to a lineup of images of
04:09 - real mice this is the training data set
04:11 - actual photographs of mice
04:13 - from the real world only that
04:14 - discriminator doesn't know which is
04:16 - which
04:17 - it's got to make a determination if it
04:19 - can guess correctly which one is the
04:20 - fake
04:21 - the generator has to go back and try
04:23 - again and it tries again
04:24 - and back and forth this is the a in gan
04:27 - adversarial these two networks are
04:30 - pitted together against each other
04:32 - in a game of cat and mouse if you will
04:35 - early gans produced fuzzy low resolution
04:38 - images ones that aren't very realistic
04:40 - into the human eye are quite
04:42 - distinguishable from real images however
04:45 - over the years these synthetic images
04:47 - have become
04:47 - more and more photorealistic in december
04:50 - 2018
04:52 - nvidia researchers released something
04:53 - called stylegan a novel technique for
04:56 - generated images with finer control over
04:58 - the style of the image itself
05:00 - in 2020 further improvements were made
05:03 - to this model known as
05:04 - stylegan 2. and this is where i find a
05:07 - really big challenge
05:08 - i can't help but get really excited by
05:10 - this technology and think about
05:12 - the possibilities that sort of like
05:14 - magic output of these pre-trained models
05:16 - and what doors
05:17 - they might open when placed in the hands
05:19 - of artists creative people
05:20 - people like who i'm imagining are
05:22 - watching this video right now
05:24 - however it is absolutely critical that
05:26 - my
05:27 - enthusiastic sharing of how to generate
05:29 - your own synthetic images
05:30 - be tempered with a really serious look
05:33 - at the harmful
05:34 - unintended and sometimes rather
05:36 - unfortunately intended
05:38 - consequences of this kind of technology
05:41 - machine learning models are making
05:42 - decisions for us impacting
05:44 - our daily lives all the time there are
05:46 - countless examples of deeply ingrained
05:48 - biases of harm that's being caused
05:50 - this harm disproportionately affects
05:53 - communities of color
05:54 - and other misrepresented identities
05:57 - synthetic media is being used on a
05:58 - regular basis to spread misinformation
06:00 - so here's just one
06:02 - example of a generated twitter profile
06:04 - pictures of fictional amazon workers
06:07 - who love working at amazon and this was
06:10 - used as an
06:11 - attempt to prevent the unionization of
06:13 - workers
06:14 - still there is beauty to be found and
06:16 - much to be learned by these ai dreams
06:19 - as evidenced by a global diverse array
06:21 - of artists and activists making
06:23 - thoughtful impactful work with the new
06:24 - technology
06:25 - these synthetic artifacts can help us to
06:28 - dream and see the small beauties of life
06:31 - take as an example helena sarin's leaves
06:34 - which showcase intricate outputs of a
06:36 - model trained on the colorful leaves
06:37 - fallen in her
06:38 - backyard others like dr nettress r
06:41 - gaskins use a related technique called
06:43 - style transfer
06:44 - to generate portraiture drawing
06:46 - inspiration from other
06:48 - generative techniques and treating the
06:49 - imagery with neural networks
06:51 - these are beautiful examples but ai
06:53 - dreams are
06:54 - often just so ridiculous that they can
06:56 - also bring laughter and levity to our
06:59 - day
06:59 - take for example this foot does not
07:01 - exist by the collective mschf
07:04 - as well as bivalent friends created by
07:06 - golon levin
07:07 - and ling donghuang so who am i to just
07:10 - make some absurd style gan mice
07:12 - i don't know but it's my hope that if i
07:14 - share this technique with you the
07:15 - creative and beautiful people of the
07:17 - world
07:17 - you can educate others spread awareness
07:20 - share your artistic vision and affect
07:21 - positive change
07:23 - so here's how i trained my own stylegan
07:25 - mice to simulate the machine
07:26 - dreaming of the computer mouse and to be
07:29 - clear there's no
07:30 - actual dreaming going on here machine
07:32 - learning these ai dreams really boil
07:34 - down
07:34 - to just numbers and spreadsheets and
07:36 - those spreadsheets being you know
07:37 - multiplied together
07:38 - a lot of times i began by first
07:40 - collecting a data set of mouse imagery
07:42 - so i scraped from flickr i scraped from
07:44 - google images
07:45 - and honestly i just i took a lot of
07:47 - pictures of mice
07:48 - i've been buying some off ebay making a
07:50 - collection and just taking lots of
07:51 - pictures of them from multiple angles
07:53 - using runway ml i uploaded the entire
07:56 - data sets of their servers and started
07:58 - the training process
07:59 - runway makes this process of training a
08:01 - stylegan model easy so first it's got a
08:03 - cloud
08:04 - server that just has all the
08:05 - configurations set up for you so all of
08:07 - the time that you might
08:08 - be configuring your environment and
08:10 - setting settings and trying to like
08:12 - get your machine learning gpu system to
08:15 - not
08:15 - throw an error runway handles all of
08:17 - that for you but even
08:19 - better than that runway has a set of
08:21 - base models from which to train
08:23 - so it would be you need a much larger
08:25 - data set and much more time
08:26 - to train a style gain model from scratch
08:28 - but in this case i started from one
08:30 - trained on objects mostly cars and so
08:32 - without starting from nothing
08:33 - i've got my synthetic mice in no time
08:36 - really just about an hour
08:37 - once the training is complete i can
08:39 - execute the model in runway itself and
08:41 - browse what is known as the latent space
08:43 - so the latent space refers to the
08:45 - universe of all
08:47 - possible dreams the model might have
08:49 - about these mice
08:50 - it's essentially a really really really
08:52 - big graph in many many dimensions
08:55 - when you see a dreamlike sequence of
08:56 - morphing synthetic imagery what you are
08:58 - really seeing is a walk
09:00 - through that latent space anna ridler's
09:03 - mosaic virus
09:04 - is a wonderful example of a walk through
09:06 - a latent space of beautiful flowers
09:08 - what's interesting is how you choose to
09:10 - make that walk and in anna riddler's
09:12 - piece
09:12 - that walk is directed by the price of
09:14 - bitcoin runway ml also has a feature
09:17 - where you can host the model that you've
09:18 - trained so once i've hosted it
09:20 - i can then write my own program to
09:22 - request images
09:23 - and i can make the walk happen however i
09:25 - want what i chose to do is program a
09:27 - node server that would request these
09:28 - images and then a processing sketch
09:30 - that would ask the node server to
09:32 - request the images and the reason why i
09:34 - wanted to use processing
09:35 - is i love this algorithm called open
09:37 - simplex noise
09:38 - it's a wonderful way to smoothly
09:40 - navigate the latent space
09:42 - and by calculating the noise in
09:44 - processing sending the results to node
09:46 - node getting the image from the server
09:48 - then back to processing
09:49 - i'm able to render what i'm about to
09:51 - present to you my
09:53 - short film entitled computer mouse
09:56 - dreams and uh just in case you know i
09:59 - did throw in a couple
10:00 - real mice in there so hopefully this is
10:02 - not gonna haunt your nightmares
10:05 - [Music]
10:14 - [Music]
10:16 - sorry
10:22 - [Music]
10:25 - and there we have it cat versus mouse
10:27 - score one for the mouse
10:28 - but i think i might have jumped ahead
10:30 - maybe done this out of order here i am
10:32 - using generative adversarial networks
10:34 - two neural networks pitted
10:36 - against each other in a fiery battle to
10:38 - generate synthetic mice
10:39 - let's take a step back what is machine
10:42 - learning what is a neural network and
10:44 - can the computer mouse help us to
10:47 - understand the answer to these questions
11:02 - gloria
11:03 - [Music]
11:06 - come here
11:15 - [Music]
11:26 - kyle mcdonald in his 2018 kik festival
11:29 - talk weird
11:30 - intelligence defines machine learning as
11:33 - programming with examples not
11:36 - instructions
11:37 - so what does this mean exactly
11:39 - programming with instructions
11:40 - is like me attempting to explain and
11:43 - provide instructions
11:44 - to my children on how to do something
11:47 - code works the same way
11:48 - only it requires a highly formal and
11:50 - very specific syntax
11:52 - and generally speaking code tends to
11:55 - listen to me
11:56 - here's a beginner example from my
11:57 - tutorials on conditional logic this is
11:59 - something that you learn
12:01 - in the first couple weeks of learning to
12:03 - code
12:04 - if the mouse's x position is on the
12:07 - right hand side
12:08 - of the canvas or greater than 200 pixels
12:11 - draw a red background else or otherwise
12:14 - if it's on the left hand side
12:16 - less than 200 pixels draw a blue
12:18 - background
12:19 - programming by example works differently
12:22 - instead of you
12:23 - the programmer writing the explicit
12:24 - instructions you show
12:26 - examples to the machine itself and say
12:29 - hey
12:29 - why don't you based on these examples
12:31 - learn what those instructions should be
12:33 - to reproduce what i'm showing you but i
12:35 - can't just show the machine something
12:38 - in the case of machine learning what i
12:40 - really mean is
12:41 - take a data set of examples of inputs
12:45 - paired with outputs
12:46 - and feed those into a particular
12:48 - algorithm so that
12:49 - the machine learning system can learn
12:51 - the instructions to reproduce
12:54 - those same outputs with those inputs a
12:56 - collection
12:57 - of example inputs and outputs is what is
13:00 - known as a training data set
13:02 - and probably one of the most famous
13:04 - well-known training data sets for
13:05 - machine learning
13:06 - is something called mnist or modified
13:09 - national institute of standards and
13:11 - technology database
13:12 - a large database of handwritten digits
13:14 - that is commonly used
13:15 - for training various i don't have the
13:18 - next page training various image
13:20 - processing systems
13:22 - personally i'm a fan of a twist on that
13:24 - there's a data set called
13:25 - fashion mnist which is an alternative
13:27 - database it has 60 thousand example
13:29 - images of
13:30 - shirts and pants shoes and more all
13:32 - paired with a label so if i want to
13:34 - demonstrate image classification i can
13:36 - show
13:36 - a machine learning system here's all of
13:38 - these fashion mnist examples
13:40 - here's what their labels are now could
13:42 - you go look at some real clothes
13:43 - and correctly classify them but what is
13:46 - the algorithm that we're giving these
13:47 - examples
13:48 - to while there are many machine learning
13:50 - algorithms and sometimes these are
13:51 - called recipes
13:52 - the n in stylegan stands for network or
13:56 - more specifically artificial neural
13:58 - network
13:58 - a computational model based on the brain
14:01 - and probably
14:02 - what is the most popular machine
14:03 - learning recipe being used in today's
14:06 - modern so-called ai systems
14:09 - oh i got it that was really good back to
14:12 - the mouse interaction
14:13 - i've written code very specific
14:16 - instructions for the computer to follow
14:18 - could i reproduce this exact same result
14:21 - but instead of explicitly writing the
14:23 - instructions just create a data set
14:25 - examples
14:25 - of what are some points that happen to
14:27 - be on the left hand side of the canvas
14:29 - versus on the right hand side of the
14:30 - canvas i love this example because it's
14:33 - kind of ridiculous it's very silly and
14:35 - trivial i'm asking a big
14:36 - fancy neural network to learn one of the
14:38 - most basic things
14:39 - that a beginner coder learns how to do
14:42 - when learning to program
14:44 - i find this to be a great entry point
14:46 - into machine learning i can demonstrate
14:47 - the entire machine learning pipeline
14:49 - see it from start to finish with
14:50 - something so obvious that it's very easy
14:52 - to test the results and understand all
14:54 - the pieces of what's going on
14:55 - and figure out whether or not it's
14:57 - working so let's put this into practice
14:59 - i'll use the ml5.js library built on top
15:01 - of google's open source
15:02 - machine learning library tensorflow.js
15:05 - it has a neural network object
15:06 - ready to go first step is to collect
15:09 - training data
15:10 - i can write a quick p5.js sketch that
15:12 - collects data through mouse clicks
15:14 - click on the right hand side of the
15:15 - canvas a bunch of times then on the left
15:17 - hand side
15:18 - all the while making sure to pair those
15:21 - clicks with the correct label
15:23 - now it's time for me to configure the
15:25 - ml5.js neural network
15:27 - i just have two inputs the x and y value
15:29 - of each click and i should note i don't
15:31 - actually even need
15:32 - the y value so that would be another
15:34 - version of this that
15:35 - doesn't bother to use the y and one
15:37 - output a classification
15:39 - that has two discrete possibilities left
15:41 - or right
15:42 - next i call the train function and this
15:45 - is what is known
15:46 - as supervised learning the neural
15:48 - network for each
15:49 - data point makes a guess left or right
15:52 - if it makes the correct guess it doesn't
15:54 - have to do anything just move on to the
15:55 - next point
15:56 - if it makes an incorrect guess there's
15:58 - an error and it can go back inside of
16:00 - itself
16:00 - and just all of its weights and
16:02 - parameters and various internal
16:04 - mechanics
16:04 - to try to get the correct answer the
16:06 - next time over and over
16:08 - again we go many many times with the
16:10 - data every time i send all the data the
16:13 - training data through the neural network
16:14 - that is one epoch the amount of times
16:17 - the neural network has gotten things
16:19 - right or wrong that's all summarized in
16:21 - what's known as a loss function
16:23 - the lower the total loss the fewer the
16:25 - mistakes the loss is going down
16:27 - during your training things are working
16:29 - as planned
16:30 - once the model is trained i can go back
16:32 - to that original
16:33 - if statement take it out and replace it
16:36 - with
16:37 - sending the xy position into a neural
16:39 - network and
16:40 - getting the output from that neural
16:41 - network same exact thing this time
16:43 - neural network classification the fun
16:45 - thing about this is i can start to play
16:47 - with all
16:47 - different kinds of delineations i don't
16:49 - have to just look at left versus right
16:51 - maybe i look at a circle maybe i have
16:52 - sort of like a fuzzy collection of
16:54 - points
16:54 - and ultimately the question i want to
16:56 - ask of you is
16:57 - could you reproduce every single mouse
17:00 - interaction we take
17:01 - for granted that is generally programmed
17:04 - with instructions
17:05 - with machine learning and neural
17:07 - networks so this is the example i use in
17:09 - my introduction to machine learning for
17:10 - the arts course
17:11 - in my video tutorial series about ml5.js
17:14 - i hope that it's helpful i'd be curious
17:15 - to hear your feedback
17:17 - on how how you imagine teaching machine
17:19 - learning
17:20 - uh with thinking about programming with
17:22 - instructions and programming with
17:23 - examples
17:24 - and i also want to give a big thank you
17:26 - to dr rebecca freebrink
17:27 - her work um building the wekinator
17:29 - project another tool for machine
17:31 - learning and
17:32 - all of her examples of interactive
17:33 - machine learning for design systems
17:35 - was a huge inspiration for these
17:38 - examples and the ml5.js project itself
17:47 - [Music]
17:58 - gloria
18:01 - [Music]
18:05 - now that i've demonstrated how the
18:07 - machine can dream
18:08 - about the mouse how the machine can be
18:10 - trained by data from the mouse
18:13 - i want to explore what can the mouse
18:15 - teach us about ourselves the human
18:17 - beings
18:18 - and how we use the machine physical
18:20 - objects show
18:21 - signs of use over time wear on a piece
18:23 - of clothing
18:24 - or a desire path cutting through a patch
18:26 - of grass whatever the landscape design
18:29 - says about how to get from point a to
18:30 - point b we're going to go the most
18:32 - useful and direct way
18:34 - technology shows this kind of wear in
18:36 - hardware f1 keys are sometimes removed
18:39 - command keys are worn down similarly
18:42 - faded patterns on a mouse pad
18:44 - can give indications on how the mouse
18:46 - itself moves
18:47 - where can we see this where left by our
18:50 - digital
18:51 - presence the mouse is an extension of
18:53 - the self in the digital space which is
18:55 - easy to forget
18:56 - char styles in her sketch nails doesn't
18:58 - let you forget this as you
18:59 - physically grimace and wince scratching
19:02 - the mouse as nails across the screen
19:04 - which has now become a chalkboard in
19:07 - maya man's can i go
19:08 - where you go maya frees us from the
19:10 - limitations of the mouse as
19:12 - agent of the body and the whole body is
19:14 - able to act
19:15 - and move freely in the digital space in
19:18 - do not
19:19 - touch by the amsterdam-based studio
19:20 - moniker real-time cursor movements of
19:23 - hundreds of users
19:24 - are collaged into an interactive
19:27 - crowd-sourced music video
19:28 - a collaborative symphony of pointers i
19:32 - began
19:32 - my own exploration of these ideas by
19:34 - writing a processing sketch to collect
19:36 - mouse data
19:37 - over long periods of time i was teaching
19:39 - writing emails
19:40 - endless and endless and endless zoom
19:43 - meetings
19:44 - creative coding libraries like
19:46 - processing and p5.js they have built
19:48 - into them
19:49 - mouse x and mouse y variables but you
19:52 - can't actually use these variables they
19:54 - limit you
19:54 - to the pixels of your graphics display
19:57 - window or canvas themselves
19:59 - processing in case you didn't know is
20:01 - built however on top of the java
20:03 - programming language and you have access
20:06 - to all that there is to do
20:08 - with java itself and one of java's
20:10 - classes it's part of the awt package or
20:13 - abstract window toolkit can both track
20:17 - and control mouse movements in real time
20:21 - across any use of the operating system
20:24 - or any application whatsoever
20:26 - so i wrote some code to collect and save
20:28 - all of my mouse pointer positions
20:30 - into a csv file csv stands for comma
20:33 - separated values it's a very standard
20:35 - data format that you can easily reload
20:37 - into all sorts of other things like a
20:39 - spreadsheet you can then take this data
20:40 - and visualize it in a myriad of ways
20:42 - time lapse animations heat maps i tried
20:45 - a few different ones
20:47 - with all of these mouse movements saved
20:49 - i could also use them as training data i
20:51 - can analyze the probability of any
20:53 - moment of the mouse going up down left
20:55 - right
20:55 - and replay a sequence based on those
20:57 - probabilities with something known as a
20:59 - markov chain
21:00 - i could feed this data into something
21:01 - called a recurrent neural network
21:03 - a kind of neural network that's very
21:05 - well suited for sequential data
21:07 - time series text music
21:10 - vector paths there's a well-known
21:12 - machine learning model called sketch rnn
21:14 - that was trained off of the google
21:15 - quickdraw data set to generate
21:18 - doodles of all sorts of different types
21:20 - of things and i can take the results
21:22 - of that recurrent neural network trained
21:24 - off of my mouse movements
21:25 - to take a nap and let it just take over
21:28 - and control my computer now i didn't
21:29 - include
21:30 - mouse clicks in here well although i
21:32 - could have this way i
21:34 - just you know i'm safe and i'm not going
21:35 - to end up you know who knows what
21:36 - nefarious
21:37 - business the sort of dream version of my
21:39 - recurrent neural network mouse
21:41 - controller might get itself
21:42 - up to thank you so much for spending
21:44 - your time with me at the 2021 computer
21:46 - mouse conference for indulging me in
21:48 - this experiment i have learned a lot
21:50 - about what it means to
21:52 - operate a camera frankly to try to work
21:55 - with a script
21:56 - and i've really enjoyed having the
21:57 - opportunity to sort of put this set of
21:59 - demos uh together for you
22:01 - um i've made all the example code from
22:04 - this video available for you it's at
22:06 - thecodingtrain.com mouse learning
22:09 - and i hope that you find some
22:10 - inspiration to explore dreaming
22:12 - learning and teaching with a computer
22:14 - mouse and that you will
22:16 - share it with me thank you to emma and
22:18 - ashley and everyone who helped put
22:20 - together the computer mouse conference
22:22 - and i will see you sometime later
22:26 - that's my ending
22:32 - gloria come here how did i do it
22:35 - yes no no no no yes help me
22:40 - [Music]
22:51 - come in go come on come in yeah
22:55 - [Music]
23:06 - okay
23:11 - [Music]
23:45 - you

Cleaned transcript:

sorry where'd you go hello computer mouse conference attendees maybe some viewers from the coding train i don't know why are you watching this video i'm gonna try something new i would like to invite the computer mouse to be with me out here in the garden with my desktop i just realized i forgot my train whistle but let's take a trip you and me and explore what it means to learn with the mouse so where do i want to begin 40 years ago a man named douglas engelbart created what he called the xy position indicator for a display system or more commonly as we know it today the computer mouse he presented it in what is famously known as the mother of all demos and so much of what we take for granted today every time we interface with technology can be traced back to this original demo perhaps a little bit less known was that a few weeks before engelbart's mouse was demoed to the public the german company telefunken presented the first official device that displaced a cursor on a monitor this invention was called a roll hugo i'm not sure if i'm pronouncing that correctly but that directly translates to rolling ball and it is just the cutest most delightful rolling ball i have ever seen over the decades the mouse has changed fundamentally in terms of its design in terms of how we the human beings here in the world use the mouse and interface with our computers and here i find myself today at an actual conference all about the computer mouse itself thank you so much mn ashley for inviting me i'm really thrilled to try to do something a little bit out of my comfort zone here i have a bunch of demos to show but ultimately i want to take the question presented by the computer mouse conference itself the fundamental question of computer mouse scholarship and explore it what is it exactly that the mouse sees and how does that shape what humans come on see come on come on come on i want to begin this journey with the image of a mouse itself you might have seen in the news or on social media lots of examples of this blank does not exist or perhaps this ai dreams about cats and it'll haunt your nightmares and you might be wondering should you be worried i mean i'm a little bit scared i have of nightmares i don't want to have nightmares this blank does not exist typically refers to the concept of synthetic media and there are lots of different kinds of techniques to generate imagery but what is grabbing all the headlines today what you're often hearing about is something called a gan or generative adversarial network a gan is a system of two neural networks one is known as the generator the other is the discriminator the generator is just making images generate images images images images maybe it's trying to create images of mice the discriminator's job is to look at an image made by the generator and compare it to a lineup of images of real mice this is the training data set actual photographs of mice from the real world only that discriminator doesn't know which is which it's got to make a determination if it can guess correctly which one is the fake the generator has to go back and try again and it tries again and back and forth this is the a in gan adversarial these two networks are pitted together against each other in a game of cat and mouse if you will early gans produced fuzzy low resolution images ones that aren't very realistic into the human eye are quite distinguishable from real images however over the years these synthetic images have become more and more photorealistic in december 2018 nvidia researchers released something called stylegan a novel technique for generated images with finer control over the style of the image itself in 2020 further improvements were made to this model known as stylegan 2. and this is where i find a really big challenge i can't help but get really excited by this technology and think about the possibilities that sort of like magic output of these pretrained models and what doors they might open when placed in the hands of artists creative people people like who i'm imagining are watching this video right now however it is absolutely critical that my enthusiastic sharing of how to generate your own synthetic images be tempered with a really serious look at the harmful unintended and sometimes rather unfortunately intended consequences of this kind of technology machine learning models are making decisions for us impacting our daily lives all the time there are countless examples of deeply ingrained biases of harm that's being caused this harm disproportionately affects communities of color and other misrepresented identities synthetic media is being used on a regular basis to spread misinformation so here's just one example of a generated twitter profile pictures of fictional amazon workers who love working at amazon and this was used as an attempt to prevent the unionization of workers still there is beauty to be found and much to be learned by these ai dreams as evidenced by a global diverse array of artists and activists making thoughtful impactful work with the new technology these synthetic artifacts can help us to dream and see the small beauties of life take as an example helena sarin's leaves which showcase intricate outputs of a model trained on the colorful leaves fallen in her backyard others like dr nettress r gaskins use a related technique called style transfer to generate portraiture drawing inspiration from other generative techniques and treating the imagery with neural networks these are beautiful examples but ai dreams are often just so ridiculous that they can also bring laughter and levity to our day take for example this foot does not exist by the collective mschf as well as bivalent friends created by golon levin and ling donghuang so who am i to just make some absurd style gan mice i don't know but it's my hope that if i share this technique with you the creative and beautiful people of the world you can educate others spread awareness share your artistic vision and affect positive change so here's how i trained my own stylegan mice to simulate the machine dreaming of the computer mouse and to be clear there's no actual dreaming going on here machine learning these ai dreams really boil down to just numbers and spreadsheets and those spreadsheets being you know multiplied together a lot of times i began by first collecting a data set of mouse imagery so i scraped from flickr i scraped from google images and honestly i just i took a lot of pictures of mice i've been buying some off ebay making a collection and just taking lots of pictures of them from multiple angles using runway ml i uploaded the entire data sets of their servers and started the training process runway makes this process of training a stylegan model easy so first it's got a cloud server that just has all the configurations set up for you so all of the time that you might be configuring your environment and setting settings and trying to like get your machine learning gpu system to not throw an error runway handles all of that for you but even better than that runway has a set of base models from which to train so it would be you need a much larger data set and much more time to train a style gain model from scratch but in this case i started from one trained on objects mostly cars and so without starting from nothing i've got my synthetic mice in no time really just about an hour once the training is complete i can execute the model in runway itself and browse what is known as the latent space so the latent space refers to the universe of all possible dreams the model might have about these mice it's essentially a really really really big graph in many many dimensions when you see a dreamlike sequence of morphing synthetic imagery what you are really seeing is a walk through that latent space anna ridler's mosaic virus is a wonderful example of a walk through a latent space of beautiful flowers what's interesting is how you choose to make that walk and in anna riddler's piece that walk is directed by the price of bitcoin runway ml also has a feature where you can host the model that you've trained so once i've hosted it i can then write my own program to request images and i can make the walk happen however i want what i chose to do is program a node server that would request these images and then a processing sketch that would ask the node server to request the images and the reason why i wanted to use processing is i love this algorithm called open simplex noise it's a wonderful way to smoothly navigate the latent space and by calculating the noise in processing sending the results to node node getting the image from the server then back to processing i'm able to render what i'm about to present to you my short film entitled computer mouse dreams and uh just in case you know i did throw in a couple real mice in there so hopefully this is not gonna haunt your nightmares sorry and there we have it cat versus mouse score one for the mouse but i think i might have jumped ahead maybe done this out of order here i am using generative adversarial networks two neural networks pitted against each other in a fiery battle to generate synthetic mice let's take a step back what is machine learning what is a neural network and can the computer mouse help us to understand the answer to these questions gloria come here kyle mcdonald in his 2018 kik festival talk weird intelligence defines machine learning as programming with examples not instructions so what does this mean exactly programming with instructions is like me attempting to explain and provide instructions to my children on how to do something code works the same way only it requires a highly formal and very specific syntax and generally speaking code tends to listen to me here's a beginner example from my tutorials on conditional logic this is something that you learn in the first couple weeks of learning to code if the mouse's x position is on the right hand side of the canvas or greater than 200 pixels draw a red background else or otherwise if it's on the left hand side less than 200 pixels draw a blue background programming by example works differently instead of you the programmer writing the explicit instructions you show examples to the machine itself and say hey why don't you based on these examples learn what those instructions should be to reproduce what i'm showing you but i can't just show the machine something in the case of machine learning what i really mean is take a data set of examples of inputs paired with outputs and feed those into a particular algorithm so that the machine learning system can learn the instructions to reproduce those same outputs with those inputs a collection of example inputs and outputs is what is known as a training data set and probably one of the most famous wellknown training data sets for machine learning is something called mnist or modified national institute of standards and technology database a large database of handwritten digits that is commonly used for training various i don't have the next page training various image processing systems personally i'm a fan of a twist on that there's a data set called fashion mnist which is an alternative database it has 60 thousand example images of shirts and pants shoes and more all paired with a label so if i want to demonstrate image classification i can show a machine learning system here's all of these fashion mnist examples here's what their labels are now could you go look at some real clothes and correctly classify them but what is the algorithm that we're giving these examples to while there are many machine learning algorithms and sometimes these are called recipes the n in stylegan stands for network or more specifically artificial neural network a computational model based on the brain and probably what is the most popular machine learning recipe being used in today's modern socalled ai systems oh i got it that was really good back to the mouse interaction i've written code very specific instructions for the computer to follow could i reproduce this exact same result but instead of explicitly writing the instructions just create a data set examples of what are some points that happen to be on the left hand side of the canvas versus on the right hand side of the canvas i love this example because it's kind of ridiculous it's very silly and trivial i'm asking a big fancy neural network to learn one of the most basic things that a beginner coder learns how to do when learning to program i find this to be a great entry point into machine learning i can demonstrate the entire machine learning pipeline see it from start to finish with something so obvious that it's very easy to test the results and understand all the pieces of what's going on and figure out whether or not it's working so let's put this into practice i'll use the ml5.js library built on top of google's open source machine learning library tensorflow.js it has a neural network object ready to go first step is to collect training data i can write a quick p5.js sketch that collects data through mouse clicks click on the right hand side of the canvas a bunch of times then on the left hand side all the while making sure to pair those clicks with the correct label now it's time for me to configure the ml5.js neural network i just have two inputs the x and y value of each click and i should note i don't actually even need the y value so that would be another version of this that doesn't bother to use the y and one output a classification that has two discrete possibilities left or right next i call the train function and this is what is known as supervised learning the neural network for each data point makes a guess left or right if it makes the correct guess it doesn't have to do anything just move on to the next point if it makes an incorrect guess there's an error and it can go back inside of itself and just all of its weights and parameters and various internal mechanics to try to get the correct answer the next time over and over again we go many many times with the data every time i send all the data the training data through the neural network that is one epoch the amount of times the neural network has gotten things right or wrong that's all summarized in what's known as a loss function the lower the total loss the fewer the mistakes the loss is going down during your training things are working as planned once the model is trained i can go back to that original if statement take it out and replace it with sending the xy position into a neural network and getting the output from that neural network same exact thing this time neural network classification the fun thing about this is i can start to play with all different kinds of delineations i don't have to just look at left versus right maybe i look at a circle maybe i have sort of like a fuzzy collection of points and ultimately the question i want to ask of you is could you reproduce every single mouse interaction we take for granted that is generally programmed with instructions with machine learning and neural networks so this is the example i use in my introduction to machine learning for the arts course in my video tutorial series about ml5.js i hope that it's helpful i'd be curious to hear your feedback on how how you imagine teaching machine learning uh with thinking about programming with instructions and programming with examples and i also want to give a big thank you to dr rebecca freebrink her work um building the wekinator project another tool for machine learning and all of her examples of interactive machine learning for design systems was a huge inspiration for these examples and the ml5.js project itself gloria now that i've demonstrated how the machine can dream about the mouse how the machine can be trained by data from the mouse i want to explore what can the mouse teach us about ourselves the human beings and how we use the machine physical objects show signs of use over time wear on a piece of clothing or a desire path cutting through a patch of grass whatever the landscape design says about how to get from point a to point b we're going to go the most useful and direct way technology shows this kind of wear in hardware f1 keys are sometimes removed command keys are worn down similarly faded patterns on a mouse pad can give indications on how the mouse itself moves where can we see this where left by our digital presence the mouse is an extension of the self in the digital space which is easy to forget char styles in her sketch nails doesn't let you forget this as you physically grimace and wince scratching the mouse as nails across the screen which has now become a chalkboard in maya man's can i go where you go maya frees us from the limitations of the mouse as agent of the body and the whole body is able to act and move freely in the digital space in do not touch by the amsterdambased studio moniker realtime cursor movements of hundreds of users are collaged into an interactive crowdsourced music video a collaborative symphony of pointers i began my own exploration of these ideas by writing a processing sketch to collect mouse data over long periods of time i was teaching writing emails endless and endless and endless zoom meetings creative coding libraries like processing and p5.js they have built into them mouse x and mouse y variables but you can't actually use these variables they limit you to the pixels of your graphics display window or canvas themselves processing in case you didn't know is built however on top of the java programming language and you have access to all that there is to do with java itself and one of java's classes it's part of the awt package or abstract window toolkit can both track and control mouse movements in real time across any use of the operating system or any application whatsoever so i wrote some code to collect and save all of my mouse pointer positions into a csv file csv stands for comma separated values it's a very standard data format that you can easily reload into all sorts of other things like a spreadsheet you can then take this data and visualize it in a myriad of ways time lapse animations heat maps i tried a few different ones with all of these mouse movements saved i could also use them as training data i can analyze the probability of any moment of the mouse going up down left right and replay a sequence based on those probabilities with something known as a markov chain i could feed this data into something called a recurrent neural network a kind of neural network that's very well suited for sequential data time series text music vector paths there's a wellknown machine learning model called sketch rnn that was trained off of the google quickdraw data set to generate doodles of all sorts of different types of things and i can take the results of that recurrent neural network trained off of my mouse movements to take a nap and let it just take over and control my computer now i didn't include mouse clicks in here well although i could have this way i just you know i'm safe and i'm not going to end up you know who knows what nefarious business the sort of dream version of my recurrent neural network mouse controller might get itself up to thank you so much for spending your time with me at the 2021 computer mouse conference for indulging me in this experiment i have learned a lot about what it means to operate a camera frankly to try to work with a script and i've really enjoyed having the opportunity to sort of put this set of demos uh together for you um i've made all the example code from this video available for you it's at thecodingtrain.com mouse learning and i hope that you find some inspiration to explore dreaming learning and teaching with a computer mouse and that you will share it with me thank you to emma and ashley and everyone who helped put together the computer mouse conference and i will see you sometime later that's my ending gloria come here how did i do it yes no no no no yes help me come in go come on come in yeah okay you
