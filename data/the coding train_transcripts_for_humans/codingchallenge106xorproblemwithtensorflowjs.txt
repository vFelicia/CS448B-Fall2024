With timestamps:

00:00 - oh hello welcome to a Cody challenge
00:03 - yeah I know what you're thinking I don't
00:05 - know what you're thinking I know what
00:06 - I'm thinking that looks like Cody
00:09 - challenge number 92 XOR which is
00:12 - probably one of the less interesting
00:15 - creative like sort of just technical
00:18 - coding challenge demonstrations that
00:20 - you've done why why why are you doing it
00:23 - again
00:24 - well Eric from the coding train
00:26 - community writes a nice explanation
00:29 - because I was before I started this
00:30 - having a real hang-up about this
00:31 - sometimes it's important when learning
00:33 - something new to base your exploration
00:36 - around an example which is fairly
00:38 - trivial and you understand intimately
00:40 - well so here's the thing I I'm learning
00:43 - something new I'll come back here and
00:45 - the thing that I am learning something
00:46 - new is this tension floaty a thing and
00:49 - wouldn't it be fun to make like play
00:51 - pac-man with it or the emoji scavenger
00:54 - hunt project or teachable machine or
00:56 - play a piano with it all these things
00:57 - all pose that oh my god we got it we got
00:58 - I want to get to that I could just sort
01:00 - of go there right now I will get there
01:02 - eventually but I'm trying to learn the
01:04 - basics of how the library works and I'm
01:07 - trying to step through this slowly so I
01:09 - will say that we're if you're watching
01:11 - this video right now where you are is
01:12 - not necessarily in the most beginner or
01:14 - friendly place because I'm working with
01:16 - tensorflow das natively to implement
01:19 - basically like a weird math problem it's
01:21 - not that weird of a problem actually but
01:22 - a very basic trivial math problem just
01:24 - to see how tensorflow Gs works that's
01:27 - what I'm trying to do with this coding
01:28 - challenge and about 20 or 30 minutes
01:31 - he'll be coding this coding challenges
01:32 - just look it's by like four hours in 72
01:34 - minutes long which is why would I say 72
01:36 - minutes cuz that's five hours and twelve
01:37 - minutes but I don't know but the
01:39 - trajectory that I'm on is I'm going to
01:41 - start doing some stuff inching my way
01:43 - towards hell let's actually use some
01:44 - data let's use some more data and maybe
01:46 - some images and so I've got a bunch of
01:48 - things that I'm stepping through and I'm
01:49 - trying to get to the point where I'm
01:51 - going to use this other machine learning
01:52 - library called ml five which at the time
01:54 - of this recording hasn't really
01:56 - officially been released yet but builds
01:58 - on top of tensorflow das thank you
02:00 - everyone who made floats wherever you
02:02 - are to try to create some more
02:04 - accessible interfaces to some of the
02:06 - algorithms and models that you things
02:09 - that you can do with tensor flow digest
02:10 - without having to do the
02:11 - level memory management and math
02:13 - operations stuff so all that is coming
02:16 - and I just took a lot of time to this
02:18 - coding job to say that to you so but as
02:22 - much as I kind of don't I haven't I'm
02:24 - not so sure but we're but it's a why is
02:26 - XOR so here's the thing this is why I
02:28 - want to need an example this is the
02:30 - first time I'm going to ever in any of
02:33 - my videos except for the other one that
02:35 - I made but this is the first time that
02:36 - I'm actually going to use the TF layers
02:40 - API to train to train a model with a
02:46 - data set to produce a certain output
02:49 - okay I did two tutorials about what the
02:52 - TF layers API is you could pause now and
02:54 - go and watch those and then come back
02:56 - here but in those videos they didn't
02:58 - actually do anything with TF layers just
02:59 - sort of talked through it typed out some
03:01 - code so the problem that I want to solve
03:03 - and apologies for explaining this
03:05 - probably for like the fifteenth time on
03:07 - this YouTube channel is very well known
03:10 - from machine learning X or because when
03:13 - the original perceptron was invented the
03:16 - single perceptron the model of an
03:18 - individual neuron that could receive
03:20 - inputs and generate an output it could
03:24 - not solve X or it just couldn't it's not
03:28 - a linearly separable problem and I've
03:30 - talked about that in other videos about
03:31 - why we need multi-layer perceptrons so
03:34 - the nice thing about XOR is I can
03:36 - diagram for you the the architecture of
03:39 - the model that we need to create there
03:42 - are two inputs there is one output so
03:47 - that the inputs to the XOR problem are
03:50 - true and false values so unlike a so if
03:55 - I made a little truth table and or XOR
04:00 - right I can have true and true true true
04:04 - false false true false false and and
04:08 - operation would only ever give me true
04:10 - when both are true false false false and
04:14 - or operation would only ever give me
04:18 - would gives me true if just one of
04:20 - is true true true true false can you
04:29 - even see that I can't see on my monitor
04:30 - but hopefully you can now X or the X 4
04:33 - exclusive gives me true only if 1 is
04:38 - true they can't both be true only one so
04:41 - in that case I get false true true false
04:45 - and the idea of linearly separable comes
04:49 - up here because I can draw a line here
04:51 - to separate true from false I can draw a
04:53 - line here to separate true from false
04:55 - but here I could do this but I can't
05:01 - draw a single line to separate true from
05:03 - false we need a more sophisticated model
05:06 - with a hidden layer so the inputs are
05:09 - things like a 1 and a 0 feed forward
05:17 - into the hidden layer activate feed to
05:20 - the output and the output should be a 0
05:23 - or a 1 because ultimately I'm going to
05:24 - visualize the output as grayscale values
05:27 - and I want to see number I want to see
05:28 - grayscale values all the way between 0 &
05:30 - 1 it's the same thing I did in the
05:31 - previous coding challenge if you if
05:32 - you're happened to have watched that one
05:34 - alright so now I actually I'm gonna also
05:36 - do something where I start from the code
05:39 - from the previous coding challenge and
05:42 - so we can see there's this idea of
05:44 - training data the inputs to the XOR
05:47 - problem are 0 0 gives me a 0 0 1 gives
05:51 - me a 1 1 0 gives me a 1 at 1 1 gives me
05:54 - a 0 this is the training data and in my
05:56 - previous version of this I used my own
05:59 - neural network library so in theory I'm
06:01 - gonna get rid of the idea of a learning
06:03 - rate slider just before we can add that
06:05 - back in later but let me get rid of the
06:08 - learning rate slider basically I want to
06:12 - do exactly the same thing the difference
06:16 - is I'm going to say neural network
06:17 - equals
06:18 - TF layers sequential and maybe I'll call
06:24 - this the model instead of neural network
06:27 - so the idea of this is going out here so
06:29 - the idea here is that I want to
06:34 - replace my neural network library with
06:39 - tensorflow yes and so this for me what
06:42 - what the usefulness of this video is a
06:44 - me learn I spent all this time trying to
06:47 - build my own rather sort of terrible
06:48 - normal Network JavaScript library and
06:51 - going through that was sort of helpful
06:52 - in thinking about how the stuff works
06:54 - now if I can translate that into
06:56 - attention flow dot yes I'm gonna things
06:58 - are gonna hopefully start to settle and
07:00 - make more sense into my brain okay so
07:02 - now we need to what then this
07:06 - constructor here said and let's just put
07:07 - this back to this this constructor here
07:09 - said would make a neural network with
07:13 - two inputs two hidden nodes and one
07:15 - output so I need to duplicate that idea
07:18 - here with PF layers so let's go to the
07:21 - 10th floor is API reference and we're
07:25 - gonna go all scroll down to TF layers
07:28 - and what I want to make is a dense layer
07:33 - TF layers dint a dense layer is a fully
07:37 - connected layer so what I'm going to do
07:39 - is I am going to say let hidden equal TF
07:46 - layers dense and then I can put inside
07:51 - there an object that has the parameters
07:55 - of how I want to configure that layer
07:57 - and so how do I want to configure it the
07:59 - two things that I want need really need
08:01 - to do is this is the hidden layer right
08:03 - I need to give it an input shape right
08:07 - he just say what's coming in what's
08:09 - coming in that's what this is here I
08:11 - need to say how many nodes it has that's
08:13 - the number of units and then I probably
08:16 - has a default one but I can specify an
08:18 - activation function and again I'm just
08:20 - gonna use sigmoid as this historical
08:22 - activation function that I've been using
08:24 - in all my videos to date I'm going to
08:27 - assume talk about softmax what that is
08:29 - as well as some other activation
08:31 - functions like lading which is maybe
08:34 - more commonly used okay by the way
08:37 - nobody pronounces that way put me so
08:39 - don't get confused alright so I want to
08:42 - say input shape I believe is just
08:45 - there's just two
08:46 - inputs I also want to have two units two
08:53 - notes and activation is going to be
08:56 - sigmoid so now I have created the hidden
09:00 - layer yay the other layer that I need to
09:04 - create is the output layer and so what
09:08 - am I know the output layer I don't need
09:09 - to provide an input shape because the
09:11 - input shape can be inferred if I add
09:14 - them sequentially the inputs are not a
09:16 - layer so for this first layer the hidden
09:18 - layer I've got to say how many there are
09:20 - but now once I'm creating this next
09:22 - layer it can just the input shape is
09:24 - going to be defined by what was before
09:25 - it so now I'm going to say we have to
09:30 - stop it at the sound effects by
09:31 - excellent and now I'm going to say let
09:34 - output equal TF layers dense and all I
09:42 - need to say is units one activation
09:45 - sigmoid okay then ought to do is say
09:49 - Model Model dot add hidden model dot add
09:54 - output okay so this is the model now one
09:59 - thing I need to do is I definitely need
10:02 - to import the tensorflow JS library
10:05 - which I happen to have from one of my
10:07 - previous examples so I'm going right now
10:09 - I only have I have the p5 libraries in
10:12 - my index.html plus my crazy a neural
10:14 - network thing and my actual code and
10:16 - sketch a yes someday maybe I'll use the
10:19 - fancy new imports and tech stuff let me
10:22 - just just have everything kind of line
10:24 - up let me add this in here so now TF
10:25 - digest should be there I should be able
10:27 - to go back and run this and not see any
10:30 - errors aha
10:33 - TF dot layers dot sequential is not a
10:38 - function so I probably just didn t have
10:40 - cut layers dot sequential the right
10:42 - thing you know I could go look I by the
10:44 - way I made an example oh it's just TF
10:47 - dot sequential
10:48 - okay so all I all I want to say is I
10:50 - just got that wrong it's a TF dot
10:52 - sequential so you know I could go look
10:55 - you know hopefully I would find this
10:57 - here at EF dot sequential yeah
11:00 - models creation there it is TF
11:02 - non-sequential so I just had that wrong
11:03 - okay let's try refreshing this yet again
11:08 - slider is not defined all right let me
11:12 - fix this learning rate issue 0.1 I just
11:15 - want the thing to run okay so it's going
11:18 - it's still working with the my neural
11:24 - network library not the new tension
11:27 - photo gs1 but let's keep stepping
11:29 - through so ah so what am I missing here
11:32 - so when I make a model this is now I've
11:35 - architected the model
11:36 - I've architected this particular
11:38 - architecture but I need to do another
11:41 - step I need to compile the model and I
11:45 - need to define the loss function and the
11:49 - optimizer basically I need to say like
11:53 - okay well this is how I'm going to
11:55 - determine how well the model is
11:58 - currently performing with the training
12:00 - data and testing data potentially but
12:03 - I'm not getting testing data will come
12:04 - in my next video about classification
12:07 - but here I'm not making a distinction
12:09 - between training and testing date I'm
12:10 - conflating those two concepts which is a
12:12 - big mistake and a problem but we're
12:14 - stepping through this stuff later by
12:15 - little by little like a butterfly
12:17 - flapping its wings it's not at all like
12:20 - a butterfly but I felt like I was being
12:21 - like a butterfly and then an optimizer
12:24 - is what sort of function what sort of
12:28 - algorithm am I using to adjust all of
12:32 - the weights of all these connections
12:33 - according to the loss function itself so
12:37 - I need to define those things so let me
12:44 - try to type it out how I think it is and
12:47 - then we'll go check so I know I need to
12:50 - create an optimizer TF optimizer like
12:57 - this and with a learning rate something
13:00 - like this like I'm fauna I want to have
13:03 - used to cast a create the set with some
13:05 - learning rate that's not correct this is
13:07 - me like trying to remember what the what
13:10 - the code is and then I need to say like
13:12 - model dot compile
13:14 - and then I think when I compile it I'll
13:16 - say things like this I'm going to
13:19 - compile it with this optimizer and this
13:21 - loss function like like root mean
13:26 - squared or something like that
13:28 - so this is what I'm remembering from
13:30 - when I looked at this at one time and I
13:32 - probably got this wrong so let's
13:33 - actually go look at the API Docs
13:35 - well first what's the chance that any of
13:37 - this actually makes sense okay TF
13:38 - optimizer is not a function so let's see
13:40 - how do we create the optimizer optimizer
13:47 - yes so it's this is what I want I want a
13:51 - TF train SGD this is how I create the
13:54 - optimizer is not a keyword in the API
13:57 - just I imagine that for myself so I need
14:00 - to say TF train SGD and then give it a
14:02 - learning rate so TF train SGD and there
14:07 - are other kinds of optimizers that will
14:09 - that i think i've even shown you and
14:11 - will use more and give it a learning
14:12 - rate like point 1 then i want to look at
14:17 - model compile so for compile well we can
14:24 - see in some examples here what I'm
14:26 - looking for is where the actual compile
14:29 - there it is compile
14:31 - so the compile function compiles it and
14:34 - give an optimizer a loss and I can also
14:36 - do some metric stuff I'm not going to
14:37 - worry about the metrics too much
14:39 - although maybe I'll try to come back
14:40 - towards the end of this video ok model
14:42 - dot compile optimizer loss I think this
14:44 - might actually be fine is it root mean
14:46 - squared so let's look for the loss
14:48 - functions all right apologies I've been
14:51 - saying root mean squared error for
14:54 - because I'm stuck in this world where
14:56 - you have to take the square root which
14:57 - you don't need to do here so just mean
14:59 - squared error that's all I need this is
15:01 - my loss function mean squared error now
15:07 - let us now go back here hit refresh all
15:13 - right things are happening things are
15:15 - going so the model is built the model is
15:19 - compiled and the next thing that I am
15:22 - ready to
15:22 - is now actually start putting data in
15:25 - the model the two things that we need to
15:28 - do now what are the two main steps I
15:30 - don't know why I came over here but
15:32 - things I'm over here first of all I drew
15:34 - this truth table thing a little bit
15:36 - weirdly and so you might recall just to
15:39 - be clear about what's going on this is
15:40 - my little drawing of the canvas right
15:42 - now and the idea of the canvas is that I
15:45 - want to see what the neural network
15:47 - thinks false false is at 0 0 I want to
15:51 - see what it thinks true false is at this
15:54 - right hands top right hand side the
15:57 - bottom left hand side I wanted to see it
15:58 - 0 1 and then I want to see here 1 1 so
16:02 - false is black 4 0 and true is white for
16:07 - 1 that's the way I'm gonna map the color
16:10 - so I should see some kind of bands of
16:13 - like I should be getting like something
16:15 - like this
16:16 - so darker here and like this so let's go
16:19 - look does that match yeah that's exactly
16:21 - what I'm seeing here so the reason why I
16:25 - came over here is what I need what I
16:27 - think I'd there's two things that I need
16:28 - to do number one is I need to train the
16:30 - model to produce this output my desired
16:33 - output that I think it should do and
16:35 - then I also need to ask the model to
16:37 - predict so I can draw what it thinks its
16:41 - output is so the two and the and so the
16:44 - two steps here I don't run out of space
16:46 - but in the attention flow chess library
16:49 - I wanted you I need to look at the
16:51 - predict function and the fit function
16:54 - predict for just saying here's the
16:57 - inputs what is your output the fit
16:59 - function for saying here's labeled
17:01 - inputs inputs with known outputs adjust
17:05 - optimize yourself according to that so
17:07 - I'm gonna do things backwards I'm gonna
17:09 - do just the predict step first I just
17:11 - want to see when you starts up with no
17:13 - training
17:14 - what visual output - again so coming
17:20 - back to the code let's look here so this
17:23 - this is what I need to replace so I need
17:26 - to now I need to ask the tensorflow
17:29 - layer sequential model thingy to give me
17:33 - the Y neural model dot predict
17:36 - but what does it expect it's predict
17:40 - function unlike my predict function
17:41 - cannot get a regular array it expects a
17:44 - tensor so I need to make the X's into TF
17:49 - tensor one D with those inputs and pass
17:54 - those through predicts now here's the
17:57 - thing there there's a lot of issues with
18:00 - this that I need to resolve and this is
18:03 - gonna run really slow I need to actually
18:04 - do this as a batch process I'm gonna get
18:06 - to all that but just looking at what
18:07 - I've got so far
18:09 - model dot predict there's there's a
18:12 - question of like is this happen
18:13 - synchronously or asynchronously this
18:15 - actually is happening synchronously but
18:18 - the problem is I need to say fill with
18:22 - the result like I need to look get that
18:25 - number out and to get the number out I
18:28 - actually want to call dot data and that
18:30 - happens asynchronously so because I'm
18:33 - working with such teeny bits of data
18:35 - right now I think I'm gonna use data
18:37 - sync and there could be issues with that
18:39 - and as I move more forward we're gonna
18:41 - see when I really need to be more
18:43 - thoughtful about callbacks and promises
18:45 - but I'm gonna use data sync right now so
18:48 - I should be able to predict the output
18:50 - with this input get that data and then
18:53 - let me just say console.log Y and I'm
18:59 - gonna make the resolution here of of the
19:05 - oh yeah the resolution really big like
19:08 - 50 because I just want to like look at
19:10 - very very little data to start with and
19:13 - let's look I'm not gonna draw anything
19:15 - let's just look and see what's coming
19:16 - out what's coming out here Y and then
19:18 - let me just say no loop so let's look in
19:20 - the console and see if we get anything
19:23 - error expected when checking dense dense
19:28 - one input to have two dimensions but it
19:30 - got array with shape to Oh once again I
19:33 - have the same problem I've had every
19:34 - single time I've done this with ten to
19:36 - flow guess so the good news is I want I
19:42 - don't want to just give this one D
19:45 - tensor though even though my data is
19:48 - just two values zero
19:50 - one zero one one and it's a
19:52 - one-dimensional array with two numbers
19:54 - in it I actually want to be able to do
19:56 - something like hey take these 15 data
19:58 - points and give me the results the
20:01 - predictions for all 15 of those and so
20:03 - what I really want to be doing is I
20:05 - always need to send in kind of like one
20:07 - order higher one degree one rank higher
20:12 - so this actually I'm just sending in one
20:16 - data it piece of data in point in point
20:20 - input frame stopped work and this now I
20:27 - also have to say tensor 2d now cuz it's
20:30 - a 2d tensor
20:32 - there we go ah so we could see look at
20:34 - this the results came out for all those
20:36 - little spots you can see in little
20:38 - numbers between zero and one in an array
20:39 - so now I can instead of console logging
20:43 - Y and I just want that it comes back in
20:47 - an array but there's only one number I
20:49 - care about I could put this back in here
20:53 - I can take out no loop and I can run it
20:57 - let me can see look there is my current
21:00 - visualization of X or I'm not really
21:07 - done I've so much left to do in this
21:09 - video that is been recording for the
21:11 - last three or four days all right
21:15 - one thing I want to do is I just want to
21:17 - say stroke 255 I just want to sort of
21:20 - see a little bit more okay that's
21:22 - actually what I'm looking at here I
21:24 - actually want to make the resolution for
21:26 - debugging debugging wise I'm I also want
21:29 - to make the resolution a little bit
21:30 - bigger so let's see now one thing I'm
21:33 - curious about
21:33 - let's look at the frame rate here that's
21:37 - running at 30 frames per second so
21:38 - that's fine
21:39 - let me now actually make the resolution
21:41 - much much higher like this oh my
21:49 - goodness oh it's not even getting to the
21:51 - first frame there we go look at the
21:55 - frame rate can't even give me a frame
21:56 - rate it's so stuck it can't even get one
21:58 - frame per second so here's the thing I
22:01 - have done something very very
22:03 - very bad and III needed to stop it no
22:08 - Luke stop you don't have to do any more
22:11 - work and let's put the resolution back
22:13 - at 100 and let's think about this what's
22:16 - going on here look at this look at this
22:18 - predict function and look at this data
22:20 - sync function what am i doing I am
22:22 - calling that function multiple times
22:25 - every single for every single spot on
22:28 - that grid when I'm working with
22:31 - something like tensorflow Jas whenever I
22:33 - create a tensor or feed data into a
22:36 - model the data has to go from my code
22:40 - onto the GPU and then when it's done
22:43 - that data sync is pulling it off of the
22:46 - GPU so I can use it again in my code
22:47 - that graphics processing unit where all
22:50 - the math is happening behind the scenes
22:51 - I want to do that as few times as
22:54 - possible look how this is I'm creating
22:58 - this two-dimensional array with one
23:00 - thing in it you know ten hundred times I
23:02 - could just create one array with a
23:06 - hundred things in it and call predict
23:08 - once that's what I want to do so I what
23:11 - I need is for this nested loop to happen
23:13 - twice once to actually wants to set up
23:18 - the data and another to draw all the
23:19 - results so I'm going to copy paste this
23:21 - just put it right below so this now what
23:25 - we need to do is create the input data
23:32 - so I'm gonna say let inputs be a blank
23:38 - array then I'm going to say inputs dot
23:43 - push and I'm going to just push in x1 x2
23:52 - so I'm going to put every single x1 x2
23:55 - all the way along I don't want to create
23:58 - the tensor or do this here I don't want
24:01 - to do the drawing stuff here I just want
24:03 - to create I just want to have a loop
24:04 - that creates all the data now I can get
24:07 - the X's is all of those inputs into a 2d
24:11 - tensor and the Y's this is now the Y's
24:15 - is
24:17 - and now here's the thing I don't just
24:20 - let's Oh hold on I got a look at what
24:21 - that's gonna look like let's comment
24:23 - this out for a second let's look at the
24:26 - Y's and see what that looks like oh okay
24:35 - SketchUp 78 error
24:37 - OOP oh no let there just inputs push
24:44 - okay oh I want to say no loop let me
24:48 - leave that no loop in put it back
24:50 - I just want look at it once so you can
24:53 - see what did I get I got a big array of
24:56 - sixteen numbers I got all the results so
25:00 - now what I want to do is back here now I
25:05 - just need to do the drawing and I don't
25:09 - need to the input data I don't need the
25:10 - model all I need to do is draw and I
25:13 - need to say fill wise index what I plus
25:17 - J times the number of columns maybe
25:20 - right because this is a one dimensional
25:23 - array to describe all each spot in that
25:27 - grid I could do something like let me
25:29 - just do this let index equal zero I'm
25:32 - going to say fill based on this
25:34 - particular one and I don't need this
25:37 - even sorry and I just need to say then
25:41 - index plus plus right so what are the
25:43 - steps here create the data get the
25:47 - predictions draw the results okay there
25:59 - we go so now we can see this is working
26:02 - I mean it's not doing anything but now
26:04 - let's check this framerate question we
26:06 - don't need to console.log the Y's I'm
26:08 - going to get rid of the no loop let's
26:12 - let's refresh this let's look at the
26:16 - framerate 30 frames per second let's
26:19 - let's pump it up a little but pump you
26:23 - up a little and where was the resolution
26:28 - there let's make this 20
26:30 - go crazy and look at the framerate there
26:34 - we go 30 frames per second no problem
26:36 - because I'm only one time through draw
26:39 - trying to copy data onto the GPU and get
26:43 - it I'm only calling predict once and we
26:45 - can just to check we can go to 10 and we
26:50 - can look at the framerate yeah you could
26:53 - see it's like kind of running a little
26:54 - bit slow but this is because I'm not
26:56 - being so thoughtful about the
26:57 - asynchronous nature of this stuff I
26:59 - could do other things to optimize it but
27:00 - I'm just gonna ignore that and leave it
27:03 - at let me make it 25 the chat is giving
27:07 - me some even further optimization which
27:09 - is why am i bothering to do this in draw
27:13 - this is something that the these inputs
27:16 - never change I could just do them once
27:18 - at the beginning because they're and and
27:21 - I can I can ask for I spent many many
27:23 - times in draw so let's actually fix that
27:25 - so I'm actually gonna I'm gonna take
27:27 - this and say let I'm gonna make this
27:29 - globe these global variables I don't
27:33 - know if you guys can hear the music
27:34 - that's coming from the room next to me
27:35 - but it's the air alright then
27:41 - oh but the width and height does not
27:44 - exist until after create canvas so let
27:51 - me do this and let me do this okay
27:59 - so now that's there now I should be able
28:02 - to take this the input data and put this
28:08 - right here in the beginning and then I'm
28:12 - going to make a variable called X's and
28:16 - X is and where did I do that here and
28:24 - then create those X's so I'm now doing
28:27 - this in setup and then in draw the only
28:32 - thing I need to do in draw is run the
28:34 - predict this is going to make things run
28:35 - a lot faster let's make sure it still
28:37 - works
28:42 - here we go okay so you notice we get
28:46 - like a different color each time i
28:47 - refresh because the neural network model
28:50 - the sequential model is initializing
28:51 - everything randomly but now I get to
28:53 - Train it now I think we're ready to
28:58 - train it so here is what I did when I
29:04 - had my previous my own JavaScript neural
29:07 - network library I called neural network
29:09 - trained data inputs data outputs all
29:13 - right so if I only I could remember
29:15 - exactly what I wrote when I made that TF
29:18 - layers tutorial but I know that what I
29:21 - need to do here and is I need to do
29:24 - something like this model dot fit some
29:27 - X's and some wise that's the training
29:30 - that's the equivalent and the learning
29:32 - rate is irrelevant and I don't
29:35 - necessarily need to do it look this is
29:36 - basically what I want to do every time
29:38 - through draw I want to try to fit the
29:40 - model with some training data so let's
29:42 - first make the training data this is not
29:44 - exactly right I need to figure out and I
29:46 - need to use a weight that need to think
29:47 - asynchronously but this is the idea so
29:51 - if I go back to the top here this is my
29:53 - training data now one thing I definitely
29:56 - need to change is I want to keep the X's
30:03 - and Y's separate in training so I'm
30:06 - going to do this is I'm just going to do
30:10 - this kind of manually because I what's
30:12 - the big deal so let me make the training
30:16 - set and then one one those are the the
30:23 - X's now let me look at the Y's and the
30:29 - Y's would be 0 1 1 0 then I need those
30:37 - to be tensors so I need to say Const
30:43 - trait TF X's
30:52 - so I've got to think of a good naming
30:54 - for this I kind of want them to call
30:55 - actually you know what I'm just gonna
30:56 - call it do I have a global X yeah I have
30:59 - a global X's already hmm-hmm-hmm tray TF
31:04 - X's equals 10 sir 2 D tensor 2 D o TF
31:11 - tensor 2 D you know what I'm gonna do I
31:15 - don't need these I don't need two
31:17 - separate sets of variables I'm just
31:19 - gonna create it I'm gonna call this ah
31:21 - everything is so much more complicated
31:23 - than I make it so if we're simple then I
31:25 - make it I'm just gonna make these
31:27 - tensors directly by saying TF dot tensor
31:31 - to D and then I'll put the parentheses
31:35 - around this and there now I'm in a
31:37 - tensor then F is a TF tensor to D and
31:41 - now I made this a tensor okay now I've
31:46 - got the training data and I'm gonna get
31:48 - rid of this this is the old way that I
31:50 - had the training data which is totally
31:51 - unnecessary so this the training X's and
31:54 - the training wise you with me if you're
31:57 - still watching I don't know dude get up
32:00 - and do it some jumping jacks let's see
32:06 - now I need to do model that fit now
32:09 - model dot fit happens asynchronously so
32:13 - let's put it in its own async function
32:18 - called train model now if you don't know
32:24 - what it means to write a function that
32:27 - is tagged with the keyword a think this
32:30 - is part of es8
32:31 - a very newish version of JavaScript and
32:34 - I've made a bunch of videos about what
32:35 - that is that you can go back and watch
32:37 - but this is basically a way for me to
32:39 - now say wait model dot fit and then
32:46 - let's look at actually let's look at the
32:48 - fit function model dot evaluate compile
32:53 - predict fit so what I need is to give it
32:59 - the X's and the wise there's batch size
33:01 - I'm not going to worry about there's
33:02 - epochs I'm not going to
33:04 - worried about our epics and so H will
33:08 - give me back the history so let's just
33:11 - see here I'm now gonna say train model
33:15 - dot then H console dot log H dot loss
33:23 - index 0 let's say no loop again so
33:28 - basically what I'm doing here is I want
33:33 - to call this function train model and
33:35 - I'm using this idea of promises it's
33:37 - going to await the model on I need to
33:39 - return do I say a weight return or
33:43 - returned a weight no I must say return
33:47 - oh wait return oh wait model dot fit so
33:50 - I'm going to return a promise which will
33:53 - have the result of the fit function and
33:55 - I don't know if this is right I want to
33:57 - just look I'm going to do that I'm going
33:59 - to call to train model every time and
34:01 - draw I might need to do this somewhere
34:02 - else just right now and then see what
34:04 - the loss is okay
34:10 - onyx 73 async function async function
34:16 - I've got to say function function
34:18 - it's an async function not an async
34:20 - there we go wise is not to find where a
34:24 - train model oh right this is I forgot to
34:28 - call it train X's and train wise this is
34:31 - my training data train X's and train
34:34 - wise nice other word train just appears
34:37 - of around we're doing machine learning
34:41 - you drink your glass of milk or whatever
34:43 - it is you're having while you're
34:44 - watching this coding train stuff cannot
34:48 - read property zero of undefined a train
34:51 - model than H all right let's look just
34:55 - console.log H note that is what I've
34:58 - done okay
35:03 - history loss zero okay oh no by the way
35:08 - I didn't give it any testing data so
35:10 - what's our computing the loss from
35:13 - history so this is I'm gonna call this
35:16 - result
35:17 - result history dot loss index zero all
35:23 - right there we go
35:24 - there we go now let's let it do that
35:28 - over and over again in draw so I let
35:37 - this run a little bit and unfortunately
35:39 - see it's getting nowhere this loss which
35:41 - I'm not sure exactly how things I don't
35:43 - think about that come back to it is not
35:46 - going down anymore so what could be some
35:49 - problems here remember one is maybe my
35:51 - learning rate is no good
35:52 - not that it's no good it may be it's too
35:54 - low so where did I set up that learning
35:56 - rate again let me get rid of by the way
35:59 - I just want to now delete I want to make
36:01 - sure I'm not using any of my old neural
36:05 - network code so I'm deleting all
36:07 - references to that so this is now purely
36:12 - tensorflow Jess and and let me refresh
36:15 - and run this again and let me look into
36:19 - sketch dot yes and find where did I set
36:21 - the learning rate right here let's set
36:23 - it to point five and see what we get yes
36:30 - getting better I'm gonna let this run
36:32 - for a little bit and I'll be back
36:34 - [Music]
36:38 - alright I'm back and you can see the
36:40 - losses now kind of much lower and you
36:42 - can start to see the visual that I'm
36:43 - expecting which has a true value in this
36:45 - corner true value in the top corner of
36:47 - darker false values in those corners but
36:49 - it's still kind of performing rather
36:51 - poorly one thing that I forgot to do is
36:54 - when I call the fit function there are a
37:00 - set of options that I can pass in for
37:04 - the number of epochs and all sort of
37:06 - thing but one of the ones that I really
37:07 - want to pass in here is called shuffle
37:09 - shuffle takes the training data and it
37:12 - shuffles the order of it each time right
37:14 - now I'm trading it with the same four
37:17 - data points in the same order every time
37:18 - which could be a bit of a problem and
37:21 - okay so let me now let me hit refresh
37:24 - here and run this again
37:26 - [Music]
37:30 - all right so you know it actually it
37:33 - actually is working it got the correct
37:35 - training result it's a little gray scaly
37:37 - in a way that I would like to be able to
37:39 - like emphasize visually what it's do but
37:41 - you could see the loss has gone way down
37:42 - but it took a while to get there but I
37:45 - want to add a few things to this and try
37:47 - to fix it up a little bit before I do
37:49 - anything I was reminded by the chat
37:51 - being over here that I haven't thought
37:53 - about memory management at all so I'm
37:55 - gonna say like no Luke for a second to
37:57 - just sort of turn this off then I'm
38:00 - going to say memory numb tensors whoops
38:03 - no no way TF memory dot numb tensor I
38:09 - know I'm trying to use ah I'm gonna say
38:12 - numb tensors TF dot memory numb numb
38:18 - there it is there it is I can get it
38:20 - there it is thirty-two thousand two
38:23 - hundred five tensors that's crazy so I
38:25 - need to deal with that
38:26 - I'm just making temperatures and letting
38:28 - them leak everywhere so I can manually
38:30 - run dispose but I've got kind of an
38:32 - issue whereas predict is gonna like make
38:34 - a lot of tensors behind the scenes as
38:36 - well as a model dot fit so I can use the
38:39 - TF tidy function so I'm going to say TF
38:42 - tidy and then I just need to I'm gonna
38:47 - use the es6 arrow notation which you can
38:51 - watch my videos about what that is but
38:52 - and i've kind of gone through what tidy
38:54 - is tidy says anything inside of this
38:56 - code clean up the memory afterwards
38:59 - basically and then I'm gonna I probably
39:01 - could put this around everything but I
39:03 - just want to and I don't need this stuff
39:04 - anymore I just want to keep these two
39:06 - areas separate because I think I'm gonna
39:09 - at some point I really should change the
39:11 - way doing the fitting of the model
39:13 - excuse me in draw is somewhat
39:15 - problematic so now I'm gonna just tidy
39:18 - all of this I think that's right drive
39:25 - an extra parenthesis there yes okay so
39:28 - now let's run this again and let me
39:33 - comment out this console.log I don't
39:37 - want to see that right now
39:40 - oh alright and now let's look at the
39:46 - number of tensors 15-15-15
39:49 - so now I've gotten rid of the memory
39:50 - leak let's check out the frame rate 30
39:55 - frames per second so this is running for
39:57 - on now
39:57 - let's I just want to be able to look at
39:59 - what's happening a little bit better so
40:00 - I'm actually gonna draw the number of
40:02 - the output inside each one of these
40:04 - things so let's do that so where am i
40:07 - drawing the rectangles here I'm going to
40:09 - say let the brightness value equal this
40:14 - and I'm going to fill the rectangle with
40:17 - that brightness and then I'm going to
40:19 - say fill 255 mind it like the inverse
40:22 - color I'm going to say text number
40:26 - format the y-values index with just two
40:33 - decimal places and I'm gonna put that at
40:37 - boy this is awkward
40:39 - alright the exercise this is gonna be
40:41 - height x resolution plus resolution / -
40:44 - I'm gonna say text-align:center
40:47 - text-align:center kind of center and
40:51 - then I'm gonna put the I'm just going to
40:53 - draw in the center of the rectangle and
40:57 - this should be J the text so let's see
41:00 - let's do this now so we can see you look
41:05 - there's lots of numbers there I think my
41:06 - number format thing didn't work 1 comma
41:11 - 2 and let's use a lower resolution just
41:15 - so I can see it better
41:18 - there we go now interestingly I can't
41:21 - see the numbers but there they go right
41:23 - you can see this is what it's getting
41:25 - the output for each one of these and I
41:28 - want to look at the law so you can see
41:30 - because it's just going so slow it's
41:33 - getting so overtime it's getting a
41:38 - little better but I really want to see a
41:40 - train much faster so let me see I have
41:43 - one idea one last thing I can add to
41:45 - this even though I and I have some
41:46 - suggestions for what I might do next but
41:48 - you can see all the numbers are starting
41:49 - to appear lovely because I forgot when
41:52 - it's gray when it's at point five
41:54 - 2:55 90.5 it's gonna be the same car I
41:56 - kind of like that effect so so what I
41:59 - want to do is what happens here if I
42:02 - actually give it tell it don't just do
42:06 - it once like do it 10 times do 10 a pox
42:10 - per cycle of fitting let me run this
42:15 - again and let me look at the let's
42:20 - actually have the loss continue to print
42:22 - out and there we go
42:27 - still running pretty fast you can see
42:29 - the losses going down and relatively
42:31 - quickly I am getting myself to the point
42:35 - where I'm starting to see you know this
42:37 - is definitely all the way cut getting
42:38 - all the way down to zero there this is
42:40 - getting way up to one there it's getting
42:42 - a little bit stuck it's having trouble
42:43 - with this size outside I imagine it'll
42:45 - get there eventually we could do some
42:47 - fun stuff so I'm gonna give it for no
42:50 - real reason all but just for fun four
42:51 - hidden nodes and I'm also gonna let me
42:54 - change the resolution to 25 let me make
42:57 - the text size something like eight point
43:02 - and let me whoops
43:08 - refresh it alright I'll let this run for
43:14 - a bit and you can kind of see here now
43:17 - you can see all the X or values here's a
43:19 - nice beautiful little map grayscale map
43:21 - of all X sorts getting all the way up to
43:23 - true and all the way down to zero at the
43:26 - corners this is pretty good so this is
43:29 - running at if I take out this
43:32 - console.log I can now take a look at the
43:39 - frame rate it's running kind of slow
43:42 - alright so the chat has given me some
43:44 - really helpful tips I've made quite a
43:46 - few little like weird little errors and
43:47 - mistakes here and I want to just fix
43:50 - this up a bit I think it's actually
43:51 - gonna be easier to look at and watch if
43:54 - I just go back to a lower resolution so
43:58 - let's make this 40 and let me refresh
44:02 - this okay so here we go so this is now
44:06 - working
44:07 - training itself for X or you can see
44:10 - it's kind of moving along here now what
44:12 - what the real the real thing that's
44:14 - problem problematic here is the draw
44:17 - loop is happening over and over again
44:18 - and then I'm triggering something
44:20 - asynchronous in draw and I could be
44:22 - asking to train the model before it's
44:24 - even done with the previous training
44:25 - cycle so this really should not be
44:28 - happening in draw now tensorflow Digest
44:31 - has a function called TF next frame I
44:34 - want you to explore it and make a
44:37 - version of this with Tia next frame as
44:39 - like an exercise after this video is
44:40 - over but I'm gonna do it a different way
44:42 - without that because I got to come back
44:43 - to that in a different video but first
44:45 - of all this I also learned this is
44:47 - totally unnecessary the the await a
44:52 - couple things number one is because
44:54 - there's just one thing happening in here
44:56 - I could just return the promise this
44:58 - doesn't actually have to be an async
45:00 - function and then I do not need the TF
45:03 - tidy because model dot fit kind of will
45:05 - clean itself up automatically for you so
45:08 - this this should still work just fine
45:11 - and I should be able to see the number
45:14 - of tensors is still fifteen so that was
45:16 - something that I didn't need that I've
45:17 - now fixed now what I really want to do
45:20 - is I want to get this out of draw so
45:22 - let's comment this out here and what I'm
45:24 - actually going to do is I'm going to
45:26 - write a separate function called train
45:29 - and in that function I'm going to say
45:35 - set time okay wait wait
45:39 - I'm gonna call train model wait hold on
45:42 - I'm gonna call train model yes yes in
45:46 - that function I'm gonna do this
45:52 - I'm a separate function that does this
45:55 - piece of it that console logs the
45:57 - history and I couldn't and what I want
46:01 - to do is I want to say set timeout call
46:05 - the train function in 100 milliseconds
46:07 - so I want to just let the program start
46:09 - 200 Phyllis X late later call this train
46:11 - function train the model which does the
46:13 - fitting when that's done log the history
46:16 - and now say set timeout train 100 so I'm
46:23 - good this is sort of like recurse like
46:25 - like I don't want you set interval here
46:27 - because I only want to call train again
46:28 - once it's finished with training the
46:30 - model itself so this is kind of like hey
46:32 - train and and by the way I could just
46:34 - increase the number of epochs or maybe
46:36 - do some kind of luke but I think this
46:38 - would be a sort of nice way to
46:39 - demonstrate it and if I just called
46:40 - train directly without a set timeout I'm
46:46 - never going to be giving back control
46:47 - for a second I could end up with sort of
46:49 - like blocking so I might even be able to
46:51 - get this down to like 10 milliseconds
46:52 - just something really really low so
46:54 - let's run this and sort of see same
46:58 - result we can see there we go
47:00 - things are working but at least now I
47:02 - have gotten that out of draw so draw is
47:05 - happening on its own and in fact what I
47:07 - could really do is I could say hey try
47:09 - doing this with like a hundred epochs
47:11 - each time epochs what is it and you can
47:15 - see the lost function is coming out much
47:18 - more slowly but whoops but the frame
47:21 - rate let me just clear this for a second
47:25 - yeah the frame rate is quite fast hold
47:32 - on this is too confusing I probably need
47:35 - to give give it back more time let's do
47:38 - like another little break let me let me
47:40 - I just want to like take out the
47:43 - console.log thing so I can look at the
47:44 - frame rate I should just put the frame
47:46 - rate in the Dom wouldn't that be smart
47:49 - but you can see now I'm getting 60
47:51 - frames 30 frames of getting like a
47:53 - really pretty high frame rate even
47:54 - though the training is happening it's
47:56 - almost it's kind of like that there is
47:59 - no threading in JavaScript so these
48:01 - things are just like passing off and
48:02 - really this might be a place where like
48:04 - web worker
48:05 - something could do the training behind
48:06 - the scenes in some fancy way which maybe
48:08 - I will get to at some point oh my
48:10 - goodness
48:12 - so Alka is suggesting it might be better
48:17 - to use the draw loop and a boolean to
48:19 - know when it's safe to call it again
48:20 - that would also be a good idea so you
48:22 - can see though you can see what kind of
48:24 - like employed my hair out what kind of
48:26 - sort of like hassle situation we've
48:28 - gotten in but really let's just put this
48:30 - back to like two epochs
48:32 - let's put this to like a little 10
48:34 - milliseconds and we can sort of feel
48:37 - like there we go and I can look at the
48:39 - frame rate it's running nice 30 frames
48:41 - per second and even though and it's and
48:43 - at some point it's gonna get there
48:45 - what's that loss I forgot to console.log
48:47 - loss come back to me come on oh but
48:53 - let's see I have an idea what if just
48:55 - before I go just before I go what if we
48:57 - try using a different optimizer what if
49:03 - we try using for example a different
49:08 - loss function hold on this video is
49:16 - already 18 hours long
49:17 - what if oh no no no a different
49:20 - optimizer sorry what if I tried using
49:22 - the the Adam optimizer so let's just try
49:29 - that just for fun times
49:32 - let's give it a lower learning rate that
49:51 - was pretty exciting let's go let's go
49:55 - let's uh let's make the resolution back
49:59 - to like 20 let me make the font size
50:02 - like nice and tiny for us we give myself
50:05 - some more space here hit refresh and
50:08 - then let's look at this look at it
50:11 - learning there wow look at that that is
50:13 - beautiful look at it learning XOR so
50:15 - nice and fast I'm just gonna hit refresh
50:17 - again
50:21 - I really should explain what these
50:23 - optimizers are but if I go back and look
50:25 - under here we can see what some of these
50:27 - are Adam the the a da coming from the
50:30 - word adaptive and you could always click
50:32 - here and look at this paper which
50:33 - describes this particular method for
50:36 - optimization which is a little bit
50:38 - different than stochastic gradient
50:39 - descent and apparently things work a lot
50:42 - faster with this XOR problem so as I go
50:44 - forward into more of these videos
50:46 - hopefully we can dig into what some of
50:48 - these different optimizers do and kind
50:50 - of understand why I might pick one over
50:52 - the other in certain situations all
50:54 - right but I'm just going to just leave
50:56 - this B I'm going to hit refresh
50:58 - I'm going to watch it learn and train
51:00 - train train train XOR oh I don't know
51:04 - some things you could do investigate TF
51:06 - dot frame give me a little slider try
51:09 - different architectures different
51:10 - optimizers types of different activation
51:12 - functions I don't know if you actually
51:14 - made it all the way to the end of this
51:17 - video I don't know hashtag something I
51:22 - should Eric has taught me to watch one
51:25 - of those videos reviewing the JavaScript
51:26 - event loop which I definitely need to do
51:28 - so I mean I'm gonna be back with more
51:30 - someday yeah goodbye goodbye goodbye
51:34 - thank you
51:40 - [Music]

Cleaned transcript:

oh hello welcome to a Cody challenge yeah I know what you're thinking I don't know what you're thinking I know what I'm thinking that looks like Cody challenge number 92 XOR which is probably one of the less interesting creative like sort of just technical coding challenge demonstrations that you've done why why why are you doing it again well Eric from the coding train community writes a nice explanation because I was before I started this having a real hangup about this sometimes it's important when learning something new to base your exploration around an example which is fairly trivial and you understand intimately well so here's the thing I I'm learning something new I'll come back here and the thing that I am learning something new is this tension floaty a thing and wouldn't it be fun to make like play pacman with it or the emoji scavenger hunt project or teachable machine or play a piano with it all these things all pose that oh my god we got it we got I want to get to that I could just sort of go there right now I will get there eventually but I'm trying to learn the basics of how the library works and I'm trying to step through this slowly so I will say that we're if you're watching this video right now where you are is not necessarily in the most beginner or friendly place because I'm working with tensorflow das natively to implement basically like a weird math problem it's not that weird of a problem actually but a very basic trivial math problem just to see how tensorflow Gs works that's what I'm trying to do with this coding challenge and about 20 or 30 minutes he'll be coding this coding challenges just look it's by like four hours in 72 minutes long which is why would I say 72 minutes cuz that's five hours and twelve minutes but I don't know but the trajectory that I'm on is I'm going to start doing some stuff inching my way towards hell let's actually use some data let's use some more data and maybe some images and so I've got a bunch of things that I'm stepping through and I'm trying to get to the point where I'm going to use this other machine learning library called ml five which at the time of this recording hasn't really officially been released yet but builds on top of tensorflow das thank you everyone who made floats wherever you are to try to create some more accessible interfaces to some of the algorithms and models that you things that you can do with tensor flow digest without having to do the level memory management and math operations stuff so all that is coming and I just took a lot of time to this coding job to say that to you so but as much as I kind of don't I haven't I'm not so sure but we're but it's a why is XOR so here's the thing this is why I want to need an example this is the first time I'm going to ever in any of my videos except for the other one that I made but this is the first time that I'm actually going to use the TF layers API to train to train a model with a data set to produce a certain output okay I did two tutorials about what the TF layers API is you could pause now and go and watch those and then come back here but in those videos they didn't actually do anything with TF layers just sort of talked through it typed out some code so the problem that I want to solve and apologies for explaining this probably for like the fifteenth time on this YouTube channel is very well known from machine learning X or because when the original perceptron was invented the single perceptron the model of an individual neuron that could receive inputs and generate an output it could not solve X or it just couldn't it's not a linearly separable problem and I've talked about that in other videos about why we need multilayer perceptrons so the nice thing about XOR is I can diagram for you the the architecture of the model that we need to create there are two inputs there is one output so that the inputs to the XOR problem are true and false values so unlike a so if I made a little truth table and or XOR right I can have true and true true true false false true false false and and operation would only ever give me true when both are true false false false and or operation would only ever give me would gives me true if just one of is true true true true false can you even see that I can't see on my monitor but hopefully you can now X or the X 4 exclusive gives me true only if 1 is true they can't both be true only one so in that case I get false true true false and the idea of linearly separable comes up here because I can draw a line here to separate true from false I can draw a line here to separate true from false but here I could do this but I can't draw a single line to separate true from false we need a more sophisticated model with a hidden layer so the inputs are things like a 1 and a 0 feed forward into the hidden layer activate feed to the output and the output should be a 0 or a 1 because ultimately I'm going to visualize the output as grayscale values and I want to see number I want to see grayscale values all the way between 0 & 1 it's the same thing I did in the previous coding challenge if you if you're happened to have watched that one alright so now I actually I'm gonna also do something where I start from the code from the previous coding challenge and so we can see there's this idea of training data the inputs to the XOR problem are 0 0 gives me a 0 0 1 gives me a 1 1 0 gives me a 1 at 1 1 gives me a 0 this is the training data and in my previous version of this I used my own neural network library so in theory I'm gonna get rid of the idea of a learning rate slider just before we can add that back in later but let me get rid of the learning rate slider basically I want to do exactly the same thing the difference is I'm going to say neural network equals TF layers sequential and maybe I'll call this the model instead of neural network so the idea of this is going out here so the idea here is that I want to replace my neural network library with tensorflow yes and so this for me what what the usefulness of this video is a me learn I spent all this time trying to build my own rather sort of terrible normal Network JavaScript library and going through that was sort of helpful in thinking about how the stuff works now if I can translate that into attention flow dot yes I'm gonna things are gonna hopefully start to settle and make more sense into my brain okay so now we need to what then this constructor here said and let's just put this back to this this constructor here said would make a neural network with two inputs two hidden nodes and one output so I need to duplicate that idea here with PF layers so let's go to the 10th floor is API reference and we're gonna go all scroll down to TF layers and what I want to make is a dense layer TF layers dint a dense layer is a fully connected layer so what I'm going to do is I am going to say let hidden equal TF layers dense and then I can put inside there an object that has the parameters of how I want to configure that layer and so how do I want to configure it the two things that I want need really need to do is this is the hidden layer right I need to give it an input shape right he just say what's coming in what's coming in that's what this is here I need to say how many nodes it has that's the number of units and then I probably has a default one but I can specify an activation function and again I'm just gonna use sigmoid as this historical activation function that I've been using in all my videos to date I'm going to assume talk about softmax what that is as well as some other activation functions like lading which is maybe more commonly used okay by the way nobody pronounces that way put me so don't get confused alright so I want to say input shape I believe is just there's just two inputs I also want to have two units two notes and activation is going to be sigmoid so now I have created the hidden layer yay the other layer that I need to create is the output layer and so what am I know the output layer I don't need to provide an input shape because the input shape can be inferred if I add them sequentially the inputs are not a layer so for this first layer the hidden layer I've got to say how many there are but now once I'm creating this next layer it can just the input shape is going to be defined by what was before it so now I'm going to say we have to stop it at the sound effects by excellent and now I'm going to say let output equal TF layers dense and all I need to say is units one activation sigmoid okay then ought to do is say Model Model dot add hidden model dot add output okay so this is the model now one thing I need to do is I definitely need to import the tensorflow JS library which I happen to have from one of my previous examples so I'm going right now I only have I have the p5 libraries in my index.html plus my crazy a neural network thing and my actual code and sketch a yes someday maybe I'll use the fancy new imports and tech stuff let me just just have everything kind of line up let me add this in here so now TF digest should be there I should be able to go back and run this and not see any errors aha TF dot layers dot sequential is not a function so I probably just didn t have cut layers dot sequential the right thing you know I could go look I by the way I made an example oh it's just TF dot sequential okay so all I all I want to say is I just got that wrong it's a TF dot sequential so you know I could go look you know hopefully I would find this here at EF dot sequential yeah models creation there it is TF nonsequential so I just had that wrong okay let's try refreshing this yet again slider is not defined all right let me fix this learning rate issue 0.1 I just want the thing to run okay so it's going it's still working with the my neural network library not the new tension photo gs1 but let's keep stepping through so ah so what am I missing here so when I make a model this is now I've architected the model I've architected this particular architecture but I need to do another step I need to compile the model and I need to define the loss function and the optimizer basically I need to say like okay well this is how I'm going to determine how well the model is currently performing with the training data and testing data potentially but I'm not getting testing data will come in my next video about classification but here I'm not making a distinction between training and testing date I'm conflating those two concepts which is a big mistake and a problem but we're stepping through this stuff later by little by little like a butterfly flapping its wings it's not at all like a butterfly but I felt like I was being like a butterfly and then an optimizer is what sort of function what sort of algorithm am I using to adjust all of the weights of all these connections according to the loss function itself so I need to define those things so let me try to type it out how I think it is and then we'll go check so I know I need to create an optimizer TF optimizer like this and with a learning rate something like this like I'm fauna I want to have used to cast a create the set with some learning rate that's not correct this is me like trying to remember what the what the code is and then I need to say like model dot compile and then I think when I compile it I'll say things like this I'm going to compile it with this optimizer and this loss function like like root mean squared or something like that so this is what I'm remembering from when I looked at this at one time and I probably got this wrong so let's actually go look at the API Docs well first what's the chance that any of this actually makes sense okay TF optimizer is not a function so let's see how do we create the optimizer optimizer yes so it's this is what I want I want a TF train SGD this is how I create the optimizer is not a keyword in the API just I imagine that for myself so I need to say TF train SGD and then give it a learning rate so TF train SGD and there are other kinds of optimizers that will that i think i've even shown you and will use more and give it a learning rate like point 1 then i want to look at model compile so for compile well we can see in some examples here what I'm looking for is where the actual compile there it is compile so the compile function compiles it and give an optimizer a loss and I can also do some metric stuff I'm not going to worry about the metrics too much although maybe I'll try to come back towards the end of this video ok model dot compile optimizer loss I think this might actually be fine is it root mean squared so let's look for the loss functions all right apologies I've been saying root mean squared error for because I'm stuck in this world where you have to take the square root which you don't need to do here so just mean squared error that's all I need this is my loss function mean squared error now let us now go back here hit refresh all right things are happening things are going so the model is built the model is compiled and the next thing that I am ready to is now actually start putting data in the model the two things that we need to do now what are the two main steps I don't know why I came over here but things I'm over here first of all I drew this truth table thing a little bit weirdly and so you might recall just to be clear about what's going on this is my little drawing of the canvas right now and the idea of the canvas is that I want to see what the neural network thinks false false is at 0 0 I want to see what it thinks true false is at this right hands top right hand side the bottom left hand side I wanted to see it 0 1 and then I want to see here 1 1 so false is black 4 0 and true is white for 1 that's the way I'm gonna map the color so I should see some kind of bands of like I should be getting like something like this so darker here and like this so let's go look does that match yeah that's exactly what I'm seeing here so the reason why I came over here is what I need what I think I'd there's two things that I need to do number one is I need to train the model to produce this output my desired output that I think it should do and then I also need to ask the model to predict so I can draw what it thinks its output is so the two and the and so the two steps here I don't run out of space but in the attention flow chess library I wanted you I need to look at the predict function and the fit function predict for just saying here's the inputs what is your output the fit function for saying here's labeled inputs inputs with known outputs adjust optimize yourself according to that so I'm gonna do things backwards I'm gonna do just the predict step first I just want to see when you starts up with no training what visual output again so coming back to the code let's look here so this this is what I need to replace so I need to now I need to ask the tensorflow layer sequential model thingy to give me the Y neural model dot predict but what does it expect it's predict function unlike my predict function cannot get a regular array it expects a tensor so I need to make the X's into TF tensor one D with those inputs and pass those through predicts now here's the thing there there's a lot of issues with this that I need to resolve and this is gonna run really slow I need to actually do this as a batch process I'm gonna get to all that but just looking at what I've got so far model dot predict there's there's a question of like is this happen synchronously or asynchronously this actually is happening synchronously but the problem is I need to say fill with the result like I need to look get that number out and to get the number out I actually want to call dot data and that happens asynchronously so because I'm working with such teeny bits of data right now I think I'm gonna use data sync and there could be issues with that and as I move more forward we're gonna see when I really need to be more thoughtful about callbacks and promises but I'm gonna use data sync right now so I should be able to predict the output with this input get that data and then let me just say console.log Y and I'm gonna make the resolution here of of the oh yeah the resolution really big like 50 because I just want to like look at very very little data to start with and let's look I'm not gonna draw anything let's just look and see what's coming out what's coming out here Y and then let me just say no loop so let's look in the console and see if we get anything error expected when checking dense dense one input to have two dimensions but it got array with shape to Oh once again I have the same problem I've had every single time I've done this with ten to flow guess so the good news is I want I don't want to just give this one D tensor though even though my data is just two values zero one zero one one and it's a onedimensional array with two numbers in it I actually want to be able to do something like hey take these 15 data points and give me the results the predictions for all 15 of those and so what I really want to be doing is I always need to send in kind of like one order higher one degree one rank higher so this actually I'm just sending in one data it piece of data in point in point input frame stopped work and this now I also have to say tensor 2d now cuz it's a 2d tensor there we go ah so we could see look at this the results came out for all those little spots you can see in little numbers between zero and one in an array so now I can instead of console logging Y and I just want that it comes back in an array but there's only one number I care about I could put this back in here I can take out no loop and I can run it let me can see look there is my current visualization of X or I'm not really done I've so much left to do in this video that is been recording for the last three or four days all right one thing I want to do is I just want to say stroke 255 I just want to sort of see a little bit more okay that's actually what I'm looking at here I actually want to make the resolution for debugging debugging wise I'm I also want to make the resolution a little bit bigger so let's see now one thing I'm curious about let's look at the frame rate here that's running at 30 frames per second so that's fine let me now actually make the resolution much much higher like this oh my goodness oh it's not even getting to the first frame there we go look at the frame rate can't even give me a frame rate it's so stuck it can't even get one frame per second so here's the thing I have done something very very very bad and III needed to stop it no Luke stop you don't have to do any more work and let's put the resolution back at 100 and let's think about this what's going on here look at this look at this predict function and look at this data sync function what am i doing I am calling that function multiple times every single for every single spot on that grid when I'm working with something like tensorflow Jas whenever I create a tensor or feed data into a model the data has to go from my code onto the GPU and then when it's done that data sync is pulling it off of the GPU so I can use it again in my code that graphics processing unit where all the math is happening behind the scenes I want to do that as few times as possible look how this is I'm creating this twodimensional array with one thing in it you know ten hundred times I could just create one array with a hundred things in it and call predict once that's what I want to do so I what I need is for this nested loop to happen twice once to actually wants to set up the data and another to draw all the results so I'm going to copy paste this just put it right below so this now what we need to do is create the input data so I'm gonna say let inputs be a blank array then I'm going to say inputs dot push and I'm going to just push in x1 x2 so I'm going to put every single x1 x2 all the way along I don't want to create the tensor or do this here I don't want to do the drawing stuff here I just want to create I just want to have a loop that creates all the data now I can get the X's is all of those inputs into a 2d tensor and the Y's this is now the Y's is and now here's the thing I don't just let's Oh hold on I got a look at what that's gonna look like let's comment this out for a second let's look at the Y's and see what that looks like oh okay SketchUp 78 error OOP oh no let there just inputs push okay oh I want to say no loop let me leave that no loop in put it back I just want look at it once so you can see what did I get I got a big array of sixteen numbers I got all the results so now what I want to do is back here now I just need to do the drawing and I don't need to the input data I don't need the model all I need to do is draw and I need to say fill wise index what I plus J times the number of columns maybe right because this is a one dimensional array to describe all each spot in that grid I could do something like let me just do this let index equal zero I'm going to say fill based on this particular one and I don't need this even sorry and I just need to say then index plus plus right so what are the steps here create the data get the predictions draw the results okay there we go so now we can see this is working I mean it's not doing anything but now let's check this framerate question we don't need to console.log the Y's I'm going to get rid of the no loop let's let's refresh this let's look at the framerate 30 frames per second let's let's pump it up a little but pump you up a little and where was the resolution there let's make this 20 go crazy and look at the framerate there we go 30 frames per second no problem because I'm only one time through draw trying to copy data onto the GPU and get it I'm only calling predict once and we can just to check we can go to 10 and we can look at the framerate yeah you could see it's like kind of running a little bit slow but this is because I'm not being so thoughtful about the asynchronous nature of this stuff I could do other things to optimize it but I'm just gonna ignore that and leave it at let me make it 25 the chat is giving me some even further optimization which is why am i bothering to do this in draw this is something that the these inputs never change I could just do them once at the beginning because they're and and I can I can ask for I spent many many times in draw so let's actually fix that so I'm actually gonna I'm gonna take this and say let I'm gonna make this globe these global variables I don't know if you guys can hear the music that's coming from the room next to me but it's the air alright then oh but the width and height does not exist until after create canvas so let me do this and let me do this okay so now that's there now I should be able to take this the input data and put this right here in the beginning and then I'm going to make a variable called X's and X is and where did I do that here and then create those X's so I'm now doing this in setup and then in draw the only thing I need to do in draw is run the predict this is going to make things run a lot faster let's make sure it still works here we go okay so you notice we get like a different color each time i refresh because the neural network model the sequential model is initializing everything randomly but now I get to Train it now I think we're ready to train it so here is what I did when I had my previous my own JavaScript neural network library I called neural network trained data inputs data outputs all right so if I only I could remember exactly what I wrote when I made that TF layers tutorial but I know that what I need to do here and is I need to do something like this model dot fit some X's and some wise that's the training that's the equivalent and the learning rate is irrelevant and I don't necessarily need to do it look this is basically what I want to do every time through draw I want to try to fit the model with some training data so let's first make the training data this is not exactly right I need to figure out and I need to use a weight that need to think asynchronously but this is the idea so if I go back to the top here this is my training data now one thing I definitely need to change is I want to keep the X's and Y's separate in training so I'm going to do this is I'm just going to do this kind of manually because I what's the big deal so let me make the training set and then one one those are the the X's now let me look at the Y's and the Y's would be 0 1 1 0 then I need those to be tensors so I need to say Const trait TF X's so I've got to think of a good naming for this I kind of want them to call actually you know what I'm just gonna call it do I have a global X yeah I have a global X's already hmmhmmhmm tray TF X's equals 10 sir 2 D tensor 2 D o TF tensor 2 D you know what I'm gonna do I don't need these I don't need two separate sets of variables I'm just gonna create it I'm gonna call this ah everything is so much more complicated than I make it so if we're simple then I make it I'm just gonna make these tensors directly by saying TF dot tensor to D and then I'll put the parentheses around this and there now I'm in a tensor then F is a TF tensor to D and now I made this a tensor okay now I've got the training data and I'm gonna get rid of this this is the old way that I had the training data which is totally unnecessary so this the training X's and the training wise you with me if you're still watching I don't know dude get up and do it some jumping jacks let's see now I need to do model that fit now model dot fit happens asynchronously so let's put it in its own async function called train model now if you don't know what it means to write a function that is tagged with the keyword a think this is part of es8 a very newish version of JavaScript and I've made a bunch of videos about what that is that you can go back and watch but this is basically a way for me to now say wait model dot fit and then let's look at actually let's look at the fit function model dot evaluate compile predict fit so what I need is to give it the X's and the wise there's batch size I'm not going to worry about there's epochs I'm not going to worried about our epics and so H will give me back the history so let's just see here I'm now gonna say train model dot then H console dot log H dot loss index 0 let's say no loop again so basically what I'm doing here is I want to call this function train model and I'm using this idea of promises it's going to await the model on I need to return do I say a weight return or returned a weight no I must say return oh wait return oh wait model dot fit so I'm going to return a promise which will have the result of the fit function and I don't know if this is right I want to just look I'm going to do that I'm going to call to train model every time and draw I might need to do this somewhere else just right now and then see what the loss is okay onyx 73 async function async function I've got to say function function it's an async function not an async there we go wise is not to find where a train model oh right this is I forgot to call it train X's and train wise this is my training data train X's and train wise nice other word train just appears of around we're doing machine learning you drink your glass of milk or whatever it is you're having while you're watching this coding train stuff cannot read property zero of undefined a train model than H all right let's look just console.log H note that is what I've done okay history loss zero okay oh no by the way I didn't give it any testing data so what's our computing the loss from history so this is I'm gonna call this result result history dot loss index zero all right there we go there we go now let's let it do that over and over again in draw so I let this run a little bit and unfortunately see it's getting nowhere this loss which I'm not sure exactly how things I don't think about that come back to it is not going down anymore so what could be some problems here remember one is maybe my learning rate is no good not that it's no good it may be it's too low so where did I set up that learning rate again let me get rid of by the way I just want to now delete I want to make sure I'm not using any of my old neural network code so I'm deleting all references to that so this is now purely tensorflow Jess and and let me refresh and run this again and let me look into sketch dot yes and find where did I set the learning rate right here let's set it to point five and see what we get yes getting better I'm gonna let this run for a little bit and I'll be back alright I'm back and you can see the losses now kind of much lower and you can start to see the visual that I'm expecting which has a true value in this corner true value in the top corner of darker false values in those corners but it's still kind of performing rather poorly one thing that I forgot to do is when I call the fit function there are a set of options that I can pass in for the number of epochs and all sort of thing but one of the ones that I really want to pass in here is called shuffle shuffle takes the training data and it shuffles the order of it each time right now I'm trading it with the same four data points in the same order every time which could be a bit of a problem and okay so let me now let me hit refresh here and run this again all right so you know it actually it actually is working it got the correct training result it's a little gray scaly in a way that I would like to be able to like emphasize visually what it's do but you could see the loss has gone way down but it took a while to get there but I want to add a few things to this and try to fix it up a little bit before I do anything I was reminded by the chat being over here that I haven't thought about memory management at all so I'm gonna say like no Luke for a second to just sort of turn this off then I'm going to say memory numb tensors whoops no no way TF memory dot numb tensor I know I'm trying to use ah I'm gonna say numb tensors TF dot memory numb numb there it is there it is I can get it there it is thirtytwo thousand two hundred five tensors that's crazy so I need to deal with that I'm just making temperatures and letting them leak everywhere so I can manually run dispose but I've got kind of an issue whereas predict is gonna like make a lot of tensors behind the scenes as well as a model dot fit so I can use the TF tidy function so I'm going to say TF tidy and then I just need to I'm gonna use the es6 arrow notation which you can watch my videos about what that is but and i've kind of gone through what tidy is tidy says anything inside of this code clean up the memory afterwards basically and then I'm gonna I probably could put this around everything but I just want to and I don't need this stuff anymore I just want to keep these two areas separate because I think I'm gonna at some point I really should change the way doing the fitting of the model excuse me in draw is somewhat problematic so now I'm gonna just tidy all of this I think that's right drive an extra parenthesis there yes okay so now let's run this again and let me comment out this console.log I don't want to see that right now oh alright and now let's look at the number of tensors 151515 so now I've gotten rid of the memory leak let's check out the frame rate 30 frames per second so this is running for on now let's I just want to be able to look at what's happening a little bit better so I'm actually gonna draw the number of the output inside each one of these things so let's do that so where am i drawing the rectangles here I'm going to say let the brightness value equal this and I'm going to fill the rectangle with that brightness and then I'm going to say fill 255 mind it like the inverse color I'm going to say text number format the yvalues index with just two decimal places and I'm gonna put that at boy this is awkward alright the exercise this is gonna be height x resolution plus resolution / I'm gonna say textaligncenter textaligncenter kind of center and then I'm gonna put the I'm just going to draw in the center of the rectangle and this should be J the text so let's see let's do this now so we can see you look there's lots of numbers there I think my number format thing didn't work 1 comma 2 and let's use a lower resolution just so I can see it better there we go now interestingly I can't see the numbers but there they go right you can see this is what it's getting the output for each one of these and I want to look at the law so you can see because it's just going so slow it's getting so overtime it's getting a little better but I really want to see a train much faster so let me see I have one idea one last thing I can add to this even though I and I have some suggestions for what I might do next but you can see all the numbers are starting to appear lovely because I forgot when it's gray when it's at point five 255 90.5 it's gonna be the same car I kind of like that effect so so what I want to do is what happens here if I actually give it tell it don't just do it once like do it 10 times do 10 a pox per cycle of fitting let me run this again and let me look at the let's actually have the loss continue to print out and there we go still running pretty fast you can see the losses going down and relatively quickly I am getting myself to the point where I'm starting to see you know this is definitely all the way cut getting all the way down to zero there this is getting way up to one there it's getting a little bit stuck it's having trouble with this size outside I imagine it'll get there eventually we could do some fun stuff so I'm gonna give it for no real reason all but just for fun four hidden nodes and I'm also gonna let me change the resolution to 25 let me make the text size something like eight point and let me whoops refresh it alright I'll let this run for a bit and you can kind of see here now you can see all the X or values here's a nice beautiful little map grayscale map of all X sorts getting all the way up to true and all the way down to zero at the corners this is pretty good so this is running at if I take out this console.log I can now take a look at the frame rate it's running kind of slow alright so the chat has given me some really helpful tips I've made quite a few little like weird little errors and mistakes here and I want to just fix this up a bit I think it's actually gonna be easier to look at and watch if I just go back to a lower resolution so let's make this 40 and let me refresh this okay so here we go so this is now working training itself for X or you can see it's kind of moving along here now what what the real the real thing that's problem problematic here is the draw loop is happening over and over again and then I'm triggering something asynchronous in draw and I could be asking to train the model before it's even done with the previous training cycle so this really should not be happening in draw now tensorflow Digest has a function called TF next frame I want you to explore it and make a version of this with Tia next frame as like an exercise after this video is over but I'm gonna do it a different way without that because I got to come back to that in a different video but first of all this I also learned this is totally unnecessary the the await a couple things number one is because there's just one thing happening in here I could just return the promise this doesn't actually have to be an async function and then I do not need the TF tidy because model dot fit kind of will clean itself up automatically for you so this this should still work just fine and I should be able to see the number of tensors is still fifteen so that was something that I didn't need that I've now fixed now what I really want to do is I want to get this out of draw so let's comment this out here and what I'm actually going to do is I'm going to write a separate function called train and in that function I'm going to say set time okay wait wait I'm gonna call train model wait hold on I'm gonna call train model yes yes in that function I'm gonna do this I'm a separate function that does this piece of it that console logs the history and I couldn't and what I want to do is I want to say set timeout call the train function in 100 milliseconds so I want to just let the program start 200 Phyllis X late later call this train function train the model which does the fitting when that's done log the history and now say set timeout train 100 so I'm good this is sort of like recurse like like I don't want you set interval here because I only want to call train again once it's finished with training the model itself so this is kind of like hey train and and by the way I could just increase the number of epochs or maybe do some kind of luke but I think this would be a sort of nice way to demonstrate it and if I just called train directly without a set timeout I'm never going to be giving back control for a second I could end up with sort of like blocking so I might even be able to get this down to like 10 milliseconds just something really really low so let's run this and sort of see same result we can see there we go things are working but at least now I have gotten that out of draw so draw is happening on its own and in fact what I could really do is I could say hey try doing this with like a hundred epochs each time epochs what is it and you can see the lost function is coming out much more slowly but whoops but the frame rate let me just clear this for a second yeah the frame rate is quite fast hold on this is too confusing I probably need to give give it back more time let's do like another little break let me let me I just want to like take out the console.log thing so I can look at the frame rate I should just put the frame rate in the Dom wouldn't that be smart but you can see now I'm getting 60 frames 30 frames of getting like a really pretty high frame rate even though the training is happening it's almost it's kind of like that there is no threading in JavaScript so these things are just like passing off and really this might be a place where like web worker something could do the training behind the scenes in some fancy way which maybe I will get to at some point oh my goodness so Alka is suggesting it might be better to use the draw loop and a boolean to know when it's safe to call it again that would also be a good idea so you can see though you can see what kind of like employed my hair out what kind of sort of like hassle situation we've gotten in but really let's just put this back to like two epochs let's put this to like a little 10 milliseconds and we can sort of feel like there we go and I can look at the frame rate it's running nice 30 frames per second and even though and it's and at some point it's gonna get there what's that loss I forgot to console.log loss come back to me come on oh but let's see I have an idea what if just before I go just before I go what if we try using a different optimizer what if we try using for example a different loss function hold on this video is already 18 hours long what if oh no no no a different optimizer sorry what if I tried using the the Adam optimizer so let's just try that just for fun times let's give it a lower learning rate that was pretty exciting let's go let's go let's uh let's make the resolution back to like 20 let me make the font size like nice and tiny for us we give myself some more space here hit refresh and then let's look at this look at it learning there wow look at that that is beautiful look at it learning XOR so nice and fast I'm just gonna hit refresh again I really should explain what these optimizers are but if I go back and look under here we can see what some of these are Adam the the a da coming from the word adaptive and you could always click here and look at this paper which describes this particular method for optimization which is a little bit different than stochastic gradient descent and apparently things work a lot faster with this XOR problem so as I go forward into more of these videos hopefully we can dig into what some of these different optimizers do and kind of understand why I might pick one over the other in certain situations all right but I'm just going to just leave this B I'm going to hit refresh I'm going to watch it learn and train train train train XOR oh I don't know some things you could do investigate TF dot frame give me a little slider try different architectures different optimizers types of different activation functions I don't know if you actually made it all the way to the end of this video I don't know hashtag something I should Eric has taught me to watch one of those videos reviewing the JavaScript event loop which I definitely need to do so I mean I'm gonna be back with more someday yeah goodbye goodbye goodbye thank you
