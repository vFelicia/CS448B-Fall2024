With timestamps:

00:08 - hello good evening this is not the
00:11 - evening at all this is the afternoon
00:12 - though
00:13 - welcome to the coding train on a Friday
00:15 - which is my usual day for the coding
00:17 - train but this summer it has not been my
00:19 - usual day I was just having a lot of
00:22 - trouble getting the start streaming
00:24 - button to start
00:26 - I had restricted mode enabled on this
00:28 - laptop for a variety of reasons that I'm
00:31 - not entirely sure of and interestingly
00:34 - enough I cannot start streaming I cannot
00:37 - actually I have to bat it's a long story
00:39 - but I couldn't get that to start
00:40 - streaming the button to work while those
00:41 - in restricted mode I fixed that so here
00:43 - I am how are you what's going on what's
00:46 - what's up what's what's happening got
00:54 - totally weird how you can't speak to me
00:56 - all right there's a chat so you sort of
00:58 - can you could type to me in the chat
01:00 - it's night in India writes Melvin in
01:03 - Israel it is 20 o'clock which i think is
01:07 - 8 p.m. all right so I have to admit
01:12 - something although this is nothing new
01:14 - you know I feel like normally I'm so
01:17 - well prepared and I spent all week big
01:20 - making notes and scheduling things out
01:22 - and knowing exactly what I'm gonna do
01:23 - and then I like I've got all this energy
01:25 - and I turn on the streaming and I'm go
01:27 - go go and I do a tutorial and I turn off
01:29 - the streaming and the day is over right
01:32 - now I feel like it was that I had to
01:34 - make a heroic effort just to make it
01:35 - here and press the start streaming
01:37 - button so I have to admit that I'm a
01:40 - little bit out of sorts but and I but I
01:43 - I have two hours today good news is I am
01:47 - planning for two live streams next week
01:49 - so I am planning to livestream both
01:53 - Wednesday and Thursday it looks like
01:55 - next week and I will publish Tynes to
01:57 - the homepage of this YouTube channel
01:59 - thing soon enough and so I'm hoping to
02:04 - get some get get into I but I really
02:07 - want to do I really want to do something
02:10 - practical a practical is the wrong word
02:12 - I don't do anything Pratt
02:13 - whatsoever I want to do some machine
02:17 - learning demonstrations in the browser
02:20 - with data and data that might interest
02:27 - you or inspire you to use your own data
02:29 - and do something else with it
02:30 - but I'm not there yet let me try to
02:32 - figure out where I am I'm gonna go to
02:38 - YouTube the coding train and I'm going
02:43 - to go to neural networks and machine
02:46 - learning I'm gonna go to session 6 hello
02:50 - and welcome I'm not gonna watch that
02:52 - alright so this is what I this is what I
02:55 - have so far I have to have to do this as
02:56 - beginning of every live stream to sort
02:58 - of recap and refrain recenter myself
03:04 - hi I'm gonna put this over here so it
03:06 - doesn't block the view you may or may
03:10 - not be aware there is something out in
03:12 - the world called tensor flow J yes this
03:16 - is a tensor flow a implementation of the
03:21 - tensor flow API in JavaScript that runs
03:24 - in the browser with no other
03:26 - dependencies all of the math is done and
03:29 - computed using WebGL and shaders and all
03:31 - sorts of amazing gymnastics and ways
03:33 - that I might never understand but enable
03:36 - opens the door and enables possibilities
03:38 - for us the people who like program like
03:42 - this it's a high program to try and
03:47 - experiment and learn a bit about machine
03:49 - learning and get our get our hands in
03:52 - there and and ask the right questions
03:55 - would be critical about the role of AI
03:57 - and machine learning in our world today
03:59 - so that's this I have started doing a
04:03 - series of tutorials these are not super
04:05 - beginner friendly you know there's some
04:09 - advanced JavaScript advanced aspects of
04:13 - the JavaScript language that I'm using
04:14 - that are confusing like promises and
04:16 - weight and they sink you have to do
04:18 - lower level memory management yourself
04:20 - when you make these arrays of data you
04:22 - have to allocate the memory and
04:24 - deallocate the memory
04:27 - and then there's all these like scary
04:29 - weird terms likes get the stochastic
04:31 - gradient descent and that's that's one
04:35 - of them
04:36 - you know optimizer root mean squared so
04:40 - but I'm doing this series for an
04:45 - audience who perhaps has already learned
04:47 - a bit about JavaScript programming maybe
04:48 - watch some of my other basic intro to
04:51 - neural network videos and just kind of
04:54 - like follow along and see how a larger
04:58 - machine learning library works in the
05:00 - browser and can be used so the tutorials
05:02 - that I have so far are sort of an
05:05 - introduction to what tensorflow DOJ's is
05:07 - talking about what it is to be a tensor
05:09 - what is it to be a tensor I want to be
05:16 - more relaxed I want to be a variable
05:17 - instead of a tensor oh look that tents
05:20 - are very bad whatever okay then
05:22 - variables and operations talking about
05:23 - memory management I implemented a
05:26 - version of linear regression which is a
05:30 - kind of like classic machine learning
05:32 - algorithm where you try to fit a line to
05:34 - a bunch of data points kind of serves as
05:36 - the foundation for a lot of machine
05:38 - learning research I also looked at
05:41 - polynomial regression where instead of a
05:42 - line we could fit a polynomial function
05:44 - which curves around and then ah then
05:48 - then then then then I finally finally
05:51 - finally started looking at the layers
05:52 - API and the layers API is a higher level
05:58 - API inside of tension flow j/s which
06:01 - allows you to create machine learning
06:04 - models as sequences of layers and that
06:07 - has you know how that works has to do
06:09 - with how neural networks are architected
06:12 - with inputs and outputs and layers that
06:14 - are in between and there's different
06:15 - kinds of layers and different kinds of
06:17 - math functions that happen with those
06:19 - layers all that sort of stuff so this is
06:21 - where I am at the moment this is where I
06:25 - am where am I going to where am I going
06:30 - to don't ask any more I forgot my
06:34 - ukulele I learned to play the ukulele a
06:36 - couple weeks ago thinking of routing
06:38 - this YouTube
06:39 - with me playing ukulele where am I going
06:42 - - don't ask anymore okay ah so what's
06:50 - next
06:51 - let me make a list really for me but
06:53 - you're watching so you can watch X or
06:59 - with T F layers I want to do a
07:06 - classification example I'm thinking of
07:11 - do what I'm thinking of right I want I
07:13 - so I'm gonna go through these one at a
07:14 - time classification and I'll come back
07:16 - to the details of these I want to do
07:18 - image then I want to do image
07:21 - classification then I want to do image
07:27 - classification again with convolutional
07:34 - layer so now I want to talk about what
07:38 - that is do I want to do some type of
07:43 - regression example maybe maybe so
07:48 - classification and then maybe sort of
07:51 - like as inside a Part B of like a basic
07:54 - like regression so this is kind of what
07:59 - I want to do in terms of the basic
08:01 - building blocks of machine learning with
08:03 - neural networks and I want to build all
08:05 - of these with the tensor flow das
08:09 - library so just just just to lower your
08:15 - expectations for a minute if I were able
08:18 - to get even just this done today I will
08:22 - be very happy about that okay so this
08:24 - this is kind of my goal that's my goal
08:27 - for today now I am doing things
08:29 - backwards in one in one sense I'm doing
08:33 - that these these are not super beginner
08:35 - friendly I mean they're as beginner
08:37 - friendly as I can make them I want them
08:39 - to be friendly but you know if I was my
08:43 - first day watching a coding video I
08:45 - might not want to jump
08:46 - into the image classification with the
08:48 - tensorflow layers api video which
08:51 - doesn't exist yet but but I once I get
08:54 - through this or actually once once June
08:56 - 15th hits or maybe even a little bit
08:58 - before June 15 that's next week
09:00 - I'm gonna start doing some
09:02 - beginner-friendly and hopefully gonna
09:08 - guests come and do these with me maybe
09:10 - somebody was watching this right now
09:11 - who's worked on this ml5 project would
09:13 - like to come beginner-friendly
09:14 - ml with some hearts and some stars like
09:22 - a little like rainbow and then like
09:27 - there's a train train going by I can't
09:31 - draw a train okay and with a library
09:45 - called ml v dot Jas so I'm gonna come
09:51 - back to this a second so just briefly
09:52 - let me just show you you catch a guess
09:57 - that's a great idea for a so if you go
10:00 - right now on the Internet to a URL ml v
10:05 - JSTOR G you will find this website and
10:09 - this is gonna be fun let's do something
10:11 - fun here we'll see here right here on
10:12 - the homepage is this interactive
10:15 - demonstration this is a picture of a
10:17 - Robin which the mobile net model labeled
10:20 - this as a Robin American Robin turdus
10:23 - migratorius with the confidence of 98.7
10:27 - so I'm going to upload an image do I
10:30 - have any images here this is good hold
10:32 - on let's go get let's go get a rainbow
10:39 - image is this looks good
10:44 - download we should done a train let's
10:48 - try a rainbow dot jpg where am I going
10:55 - desktop that looks good
10:57 - let's go for a train this one looks good
11:03 - Oh save image as train let's go back to
11:10 - the ml5 webpage and what I'm gonna do I
11:13 - could drag and drop it but I'm just
11:14 - gonna do this let's looks let's try the
11:20 - train trailer truck tractor trailer
11:26 - trucking rig rig articulated lorry semi
11:30 - with a confidence of forty two point
11:32 - ninety four percent let's try doing the
11:34 - dragging and dropping thing with the
11:37 - rainbow bring it over here and now we
11:39 - have a parachute shoot with a confidence
11:42 - of forty nine point twenty nine percent
11:44 - so how does this work well I will show
11:47 - you if you scroll down here you will see
11:49 - here is the code for such a thing and so
11:54 - the ml5 library is a machine learning
11:57 - library built on top of tensorflow das
12:00 - this library would not at all be
12:02 - possible without tension flow chess
12:04 - running behind the scenes to try to
12:06 - create some simple code examples to work
12:09 - with at the moment mostly pre-trained
12:11 - models in the browser and there's lots
12:13 - more coming here and I'm gonna do a
12:14 - bunch of tutorials with this but this
12:16 - project is I'm mentioning it now because
12:18 - Hannah Davis at the IO Festival I'm
12:21 - totally not in the slack channel here at
12:28 - the IO festival made talked about ml5 in
12:31 - her presentation when hopefully the IO
12:33 - Festival which is just finished up in
12:35 - Minneapolis the videos that all the
12:37 - talks that are amazing go find their
12:39 - Vimeo channel and all the ones from last
12:41 - year you can watch them the new ones
12:42 - from this year be out soon and we're
12:45 - looking at trying to launch this more
12:47 - officially on June 15th and I will
12:49 - mention that if you're interested in
12:50 - kind of poking around if you go to all
12:52 - of our github repos first let me just go
12:54 - over here oh we can see here's all sorts
12:56 - of wonderful people who have been
12:58 - working on this project more than just
13:01 - these ten people but here are 10 people
13:02 - and what I want to show you here is
13:07 - under projects this is my first
13:10 - or a into using the project management
13:13 - tool that's part of not to okay so but
13:31 - this project management tools the first
13:33 - time I've used it if anyone wants to
13:35 - jump on in and get involved and get
13:36 - bored kind of have a little sprint here
13:38 - from now until June 15th there's a lot
13:39 - if you look at that website there's a
13:41 - lot of stuff that's missing there's a
13:42 - lot of stuff that's broken and so you
13:45 - know reach out to me on Twitter at
13:47 - Schiffman type in a comment somewhere on
13:49 - github if you want to get involved and
13:51 - help us kind of push forward some of
13:52 - this code stuff for the release and okay
13:55 - so that's what I wanted to mention there
14:02 - too looking in the chat looking in the
14:05 - chat looking in the chat okay so now
14:07 - coming back over here so this is what's
14:10 - coming this is what I hope to eventually
14:12 - have on this YouTube channel is a
14:14 - playlist which is machine learning for
14:17 - beginners in the browser and what's to
14:18 - call it exactly and I'm gonna be using
14:20 - ml 5 NP 5gs for that together by the way
14:23 - the 5 in ml 5 is in omage 2 P 5 and
14:27 - processing because the ml 5 library
14:32 - aspires to be friendly and accessible in
14:36 - the same ways that processing in P 5
14:38 - have been over the years and there we go
14:40 - my camera's still shut off now back to
14:44 - today it's 1 o'clock already so what now
14:49 - I have a pretty clear picture what I
14:53 - want to do with my image classification
14:55 - examples that I'm going to build I want
14:59 - to use the quick-draw data set so one of
15:04 - my goals with making my machine learning
15:06 - tutorials is to use non traditional data
15:09 - sets and but maybe non-traditional is
15:11 - wrong word but data sets that are
15:13 - outside of what you would typically find
15:16 - in machine learning and data science
15:19 - curriculum want them I want to use
15:21 - different ones that are kind of in
15:23 - creative space to get people thinking
15:25 - more creatively about what kinds of data
15:27 - they encounter in their life that they
15:29 - could maybe use I want to use data sets
15:32 - that are really simple and kind of like
15:34 - easy to understand and look at and it
15:36 - also want to use data sets that are
15:38 - representative of the world that we live
15:42 - in and all of the all of the cultures
15:44 - and people that we share this great
15:46 - earth with so you know things like I'm
15:49 - trying to avoid things like M mist which
15:51 - is the classic handwritten digits data
15:54 - set things like the iris data set which
15:56 - is wonderful I have loved flowers
15:57 - nothing could possibly be wrong with the
15:59 - flowers data set but but things that you
16:01 - wouldn't that that kind of made me feel
16:03 - a bit more approachable so I really
16:05 - asked this question a bunch of places a
16:08 - bunch of times don't get any responses
16:12 - because I don't maybe it's hard to find
16:13 - these kind of data sets I feel like the
16:15 - Google quick-draw dataset is a great one
16:17 - for learning about image classification
16:19 - and then what I'm thinking of doing an X
16:23 - or is like it's not really a dataset
16:25 - it's just made up oh wow something else
16:27 - someone add in here okay hold on hold on
16:28 - and then classification what I'm right
16:33 - now the only thing I'm so tensorflow dot
16:36 - yes there's a node version tensorflow -
16:37 - yes that used this MLB Major League
16:41 - Baseball dataset to classify pitches and
16:43 - I love that football kind of a little
16:45 - bit of a baseball nerd you know any word
16:49 - so so I so that's kind of interests me
16:52 - but I don't know that baseball is
16:54 - perfect for what I want to do that's
16:56 - going to be you know a lot of people
17:03 - don't know about baseball and ranged in
17:04 - baseball and it's maybe not reaching the
17:05 - the more general audience that I'm
17:07 - imagining for this channel so but
17:09 - something like that that's really simple
17:11 - so all I could think of right now
17:12 - because I from gibreel go check out a
17:15 - CFD science that's the assess of science
17:19 - I can't ever say his channel name it's
17:21 - probably like there's like a really easy
17:23 - way to say that channel name and I just
17:24 - can't do it I don't know why I'm almost
17:26 - falling over for no reason I was talking
17:29 - about something datasets gibreel had
17:34 - this demonstration of a color predictor
17:35 - and what am I
17:37 - it's actually this semester created a
17:39 - variation on that which was kind of
17:41 - predicting more about a color than just
17:43 - a or B and so I was thinking of kind of
17:47 - using that as an inspiration and so like
17:51 - what if what if I made a color
17:54 - classifier that classified colors into
17:57 - like bluish grayish or like like the C I
18:01 - don't know some kind of set of arbitrary
18:02 - labels like five to ten labels that and
18:06 - what I need to so maybe I would
18:07 - crowdsource that Dave said I'm not sure
18:08 - yet so that's what I'm thinking about
18:09 - for classification I kind of hope to do
18:12 - that today but I talk too much and
18:14 - there's a little bit of time but that's
18:16 - that's coming next week another thing I
18:18 - forgot in here I wanted to make like
18:21 - make your own TF playground so just
18:29 - briefly one last thing that I'll mention
18:31 - here on this to-do list if you go to I
18:33 - believe it's playground tensorflow dot J
18:38 - s what's that well hold on J or whatever
18:43 - tensorflow playgrounds this is a project
18:48 - from the big picture group the research
18:50 - group from Google that created that
18:52 - where 10th floor address itself came out
18:54 - of and you can kind of create this
18:56 - little playground in the browser where
18:58 - you can configure a neural network you
18:59 - can have this kind of 2d data set you
19:02 - can actually there's like a play button
19:03 - so you can run it and you can watch it
19:05 - try to either classify and there we go
19:08 - sort of classify or I don't know it's
19:10 - doing classification regression it looks
19:11 - like classification to me they're sort
19:12 - of blue and orange so I have no interest
19:15 - in building out something that has this
19:18 - level of sophistication and design
19:20 - visual design but I would like to show
19:23 - you well could you similarly to how I
19:26 - made the linear regression the
19:28 - polynomial regression examples maybe
19:30 - I'll just do like a basic 2d
19:31 - classification problem with drawing
19:35 - stuff okay so that's that's my
19:40 - introductory talk I love this go you
19:47 - little neurons
19:48 - that data whoo all right oh oh I can
19:58 - break it I'm very keen for a fix
20:00 - all right so that's where I am so I
20:02 - think when all is said and done I think
20:05 - right now I'm just going to tackle today
20:08 - from now until about 2:30 which is an
20:11 - hour and a half X or and I really am
20:14 - torn like I don't want to do it because
20:16 - is it it's sort of but it's good for me
20:19 - so I'm gonna do all right but before I
20:23 - do that let me see if I can get some
20:25 - questions and get myself organized
20:30 - anybody have any questions about ml5
20:33 - tensorflow J s life the universe how to
20:44 - play the ukulele all right so what do I
20:48 - need to do here it's a couple things
20:52 - that I need number one is if anyone who
21:10 - is a sponsor and patron and there's an
21:16 - interesting question in the YouTube chat
21:18 - that's the might that I might like to
21:20 - answer you can paste it into the sock
21:24 - channel
21:27 - this is not right
21:30 - no no oh
21:36 - [Music]
21:38 - totally
21:39 - Papa Delhi doobie doobie wahh
21:42 - there we go alright there's two bits of
22:01 - code that I need to get started with
22:03 - this one is the actual previous XOR
22:12 - [Music]
22:17 - there we go coding challenge 92 and then
22:21 - I also want to get under maybe it's
22:31 - under courses intelligence and learning
22:33 - session wait
22:36 - [Music]
22:39 - no how come it's not there is this a
22:42 - different go past in a different place I
22:46 - [Music]
22:49 - have this in too many places courses
22:53 - intelligence and learning session 6 the
22:57 - layers API I need that and then p5
23:02 - tensorflow let's put these in there and
23:08 - I don't leave these anymore
23:09 - and let's go over here I don't know
23:20 - which would be better to start from
23:22 - let's start from this
23:25 - [Music]
23:29 - this'll be coding challenge what what
23:32 - coding challenge number isn't this the
23:35 - third time we do X or is it really oh
23:41 - you're really torching me because I
23:43 - really feel like I have a thing I have a
23:47 - little bit of a thing like it's kind of
23:49 - a little like you know like I can't if
23:51 - I'm listening to podcasts like I I have
23:54 - to listen to every all of it every
23:56 - minute I can't like not listen to one
23:58 - episode and I have to like so somehow in
24:01 - my stuff like guys like I just have to
24:03 - do I have to do the X or now with ten to
24:04 - flow Jay s because I have to but maybe I
24:07 - should skip it what coding challenge
24:12 - number am I on did somebody tell me no
24:16 - but I can find that out by going here
24:24 - and oops
24:27 - 105 was polynomial regression so this
24:33 - would be 106 okay I totally didn't do
24:38 - this right yeah how come there we go
24:42 - now do I have the atom editor open
25:04 - see here's the thing what's interesting
25:06 - about doing this YouTube channel is if I
25:08 - were teaching a course like I do
25:11 - supposedly he's at NYU I just would not
25:18 - I would just skip a lot of stuff because
25:20 - there's like limited amounts of time and
25:22 - I do to some extent to do that here but
25:23 - I I have this like false it's like this
25:26 - false sense of infinite time and I must
25:28 - do every single step which I need to
25:30 - move away from
25:41 - Cain wheat bun writes it's a good
25:44 - example comma but dot dot and I get it I
25:47 - get it but I'm waiting for what's coming
25:50 - next
25:50 - yeah I think I could I could fill it in
25:52 - but but but I think it's weird that's
25:59 - what I'm doing today
26:00 - unless I'm into that
26:06 - no no okay that's what I do all right
26:10 - all right all right so let's see here we
26:18 - are this is my list huh thank you thank
26:36 - you XOR Shipman see I think I only
26:46 - actually made one even though it's like
26:47 - probably the third or fourth time I'm
26:48 - doing it on this channel I didn't I
26:50 - don't only have one video it appears
26:55 - yeah look here's here's Simon talking
27:00 - about X or some other videos okay all
27:06 - right and now Joe server yes let me open
27:14 - up the browser I would love to do like
27:16 - some kind of little just fun algorithmic
27:18 - thing the equivalent of like phyllotaxis
27:21 - today before I go if there's time but I
27:23 - doubt there is okay
27:31 - all right you had got CGI rights I
27:35 - missed the non machine learning coding
27:37 - challenges totally agree yeah I don't
27:43 - know burger Bob asked could you please
27:46 - read the chat more often I totally get
27:51 - the sentiment I appreciate the question
27:52 - I will sir I can certainly try it is
27:55 - very hard to follow the chat and do the
27:59 - livestream at the same time and maybe
28:03 - someday I will have a better system for
28:05 - doing that I have some ideas for how to
28:07 - do that but it just I need time to get
28:09 - some more screens and maybe have some
28:11 - help with that and the sort of thing all
28:13 - right ADA she writes did he say filing
28:24 - taxes yes and now coding challenge
28:27 - number 327 filing your taxes let's go
28:32 - see IRS tax filing API ooh
28:41 - IRS gov a file providers software
28:44 - developer
28:49 - [Music]
28:55 - [Music]
29:12 - okay I guess I guess the XOR isn't so
29:14 - bad
29:16 - phyllotaxis the spiral beautiful
29:19 - Fibonacci spiral pattern of a sunflower
29:21 - that's what I was saying
29:24 - alright see what happens
29:31 - burgerbob that's exactly burgerbob Oh
29:33 - what about a giant screen which shows
29:35 - the chat behind the camera that is
29:37 - exactly what I would like and I will
29:40 - snap my fingers and giant screen will be
29:42 - mounted there behind the camera somehow
29:45 - that didn't happen I'm not sure why so I
29:48 - will I I do like that suggestion looks
29:55 - whoops yes Chris writes might be worth
29:58 - having a mod pull interesting questions
30:00 - out of both chats and give them to Dan
30:03 - at Q&A breaks I'm absolutely game for
30:05 - trying that and wants to sort of
30:07 - volunteer at this point I think I would
30:08 - need a volunteer to help facilitate that
30:11 - and that would be great
30:12 - all right let's let's let's let me get
30:22 - over all of my anxiety and hang-ups
30:23 - about you doing X or again and talk
30:26 - about them when I start and then and
30:34 - then and then and then I will begin I
30:43 - see all these people typing and Ada and
30:46 - Kate week Mon and Eric but I gotta move
30:49 - on I think I got to start because time
30:51 - time is a-wastin
30:59 - yes thank you okay Eric in the chat
31:02 - rightz sometimes it's important when
31:04 - learning something new to base your
31:06 - exploration around an example which is
31:08 - fairly trivial and you understand
31:10 - intimately well true or were better I
31:13 - could not have put it better myself
31:15 - thank you I'm gonna read that sentence
31:18 - at the beginning of this coding
31:20 - challenge if you don't mind
31:23 - hello welcome to a coding challenge yeah
31:26 - I know what you're thinking I mean I
31:28 - don't know what you're thinking I know
31:29 - what I'm thinking that looks like coding
31:31 - challenge number 92 XOR which is
31:35 - probably one of the less interesting
31:37 - creative like sort of just technical
31:40 - coding challenge demonstrations that
31:42 - you've done why why why are you doing it
31:46 - again
31:46 - well Eric from the coding train
31:49 - community writes
31:50 - thanks explanation because I was just
31:52 - before I started this having a real
31:53 - hang-up about this sometimes it's
31:55 - important when learning something new to
31:57 - base your exploration around an example
31:59 - which is fairly trivial and you
32:01 - understand intimately well so here's the
32:03 - thing
32:04 - I I'm learning something new I'll come
32:06 - back here and the thing that I am
32:08 - learning something new is this tension
32:10 - flow a thing and wouldn't it be fun to
32:12 - make like play pac-man with it or the
32:15 - emoji scavenger hunt project or
32:17 - teachable machine or play a piano with
32:19 - it all these things all pose that oh my
32:20 - god we got it we're gonna get this I
32:21 - could just sort of go there right now I
32:23 - will get there eventually but I'm trying
32:26 - to learn the basics of how the library
32:27 - works and I'm trying to step through
32:30 - this slowly so I will say that we're if
32:33 - you're watching this video right now
32:34 - where you are is not necessarily in the
32:35 - most beginner or friendly place because
32:38 - I'm working with tensorflow tas natively
32:40 - to implement basically like a weird math
32:43 - problem it's not that weird of a problem
32:44 - actually but a very basic trivial math
32:46 - problem just to see how tensorflow dachi
32:48 - has works that's what I'm trying to do
32:50 - with this coding challenge and about 20
32:52 - or 30 minutes he'll be coding this
32:54 - coding challenges just look it's right
32:55 - like four hours and 72 minutes long
32:57 - which is why i say 72 minutes cuz that's
32:58 - five hours and twelve minutes I don't
32:59 - know but the trajectory that I'm on is
33:03 - I'm gonna start doing some stuff inching
33:05 - my way towards hell let's actually use
33:06 - some data let's use some more data and
33:08 - maybe some images and so I've got a
33:10 - bunch of things that I'm stepping
33:11 - and I'm trying to get to the point where
33:13 - I'm going to use this other machine
33:14 - learning library called ml5 which at the
33:16 - time of this recording hasn't really
33:18 - officially been released yet but builds
33:20 - on top of tensorflow das thank you
33:22 - everyone who votes wherever you are to
33:26 - try to create some more accessible
33:27 - interfaces to some of the algorithms and
33:29 - models that you things that you can do
33:31 - with tensorflow digest without having to
33:33 - do the lower-level memory management and
33:35 - math operations stuff so all that is
33:38 - coming and I just took a lot of time in
33:40 - this coding job to say that to you so
33:43 - but as much as I kind of don't I haven't
33:47 - I'm not so sure but well but it's a why
33:49 - is XOR so here's the thing this is why I
33:51 - want I need an example this is the first
33:52 - time I'm going to ever in any of my
33:55 - videos except for the other one that I
33:57 - made but this is the first time that I'm
33:59 - actually going to use the TF layers API
34:04 - to Train Oh to train a model with a data
34:09 - set to produce a certain output okay I
34:12 - did two tutorials about what the TF
34:14 - layers API is you could pause down and
34:16 - go and watch those and then come back
34:18 - here but in those videos I didn't
34:20 - actually do anything with TF layers just
34:22 - sort of talk through and typed out some
34:23 - code so the problem that I want to solve
34:25 - and apologies for explaining this
34:27 - probably for like the fifteenth time on
34:29 - this YouTube channel is very well known
34:32 - from machine learning X or because when
34:35 - the original perceptron was invented the
34:38 - single perceptron the model of an
34:40 - individual neuron that could receive
34:42 - inputs and generate an output it could
34:46 - not solve X or it just couldn't it's not
34:50 - a linearly separable problem and I've
34:52 - talked about that in other videos about
34:54 - why we need multi-layer perceptrons so
34:56 - the nice thing about XOR is I can
34:58 - diagram for you hold on a second
35:07 - look back I can diagram for you the the
35:13 - architecture of the model that we need
35:14 - to create there are two inputs there is
35:19 - one output so that the inputs to the XOR
35:24 - problem are true and false values so
35:27 - unlike a so if I made a little truth
35:30 - table and or XOR right I can have true
35:36 - and true true true false false true
35:40 - false false and and operation would only
35:43 - ever give me true when both are true
35:45 - false false false and or operation would
35:50 - only ever give me would gives me true if
35:53 - just one of them is true true false can
36:02 - you even see that I can't see on my
36:04 - monitor but hopefully you can now XOR
36:06 - the X for exclusive gives me true only
36:11 - if one is true they can't both be true
36:13 - only one so in that case I get false
36:16 - true true false and the idea if linearly
36:21 - separable comes up here because I can
36:24 - draw a line here to separate true from
36:26 - false I can draw a line here to separate
36:28 - true from false but here I could do this
36:34 - but I can't draw a single line to
36:36 - separate true from false we need a more
36:39 - sophisticated model with a hidden layer
36:41 - so the inputs are things like a 1 and a
36:44 - 0 feed forward into the hidden layer
36:51 - activate feed to the output and the
36:55 - output should be a 0 or a 1 it's really
36:57 - in some ways a classification problem
36:59 - but I'm gonna do this as a regression
37:01 - essentially where I'm just gonna get
37:03 - some number between 0 & 1 if you watch
37:05 - the previous coding challenge the reason
37:07 - why that is is because thank you very
37:11 - much good night this video is now
37:12 - I hid that my sound effect by accident
37:16 - because what I'm trying to do is
37:18 - visualize the true-false space alright
37:24 - pause for a second
37:33 - just taking a pause for a second
37:49 - how long's 10 minutes at least right but
37:53 - I you know it's important it's important
37:55 - for me to talk about what I'm doing all
38:00 - right
38:04 - so I'm just thinking here where am I
38:08 - going next looking at the chat no one's
38:10 - complaining too terribly and I think I'm
38:13 - going to move on I guess I could
38:21 - transition back over here case I'm not -
38:24 - you want to edit out the weird sound
38:25 - effect thing
38:35 - oh they're good sounding okay that's
38:37 - good
38:38 - Hugo asks will you ever do non
38:41 - JavaScript videos well I do I do some
38:44 - processing and Java videos those aren't
38:46 - JavaScript but you know at this point
38:50 - I'm kind of I'm kind of I'm kind of
38:55 - doing the JavaScript thing okay let me
39:02 - let me transition back and that to me
39:04 - may be the sound effect little thing was
39:06 - like a funny little bit but I think I'm
39:08 - gonna just transition back because
39:12 - ultimately what I'm going to do is
39:14 - visualize the output of the model and
39:18 - I'm gonna send in numbers all the way
39:21 - between zero and one I know I don't even
39:24 - know what I'm saying it's fine because
39:32 - ultimately I'm gonna visualize the
39:33 - output as grayscale values and I want to
39:35 - see number I want to see grayscale
39:36 - values all the way between zero and once
39:38 - the same thing I did in the previous
39:39 - coding challenge if you if you're
39:41 - happened to have watched that one
39:42 - alright so now I actually I'm gonna also
39:44 - do something where I start from the code
39:48 - from the previous coding challenge and
39:50 - so we can see there's this idea of
39:52 - training data the inputs to the X or
39:55 - problem are zero zero gives me a 0 0 1
39:59 - gives me a 1 1 0 gives me a 1 at 1 1
40:01 - gives me a 0 this is the training data
40:03 - and in my previous version of this I
40:07 - used my own neural network library so in
40:09 - theory I'm gonna get rid of the idea of
40:10 - the learning rate slider just before we
40:13 - can add that back in later but let me
40:16 - get rid of the learning rate slider
40:19 - basically I want to do exactly the same
40:22 - thing the difference is I'm going to say
40:25 - neural network equals
40:26 - TF layers sequential and maybe I'll call
40:32 - this the model instead of neural network
40:36 - so the I do this is a neural net here so
40:38 - the idea here is that I want to replace
40:44 - my neural network library with
40:48 - tensorflow yes and so this for me what
40:51 - what the usefulness of this video is a
40:53 - me learn I spent all this time trying to
40:55 - build my own rather sort of terrible
40:57 - neural network javascript library and
40:59 - going through that was sort of helpful
41:01 - in thinking about how the stuff works
41:02 - now if I can translate that into
41:05 - attention flow dot yes I'm gonna things
41:07 - are gonna hopefully start to sell and
41:08 - make more sense into my brain Bruno is
41:17 - asking something in the chat about the
41:18 - true/false table yeah usually you draw
41:20 - it as a I might come back to this later
41:23 - usually you draw it as a matrix and I
41:28 - sort of did something weird there but I
41:30 - think it's fine sorry I'm looking see
41:38 - this is what happens wait look at the
41:38 - chat too much all right okay
41:57 - alright okay so now we need to what this
42:05 - constructor here said and let's just put
42:06 - this back to this this constructor here
42:08 - said would make a neural network with
42:12 - two inputs two hidden nodes and one
42:14 - output so I need to duplicate that idea
42:17 - here with PF layers so let's go to the
42:20 - 10th floor Jas API reference and we're
42:24 - gonna go all scroll down to TF layers
42:28 - and what I want to make is a dense layer
42:32 - TF layers dint a dense layer is a fully
42:36 - connected layer so what I'm going to do
42:38 - is I am going to say let hidden equal TF
42:45 - layers dense and then I can put inside
42:51 - there an object that has the parameters
42:54 - of how I want to configure that layer
42:56 - and so how do I want to configure it the
42:58 - two things that I want need really need
43:00 - to do is this is the hidden layer right
43:02 - I need to give it an input shape right
43:06 - he just say what's coming in what's
43:08 - coming in that's what this is here I
43:10 - need to say how many nodes it has that's
43:12 - the number of units and then I probably
43:15 - has a default one but I can specify an
43:17 - activation function and again I'm just
43:19 - going to use sigmoid as this historical
43:21 - activation function that I've been using
43:23 - in all my videos to date I'm gonna soon
43:26 - talk about softmax what that is as well
43:28 - as some other activation functions like
43:30 - lazy which is maybe more commonly used
43:34 - okay but like nobody pronounces that way
43:37 - foot B so don't get confused
43:39 - alright so I want to say input shape I
43:43 - believe is just there's just two inputs
43:46 - I also want to have two units two nodes
43:52 - and activation is going to be sigmoid so
43:57 - now I have created the hidden layer yay
44:01 - the other layer that I need to create is
44:04 - the output layer and so what am I know
44:07 - the app and layer I don't need to
44:08 - provide an input shape because the input
44:11 - shape can be inferred if I add them
44:13 - sequentially the inputs are not a layer
44:16 - so for this first layer the hidden layer
44:17 - I've got to say how many there are but
44:19 - now once I'm creating this next layer it
44:21 - can just the input shape is gonna be
44:23 - defined by what was before it so now I'm
44:26 - going to say really have to stop it at
44:29 - the sound effects by excellent and now
44:32 - I'm going to say let output equal TF
44:35 - layers dense and all I need to say is
44:41 - units one activation sigmoid okay then
44:47 - ought to do is say Model Model dot add
44:50 - hidden model dot add output okay so this
44:55 - is the model now one thing I need to do
44:59 - is I definitely need to import the
45:03 - tensor flow J's library which I happen
45:05 - to have from one of my previous examples
45:07 - so I'm going right now I only have I
45:09 - have the p5 libraries in my index.html
45:11 - plus my crazy neural network thing and
45:14 - my actual code and sketch J yes someday
45:18 - maybe I'll use the fancy new import
45:19 - syntax stuff let me just just have
45:21 - everything kind of line up let me add
45:23 - this in here so now TFS should be there
45:25 - I should be able to go back and run this
45:27 - and not see any errors aha
45:32 - TF dot layers dot sequential is not a
45:37 - function so I'm seeing things in the
45:41 - chat chats really off the rails with
45:44 - this XOR thing
45:48 - [Music]
45:52 - all right so I probably just so I
45:56 - probably just didn't even see half cut
45:58 - layers dot sequential the right thing
46:00 - you know I could go look I by the way
46:02 - made an example oh it's just TF dot
46:05 - sequential okay so all I all I want to
46:07 - say is I just got that wrong it's TF dot
46:10 - sequential so you know I could go look
46:13 - you know hopefully I would find this
46:15 - here at EF dot sequential yeah models
46:18 - creation there it is TF not sequential
46:20 - so I just had that wrong
46:21 - okay let's try refreshing this yet again
46:26 - slider is not defined hold on sorry much
46:31 - ya have to turn the notifications off on
46:35 - my watch I'm getting like phone calls
46:36 - and buzzing things okay
46:39 - take a minute here somebody said that I
46:41 - look I'm good at drinking drinks in
46:44 - profile that I could be like a coating
46:46 - train brought to you I'm buzz marketing
46:48 - Klean Kanteen that's not an official
46:52 - sponsor where was I
47:08 - right alright let me fix this learning
47:14 - rate issue 0.1 I just want the thing to
47:17 - run okay so it's going it's still
47:20 - working with the my neural network
47:25 - library not the new ten so Jeff's one
47:28 - but let's keep stepping through so ah so
47:32 - what am I missing here
47:33 - so when I make a model this is now I've
47:36 - architected the model
47:38 - I've architected this particular
47:40 - architecture but I need to do another
47:42 - step I need to compile the model and I
47:46 - need to define the loss function and the
47:50 - optimizer basically I need to say like
47:54 - okay well this is how I'm going to
47:56 - determine how well the model is
47:59 - currently performing with the training
48:01 - data and testing data potentially but
48:04 - I'm not getting testing data will come
48:06 - in my next video about classification
48:08 - but here I'm not making a distinction
48:10 - between training and testing date I'm
48:11 - conflating those two concepts which is a
48:13 - big mistake and a problem but we're
48:15 - stepping through this stuff later by
48:16 - little by little like a butterfly
48:18 - flapping its wings it's not at all a
48:21 - butterfly but I felt like I was being
48:23 - like a butterfly and then an optimizer
48:25 - is what sort of function what sort of
48:30 - algorithm am I using to adjust all of
48:33 - the weights of all these connections
48:35 - according to the loss function itself so
48:38 - I need to define those things so let me
48:45 - try to type it out how I think it is
48:47 - and then we'll go check so I know I need
48:51 - to create an optimizer GF optimizer like
48:58 - this and with a learning rate something
49:01 - like this like I'm gonna I want to have
49:04 - used to cast a crate to set with some
49:06 - learning rate that's not correct
49:07 - this is me like trying to remember what
49:10 - what the code is and then I need to say
49:13 - like model dot compile and then I think
49:16 - when I compile it I'll say things like
49:19 - this I'm going to compile it with
49:21 - optimizer and this loss function like
49:25 - like root mean squared or something like
49:29 - that
49:29 - so this is what I'm remembering from
49:31 - when I looked at this at one time and I
49:33 - probably got this wrong so let's
49:34 - actually go look at the API Docs
49:36 - well first what's the chance that any of
49:38 - this actually makes sense okay TF
49:40 - optimizers not a function so let's see
49:41 - how do we create the optimizer optimizer
49:48 - yes so it's this is what I want I want a
49:52 - TF train SGD this is how I create the
49:55 - other optimizer is not a keyword in the
49:58 - API just I imagine that for myself so I
50:01 - need to say TF train SGD and then give
50:04 - it a learning rate so TF train SGD and
50:08 - there are other kinds of optimizers that
50:10 - will will that I think I've even shown
50:12 - you and what we'll use more and give it
50:13 - a learning rate like 0.1 then I want to
50:18 - look at model dot compile so look for
50:24 - compile well we can see in some examples
50:27 - here what I'm looking for is where the
50:29 - actual compile there it is compile
50:32 - so the compile function compiles it and
50:35 - give an optimizer a loss and I can also
50:37 - do some metric stuff I'm not going to
50:39 - worry about the metrics too much
50:40 - although maybe I'll try to come back
50:41 - towards the end of this video okay
50:43 - model dot compile optimizer loss I think
50:45 - this might actually be fine is it root
50:47 - mean squared so let's look for the loss
50:49 - functions loss root means mean squared
50:59 - Oh
51:00 - Weiss ago I keep saying root because I
51:14 - have it in my head from some thing that
51:16 - I did a very long time ago where I was
51:19 - always taking the square root of the
51:21 - mean squared error so I always say root
51:23 - mean squared there's no root here
51:25 - involved I guess I have to get back up
51:30 - and continue this tutorial
51:32 - how long was I saying root for and hell
51:35 - annoying will that be for the people who
51:36 - watch this later okay well don't you
51:39 - know I get up slowly others I get kind
51:41 - of lightheaded alright apologies I've
51:45 - been saying root mean squared error for
51:48 - because I'm stuck in this world where
51:50 - you have to take the square root which
51:51 - you don't need to do here so just mean
51:53 - squared error that's all I need this is
51:55 - my loss function mean squared error now
52:01 - let us now go back here hit refresh all
52:07 - right things are happening things are
52:09 - going so the model is built the model is
52:13 - compiled and the next thing that I am
52:16 - ready to do is now actually start
52:18 - putting data in the model time out for a
52:23 - second why did I lose all right just a
52:36 - second here I really have to watch the
52:42 - time I've got an hour I have to be a
52:45 - little league practice I am NOT a
52:49 - strange forty four-year-old person who
52:51 - plays in the league but my son is
52:53 - playing Little League for the first time
52:54 - this year to be at the practice cannot
52:57 - be late so I have another hour though
53:00 - okay hmm I'm not the coach don't worry
53:04 - I'm just stand by the side a cheer do my
53:09 - little debt hold my little signs that's
53:11 - it I don't have any sign should have
53:13 - signs I was actually thinking of
53:14 - sponsoring a little league team like the
53:15 - coding train sponsored little league
53:17 - team I think get it get together this
53:18 - year maybe next year
53:19 - alright
53:27 - all right
53:35 - there too - these are two next steps
53:38 - whoops oh why is this completely died I
53:53 - just I know I'm saying 44 a lot because
53:56 - about to be 45 so I feel like it'll say
53:59 - 44 I like number 44 much better than
54:01 - number 45 for a variety of reasons that
54:03 - it will not get into can figure out what
54:06 - I'm talking about
54:07 - all right um it's pretty obvious
54:09 - probably all right so the two things
54:13 - that we need to do now what are the two
54:15 - main steps I don't know why I came over
54:16 - here but since I'm over here first of
54:20 - all I drew this truth table thing a
54:21 - little bit weirdly and so you might
54:24 - recall just to be clear about what's
54:25 - going on this is my little drawing of
54:27 - the canvas right now and the idea of the
54:29 - canvas is that I want to see what the
54:32 - neural network thinks false false is at
54:35 - 0-0
54:36 - I want to see what it thinks true false
54:39 - is at this right hands top right hand
54:41 - side the bottom left hand side I wanted
54:44 - to see it 0 1 and then I want to see
54:46 - here 1 1 so false is black 4 0 and true
54:52 - is white for one that's the way I'm
54:54 - gonna map the color so I should see some
54:57 - kind of bands of like I should be
55:00 - getting like something like this
55:01 - so darker here and like this so let's go
55:04 - look does that match yeah that's exactly
55:07 - what I'm seeing here so the reason why I
55:10 - came over here is what I need what I
55:13 - think there's two things that I need to
55:14 - do number one is I need to train the
55:16 - model to produce this output my desired
55:19 - output that I think it should do and
55:20 - then I also need to ask the model to
55:23 - predict so I can draw what it thinks its
55:27 - output is so the two and the and so the
55:30 - two steps here I don't run out of space
55:32 - but in the attention photo chess library
55:35 - I wanted you I need to look at the
55:37 - predict function and the fit function
55:40 - predict for just saying here's the
55:43 - inputs what is your output the fit
55:45 - function for saying here's labeled
55:47 - inputs inputs with no
55:49 - outputs adjust optimize yourself
55:51 - according to that so I'm gonna do things
55:53 - backwards I'm gonna do just the predict
55:55 - step first I just want to see when you
55:58 - starts up with no training
55:59 - what visual output - again so coming
56:05 - back to the code let's look here so this
56:09 - this is what I need to replace I need to
56:12 - say now I need to say let whoops y equal
56:21 - model dot predict
56:34 - model dot predict now let's go look at
56:37 - the documentation right I need to send
56:40 - in the inputs so let's go back to the
56:44 - documentation model dot predict I get a
56:50 - better way of browsing this
56:51 - documentation here it is so I need to
56:56 - sorry
56:57 - model dot predict I need to give it the
56:59 - X's what are the X's this are the X's
57:04 - but remember I'm using tensorflow de s
57:08 - now tensorflow - s oi oi oi vague volts
57:13 - I have to make them a tensor I can't use
57:15 - regular arrays so I could say let X is
57:18 - equal tensor 1d oh no it's CI sorry I
57:27 - got confused
57:28 - TF tensor one D inputs and then model
57:33 - X's now here's the thing so this is the
57:37 - ID there's many problems what I've done
57:39 - so far ok many problems which I will
57:42 - solve slowly this could be a very long
57:44 - video I apologize in advance you can
57:45 - take a break now pause take a break go
57:47 - do something else
57:47 - go back so what about what's the what's
57:51 - problem number one problem number one is
57:53 - predict happens asynchronously
57:56 - hoo-boy pause for a second here how did
58:02 - i do this and so i made an example i
58:06 - want i just need to think about this for
58:08 - a second wait my glasses are steaming up
58:11 - why is it getting all warm in here hold
58:13 - on oh and Eric thank you for that pull
58:16 - request
58:17 - I probably shouldn't that and then look
58:19 - at because he probably fixed a bunch of
58:20 - things I just want to see something can
58:22 - we do this as a batch
58:24 - oh no predict happens synchronously it's
58:28 - fit that's asynchronous oh good that's
58:30 - why I'm doing this right predict happens
58:36 - synchrony can happen synchronously let
58:40 - me look at should I should show you what
58:42 - I'm looking at I know why I'm looking on
58:44 - this other computer
58:48 - oops I'm just looking up some stuff for
58:51 - how this stuff works
58:56 - oh wait wait no hold on let me show you
59:12 - what I'm looking at because I don't know
59:14 - why I'm looking at this so I have a repo
59:21 - called 1:45 it's not called what for I
59:26 - this the time tensorflow jazz examples x
59:29 - or sketch right so I know I have to do
59:39 - as a batch neural network predict Oh or
59:44 - did I do it and so I was putting it in a
59:46 - class which I'm not going to do here
59:48 - tidy return oh I know predict happens
59:57 - synchrony synchronously but then pulling
60:00 - the data off happens asynchronously but
60:03 - I can use data sync even though I
60:05 - probably should be using TF next frame I
60:08 - don't know how to use that yet so I will
60:14 - deal with that later okay sorry okay
60:29 - okay oh here I am back all right sorry
60:34 - we can just back up a bit
60:38 - not you you can splice things however
60:41 - you so feel so inclined whoops
61:04 - so I need to now I need to ask the
61:07 - tensorflow layer sequential model thingy
61:10 - to give me the Y neuro a model dot
61:14 - predict but what does it expect its
61:18 - predict function unlike my predict
61:20 - function cannot get a regular array it
61:22 - expects a tensor so I need to make the
61:26 - X's into TF tensor 1d with those inputs
61:32 - and pass those through predicts now
61:36 - here's the thing there there's a lot of
61:38 - issues with this that I need to resolve
61:42 - and this is gonna run really slow I need
61:43 - to actually do this as a batch process
61:44 - I'm gonna get to all that but just
61:46 - looking at what I've got so far
61:48 - model dot predict there's there's a
61:51 - question of like is this happen
61:52 - synchronously or asynchronously this
61:54 - actually is happening synchronously but
61:57 - the problem is I need to say fill with
62:01 - the result like I need to look get that
62:03 - number out and to get the number out I
62:07 - actually want to call dot data and that
62:09 - happens asynchronously so because I'm
62:12 - working with some teeny bits of data
62:14 - right now
62:14 - I think I'm gonna use data sync and
62:17 - there could be issues with that and as I
62:19 - move more forward we're gonna see when I
62:21 - really need to be more thoughtful about
62:22 - callbacks and promises but I'm gonna use
62:25 - data sync right now so I should be able
62:27 - to predict the output with this input
62:30 - get that data and then let me just say
62:33 - console.log why and I'm gonna make the
62:39 - resolution here of the oh yeah the
62:46 - resolution really big like 50 because I
62:48 - just want to like look at very very
62:49 - little data to start with and let's look
62:53 - I'm not gonna draw anything let's just
62:54 - look and see what's coming out what's
62:56 - coming out here why and then let me just
62:57 - say no loop so let's look in the console
63:00 - and see if we get anything error
63:04 - expected when checking dense dense one
63:07 - input to have two dimensions but it got
63:09 - array with shape too
63:11 - I have the same problem I've had every
63:13 - single time I've done this with 10:00 to
63:15 - 4:00 guess so
63:16 - the good news is I want I don't want to
63:22 - just give this one D tensor so even
63:26 - though my data is just two values 0 1 1
63:29 - 0 1 1 and it's a one-dimensional array
63:32 - with two numbers in it I actually want
63:34 - to be able to do something like hey take
63:36 - these 15 data points and give me the
63:40 - results the predictions for all 15 of
63:41 - those and so what I really want to be
63:43 - doing is I always need to send in kind
63:46 - of like one order higher one degree one
63:50 - rank higher so this actually I'm just
63:54 - sending in one data it piece of data
63:56 - endpoint in point input frame stopped
64:00 - work and this now I also have to say
64:07 - tensor 2d now because it's a 2d tensor
64:10 - there we go ah so we could see look at
64:13 - this the results came out for all those
64:15 - little spots you can see in little
64:16 - numbers between 0 and 1 in an array so
64:18 - now I can instead of console logging Y
64:22 - and I just want that it comes back into
64:26 - the Ray but there's only one number I
64:28 - care about I can put this back in here I
64:32 - can take out no loop and I can run it
64:36 - let me see look there is my current
64:39 - visualization of X or
64:45 - I'm not really done I've so much left to
64:47 - do in this video that is recording for
64:50 - the last three or four days alright one
64:54 - thing I want to do is I just want to say
64:56 - stroke 255 I just want to sort of see a
65:00 - little bit more okay that's actually
65:01 - what I'm looking at here I actually want
65:03 - to make the resolution for debugging
65:05 - debugging wise on I also want to make
65:08 - the resolution a little bit bigger so
65:11 - let's see now one thing I'm curious
65:12 - about
65:12 - let's look at the frame rate here oh
65:15 - that's lighting at 30 frames per second
65:17 - so that's fine let me now actually make
65:20 - the resolution much much higher like
65:24 - this oh my goodness oh it's not even
65:29 - getting to the first frame oh well there
65:32 - we go look at the frame rail can't even
65:35 - give me a frame rate it's so stuck you
65:37 - can't even get one frame per second so
65:39 - here's the thing I have done something
65:41 - very very very bad and I needed to stop
65:46 - it no loop
65:48 - stop you don't have to do any more work
65:50 - and let's put the resolution back at 100
65:53 - and let's think about this what's going
65:55 - on here look at this look at this
65:57 - predict function and look at this data
65:58 - sync function what am i doing I am
66:01 - calling that function multiple times
66:04 - every single for every single spot on
66:07 - that grid when I'm working with
66:10 - something like tensorflow J s whenever I
66:12 - create a tensor or feed data into a
66:15 - model the data has to go from my code
66:19 - onto the GPU and then when it's done
66:22 - that data sync is pulling it off of the
66:24 - GPU so I can use it again in my code
66:26 - that graphics processing unit where all
66:28 - the math is happening behind the scenes
66:30 - I want to do that as few times as
66:33 - possible
66:34 - look how this is I'm creating this
66:37 - two-dimensional array with one thing in
66:39 - it you know ten hundred times I could
66:42 - just create one array with a hundred
66:45 - things in it and call predict once
66:47 - that's what I want to do so I what I
66:50 - need is for this nested loop to happen
66:52 - twice once to actually wants to setup
66:56 - the data and another to draw all the
66:58 - results
66:59 - so I'm gonna copy paste this just put it
67:01 - right below so this now what we need to
67:04 - do is create the input data so I'm gonna
67:13 - say let inputs be a blank array then I'm
67:19 - going to say inputs dot push and I'm
67:27 - going to just push in x1 x2 so I'm going
67:31 - to put every single x1 x2 all the way
67:35 - along I don't want to create the tensor
67:38 - or do this here I don't want to do the
67:40 - drawing stuff here I just want to create
67:42 - I just want to have a loop that creates
67:43 - all the data now I can get the X's is
67:48 - all of those inputs into a 2d tensor and
67:51 - the Y's this is now the Y's is and now
67:57 - here's the thing I don't just let's so
67:59 - hold on I got a look at what that's
68:00 - gonna look like let's comment this out
68:02 - for a second let's look at the Y's and
68:08 - see what that looks like oh okay
68:14 - Sketchup 78 error OOP oh no let there
68:21 - just inputs push okay oh I want to say
68:25 - no loop let me leave that no loop in put
68:28 - it back
68:29 - I just won't look at it once so you can
68:32 - see what did I get I got a big array of
68:34 - 16 numbers I got all the results so now
68:39 - what I want to do is back here now I
68:44 - just need to do the drawing and I don't
68:48 - need to the input data I don't need the
68:49 - model all I need to do is draw and I
68:51 - need to say fill wise index what I plus
68:56 - J times the number of columns maybe
68:59 - right because this is a one dimensional
69:02 - array to describe all each spot in that
69:06 - grid I could do something like let me
69:08 - just do this let index equal zero I'm
69:11 - going to say fill based on this
69:13 - particular one and I don't need this
69:15 - even sorry and I just need to say then
69:19 - index plus plus right so what are the
69:22 - steps here create the data get the
69:26 - predictions draw the results okay there
69:38 - we go so now we can see this is working
69:41 - I mean it's not doing anything but now
69:42 - let's check this framerate question we
69:45 - don't need to console.log the Y's I'm
69:47 - going to get rid of the no loop let's
69:51 - let's refresh this let's look at the
69:54 - framerate 30 frames per second let's
69:58 - let's pump it up a little but pump you
70:02 - up a little and where is the resolution
70:07 - there let's make this 20 I don't want to
70:09 - go crazy and look at the framerate there
70:13 - we go 30 frames per second no problem
70:15 - because I'm only one time through draw
70:17 - trying to copy data on to the GPU and
70:21 - get it I'm only calling predict once and
70:23 - we can just to check we can go to 10 and
70:29 - we can look at the framerate yeah you
70:31 - could see it's like kind of running a
70:33 - little bit slow but this is because I'm
70:34 - not being too thoughtful about the
70:36 - asynchronous nature of this stuff I
70:37 - could do other things to optimize it but
70:39 - I'm just going to ignore that and leave
70:42 - it at let me make it 25 hey timeout for
70:47 - a sec this should definitely be multiple
70:51 - parts
70:52 - oh yeah create inputs at the start they
70:56 - are constant oh that's such a good point
70:58 - okay I'm gonna do that right now that's
71:00 - a very good point who said that in the
71:02 - chat probably lots of people have
71:07 - precalculate them set up a bunch of
71:09 - people have yeah okay
71:15 - oh this alright
71:22 - all right everybody's saying that all
71:23 - right the chat is giving me some even
71:32 - further optimization which is why am i
71:36 - bothering to do this in draw this is
71:38 - something that the these inputs is never
71:41 - change I could just do them once at the
71:43 - beginning because they're and and I can
71:45 - I can ask for ask them many times in
71:48 - draw so let's actually fix that so I'm
71:50 - actually gonna I'm gonna take this and
71:52 - say let I'm gonna make this globe these
71:54 - global variables I don't know if you
71:58 - guys can hear the music that's coming
71:59 - from the room next to me but it's there
72:04 - alright then oh but the width and height
72:08 - does not exist until after create canvas
72:14 - so let me do this
72:19 - and let me do this okay so now that's
72:24 - there now I should be able to take this
72:28 - the input data and put this right here
72:33 - in the beginning and then I'm going to
72:37 - make a variable called X's and X is and
72:44 - where did I do that here and then create
72:50 - those X's so I'm now doing this in setup
72:53 - and then in draw the only thing I need
72:57 - to do in draw is run the predict this is
72:59 - going to make things run a lot faster
73:00 - let's make sure it still works here we
73:08 - go
73:09 - okay so you notice we getting like a
73:11 - different color each time i refresh
73:12 - because the neural network model the
73:14 - sequential model is initializing
73:16 - everything randomly but now I get to
73:17 - train it now I think we're ready to
73:22 - train it so here is what I did when I
73:29 - had my previous my own JavaScript neural
73:31 - network library I called neural network
73:33 - trained data inputs data outputs sorry
73:42 - I'm reading the chat you guys couldn't
73:45 - hear the music well alright so if I only
73:52 - I could remember exactly what I wrote
73:55 - when I made that TF layers tutorial but
73:58 - I know that what I need to do here and
74:00 - is I need to do something like this
74:03 - model dot fit some X's and some wise
74:07 - that's the training that's the
74:09 - equivalent and the learning rate is
74:10 - irrelevant and I don't necessarily need
74:14 - to do it this is basically what I want
74:15 - to do every time through draw I want to
74:17 - try to fit the model with some training
74:19 - data so let's first make the training
74:21 - data this is not exactly right I need to
74:24 - figure out and I need to use a weight
74:25 - that need to think asynchronously but
74:26 - this is the idea so if I go back to the
74:30 - top here this is my training data now
74:32 - one thing I definitely need to change is
74:39 - I'm going to keep the X's and Y's
74:41 - separate in training so I'm going to do
74:45 - this is I'm just going to do this kind
74:48 - of manually because I what's the big
74:51 - deal
74:52 - so let me make the training set and then
74:58 - one one those are the the X's now let me
75:04 - look at the Y's and the Y's would be 0 1
75:10 - 1 0 then I need those to be tensors so I
75:20 - need to say Const
75:22 - trait TF exes ah so I've got to think of
75:32 - good naming for this I kind of want them
75:33 - to call actually you know what I'm just
75:35 - gonna call it do I have a global X yeah
75:37 - I have a global X's already hmm hmm hmm
75:40 - tray TF X's equals 10 sir 2 D tensor 2 D
75:48 - OTF tensor 2 D you know what I'm gonna
75:53 - do I don't need these I don't need two
75:56 - separate sets of variables I'm just
75:57 - gonna create it I'm gonna call this ah
75:59 - everything is so much more complicated
76:01 - than I make it so if we're simple then I
76:03 - make it I'm just gonna make these
76:05 - tensors directly by saying TF tensor to
76:10 - D and then I'll put the parentheses
76:13 - around this and there now I made it a
76:16 - tensor then F is a TF tensor to D and
76:20 - now I made this a tensor ok now I've got
76:24 - the training data and I'm gonna get rid
76:27 - of this this is the old way that I had
76:29 - the training data which is totally
76:30 - unnecessary
76:30 - so this the training X's and the
76:32 - training wise are you with me if you're
76:36 - still watching I don't know dude get up
76:39 - and do it some jumping jacks
76:43 - let's see now I need to do model F it
76:47 - now model dot fit happens asynchronously
76:51 - so let's put it in its own async
76:55 - function called train model now if you
77:02 - don't know what it means to write a
77:04 - function that is tagged with the keyword
77:07 - a think this is part of es8
77:10 - a very newish version of JavaScript and
77:12 - I made a bunch of videos about what that
77:14 - is that you can go back and watch but
77:16 - this is basically a way for me to now
77:18 - say wait model dot fit and then let's
77:24 - look at actually let's look at the fit
77:26 - function model dot evaluate compile
77:31 - predict fit so what I need is
77:35 - is to give it the X's and the Y's
77:38 - there's batch size I'm not going to
77:40 - worry about there's epochs I'm not going
77:42 - to worry about or epochs and so H will
77:47 - give me back the history so let's just
77:49 - see here I'm now gonna say train model
77:53 - dot then H console dot log H dot loss
78:01 - index 0 let's say no loop again so
78:06 - basically what I'm doing here is I want
78:11 - to call this function train model and
78:13 - I'm using this idea of promises it's
78:15 - going to await the model on I need to
78:18 - return do I say a weight return or
78:22 - return a wait no I must say return oh
78:25 - wait return oh wait model dot fit so I'm
78:28 - going to return a promise which will
78:31 - have the result of the fit function and
78:33 - I don't know if this is right I want to
78:35 - just look I want to do that I want to
78:37 - call to train model every time and draw
78:39 - I might need to do this somewhere else
78:40 - just right now and then see what the
78:42 - loss is ok onyx 73 async function async
78:54 - function I've got a save function
78:55 - function it's an async function not an
78:58 - async there we go
79:00 - wise is not defined where a train model
79:04 - oh right this is I forgot to call it
79:07 - train X's and train wise so my training
79:10 - data train X's and train wise isn't that
79:14 - nice other word train just appears over
79:16 - out when you're doing machine learning
79:19 - you drink your glass of milk or whatever
79:21 - it is you're having while you're
79:22 - watching this coding training stuff
79:26 - cannot read property 0 of undefined a
79:29 - train model than H all right let's look
79:34 - just console.log H note that is what
79:37 - I've done okay
79:41 - history loss 0 okay oh no by the way
79:46 - didn't give it any testing data so
79:48 - whatsit computing the loss from history
79:52 - so this is I'm gonna call this result
79:55 - result history dot loss index 0 all
80:01 - right there we go there we go now let's
80:06 - let it do that over and over again in
80:09 - draw alright uh so this is a bit of a
80:26 - fail here
80:30 - no very returning the promise that's a
80:33 - good point
80:40 - all right
80:49 - so I let this run a little bit and
80:51 - unfortunately see it's getting nowhere
80:53 - this loss which I'm not sure exactly how
80:56 - it's country things I don't think about
80:57 - that come back to it is not going down
81:00 - anymore so what could be some problems
81:03 - here remember one is maybe my learning
81:05 - rate is no good not that it's no good
81:07 - maybe it's too low so where did I set up
81:09 - that learning rate again let me get rid
81:12 - of by the way I just want to now delete
81:14 - I want to make sure I'm not using any of
81:17 - my old neural network code so I'm
81:19 - deleting all references to that so this
81:24 - is now purely tensorflow Jess and and
81:27 - let me refresh and run this again and
81:30 - let me look into sketch dot yes and find
81:34 - where did I set the learning rate right
81:36 - here let's set it to 0.5 and see what we
81:41 - get yes getting better I'm gonna let
81:45 - this run for a little bit and I'll be
81:46 - back let me look at so one thing I want
81:58 - to do also is I want to write it looks
82:00 - nice if I write in the numbers actually
82:02 - of what the output is where it is you
82:05 - can see that it's actually getting there
82:06 - just very slowly I just want to see what
82:09 - settings I used in my other example try
82:16 - different optimizers and activation
82:17 - functions yeah I'm going to do that in a
82:19 - second I want to get it to work with
82:20 - this first I just want to see what
82:23 - settings I used
82:32 - [Music]
82:33 - oh did I forget to put shuffle in but
82:39 -  shuffle it by default I wonder if I
82:41 - ah you know what I forgot to put shuffle
82:45 - in it's actually working alright I'm
82:51 - back and you can see the losses now kind
82:53 - of much lower and you can start to see
82:55 - the visual that I'm expecting which has
82:56 - a true value in this corner a true value
82:58 - in the top corner and sort of darker
82:59 - false values in those corners but it's
83:01 - still kind of performing rather poorly
83:03 - one thing that I forgot to do is when I
83:06 - call the fit function there are a set of
83:13 - options that I can pass in for the
83:16 - number of epochs and all sort of thing
83:18 - but one of the ones that I really want
83:19 - to pass in here is called shuffle
83:21 - shuffle takes the training data and it
83:24 - shuffles the order of it each time right
83:26 - now I'm trading it with the same four
83:29 - data points in the same order every time
83:31 - which could be a bit of a problem and
83:33 - okay
83:34 - so let me now let me hit refresh here
83:37 - and run this again
83:47 - [Music]
83:49 - oops I can't see the frame rate because
83:54 - time out a sec 0.5 optimizer Oh
84:02 - optimizer yep this was all the same
84:05 - sigmoid sigmoid did I am I giving it you
84:15 - know what I probably did
84:28 - No I thought maybe I was giving it more
84:30 - epochs or something
84:32 - shuffle Forex or is not a big help
84:34 - apparently not
84:44 - I'm curious I could do just out of
84:48 - curiosity
84:58 - because if I go to that thing I was
85:01 - showing you everybody
85:17 - where this is this should be hosted
85:24 - hosted via github pages
85:42 - for loop and epochs will really help
85:44 - yeah I just wanted to see like I'm
85:47 - pretty sure I know why this isn't
85:49 - loading
85:59 - I mean the other thing is I probably
86:02 - should create a separate I mean there no
86:03 - such thing as a thread but I probably
86:06 - should have right I should just run the
86:11 - training like this async function I
86:14 - should just like I don't need to like I
86:24 - don't be calling this in draw but I just
86:27 - wanted to start doing that I was gonna
86:28 - fix that later what time is it - I have
86:35 - a half an hour and get through this
86:41 - [Music]
86:46 - whoops
86:47 - why is this not loading oh is it because
86:52 - oh it's probably cuz this is running
87:10 - I don't know why my example I wanted to
87:15 - see the performance of my example hold
87:17 - on I think I need to restart Chrome
87:35 - oops
87:40 - why is this not working here we go
87:50 - yeah I know I should do like I was gonna
87:52 - do this set interval thing but actually
87:55 - wasn't gonna do set interval I was gonna
87:57 - do set timeout and then have it like
87:59 - recursively call itself when it's done
88:02 - I'm just kidding I'm a got lost and this
88:04 - is not important I just thought I could
88:06 - run the thing that I made before just to
88:07 - see how it performed but I don't know
88:10 - what it's what it's stuck is anybody
88:12 - else anybody else try running this
88:13 - example maybe I'll merge Eric's pull
88:18 - request it's like stuck weird okay yeah
88:28 - that's what I that's what I wanted to do
88:30 - next frame so me I am so me let me come
88:36 - back to that I didn't want to do that
88:39 - first but yeah but thank you for that oh
88:48 - my goodness I forgot to tidy everything
88:53 - oh well but yeah the wise I need to
88:57 - dispose and I also need to tidy this oh
89:02 - yeah well let me at least just go back
89:06 - to where I was
89:23 - I'm gonna let this go for a little bit I
89:25 - just want to
89:36 - did anybody see what that was there it
89:41 - is where is it
89:44 - yeah look at that that's a mess okay
89:58 - okay
90:04 - all right so things are still working
90:06 - but it's still running kind of slow we
90:08 - can see I'm getting the lost down hold
90:11 - on
90:12 - okay things are working it's getting the
90:15 - kind of getting close to the right
90:16 - results you can see the losses going
90:18 - down I've realized thanks to the chat of
90:20 - course that I forgot something really
90:22 - important is I want to try to make it
90:24 - learn faster which I will kind of get to
90:26 - and I want to think about the sort of
90:27 - asynchronous nature of using p5 s draw
90:29 - loop and using the model dot fit at the
90:32 - same time but before I do any of that I
90:34 - just realized I haven't thought about
90:36 - memory management at all and there's a
90:38 - big problems if I just take out for a
90:40 - second this and let me let me do this
90:44 - here take out this console lot log and I
90:50 - run this again and I look at say if I
90:54 - say in the hooks yeah I'm like killing
90:57 - my computer it can barely run it again
90:59 - because watch what TF memory num tensors
91:06 - 3455 4042 79 i'm just generating tensors
91:09 - I'm filling up all the memory it's gonna
91:11 - perform so this is going to be a
91:12 - disaster so let me stop it before it
91:15 - gets too bad and let's see where do I
91:17 - need to do some cleanup so these X's
91:20 - these training tensors I never need to
91:23 - clean those up those I can keep forever
91:27 - but I in train model I should probably
91:32 - TF tidy this whole thing let me think
91:36 - about that let's first actually the Y's
91:39 - after I do this I can just dispose those
91:42 - so the Y's I definitely need to dispose
91:45 - let's at least just do that first and
91:47 - let's see what um let's see where that
91:49 - gets oh oh look at that look at this ah
91:55 - because I use data think I could use TF
91:57 - tidy but what I'm gonna do now is wise
92:00 - and then I'm gonna say let Y underscore
92:05 - values equals ma wise dot data sink I
92:09 - can't clean it up once it's also been
92:11 - data synced so now I do to why it's not
92:13 - disposed
92:18 - oops do I have no loop on nope and still
92:27 - going up that's weird
92:32 - oh this huh this has to be y-values also
92:35 - I have to change that to Y values so
92:37 - that helped now what I want to do is I
92:44 - need to uh let's just do this TF tidy
92:47 - let's just put the TF tidy here so like
92:51 - whatever happens in this function just
92:55 - tidy it all up I could put it properly
92:58 - up here it's that model that fit the
92:59 - need to tidy but I'm just going to do
93:01 - this let's do this five hundred forty
93:05 - nine hundred missiles being created over
93:09 - and over again that I didn't tidy
93:33 - it's to 2003 to a disaster what did I
93:39 - not tidy I guess maybe I need to put
93:43 - tidy in here like
94:05 - or I can just say like this right
94:20 - to learn
94:25 - Oh
94:33 - mmm
94:49 - I'm so confused like this no
94:59 - [Music]
95:08 - I'm so confused hold on I'll have to
95:12 - make the arrow function async yeah I
95:15 - dislike I'm like lost here let me back
95:18 - up for a second this is why I wanted to
95:20 - tidy outside of it you get rid of TF
95:23 - tidy for a second what did I have to
95:25 - start can I just put this here
95:44 - what am I missing
95:52 - I can't use promises in a tidy uh right
95:56 - so I was doing this right
96:11 - hold on a sec what what's wrong with
96:14 - what I had here what was wrong with this
96:31 - like why is this this should be fine
96:33 - right
96:43 - which version of
96:51 - well let me also go back to making the
96:59 - resolution much bigger oh uh I had it
97:09 - right before I just didn't save I'm
97:20 - gonna do a backflip ready what - no no
97:22 - I'm not dating back foot I had it right
97:26 - the whole time no that's so weird
97:38 - when the resolution is higher I have a
97:41 - memory leak
97:58 - that's weird what am I missing
98:07 - why do I have three in there that was
98:09 - weird
98:31 - [Music]
98:36 - I'm like going out of my mind here
98:45 - so what is it
98:50 - well I have some like weird bug here
98:52 - that I'm not thinking of exes
99:15 - let me see if tidy this whole thing hmm
99:24 - ah so why with a different resolution
99:35 - that's so weird I feel like that's is a
99:42 - bug he had to slower the resolution to
99:48 - less early I sort of feel like there's
99:52 - some bug that's not my code I can't
99:55 - figure it out but tidy here kind of
99:56 - fixes it
100:08 - I don't think that this needs to be this
100:11 - is just an array so this doesn't need to
100:13 - be cleaned up I mean the garbage
100:15 - collector should that's not a tensor but
100:18 - did I make some other tensor in here
100:19 - that I like it's modeled I predict maybe
100:22 - it makes some other tensors behind the
100:25 - scenes okay you know what it is I'm just
100:30 - being silly here I think I'm like
100:32 - forgetting that model that predicts like
100:33 - model dot fit does okay
100:35 - that's yeah I'm going back and talk and
100:48 - I'm gonna do my memory
101:07 - when did I make that three I'm gonna do
101:11 - my
101:17 - and go all the way back and memory leak
101:20 - it again and fix it
101:45 - yeah the epochs will definitely help
101:49 - [Music]
101:52 - whoo
101:53 - modeling evolution with tensorflow das
101:56 - from suraj rebel new video all right
102:08 - want to get this down a bit and then I'm
102:09 - going to come back
102:23 - all right here we go here we go
102:34 - I can't look at the chat right now jet
102:43 - all right so you can see it's kind of
102:46 - working I back it's sort of taking a
102:48 - while but I want to get this to train a
102:50 - little faster I want to make this I want
102:52 - to get a little further it's up first of
102:53 - all I was reminded by the chat that I've
102:54 - forgotten something really crucial
102:56 - important which is memory management and
102:58 - I really I really should stop this from
103:00 - running right now because there's a huge
103:02 - memory leak happening which I haven't
103:04 - cleaned up any of my tensors at all
103:06 - whoops look at that well not sure out
103:10 - that it was there this is there
103:30 - just do this again sorry everybody one
103:34 - more time shift ya memory leak really
104:08 - need that ukulele Wow
104:12 - [Music]
104:23 - all right wow look at this
104:27 - alright so you know it actually it
104:30 - actually is working it got the correct
104:32 - training result it's a little gray scaly
104:34 - in a way that I would like to be able to
104:36 - like emphasize visually what it's do but
104:38 - you could see the loss has gone way down
104:39 - but it took a while to get there but I
104:41 - want to add a few things to this and try
104:44 - to fix it up a little bit before I do
104:46 - anything I was reminded by the chat
104:48 - being over here that I haven't thought
104:50 - about memory management at all so I'm
104:52 - gonna say like no Luke for a second to
104:54 - just sort of turn this off then I'm
104:57 - going to say memory numb tensors oops no
105:00 - no way TF memory dot numb tensor I know
105:06 - I'm trying to use it I'm gonna say numb
105:10 - tensors TF dot memory numb numb there it
105:15 - is there it is I could get it there it
105:17 - is thirty two thousand two hundred five
105:20 - tensors that's crazy so I need to deal
105:22 - with that I'm just making tensors and
105:24 - letting them leak everywhere so I can
105:27 - manually run dispose but I've got kind
105:29 - of an issue whereas predict is gonna
105:31 - like make a lot of tensors behind the
105:32 - scenes as well as a model dot fit so I
105:36 - can use the TF tidy function so I'm
105:38 - gonna say TF dot tidy and then I just
105:43 - need to I'm gonna use the es6 arrow
105:45 - notation which you can watch my videos
105:48 - about what that is but and I've kind of
105:50 - gone through what tidy is tidy says
105:52 - anything inside of this code clean up
105:55 - the memory afterwards basically and then
105:57 - I'm gonna I probably could put this
105:58 - around everything but I just want to and
106:00 - I don't need this stuff anymore I just
106:02 - want to keep these two areas separate
106:04 - because I think I'm gonna at some point
106:07 - I really should change the way doing the
106:08 - fitting of the model excuse me in draw
106:11 - is somewhat problematic so now I'm gonna
106:14 - just tidy all of this I think that's
106:19 - right
106:21 - I tried an extra parenthesis there yes
106:24 - okay so now let's run this again
106:29 - comment out this console.log I don't
106:34 - want to see that right now oh all right
106:42 - now let's look at the number of tensors
106:44 - 15 15 15 so now I've gotten rid of the
106:47 - memory leak let's check out the frame
106:49 - rate 30 frames per second so this is
106:53 - running for up now let's I just want to
106:55 - be able to look at what's happening a
106:56 - little bit better so I'm actually gonna
106:57 - draw the number of the output inside
107:00 - each one of these things so let's do
107:03 - that so where am i drawing the
107:04 - rectangles here I'm going to say let the
107:08 - brightness value equal this and I'm
107:13 - going to fill the rectangle with that
107:14 - brightness and then I'm going to say
107:16 - fill 255 mind it like the inverse color
107:19 - I'm gonna say text number format the
107:24 - y-values index with just two decimal
107:31 - places and I'm gonna put that at boy
107:35 - this is awkward I the exercise this is
107:37 - gonna be high x resolution plus
107:39 - resolution / - I'm gonna say
107:41 - text-align:center text-align:center
107:46 - comma Center and then I'm gonna put the
107:49 - I'm just going to draw in the center of
107:52 - the rectangle and this should be J the
107:56 - text so let's see let's do this now so
108:00 - we can see you look there's lots of
108:02 - numbers there I think my number format
108:03 - thing didn't work 1 comma 2 and let's
108:10 - use a lower resolution just so I can see
108:12 - it better there we go now interestingly
108:17 - I can't see the numbers but there they
108:19 - go right you can see this is what it's
108:22 - getting the output for each one of these
108:24 - and I want to look at the law so you can
108:27 - see because it's just going so slow it's
108:30 - getting over time it's getting a little
108:35 - better but I really want to see a train
108:37 - much faster so let me see I have one
108:39 - idea one last thing I can add to this
108:42 - even though I and
108:42 - I have some suggestions for what I might
108:44 - do next but you can see all the numbers
108:45 - are starting to appear lovely I was
108:47 - because I forgot when it's gray when
108:49 - it's at point 5 2 or 55 minus 0.5 it's
108:52 - gonna be the same card I kind of like
108:53 - that effect so so what I want to do is
108:57 - what happens here if I actually give it
109:00 - tell it don't just do it once like do it
109:05 - 10 times do 10 a pox per cycle of
109:09 - fitting let me run this again and let me
109:15 - look at the let's actually have the loss
109:18 - continue to print out and there we go
109:24 - still running pretty fast you can see
109:26 - the losses going down and relatively
109:28 - quickly I am getting myself to the point
109:32 - where I'm starting to see you know this
109:34 - is definitely all the way got getting
109:35 - all the way down to zero there this is
109:37 - getting way up to 1 there it's getting a
109:39 - little bit stuck it's having trouble
109:40 - with this size side I imagine it'll get
109:42 - there eventually we could do some fun
109:44 - stuff for example I'm going to just let
109:46 - it have it's totally unnecessary but I'm
109:49 - gonna give it 4 hidden nodes and I'm
109:53 - also going to put the resin the
109:54 - resolution back to 25 and let's run this
109:58 - again and let's see how this goes so
110:03 - I'll give it a minute I'll come back oh
110:04 - the font is too big but you can see it's
110:12 - learning pretty quickly right now hold
110:13 - on
110:19 - well let me let me let me go back do
110:21 - that again
110:28 - so I'm gonna give it for no real reason
110:30 - all but just for fun four hidden nodes
110:33 - and I'm also gonna let me change the
110:35 - resolution to twenty five let me make
110:37 - the text size something like eight point
110:43 - and let me
110:45 - oops refresh it
111:09 - yeah one alright I let this run for a
111:15 - bit and you can kind of see here now you
111:17 - can see all the X or values here's a
111:19 - nice beautiful little map grayscale map
111:21 - of all X sorts getting all the way up to
111:23 - true and all the way down to zero at the
111:26 - corners this is pretty good
111:28 - so this is running at if I take out this
111:32 - console.log I can now take a look at the
111:40 - frame rate it's running kind of slow so
111:43 - here's the thing I have done something
111:45 - that I don't like which is and which is
111:50 - this train model is being called inside
111:53 - draw and I really shouldn't be doing
111:57 - that
111:58 - all right because I don't want I'm you
112:02 - know I don't want to call train model
112:03 - over I can call train model only thing
112:06 - about what I'm saying here yeah I how
112:16 - did I finish this up I have to go in
112:22 - just like a minute I'm saying of what I
112:25 - want to finish up here so I don't think
112:29 - I have time to do the TF frame thing but
112:33 - should I do
112:34 - would it make sense for me to at least
112:37 - do something like set time out train
112:43 - model let's just say hey do that and
112:47 - then
112:55 - like what if I did this function train
113:02 - set timeout train model I want to do
113:11 - like a then but well or what if I just
113:18 - did this
113:37 - like something like this right this is
113:42 - what I'm thinking
113:43 - and then this wood right does this make
113:56 - sense so I gonna explain this in a
113:58 - second like this will basically be
114:01 - happening elsewhere
114:13 - but I think I need the set timeout to
114:16 - like yeah this is never gonna release
114:18 - anything oh this is like superfluous
114:25 - because it's just returning a promise
114:28 - yeah I'll do that
114:43 - like what if I did this this is weird
114:47 - though
114:55 - kill this completely killed the browser
115:18 - yeah it's not really that much faster
115:21 - but this this is kind of like what I
115:23 - want to do right
115:40 - yeah now I can get a much faster frame
115:42 - rate and I could actually give it more
115:47 - epochs right
115:57 - yeah the training is happening more
115:59 - slowly but the framerate is happening
116:02 - faster perhaps result in a video about
116:06 - workers what tidy does nothing look look
116:14 - at this crazy way that it learned
116:21 - where's a tidy that does nothing
116:24 - me I am semi right is this an
116:27 - improvement over what I had before
117:00 - you know I I need this TF tidy what
117:06 - because this has to get tidied all right
117:17 - this is what's doing right now if I take
117:21 - that out oh you're right
117:28 - oh I don't I don't need a TF tidy for
117:35 - model that fit okay okay
117:40 - is this an improvement like to have
117:44 - pulled this out at least pulled this out
117:46 - of draw and a wit model that fit is
117:53 - tidied internally okay oh and tidy
117:57 - doesn't work with promises got it got it
117:59 - got it got it got it got it all right is
118:02 - this an improvement or should I just
118:04 - stay where I was and let it be like
118:10 - leave it and draw and talk about how
118:11 - that probably should do TF frame instead
118:13 - webworkers yadda-yadda-yadda
118:18 - answer fast I gotta go a sink while loop
118:24 - is a definitely good suggestion
118:35 - technically this is a much better way to
118:37 - do it all right thank you
118:39 - that's all I needed to know okay
119:11 - okay so I'm gonna go back to all right
119:24 - so the chat has given me some really
119:26 - helpful tips I've made quite a few
119:27 - little like weird little errors and
119:29 - mistakes here and I want to just fix
119:31 - this up a bit I think it's good actually
119:32 - to be easier to look at and watch if I
119:36 - just go back to a lower resolution so
119:40 - let's make this 40 and let me refresh
119:43 - this okay so here we go so this is now
119:48 - working training itself for X or you can
119:51 - see it's kind of moving along here now
119:53 - what what the real thing that's problem
119:56 - problematic here is the draw loop is
119:58 - happening over and over again and then
120:00 - I'm triggering something asynchronous in
120:02 - draw and I could be asking to train the
120:04 - model before it's even done with the
120:06 - previous training cycle so this really
120:08 - should not be happening in draw now
120:11 - tensorflow digest has a function called
120:13 - TF next frame I want you to explore it
120:17 - and make a version of this with Tia next
120:19 - frame as I can exercise after this video
120:22 - is over but I'm gonna do it a different
120:23 - way without that cuz I gotta come back
120:25 - then in a different video but first of
120:26 - all this I also learned this is totally
120:29 - unnecessary the via wait so a couple
120:34 - things number one is because there's
120:36 - just one thing happening in here I could
120:38 - just return the promise this doesn't
120:40 - actually have to be an async function
120:42 - and then I do not need the TF tidy
120:45 - because monal dot fit kind of will clean
120:47 - itself up automatically for you so this
120:50 - this should still work just fine and I
120:54 - should be able to see the number of
120:55 - tensors is still 15 so that was
120:57 - something that I didn't need that I've
120:59 - now fixed now what I really want to do
121:01 - is I want to get this out of draw so
121:03 - let's comment this out here and what I'm
121:06 - actually going to do is I'm going to
121:08 - write a separate function called train
121:10 - and in that function I'm going to say
121:16 - set time okay wait wait
121:20 - I'm gonna call train model
121:21 - wait hold on I'm gonna call trade model
121:25 - yes yes in that function I'm going to do
121:28 - this so I'm a separate function that
121:35 - does this piece of it that console logs
121:38 - the history and I could use and what I
121:41 - want to do is I want to say set timeout
121:46 - call the train function in 100
121:48 - milliseconds so I want to just let the
121:50 - program start 200 full effects late
121:51 - later call this train function train the
121:54 - model which does the fitting when that's
121:56 - done log the history and now say set
122:00 - timeout train 100 so I'm good this is
122:05 - sort of like workers like like don't you
122:07 - set interval here because I only want to
122:09 - call train again once it's finished
122:11 - with training the model itself so this
122:13 - is kind of like hey train and and by the
122:15 - way I could just increase the number of
122:16 - epochs or maybe do some kind of loop but
122:19 - I think this would be a sort of nice way
122:20 - to demonstrate it and if I just called
122:22 - train directly without a set timeout I'm
122:27 - never going to be giving back control
122:29 - for a second I couldn't end up with sort
122:30 - of like blocking so I might even be able
122:32 - to get this down to like 10 milliseconds
122:34 - just something really really low so
122:35 - let's run this and sort of see same
122:39 - result we can see there we go
122:41 - things are working but at least now I
122:43 - have gotten that out of draw so draw is
122:46 - happening on its own and in fact what I
122:48 - could really do is I could say hey try
122:50 - doing this with like a hundred epochs
122:52 - each time epochs what is it and you can
122:57 - see the lost function is coming out much
122:59 - more slowly but whoops but the frame
123:02 - rate let me just clear this for a second
123:06 - yeah the frame rate is quite fast hold
123:13 - on this is too confusing
123:21 - I probably need to give give it back
123:23 - more time let's do like another little
123:25 - break let me let me I just want to like
123:28 - take out the console.log thing so I can
123:30 - look at the frame rate I should just put
123:32 - the frame rate in the Dom would that be
123:34 - smart but you can see now I'm getting 60
123:37 - frames 30 frames of getting like a
123:39 - really pretty high frame rate even
123:41 - though the training is happening it's
123:43 - almost it's kind of like there is no
123:45 - threading in JavaScript so these things
123:47 - are just like passing off and really
123:49 - this might be a place where like web
123:51 - workers or something could do the
123:52 - training behind the scenes in some fancy
123:54 - way which maybe I will get to at some
123:55 - point oh my goodness
123:58 - so Alka is suggesting it might be better
124:03 - to use the draw loop and a boolean to
124:05 - know when it's safe to call it again
124:07 - that would also be a good idea so you
124:09 - can see though you can see what kind of
124:10 - like employing my hair out what kind of
124:12 - sort of like hassle situation we've
124:14 - gotten in but really let's just put this
124:16 - back to like two epochs
124:18 - let's put this to like a little 10
124:21 - milliseconds and we can sort of feel
124:23 - like there we go and I can look at the
124:25 - frame rate it's running nice 30 frames
124:27 - per second and even though and it's at
124:30 - some point it's going to get there
124:31 - what's that loss I forgot to console.log
124:33 - loss come back to me
124:37 - come on oh but let's see I have an idea
124:40 - what if just before I go just before I
124:43 - go what if we try using a different
124:48 - optimizer what if we try using for
124:51 - example a different loss function hold
124:59 - on
125:01 - this video was already 18 hours long
125:03 - what if oh no no no a different
125:06 - optimizer sorry what if I tried using
125:08 - the the atom optimizer
125:14 - so let's just try that just for fun
125:17 - times
125:18 - let's give it a lower learning rate that
125:38 - was pretty exciting let's go let's go
125:41 - let's uh let's make the resolution back
125:45 - to like 20 let me make the font size
125:48 - like nice and tiny for us we give myself
125:51 - some more space here hit refresh and
125:54 - then let's look at this look at it
125:57 - learning there wow look at that that is
125:59 - beautiful look at it learning XOR so
126:01 - nice and fast I'm just gonna hit refresh
126:03 - again
126:07 - so we could say we really should look at
126:10 - what this atom optimizer is and I will
126:13 - link to a paper and some more
126:16 - information about the some of the what
126:19 - the atom optimizer is you can find out
126:22 - actually we should just go yeah I'm
126:26 - sorry like hold on we should really I
126:41 - should really talk at some point about
126:42 - what some of these other optimizer
126:43 - functions are for now what I would
126:45 - suggest that you do is if I again if I
126:47 - go back to the API reference and I go
126:50 - all the way down and I find let me just
126:53 - look this way the OP if I go here and I
126:56 - go training out Adam we can see here
126:58 - this is what you're gonna want to click
126:59 - on this is the paper that describes the
127:00 - Adam algorithm of its a different but
127:04 - but it's optimizing the lost function in
127:06 - a slightly different way than stochastic
127:08 - gradient descent does you know we could
127:10 - also try starting to like we you know we
127:12 - could we could just never stop and I
127:14 - could start
127:15 - like I think using the rail ooh
127:19 - activation function instead whoops
127:22 - is that not what it's called where are
127:30 - the activation functions oh it's it's
127:32 - there's no it's just all lowercase so
127:36 - there's so many things you can play
127:37 - around with with these things how was a
127:45 - failure I think because I have the all
127:55 - right let's I'm not gonna add I'm gonna
127:57 - go back back to where I was explaining
128:01 - the atom optimizer oh it made this such
128:05 - a mess to edit and I really have to go
128:20 - I really should explain what these
128:22 - optimizers are but if I go back and look
128:25 - under here we can see what some of these
128:27 - are Adam the a da coming from the word
128:29 - adaptive and you could always click here
128:31 - and look at this paper which describes
128:33 - this particular method for optimization
128:36 - which is a little bit different than
128:38 - stochastic gradient descent and
128:40 - apparently things work a lot faster with
128:42 - this XOR problem so as I go forward into
128:44 - more of these videos hopefully we can
128:46 - dig into what some of these different
128:47 - optimizers do and kind of understand why
128:50 - I might pick one over the other in
128:51 - certain situations all right but I'm
128:53 - just going to just leave this B I'm
128:56 - going to hit refresh I'm going to watch
128:58 - it learn and train train train train XOR
129:02 - oh I don't know some things you could do
129:05 - investigate TF dot frame give me a
129:07 - little slider try different
129:09 - architectures different optimizers try
129:10 - some different activation functions I
129:12 - don't know if you actually made it all
129:15 - the way to the end of this video I don't
129:19 - know ashtag something I should Eric is
129:23 - telling me to watch one of those videos
129:24 - reviewing the JavaScript event loop
129:26 - which I definitely need to do so I need
129:28 - I'm gonna be back with more someday well
129:31 - good bye good bye good bye thank you
129:43 - momentum is add a knock for adaptive did
129:53 - I just make that up
129:57 - adaptive estimates so maybe the M is for
130:00 - estimates moments low-order moments and
130:03 - is for moments yeah
130:12 - all right everybody well you know today
130:15 - was one of those days I should really be
130:18 - doing my old style coding challenges
130:19 - again readout of the random numbers book
130:21 - okay okay I really I'm running late I
130:26 - gotta go everyone lie down go to sleep
130:31 - we're gonna let's let's artificially
130:36 - make this happen much slower let's train
130:46 - let's go caddy
130:48 - [Music]
130:56 - thirty two thousand nine hundred eighty
130:59 - five twenty six thousand eight hundred
131:05 - and fourteen fifty one thousand eight
131:08 - hundred thirty three fifty seven
131:10 - thousand three hundred and sixty three
131:12 - four thousand and sixty seven eighty
131:15 - four thousand six hundred forty eight
131:18 - eighty-five thousand five hundred five
131:21 - forty one thousand four hundred
131:23 - sixty-five seventy one thousand seven
131:26 - hundred sixty nine ninety nine thousand
131:28 - five hundred fifty fifty five thousand
131:31 - nine hundred four will there be a second
131:37 - live stream today no unfortunately why
131:39 - life's do on the weekend no
131:40 - unfortunately
131:41 - so apologies I wish I could live stream
131:43 - more often all the time make more stuff
131:45 - yeah blah blah the next live stream will
131:48 - likely be next Wednesday and next
131:49 - Thursday and I'm gonna be working on a
131:53 - lot more of this stuff in-between
131:56 - hopefully we will the things that I
132:01 - really need to look at or I need to look
132:03 - at that event loop article that I just
132:05 - to get a better sense of how to use this
132:07 - stuff together with the
132:10 - requestanimationframe better and TF dot
132:13 - frame alright so any last if I my real
132:20 - time that I had to leave was three
132:22 - o'clock I got two minutes to answer
132:27 - questions
132:32 - let's try like
132:34 - [Music]
132:39 - I like doing it with like a high
132:42 - learning rate you can see like it gets
132:44 - too good sort of see it bouncing
132:52 - [Music]
132:59 - maybe this learning rate is too high
133:00 - it's kind of get stuck it kind of can't
133:02 - get there and then if we could do we
133:08 - could do weird stuff like like what if I
133:10 - gave the hidden layer 16 units for like
133:12 - no reason like look how it is there's no
133:20 - correct answer there's only training
133:22 - data at the corners that learn to
133:23 - rainbow people for asking me questions
133:31 - and I are you going to do this
133:32 - convolutional 2t soon that's why I I
133:36 - hope to plan to yes
133:38 - all right everybody so this is my list
133:43 - Oh Thank You Joshua Myers that's very
133:46 - kind of you if I had my Philip's light
133:48 - bulb it would have flashed by the way oh
133:52 - let me just mention if any of you are if
133:55 - any of you are sponsors you can now
133:56 - sponsor the channel through the through
133:58 - the YouTube interface itself make sure
134:00 - you go and check the community tab and
134:03 - look for a post that links to a Google
134:05 - Form to enter your email so I can send
134:07 - you an invitation to the slack group I
134:08 - need a better system for doing that I
134:10 - have a thing set up that it actually
134:12 - like I have a I get alerts in a
134:13 - spreadsheet and I can look but I don't
134:15 - get your email address so there's no
134:16 - automatic way to invite you to slack
134:18 - other than by doing it manually yeah so
134:23 - this is where I'm this is where I am
134:27 - going to next I am going to do a I want
134:35 - to do the TF playground idea and the
134:37 - classification idea so if anybody has
134:42 - any ideas for really simple goofy just
134:45 - like basic numeric data sets probably
134:49 - going to do something where I'm going to
134:50 - look for some color data set which takes
134:52 - any RGB set of RGB numbers and like
134:56 - categorizes it it's like a few with a
134:58 - few different like labels that's one so
135:00 - if you have some suggestions for that
135:02 - please let me know at Schiffman on
135:05 - twitter is probably the best way all
135:07 - right everyone
135:08 - I don't know I had more layers
135:13 - sorry everybody I hope today I hope you
135:15 - enjoyed today somehow uh somehow I I
135:20 - just I don't know maybe I shouldn't be
135:21 - covered next week let me through really
135:23 - maybe I'll try to at least do a coding
135:24 - challenge that's not machine learning
135:25 - related I want to get thrown mean
135:27 - learning content but it is kind of
135:29 - overwhelming and taking over everything
135:30 - weather prediction but I need I want to
135:33 - do classification so I don't want to do
135:35 - time series great all these these things
135:38 - will come but I want to do a really
135:39 - basic classification I don't use images
135:42 - I don't want to use text data that's
135:44 - like I don't use time series sequential
135:47 - data I just want like you know like it's
135:49 - like house prediction the iris data say
135:51 - these are the kind of thing house price
135:53 - prediction
135:53 - well--that's prediction that's not
135:54 - classification even but regression but
135:56 - something like that but I want it to
135:58 - feel goofy creative in the art world
136:01 - space that kind of thing
136:03 - alright tic-tac-toe game okay goodbye
136:06 - everybody I'm gonna hit stop streaming I
136:07 - guess I will play you out with my weird
136:09 - trailer since that's what I do now
136:13 - [Music]
136:21 - while the trailer is playing I will
136:24 - attempt to explain why doesn't it
136:28 - dispose of tensors automatically s asks
136:30 - ray Jackson asks Arnab I don't actually
136:35 - know why it doesn't dispose of tensors
136:37 - automatically but it's this is the thing
136:40 - that there is no way to clean up the
136:42 - memory memory on the GPU with a garbage
136:44 - collector in the same way and this is
136:45 - like a lower-level question that I would
136:47 - be curious we have to investigate more
136:49 - about how tend to flow J s works
137:02 - [Music]
137:08 - [Music]
137:11 - you
137:20 - you

Cleaned transcript:

hello good evening this is not the evening at all this is the afternoon though welcome to the coding train on a Friday which is my usual day for the coding train but this summer it has not been my usual day I was just having a lot of trouble getting the start streaming button to start I had restricted mode enabled on this laptop for a variety of reasons that I'm not entirely sure of and interestingly enough I cannot start streaming I cannot actually I have to bat it's a long story but I couldn't get that to start streaming the button to work while those in restricted mode I fixed that so here I am how are you what's going on what's what's up what's what's happening got totally weird how you can't speak to me all right there's a chat so you sort of can you could type to me in the chat it's night in India writes Melvin in Israel it is 20 o'clock which i think is 8 p.m. all right so I have to admit something although this is nothing new you know I feel like normally I'm so well prepared and I spent all week big making notes and scheduling things out and knowing exactly what I'm gonna do and then I like I've got all this energy and I turn on the streaming and I'm go go go and I do a tutorial and I turn off the streaming and the day is over right now I feel like it was that I had to make a heroic effort just to make it here and press the start streaming button so I have to admit that I'm a little bit out of sorts but and I but I I have two hours today good news is I am planning for two live streams next week so I am planning to livestream both Wednesday and Thursday it looks like next week and I will publish Tynes to the homepage of this YouTube channel thing soon enough and so I'm hoping to get some get get into I but I really want to do I really want to do something practical a practical is the wrong word I don't do anything Pratt whatsoever I want to do some machine learning demonstrations in the browser with data and data that might interest you or inspire you to use your own data and do something else with it but I'm not there yet let me try to figure out where I am I'm gonna go to YouTube the coding train and I'm going to go to neural networks and machine learning I'm gonna go to session 6 hello and welcome I'm not gonna watch that alright so this is what I this is what I have so far I have to have to do this as beginning of every live stream to sort of recap and refrain recenter myself hi I'm gonna put this over here so it doesn't block the view you may or may not be aware there is something out in the world called tensor flow J yes this is a tensor flow a implementation of the tensor flow API in JavaScript that runs in the browser with no other dependencies all of the math is done and computed using WebGL and shaders and all sorts of amazing gymnastics and ways that I might never understand but enable opens the door and enables possibilities for us the people who like program like this it's a high program to try and experiment and learn a bit about machine learning and get our get our hands in there and and ask the right questions would be critical about the role of AI and machine learning in our world today so that's this I have started doing a series of tutorials these are not super beginner friendly you know there's some advanced JavaScript advanced aspects of the JavaScript language that I'm using that are confusing like promises and weight and they sink you have to do lower level memory management yourself when you make these arrays of data you have to allocate the memory and deallocate the memory and then there's all these like scary weird terms likes get the stochastic gradient descent and that's that's one of them you know optimizer root mean squared so but I'm doing this series for an audience who perhaps has already learned a bit about JavaScript programming maybe watch some of my other basic intro to neural network videos and just kind of like follow along and see how a larger machine learning library works in the browser and can be used so the tutorials that I have so far are sort of an introduction to what tensorflow DOJ's is talking about what it is to be a tensor what is it to be a tensor I want to be more relaxed I want to be a variable instead of a tensor oh look that tents are very bad whatever okay then variables and operations talking about memory management I implemented a version of linear regression which is a kind of like classic machine learning algorithm where you try to fit a line to a bunch of data points kind of serves as the foundation for a lot of machine learning research I also looked at polynomial regression where instead of a line we could fit a polynomial function which curves around and then ah then then then then then I finally finally finally started looking at the layers API and the layers API is a higher level API inside of tension flow j/s which allows you to create machine learning models as sequences of layers and that has you know how that works has to do with how neural networks are architected with inputs and outputs and layers that are in between and there's different kinds of layers and different kinds of math functions that happen with those layers all that sort of stuff so this is where I am at the moment this is where I am where am I going to where am I going to don't ask any more I forgot my ukulele I learned to play the ukulele a couple weeks ago thinking of routing this YouTube with me playing ukulele where am I going don't ask anymore okay ah so what's next let me make a list really for me but you're watching so you can watch X or with T F layers I want to do a classification example I'm thinking of do what I'm thinking of right I want I so I'm gonna go through these one at a time classification and I'll come back to the details of these I want to do image then I want to do image classification then I want to do image classification again with convolutional layer so now I want to talk about what that is do I want to do some type of regression example maybe maybe so classification and then maybe sort of like as inside a Part B of like a basic like regression so this is kind of what I want to do in terms of the basic building blocks of machine learning with neural networks and I want to build all of these with the tensor flow das library so just just just to lower your expectations for a minute if I were able to get even just this done today I will be very happy about that okay so this this is kind of my goal that's my goal for today now I am doing things backwards in one in one sense I'm doing that these these are not super beginner friendly I mean they're as beginner friendly as I can make them I want them to be friendly but you know if I was my first day watching a coding video I might not want to jump into the image classification with the tensorflow layers api video which doesn't exist yet but but I once I get through this or actually once once June 15th hits or maybe even a little bit before June 15 that's next week I'm gonna start doing some beginnerfriendly and hopefully gonna guests come and do these with me maybe somebody was watching this right now who's worked on this ml5 project would like to come beginnerfriendly ml with some hearts and some stars like a little like rainbow and then like there's a train train going by I can't draw a train okay and with a library called ml v dot Jas so I'm gonna come back to this a second so just briefly let me just show you you catch a guess that's a great idea for a so if you go right now on the Internet to a URL ml v JSTOR G you will find this website and this is gonna be fun let's do something fun here we'll see here right here on the homepage is this interactive demonstration this is a picture of a Robin which the mobile net model labeled this as a Robin American Robin turdus migratorius with the confidence of 98.7 so I'm going to upload an image do I have any images here this is good hold on let's go get let's go get a rainbow image is this looks good download we should done a train let's try a rainbow dot jpg where am I going desktop that looks good let's go for a train this one looks good Oh save image as train let's go back to the ml5 webpage and what I'm gonna do I could drag and drop it but I'm just gonna do this let's looks let's try the train trailer truck tractor trailer trucking rig rig articulated lorry semi with a confidence of forty two point ninety four percent let's try doing the dragging and dropping thing with the rainbow bring it over here and now we have a parachute shoot with a confidence of forty nine point twenty nine percent so how does this work well I will show you if you scroll down here you will see here is the code for such a thing and so the ml5 library is a machine learning library built on top of tensorflow das this library would not at all be possible without tension flow chess running behind the scenes to try to create some simple code examples to work with at the moment mostly pretrained models in the browser and there's lots more coming here and I'm gonna do a bunch of tutorials with this but this project is I'm mentioning it now because Hannah Davis at the IO Festival I'm totally not in the slack channel here at the IO festival made talked about ml5 in her presentation when hopefully the IO Festival which is just finished up in Minneapolis the videos that all the talks that are amazing go find their Vimeo channel and all the ones from last year you can watch them the new ones from this year be out soon and we're looking at trying to launch this more officially on June 15th and I will mention that if you're interested in kind of poking around if you go to all of our github repos first let me just go over here oh we can see here's all sorts of wonderful people who have been working on this project more than just these ten people but here are 10 people and what I want to show you here is under projects this is my first or a into using the project management tool that's part of not to okay so but this project management tools the first time I've used it if anyone wants to jump on in and get involved and get bored kind of have a little sprint here from now until June 15th there's a lot if you look at that website there's a lot of stuff that's missing there's a lot of stuff that's broken and so you know reach out to me on Twitter at Schiffman type in a comment somewhere on github if you want to get involved and help us kind of push forward some of this code stuff for the release and okay so that's what I wanted to mention there too looking in the chat looking in the chat looking in the chat okay so now coming back over here so this is what's coming this is what I hope to eventually have on this YouTube channel is a playlist which is machine learning for beginners in the browser and what's to call it exactly and I'm gonna be using ml 5 NP 5gs for that together by the way the 5 in ml 5 is in omage 2 P 5 and processing because the ml 5 library aspires to be friendly and accessible in the same ways that processing in P 5 have been over the years and there we go my camera's still shut off now back to today it's 1 o'clock already so what now I have a pretty clear picture what I want to do with my image classification examples that I'm going to build I want to use the quickdraw data set so one of my goals with making my machine learning tutorials is to use non traditional data sets and but maybe nontraditional is wrong word but data sets that are outside of what you would typically find in machine learning and data science curriculum want them I want to use different ones that are kind of in creative space to get people thinking more creatively about what kinds of data they encounter in their life that they could maybe use I want to use data sets that are really simple and kind of like easy to understand and look at and it also want to use data sets that are representative of the world that we live in and all of the all of the cultures and people that we share this great earth with so you know things like I'm trying to avoid things like M mist which is the classic handwritten digits data set things like the iris data set which is wonderful I have loved flowers nothing could possibly be wrong with the flowers data set but but things that you wouldn't that that kind of made me feel a bit more approachable so I really asked this question a bunch of places a bunch of times don't get any responses because I don't maybe it's hard to find these kind of data sets I feel like the Google quickdraw dataset is a great one for learning about image classification and then what I'm thinking of doing an X or is like it's not really a dataset it's just made up oh wow something else someone add in here okay hold on hold on and then classification what I'm right now the only thing I'm so tensorflow dot yes there's a node version tensorflow yes that used this MLB Major League Baseball dataset to classify pitches and I love that football kind of a little bit of a baseball nerd you know any word so so I so that's kind of interests me but I don't know that baseball is perfect for what I want to do that's going to be you know a lot of people don't know about baseball and ranged in baseball and it's maybe not reaching the the more general audience that I'm imagining for this channel so but something like that that's really simple so all I could think of right now because I from gibreel go check out a CFD science that's the assess of science I can't ever say his channel name it's probably like there's like a really easy way to say that channel name and I just can't do it I don't know why I'm almost falling over for no reason I was talking about something datasets gibreel had this demonstration of a color predictor and what am I it's actually this semester created a variation on that which was kind of predicting more about a color than just a or B and so I was thinking of kind of using that as an inspiration and so like what if what if I made a color classifier that classified colors into like bluish grayish or like like the C I don't know some kind of set of arbitrary labels like five to ten labels that and what I need to so maybe I would crowdsource that Dave said I'm not sure yet so that's what I'm thinking about for classification I kind of hope to do that today but I talk too much and there's a little bit of time but that's that's coming next week another thing I forgot in here I wanted to make like make your own TF playground so just briefly one last thing that I'll mention here on this todo list if you go to I believe it's playground tensorflow dot J s what's that well hold on J or whatever tensorflow playgrounds this is a project from the big picture group the research group from Google that created that where 10th floor address itself came out of and you can kind of create this little playground in the browser where you can configure a neural network you can have this kind of 2d data set you can actually there's like a play button so you can run it and you can watch it try to either classify and there we go sort of classify or I don't know it's doing classification regression it looks like classification to me they're sort of blue and orange so I have no interest in building out something that has this level of sophistication and design visual design but I would like to show you well could you similarly to how I made the linear regression the polynomial regression examples maybe I'll just do like a basic 2d classification problem with drawing stuff okay so that's that's my introductory talk I love this go you little neurons that data whoo all right oh oh I can break it I'm very keen for a fix all right so that's where I am so I think when all is said and done I think right now I'm just going to tackle today from now until about 230 which is an hour and a half X or and I really am torn like I don't want to do it because is it it's sort of but it's good for me so I'm gonna do all right but before I do that let me see if I can get some questions and get myself organized anybody have any questions about ml5 tensorflow J s life the universe how to play the ukulele all right so what do I need to do here it's a couple things that I need number one is if anyone who is a sponsor and patron and there's an interesting question in the YouTube chat that's the might that I might like to answer you can paste it into the sock channel this is not right no no oh totally Papa Delhi doobie doobie wahh there we go alright there's two bits of code that I need to get started with this one is the actual previous XOR there we go coding challenge 92 and then I also want to get under maybe it's under courses intelligence and learning session wait no how come it's not there is this a different go past in a different place I have this in too many places courses intelligence and learning session 6 the layers API I need that and then p5 tensorflow let's put these in there and I don't leave these anymore and let's go over here I don't know which would be better to start from let's start from this this'll be coding challenge what what coding challenge number isn't this the third time we do X or is it really oh you're really torching me because I really feel like I have a thing I have a little bit of a thing like it's kind of a little like you know like I can't if I'm listening to podcasts like I I have to listen to every all of it every minute I can't like not listen to one episode and I have to like so somehow in my stuff like guys like I just have to do I have to do the X or now with ten to flow Jay s because I have to but maybe I should skip it what coding challenge number am I on did somebody tell me no but I can find that out by going here and oops 105 was polynomial regression so this would be 106 okay I totally didn't do this right yeah how come there we go now do I have the atom editor open see here's the thing what's interesting about doing this YouTube channel is if I were teaching a course like I do supposedly he's at NYU I just would not I would just skip a lot of stuff because there's like limited amounts of time and I do to some extent to do that here but I I have this like false it's like this false sense of infinite time and I must do every single step which I need to move away from Cain wheat bun writes it's a good example comma but dot dot and I get it I get it but I'm waiting for what's coming next yeah I think I could I could fill it in but but but I think it's weird that's what I'm doing today unless I'm into that no no okay that's what I do all right all right all right so let's see here we are this is my list huh thank you thank you XOR Shipman see I think I only actually made one even though it's like probably the third or fourth time I'm doing it on this channel I didn't I don't only have one video it appears yeah look here's here's Simon talking about X or some other videos okay all right and now Joe server yes let me open up the browser I would love to do like some kind of little just fun algorithmic thing the equivalent of like phyllotaxis today before I go if there's time but I doubt there is okay all right you had got CGI rights I missed the non machine learning coding challenges totally agree yeah I don't know burger Bob asked could you please read the chat more often I totally get the sentiment I appreciate the question I will sir I can certainly try it is very hard to follow the chat and do the livestream at the same time and maybe someday I will have a better system for doing that I have some ideas for how to do that but it just I need time to get some more screens and maybe have some help with that and the sort of thing all right ADA she writes did he say filing taxes yes and now coding challenge number 327 filing your taxes let's go see IRS tax filing API ooh IRS gov a file providers software developer okay I guess I guess the XOR isn't so bad phyllotaxis the spiral beautiful Fibonacci spiral pattern of a sunflower that's what I was saying alright see what happens burgerbob that's exactly burgerbob Oh what about a giant screen which shows the chat behind the camera that is exactly what I would like and I will snap my fingers and giant screen will be mounted there behind the camera somehow that didn't happen I'm not sure why so I will I I do like that suggestion looks whoops yes Chris writes might be worth having a mod pull interesting questions out of both chats and give them to Dan at Q&A breaks I'm absolutely game for trying that and wants to sort of volunteer at this point I think I would need a volunteer to help facilitate that and that would be great all right let's let's let's let me get over all of my anxiety and hangups about you doing X or again and talk about them when I start and then and then and then and then I will begin I see all these people typing and Ada and Kate week Mon and Eric but I gotta move on I think I got to start because time time is awastin yes thank you okay Eric in the chat rightz sometimes it's important when learning something new to base your exploration around an example which is fairly trivial and you understand intimately well true or were better I could not have put it better myself thank you I'm gonna read that sentence at the beginning of this coding challenge if you don't mind hello welcome to a coding challenge yeah I know what you're thinking I mean I don't know what you're thinking I know what I'm thinking that looks like coding challenge number 92 XOR which is probably one of the less interesting creative like sort of just technical coding challenge demonstrations that you've done why why why are you doing it again well Eric from the coding train community writes thanks explanation because I was just before I started this having a real hangup about this sometimes it's important when learning something new to base your exploration around an example which is fairly trivial and you understand intimately well so here's the thing I I'm learning something new I'll come back here and the thing that I am learning something new is this tension flow a thing and wouldn't it be fun to make like play pacman with it or the emoji scavenger hunt project or teachable machine or play a piano with it all these things all pose that oh my god we got it we're gonna get this I could just sort of go there right now I will get there eventually but I'm trying to learn the basics of how the library works and I'm trying to step through this slowly so I will say that we're if you're watching this video right now where you are is not necessarily in the most beginner or friendly place because I'm working with tensorflow tas natively to implement basically like a weird math problem it's not that weird of a problem actually but a very basic trivial math problem just to see how tensorflow dachi has works that's what I'm trying to do with this coding challenge and about 20 or 30 minutes he'll be coding this coding challenges just look it's right like four hours and 72 minutes long which is why i say 72 minutes cuz that's five hours and twelve minutes I don't know but the trajectory that I'm on is I'm gonna start doing some stuff inching my way towards hell let's actually use some data let's use some more data and maybe some images and so I've got a bunch of things that I'm stepping and I'm trying to get to the point where I'm going to use this other machine learning library called ml5 which at the time of this recording hasn't really officially been released yet but builds on top of tensorflow das thank you everyone who votes wherever you are to try to create some more accessible interfaces to some of the algorithms and models that you things that you can do with tensorflow digest without having to do the lowerlevel memory management and math operations stuff so all that is coming and I just took a lot of time in this coding job to say that to you so but as much as I kind of don't I haven't I'm not so sure but well but it's a why is XOR so here's the thing this is why I want I need an example this is the first time I'm going to ever in any of my videos except for the other one that I made but this is the first time that I'm actually going to use the TF layers API to Train Oh to train a model with a data set to produce a certain output okay I did two tutorials about what the TF layers API is you could pause down and go and watch those and then come back here but in those videos I didn't actually do anything with TF layers just sort of talk through and typed out some code so the problem that I want to solve and apologies for explaining this probably for like the fifteenth time on this YouTube channel is very well known from machine learning X or because when the original perceptron was invented the single perceptron the model of an individual neuron that could receive inputs and generate an output it could not solve X or it just couldn't it's not a linearly separable problem and I've talked about that in other videos about why we need multilayer perceptrons so the nice thing about XOR is I can diagram for you hold on a second look back I can diagram for you the the architecture of the model that we need to create there are two inputs there is one output so that the inputs to the XOR problem are true and false values so unlike a so if I made a little truth table and or XOR right I can have true and true true true false false true false false and and operation would only ever give me true when both are true false false false and or operation would only ever give me would gives me true if just one of them is true true false can you even see that I can't see on my monitor but hopefully you can now XOR the X for exclusive gives me true only if one is true they can't both be true only one so in that case I get false true true false and the idea if linearly separable comes up here because I can draw a line here to separate true from false I can draw a line here to separate true from false but here I could do this but I can't draw a single line to separate true from false we need a more sophisticated model with a hidden layer so the inputs are things like a 1 and a 0 feed forward into the hidden layer activate feed to the output and the output should be a 0 or a 1 it's really in some ways a classification problem but I'm gonna do this as a regression essentially where I'm just gonna get some number between 0 & 1 if you watch the previous coding challenge the reason why that is is because thank you very much good night this video is now I hid that my sound effect by accident because what I'm trying to do is visualize the truefalse space alright pause for a second just taking a pause for a second how long's 10 minutes at least right but I you know it's important it's important for me to talk about what I'm doing all right so I'm just thinking here where am I going next looking at the chat no one's complaining too terribly and I think I'm going to move on I guess I could transition back over here case I'm not you want to edit out the weird sound effect thing oh they're good sounding okay that's good Hugo asks will you ever do non JavaScript videos well I do I do some processing and Java videos those aren't JavaScript but you know at this point I'm kind of I'm kind of I'm kind of doing the JavaScript thing okay let me let me transition back and that to me may be the sound effect little thing was like a funny little bit but I think I'm gonna just transition back because ultimately what I'm going to do is visualize the output of the model and I'm gonna send in numbers all the way between zero and one I know I don't even know what I'm saying it's fine because ultimately I'm gonna visualize the output as grayscale values and I want to see number I want to see grayscale values all the way between zero and once the same thing I did in the previous coding challenge if you if you're happened to have watched that one alright so now I actually I'm gonna also do something where I start from the code from the previous coding challenge and so we can see there's this idea of training data the inputs to the X or problem are zero zero gives me a 0 0 1 gives me a 1 1 0 gives me a 1 at 1 1 gives me a 0 this is the training data and in my previous version of this I used my own neural network library so in theory I'm gonna get rid of the idea of the learning rate slider just before we can add that back in later but let me get rid of the learning rate slider basically I want to do exactly the same thing the difference is I'm going to say neural network equals TF layers sequential and maybe I'll call this the model instead of neural network so the I do this is a neural net here so the idea here is that I want to replace my neural network library with tensorflow yes and so this for me what what the usefulness of this video is a me learn I spent all this time trying to build my own rather sort of terrible neural network javascript library and going through that was sort of helpful in thinking about how the stuff works now if I can translate that into attention flow dot yes I'm gonna things are gonna hopefully start to sell and make more sense into my brain Bruno is asking something in the chat about the true/false table yeah usually you draw it as a I might come back to this later usually you draw it as a matrix and I sort of did something weird there but I think it's fine sorry I'm looking see this is what happens wait look at the chat too much all right okay alright okay so now we need to what this constructor here said and let's just put this back to this this constructor here said would make a neural network with two inputs two hidden nodes and one output so I need to duplicate that idea here with PF layers so let's go to the 10th floor Jas API reference and we're gonna go all scroll down to TF layers and what I want to make is a dense layer TF layers dint a dense layer is a fully connected layer so what I'm going to do is I am going to say let hidden equal TF layers dense and then I can put inside there an object that has the parameters of how I want to configure that layer and so how do I want to configure it the two things that I want need really need to do is this is the hidden layer right I need to give it an input shape right he just say what's coming in what's coming in that's what this is here I need to say how many nodes it has that's the number of units and then I probably has a default one but I can specify an activation function and again I'm just going to use sigmoid as this historical activation function that I've been using in all my videos to date I'm gonna soon talk about softmax what that is as well as some other activation functions like lazy which is maybe more commonly used okay but like nobody pronounces that way foot B so don't get confused alright so I want to say input shape I believe is just there's just two inputs I also want to have two units two nodes and activation is going to be sigmoid so now I have created the hidden layer yay the other layer that I need to create is the output layer and so what am I know the app and layer I don't need to provide an input shape because the input shape can be inferred if I add them sequentially the inputs are not a layer so for this first layer the hidden layer I've got to say how many there are but now once I'm creating this next layer it can just the input shape is gonna be defined by what was before it so now I'm going to say really have to stop it at the sound effects by excellent and now I'm going to say let output equal TF layers dense and all I need to say is units one activation sigmoid okay then ought to do is say Model Model dot add hidden model dot add output okay so this is the model now one thing I need to do is I definitely need to import the tensor flow J's library which I happen to have from one of my previous examples so I'm going right now I only have I have the p5 libraries in my index.html plus my crazy neural network thing and my actual code and sketch J yes someday maybe I'll use the fancy new import syntax stuff let me just just have everything kind of line up let me add this in here so now TFS should be there I should be able to go back and run this and not see any errors aha TF dot layers dot sequential is not a function so I'm seeing things in the chat chats really off the rails with this XOR thing all right so I probably just so I probably just didn't even see half cut layers dot sequential the right thing you know I could go look I by the way made an example oh it's just TF dot sequential okay so all I all I want to say is I just got that wrong it's TF dot sequential so you know I could go look you know hopefully I would find this here at EF dot sequential yeah models creation there it is TF not sequential so I just had that wrong okay let's try refreshing this yet again slider is not defined hold on sorry much ya have to turn the notifications off on my watch I'm getting like phone calls and buzzing things okay take a minute here somebody said that I look I'm good at drinking drinks in profile that I could be like a coating train brought to you I'm buzz marketing Klean Kanteen that's not an official sponsor where was I right alright let me fix this learning rate issue 0.1 I just want the thing to run okay so it's going it's still working with the my neural network library not the new ten so Jeff's one but let's keep stepping through so ah so what am I missing here so when I make a model this is now I've architected the model I've architected this particular architecture but I need to do another step I need to compile the model and I need to define the loss function and the optimizer basically I need to say like okay well this is how I'm going to determine how well the model is currently performing with the training data and testing data potentially but I'm not getting testing data will come in my next video about classification but here I'm not making a distinction between training and testing date I'm conflating those two concepts which is a big mistake and a problem but we're stepping through this stuff later by little by little like a butterfly flapping its wings it's not at all a butterfly but I felt like I was being like a butterfly and then an optimizer is what sort of function what sort of algorithm am I using to adjust all of the weights of all these connections according to the loss function itself so I need to define those things so let me try to type it out how I think it is and then we'll go check so I know I need to create an optimizer GF optimizer like this and with a learning rate something like this like I'm gonna I want to have used to cast a crate to set with some learning rate that's not correct this is me like trying to remember what what the code is and then I need to say like model dot compile and then I think when I compile it I'll say things like this I'm going to compile it with optimizer and this loss function like like root mean squared or something like that so this is what I'm remembering from when I looked at this at one time and I probably got this wrong so let's actually go look at the API Docs well first what's the chance that any of this actually makes sense okay TF optimizers not a function so let's see how do we create the optimizer optimizer yes so it's this is what I want I want a TF train SGD this is how I create the other optimizer is not a keyword in the API just I imagine that for myself so I need to say TF train SGD and then give it a learning rate so TF train SGD and there are other kinds of optimizers that will will that I think I've even shown you and what we'll use more and give it a learning rate like 0.1 then I want to look at model dot compile so look for compile well we can see in some examples here what I'm looking for is where the actual compile there it is compile so the compile function compiles it and give an optimizer a loss and I can also do some metric stuff I'm not going to worry about the metrics too much although maybe I'll try to come back towards the end of this video okay model dot compile optimizer loss I think this might actually be fine is it root mean squared so let's look for the loss functions loss root means mean squared Oh Weiss ago I keep saying root because I have it in my head from some thing that I did a very long time ago where I was always taking the square root of the mean squared error so I always say root mean squared there's no root here involved I guess I have to get back up and continue this tutorial how long was I saying root for and hell annoying will that be for the people who watch this later okay well don't you know I get up slowly others I get kind of lightheaded alright apologies I've been saying root mean squared error for because I'm stuck in this world where you have to take the square root which you don't need to do here so just mean squared error that's all I need this is my loss function mean squared error now let us now go back here hit refresh all right things are happening things are going so the model is built the model is compiled and the next thing that I am ready to do is now actually start putting data in the model time out for a second why did I lose all right just a second here I really have to watch the time I've got an hour I have to be a little league practice I am NOT a strange forty fouryearold person who plays in the league but my son is playing Little League for the first time this year to be at the practice cannot be late so I have another hour though okay hmm I'm not the coach don't worry I'm just stand by the side a cheer do my little debt hold my little signs that's it I don't have any sign should have signs I was actually thinking of sponsoring a little league team like the coding train sponsored little league team I think get it get together this year maybe next year alright all right there too these are two next steps whoops oh why is this completely died I just I know I'm saying 44 a lot because about to be 45 so I feel like it'll say 44 I like number 44 much better than number 45 for a variety of reasons that it will not get into can figure out what I'm talking about all right um it's pretty obvious probably all right so the two things that we need to do now what are the two main steps I don't know why I came over here but since I'm over here first of all I drew this truth table thing a little bit weirdly and so you might recall just to be clear about what's going on this is my little drawing of the canvas right now and the idea of the canvas is that I want to see what the neural network thinks false false is at 00 I want to see what it thinks true false is at this right hands top right hand side the bottom left hand side I wanted to see it 0 1 and then I want to see here 1 1 so false is black 4 0 and true is white for one that's the way I'm gonna map the color so I should see some kind of bands of like I should be getting like something like this so darker here and like this so let's go look does that match yeah that's exactly what I'm seeing here so the reason why I came over here is what I need what I think there's two things that I need to do number one is I need to train the model to produce this output my desired output that I think it should do and then I also need to ask the model to predict so I can draw what it thinks its output is so the two and the and so the two steps here I don't run out of space but in the attention photo chess library I wanted you I need to look at the predict function and the fit function predict for just saying here's the inputs what is your output the fit function for saying here's labeled inputs inputs with no outputs adjust optimize yourself according to that so I'm gonna do things backwards I'm gonna do just the predict step first I just want to see when you starts up with no training what visual output again so coming back to the code let's look here so this this is what I need to replace I need to say now I need to say let whoops y equal model dot predict model dot predict now let's go look at the documentation right I need to send in the inputs so let's go back to the documentation model dot predict I get a better way of browsing this documentation here it is so I need to sorry model dot predict I need to give it the X's what are the X's this are the X's but remember I'm using tensorflow de s now tensorflow s oi oi oi vague volts I have to make them a tensor I can't use regular arrays so I could say let X is equal tensor 1d oh no it's CI sorry I got confused TF tensor one D inputs and then model X's now here's the thing so this is the ID there's many problems what I've done so far ok many problems which I will solve slowly this could be a very long video I apologize in advance you can take a break now pause take a break go do something else go back so what about what's the what's problem number one problem number one is predict happens asynchronously hooboy pause for a second here how did i do this and so i made an example i want i just need to think about this for a second wait my glasses are steaming up why is it getting all warm in here hold on oh and Eric thank you for that pull request I probably shouldn't that and then look at because he probably fixed a bunch of things I just want to see something can we do this as a batch oh no predict happens synchronously it's fit that's asynchronous oh good that's why I'm doing this right predict happens synchrony can happen synchronously let me look at should I should show you what I'm looking at I know why I'm looking on this other computer oops I'm just looking up some stuff for how this stuff works oh wait wait no hold on let me show you what I'm looking at because I don't know why I'm looking at this so I have a repo called 145 it's not called what for I this the time tensorflow jazz examples x or sketch right so I know I have to do as a batch neural network predict Oh or did I do it and so I was putting it in a class which I'm not going to do here tidy return oh I know predict happens synchrony synchronously but then pulling the data off happens asynchronously but I can use data sync even though I probably should be using TF next frame I don't know how to use that yet so I will deal with that later okay sorry okay okay oh here I am back all right sorry we can just back up a bit not you you can splice things however you so feel so inclined whoops so I need to now I need to ask the tensorflow layer sequential model thingy to give me the Y neuro a model dot predict but what does it expect its predict function unlike my predict function cannot get a regular array it expects a tensor so I need to make the X's into TF tensor 1d with those inputs and pass those through predicts now here's the thing there there's a lot of issues with this that I need to resolve and this is gonna run really slow I need to actually do this as a batch process I'm gonna get to all that but just looking at what I've got so far model dot predict there's there's a question of like is this happen synchronously or asynchronously this actually is happening synchronously but the problem is I need to say fill with the result like I need to look get that number out and to get the number out I actually want to call dot data and that happens asynchronously so because I'm working with some teeny bits of data right now I think I'm gonna use data sync and there could be issues with that and as I move more forward we're gonna see when I really need to be more thoughtful about callbacks and promises but I'm gonna use data sync right now so I should be able to predict the output with this input get that data and then let me just say console.log why and I'm gonna make the resolution here of the oh yeah the resolution really big like 50 because I just want to like look at very very little data to start with and let's look I'm not gonna draw anything let's just look and see what's coming out what's coming out here why and then let me just say no loop so let's look in the console and see if we get anything error expected when checking dense dense one input to have two dimensions but it got array with shape too I have the same problem I've had every single time I've done this with 1000 to 400 guess so the good news is I want I don't want to just give this one D tensor so even though my data is just two values 0 1 1 0 1 1 and it's a onedimensional array with two numbers in it I actually want to be able to do something like hey take these 15 data points and give me the results the predictions for all 15 of those and so what I really want to be doing is I always need to send in kind of like one order higher one degree one rank higher so this actually I'm just sending in one data it piece of data endpoint in point input frame stopped work and this now I also have to say tensor 2d now because it's a 2d tensor there we go ah so we could see look at this the results came out for all those little spots you can see in little numbers between 0 and 1 in an array so now I can instead of console logging Y and I just want that it comes back into the Ray but there's only one number I care about I can put this back in here I can take out no loop and I can run it let me see look there is my current visualization of X or I'm not really done I've so much left to do in this video that is recording for the last three or four days alright one thing I want to do is I just want to say stroke 255 I just want to sort of see a little bit more okay that's actually what I'm looking at here I actually want to make the resolution for debugging debugging wise on I also want to make the resolution a little bit bigger so let's see now one thing I'm curious about let's look at the frame rate here oh that's lighting at 30 frames per second so that's fine let me now actually make the resolution much much higher like this oh my goodness oh it's not even getting to the first frame oh well there we go look at the frame rail can't even give me a frame rate it's so stuck you can't even get one frame per second so here's the thing I have done something very very very bad and I needed to stop it no loop stop you don't have to do any more work and let's put the resolution back at 100 and let's think about this what's going on here look at this look at this predict function and look at this data sync function what am i doing I am calling that function multiple times every single for every single spot on that grid when I'm working with something like tensorflow J s whenever I create a tensor or feed data into a model the data has to go from my code onto the GPU and then when it's done that data sync is pulling it off of the GPU so I can use it again in my code that graphics processing unit where all the math is happening behind the scenes I want to do that as few times as possible look how this is I'm creating this twodimensional array with one thing in it you know ten hundred times I could just create one array with a hundred things in it and call predict once that's what I want to do so I what I need is for this nested loop to happen twice once to actually wants to setup the data and another to draw all the results so I'm gonna copy paste this just put it right below so this now what we need to do is create the input data so I'm gonna say let inputs be a blank array then I'm going to say inputs dot push and I'm going to just push in x1 x2 so I'm going to put every single x1 x2 all the way along I don't want to create the tensor or do this here I don't want to do the drawing stuff here I just want to create I just want to have a loop that creates all the data now I can get the X's is all of those inputs into a 2d tensor and the Y's this is now the Y's is and now here's the thing I don't just let's so hold on I got a look at what that's gonna look like let's comment this out for a second let's look at the Y's and see what that looks like oh okay Sketchup 78 error OOP oh no let there just inputs push okay oh I want to say no loop let me leave that no loop in put it back I just won't look at it once so you can see what did I get I got a big array of 16 numbers I got all the results so now what I want to do is back here now I just need to do the drawing and I don't need to the input data I don't need the model all I need to do is draw and I need to say fill wise index what I plus J times the number of columns maybe right because this is a one dimensional array to describe all each spot in that grid I could do something like let me just do this let index equal zero I'm going to say fill based on this particular one and I don't need this even sorry and I just need to say then index plus plus right so what are the steps here create the data get the predictions draw the results okay there we go so now we can see this is working I mean it's not doing anything but now let's check this framerate question we don't need to console.log the Y's I'm going to get rid of the no loop let's let's refresh this let's look at the framerate 30 frames per second let's let's pump it up a little but pump you up a little and where is the resolution there let's make this 20 I don't want to go crazy and look at the framerate there we go 30 frames per second no problem because I'm only one time through draw trying to copy data on to the GPU and get it I'm only calling predict once and we can just to check we can go to 10 and we can look at the framerate yeah you could see it's like kind of running a little bit slow but this is because I'm not being too thoughtful about the asynchronous nature of this stuff I could do other things to optimize it but I'm just going to ignore that and leave it at let me make it 25 hey timeout for a sec this should definitely be multiple parts oh yeah create inputs at the start they are constant oh that's such a good point okay I'm gonna do that right now that's a very good point who said that in the chat probably lots of people have precalculate them set up a bunch of people have yeah okay oh this alright all right everybody's saying that all right the chat is giving me some even further optimization which is why am i bothering to do this in draw this is something that the these inputs is never change I could just do them once at the beginning because they're and and I can I can ask for ask them many times in draw so let's actually fix that so I'm actually gonna I'm gonna take this and say let I'm gonna make this globe these global variables I don't know if you guys can hear the music that's coming from the room next to me but it's there alright then oh but the width and height does not exist until after create canvas so let me do this and let me do this okay so now that's there now I should be able to take this the input data and put this right here in the beginning and then I'm going to make a variable called X's and X is and where did I do that here and then create those X's so I'm now doing this in setup and then in draw the only thing I need to do in draw is run the predict this is going to make things run a lot faster let's make sure it still works here we go okay so you notice we getting like a different color each time i refresh because the neural network model the sequential model is initializing everything randomly but now I get to train it now I think we're ready to train it so here is what I did when I had my previous my own JavaScript neural network library I called neural network trained data inputs data outputs sorry I'm reading the chat you guys couldn't hear the music well alright so if I only I could remember exactly what I wrote when I made that TF layers tutorial but I know that what I need to do here and is I need to do something like this model dot fit some X's and some wise that's the training that's the equivalent and the learning rate is irrelevant and I don't necessarily need to do it this is basically what I want to do every time through draw I want to try to fit the model with some training data so let's first make the training data this is not exactly right I need to figure out and I need to use a weight that need to think asynchronously but this is the idea so if I go back to the top here this is my training data now one thing I definitely need to change is I'm going to keep the X's and Y's separate in training so I'm going to do this is I'm just going to do this kind of manually because I what's the big deal so let me make the training set and then one one those are the the X's now let me look at the Y's and the Y's would be 0 1 1 0 then I need those to be tensors so I need to say Const trait TF exes ah so I've got to think of good naming for this I kind of want them to call actually you know what I'm just gonna call it do I have a global X yeah I have a global X's already hmm hmm hmm tray TF X's equals 10 sir 2 D tensor 2 D OTF tensor 2 D you know what I'm gonna do I don't need these I don't need two separate sets of variables I'm just gonna create it I'm gonna call this ah everything is so much more complicated than I make it so if we're simple then I make it I'm just gonna make these tensors directly by saying TF tensor to D and then I'll put the parentheses around this and there now I made it a tensor then F is a TF tensor to D and now I made this a tensor ok now I've got the training data and I'm gonna get rid of this this is the old way that I had the training data which is totally unnecessary so this the training X's and the training wise are you with me if you're still watching I don't know dude get up and do it some jumping jacks let's see now I need to do model F it now model dot fit happens asynchronously so let's put it in its own async function called train model now if you don't know what it means to write a function that is tagged with the keyword a think this is part of es8 a very newish version of JavaScript and I made a bunch of videos about what that is that you can go back and watch but this is basically a way for me to now say wait model dot fit and then let's look at actually let's look at the fit function model dot evaluate compile predict fit so what I need is is to give it the X's and the Y's there's batch size I'm not going to worry about there's epochs I'm not going to worry about or epochs and so H will give me back the history so let's just see here I'm now gonna say train model dot then H console dot log H dot loss index 0 let's say no loop again so basically what I'm doing here is I want to call this function train model and I'm using this idea of promises it's going to await the model on I need to return do I say a weight return or return a wait no I must say return oh wait return oh wait model dot fit so I'm going to return a promise which will have the result of the fit function and I don't know if this is right I want to just look I want to do that I want to call to train model every time and draw I might need to do this somewhere else just right now and then see what the loss is ok onyx 73 async function async function I've got a save function function it's an async function not an async there we go wise is not defined where a train model oh right this is I forgot to call it train X's and train wise so my training data train X's and train wise isn't that nice other word train just appears over out when you're doing machine learning you drink your glass of milk or whatever it is you're having while you're watching this coding training stuff cannot read property 0 of undefined a train model than H all right let's look just console.log H note that is what I've done okay history loss 0 okay oh no by the way didn't give it any testing data so whatsit computing the loss from history so this is I'm gonna call this result result history dot loss index 0 all right there we go there we go now let's let it do that over and over again in draw alright uh so this is a bit of a fail here no very returning the promise that's a good point all right so I let this run a little bit and unfortunately see it's getting nowhere this loss which I'm not sure exactly how it's country things I don't think about that come back to it is not going down anymore so what could be some problems here remember one is maybe my learning rate is no good not that it's no good maybe it's too low so where did I set up that learning rate again let me get rid of by the way I just want to now delete I want to make sure I'm not using any of my old neural network code so I'm deleting all references to that so this is now purely tensorflow Jess and and let me refresh and run this again and let me look into sketch dot yes and find where did I set the learning rate right here let's set it to 0.5 and see what we get yes getting better I'm gonna let this run for a little bit and I'll be back let me look at so one thing I want to do also is I want to write it looks nice if I write in the numbers actually of what the output is where it is you can see that it's actually getting there just very slowly I just want to see what settings I used in my other example try different optimizers and activation functions yeah I'm going to do that in a second I want to get it to work with this first I just want to see what settings I used oh did I forget to put shuffle in but shuffle it by default I wonder if I ah you know what I forgot to put shuffle in it's actually working alright I'm back and you can see the losses now kind of much lower and you can start to see the visual that I'm expecting which has a true value in this corner a true value in the top corner and sort of darker false values in those corners but it's still kind of performing rather poorly one thing that I forgot to do is when I call the fit function there are a set of options that I can pass in for the number of epochs and all sort of thing but one of the ones that I really want to pass in here is called shuffle shuffle takes the training data and it shuffles the order of it each time right now I'm trading it with the same four data points in the same order every time which could be a bit of a problem and okay so let me now let me hit refresh here and run this again oops I can't see the frame rate because time out a sec 0.5 optimizer Oh optimizer yep this was all the same sigmoid sigmoid did I am I giving it you know what I probably did No I thought maybe I was giving it more epochs or something shuffle Forex or is not a big help apparently not I'm curious I could do just out of curiosity because if I go to that thing I was showing you everybody where this is this should be hosted hosted via github pages for loop and epochs will really help yeah I just wanted to see like I'm pretty sure I know why this isn't loading I mean the other thing is I probably should create a separate I mean there no such thing as a thread but I probably should have right I should just run the training like this async function I should just like I don't need to like I don't be calling this in draw but I just wanted to start doing that I was gonna fix that later what time is it I have a half an hour and get through this whoops why is this not loading oh is it because oh it's probably cuz this is running I don't know why my example I wanted to see the performance of my example hold on I think I need to restart Chrome oops why is this not working here we go yeah I know I should do like I was gonna do this set interval thing but actually wasn't gonna do set interval I was gonna do set timeout and then have it like recursively call itself when it's done I'm just kidding I'm a got lost and this is not important I just thought I could run the thing that I made before just to see how it performed but I don't know what it's what it's stuck is anybody else anybody else try running this example maybe I'll merge Eric's pull request it's like stuck weird okay yeah that's what I that's what I wanted to do next frame so me I am so me let me come back to that I didn't want to do that first but yeah but thank you for that oh my goodness I forgot to tidy everything oh well but yeah the wise I need to dispose and I also need to tidy this oh yeah well let me at least just go back to where I was I'm gonna let this go for a little bit I just want to did anybody see what that was there it is where is it yeah look at that that's a mess okay okay all right so things are still working but it's still running kind of slow we can see I'm getting the lost down hold on okay things are working it's getting the kind of getting close to the right results you can see the losses going down I've realized thanks to the chat of course that I forgot something really important is I want to try to make it learn faster which I will kind of get to and I want to think about the sort of asynchronous nature of using p5 s draw loop and using the model dot fit at the same time but before I do any of that I just realized I haven't thought about memory management at all and there's a big problems if I just take out for a second this and let me let me do this here take out this console lot log and I run this again and I look at say if I say in the hooks yeah I'm like killing my computer it can barely run it again because watch what TF memory num tensors 3455 4042 79 i'm just generating tensors I'm filling up all the memory it's gonna perform so this is going to be a disaster so let me stop it before it gets too bad and let's see where do I need to do some cleanup so these X's these training tensors I never need to clean those up those I can keep forever but I in train model I should probably TF tidy this whole thing let me think about that let's first actually the Y's after I do this I can just dispose those so the Y's I definitely need to dispose let's at least just do that first and let's see what um let's see where that gets oh oh look at that look at this ah because I use data think I could use TF tidy but what I'm gonna do now is wise and then I'm gonna say let Y underscore values equals ma wise dot data sink I can't clean it up once it's also been data synced so now I do to why it's not disposed oops do I have no loop on nope and still going up that's weird oh this huh this has to be yvalues also I have to change that to Y values so that helped now what I want to do is I need to uh let's just do this TF tidy let's just put the TF tidy here so like whatever happens in this function just tidy it all up I could put it properly up here it's that model that fit the need to tidy but I'm just going to do this let's do this five hundred forty nine hundred missiles being created over and over again that I didn't tidy it's to 2003 to a disaster what did I not tidy I guess maybe I need to put tidy in here like or I can just say like this right to learn Oh mmm I'm so confused like this no I'm so confused hold on I'll have to make the arrow function async yeah I dislike I'm like lost here let me back up for a second this is why I wanted to tidy outside of it you get rid of TF tidy for a second what did I have to start can I just put this here what am I missing I can't use promises in a tidy uh right so I was doing this right hold on a sec what what's wrong with what I had here what was wrong with this like why is this this should be fine right which version of well let me also go back to making the resolution much bigger oh uh I had it right before I just didn't save I'm gonna do a backflip ready what no no I'm not dating back foot I had it right the whole time no that's so weird when the resolution is higher I have a memory leak that's weird what am I missing why do I have three in there that was weird I'm like going out of my mind here so what is it well I have some like weird bug here that I'm not thinking of exes let me see if tidy this whole thing hmm ah so why with a different resolution that's so weird I feel like that's is a bug he had to slower the resolution to less early I sort of feel like there's some bug that's not my code I can't figure it out but tidy here kind of fixes it I don't think that this needs to be this is just an array so this doesn't need to be cleaned up I mean the garbage collector should that's not a tensor but did I make some other tensor in here that I like it's modeled I predict maybe it makes some other tensors behind the scenes okay you know what it is I'm just being silly here I think I'm like forgetting that model that predicts like model dot fit does okay that's yeah I'm going back and talk and I'm gonna do my memory when did I make that three I'm gonna do my and go all the way back and memory leak it again and fix it yeah the epochs will definitely help whoo modeling evolution with tensorflow das from suraj rebel new video all right want to get this down a bit and then I'm going to come back all right here we go here we go I can't look at the chat right now jet all right so you can see it's kind of working I back it's sort of taking a while but I want to get this to train a little faster I want to make this I want to get a little further it's up first of all I was reminded by the chat that I've forgotten something really crucial important which is memory management and I really I really should stop this from running right now because there's a huge memory leak happening which I haven't cleaned up any of my tensors at all whoops look at that well not sure out that it was there this is there just do this again sorry everybody one more time shift ya memory leak really need that ukulele Wow all right wow look at this alright so you know it actually it actually is working it got the correct training result it's a little gray scaly in a way that I would like to be able to like emphasize visually what it's do but you could see the loss has gone way down but it took a while to get there but I want to add a few things to this and try to fix it up a little bit before I do anything I was reminded by the chat being over here that I haven't thought about memory management at all so I'm gonna say like no Luke for a second to just sort of turn this off then I'm going to say memory numb tensors oops no no way TF memory dot numb tensor I know I'm trying to use it I'm gonna say numb tensors TF dot memory numb numb there it is there it is I could get it there it is thirty two thousand two hundred five tensors that's crazy so I need to deal with that I'm just making tensors and letting them leak everywhere so I can manually run dispose but I've got kind of an issue whereas predict is gonna like make a lot of tensors behind the scenes as well as a model dot fit so I can use the TF tidy function so I'm gonna say TF dot tidy and then I just need to I'm gonna use the es6 arrow notation which you can watch my videos about what that is but and I've kind of gone through what tidy is tidy says anything inside of this code clean up the memory afterwards basically and then I'm gonna I probably could put this around everything but I just want to and I don't need this stuff anymore I just want to keep these two areas separate because I think I'm gonna at some point I really should change the way doing the fitting of the model excuse me in draw is somewhat problematic so now I'm gonna just tidy all of this I think that's right I tried an extra parenthesis there yes okay so now let's run this again comment out this console.log I don't want to see that right now oh all right now let's look at the number of tensors 15 15 15 so now I've gotten rid of the memory leak let's check out the frame rate 30 frames per second so this is running for up now let's I just want to be able to look at what's happening a little bit better so I'm actually gonna draw the number of the output inside each one of these things so let's do that so where am i drawing the rectangles here I'm going to say let the brightness value equal this and I'm going to fill the rectangle with that brightness and then I'm going to say fill 255 mind it like the inverse color I'm gonna say text number format the yvalues index with just two decimal places and I'm gonna put that at boy this is awkward I the exercise this is gonna be high x resolution plus resolution / I'm gonna say textaligncenter textaligncenter comma Center and then I'm gonna put the I'm just going to draw in the center of the rectangle and this should be J the text so let's see let's do this now so we can see you look there's lots of numbers there I think my number format thing didn't work 1 comma 2 and let's use a lower resolution just so I can see it better there we go now interestingly I can't see the numbers but there they go right you can see this is what it's getting the output for each one of these and I want to look at the law so you can see because it's just going so slow it's getting over time it's getting a little better but I really want to see a train much faster so let me see I have one idea one last thing I can add to this even though I and I have some suggestions for what I might do next but you can see all the numbers are starting to appear lovely I was because I forgot when it's gray when it's at point 5 2 or 55 minus 0.5 it's gonna be the same card I kind of like that effect so so what I want to do is what happens here if I actually give it tell it don't just do it once like do it 10 times do 10 a pox per cycle of fitting let me run this again and let me look at the let's actually have the loss continue to print out and there we go still running pretty fast you can see the losses going down and relatively quickly I am getting myself to the point where I'm starting to see you know this is definitely all the way got getting all the way down to zero there this is getting way up to 1 there it's getting a little bit stuck it's having trouble with this size side I imagine it'll get there eventually we could do some fun stuff for example I'm going to just let it have it's totally unnecessary but I'm gonna give it 4 hidden nodes and I'm also going to put the resin the resolution back to 25 and let's run this again and let's see how this goes so I'll give it a minute I'll come back oh the font is too big but you can see it's learning pretty quickly right now hold on well let me let me let me go back do that again so I'm gonna give it for no real reason all but just for fun four hidden nodes and I'm also gonna let me change the resolution to twenty five let me make the text size something like eight point and let me oops refresh it yeah one alright I let this run for a bit and you can kind of see here now you can see all the X or values here's a nice beautiful little map grayscale map of all X sorts getting all the way up to true and all the way down to zero at the corners this is pretty good so this is running at if I take out this console.log I can now take a look at the frame rate it's running kind of slow so here's the thing I have done something that I don't like which is and which is this train model is being called inside draw and I really shouldn't be doing that all right because I don't want I'm you know I don't want to call train model over I can call train model only thing about what I'm saying here yeah I how did I finish this up I have to go in just like a minute I'm saying of what I want to finish up here so I don't think I have time to do the TF frame thing but should I do would it make sense for me to at least do something like set time out train model let's just say hey do that and then like what if I did this function train set timeout train model I want to do like a then but well or what if I just did this like something like this right this is what I'm thinking and then this wood right does this make sense so I gonna explain this in a second like this will basically be happening elsewhere but I think I need the set timeout to like yeah this is never gonna release anything oh this is like superfluous because it's just returning a promise yeah I'll do that like what if I did this this is weird though kill this completely killed the browser yeah it's not really that much faster but this this is kind of like what I want to do right yeah now I can get a much faster frame rate and I could actually give it more epochs right yeah the training is happening more slowly but the framerate is happening faster perhaps result in a video about workers what tidy does nothing look look at this crazy way that it learned where's a tidy that does nothing me I am semi right is this an improvement over what I had before you know I I need this TF tidy what because this has to get tidied all right this is what's doing right now if I take that out oh you're right oh I don't I don't need a TF tidy for model that fit okay okay is this an improvement like to have pulled this out at least pulled this out of draw and a wit model that fit is tidied internally okay oh and tidy doesn't work with promises got it got it got it got it got it got it all right is this an improvement or should I just stay where I was and let it be like leave it and draw and talk about how that probably should do TF frame instead webworkers yaddayaddayadda answer fast I gotta go a sink while loop is a definitely good suggestion technically this is a much better way to do it all right thank you that's all I needed to know okay okay so I'm gonna go back to all right so the chat has given me some really helpful tips I've made quite a few little like weird little errors and mistakes here and I want to just fix this up a bit I think it's good actually to be easier to look at and watch if I just go back to a lower resolution so let's make this 40 and let me refresh this okay so here we go so this is now working training itself for X or you can see it's kind of moving along here now what what the real thing that's problem problematic here is the draw loop is happening over and over again and then I'm triggering something asynchronous in draw and I could be asking to train the model before it's even done with the previous training cycle so this really should not be happening in draw now tensorflow digest has a function called TF next frame I want you to explore it and make a version of this with Tia next frame as I can exercise after this video is over but I'm gonna do it a different way without that cuz I gotta come back then in a different video but first of all this I also learned this is totally unnecessary the via wait so a couple things number one is because there's just one thing happening in here I could just return the promise this doesn't actually have to be an async function and then I do not need the TF tidy because monal dot fit kind of will clean itself up automatically for you so this this should still work just fine and I should be able to see the number of tensors is still 15 so that was something that I didn't need that I've now fixed now what I really want to do is I want to get this out of draw so let's comment this out here and what I'm actually going to do is I'm going to write a separate function called train and in that function I'm going to say set time okay wait wait I'm gonna call train model wait hold on I'm gonna call trade model yes yes in that function I'm going to do this so I'm a separate function that does this piece of it that console logs the history and I could use and what I want to do is I want to say set timeout call the train function in 100 milliseconds so I want to just let the program start 200 full effects late later call this train function train the model which does the fitting when that's done log the history and now say set timeout train 100 so I'm good this is sort of like workers like like don't you set interval here because I only want to call train again once it's finished with training the model itself so this is kind of like hey train and and by the way I could just increase the number of epochs or maybe do some kind of loop but I think this would be a sort of nice way to demonstrate it and if I just called train directly without a set timeout I'm never going to be giving back control for a second I couldn't end up with sort of like blocking so I might even be able to get this down to like 10 milliseconds just something really really low so let's run this and sort of see same result we can see there we go things are working but at least now I have gotten that out of draw so draw is happening on its own and in fact what I could really do is I could say hey try doing this with like a hundred epochs each time epochs what is it and you can see the lost function is coming out much more slowly but whoops but the frame rate let me just clear this for a second yeah the frame rate is quite fast hold on this is too confusing I probably need to give give it back more time let's do like another little break let me let me I just want to like take out the console.log thing so I can look at the frame rate I should just put the frame rate in the Dom would that be smart but you can see now I'm getting 60 frames 30 frames of getting like a really pretty high frame rate even though the training is happening it's almost it's kind of like there is no threading in JavaScript so these things are just like passing off and really this might be a place where like web workers or something could do the training behind the scenes in some fancy way which maybe I will get to at some point oh my goodness so Alka is suggesting it might be better to use the draw loop and a boolean to know when it's safe to call it again that would also be a good idea so you can see though you can see what kind of like employing my hair out what kind of sort of like hassle situation we've gotten in but really let's just put this back to like two epochs let's put this to like a little 10 milliseconds and we can sort of feel like there we go and I can look at the frame rate it's running nice 30 frames per second and even though and it's at some point it's going to get there what's that loss I forgot to console.log loss come back to me come on oh but let's see I have an idea what if just before I go just before I go what if we try using a different optimizer what if we try using for example a different loss function hold on this video was already 18 hours long what if oh no no no a different optimizer sorry what if I tried using the the atom optimizer so let's just try that just for fun times let's give it a lower learning rate that was pretty exciting let's go let's go let's uh let's make the resolution back to like 20 let me make the font size like nice and tiny for us we give myself some more space here hit refresh and then let's look at this look at it learning there wow look at that that is beautiful look at it learning XOR so nice and fast I'm just gonna hit refresh again so we could say we really should look at what this atom optimizer is and I will link to a paper and some more information about the some of the what the atom optimizer is you can find out actually we should just go yeah I'm sorry like hold on we should really I should really talk at some point about what some of these other optimizer functions are for now what I would suggest that you do is if I again if I go back to the API reference and I go all the way down and I find let me just look this way the OP if I go here and I go training out Adam we can see here this is what you're gonna want to click on this is the paper that describes the Adam algorithm of its a different but but it's optimizing the lost function in a slightly different way than stochastic gradient descent does you know we could also try starting to like we you know we could we could just never stop and I could start like I think using the rail ooh activation function instead whoops is that not what it's called where are the activation functions oh it's it's there's no it's just all lowercase so there's so many things you can play around with with these things how was a failure I think because I have the all right let's I'm not gonna add I'm gonna go back back to where I was explaining the atom optimizer oh it made this such a mess to edit and I really have to go I really should explain what these optimizers are but if I go back and look under here we can see what some of these are Adam the a da coming from the word adaptive and you could always click here and look at this paper which describes this particular method for optimization which is a little bit different than stochastic gradient descent and apparently things work a lot faster with this XOR problem so as I go forward into more of these videos hopefully we can dig into what some of these different optimizers do and kind of understand why I might pick one over the other in certain situations all right but I'm just going to just leave this B I'm going to hit refresh I'm going to watch it learn and train train train train XOR oh I don't know some things you could do investigate TF dot frame give me a little slider try different architectures different optimizers try some different activation functions I don't know if you actually made it all the way to the end of this video I don't know ashtag something I should Eric is telling me to watch one of those videos reviewing the JavaScript event loop which I definitely need to do so I need I'm gonna be back with more someday well good bye good bye good bye thank you momentum is add a knock for adaptive did I just make that up adaptive estimates so maybe the M is for estimates moments loworder moments and is for moments yeah all right everybody well you know today was one of those days I should really be doing my old style coding challenges again readout of the random numbers book okay okay I really I'm running late I gotta go everyone lie down go to sleep we're gonna let's let's artificially make this happen much slower let's train let's go caddy thirty two thousand nine hundred eighty five twenty six thousand eight hundred and fourteen fifty one thousand eight hundred thirty three fifty seven thousand three hundred and sixty three four thousand and sixty seven eighty four thousand six hundred forty eight eightyfive thousand five hundred five forty one thousand four hundred sixtyfive seventy one thousand seven hundred sixty nine ninety nine thousand five hundred fifty fifty five thousand nine hundred four will there be a second live stream today no unfortunately why life's do on the weekend no unfortunately so apologies I wish I could live stream more often all the time make more stuff yeah blah blah the next live stream will likely be next Wednesday and next Thursday and I'm gonna be working on a lot more of this stuff inbetween hopefully we will the things that I really need to look at or I need to look at that event loop article that I just to get a better sense of how to use this stuff together with the requestanimationframe better and TF dot frame alright so any last if I my real time that I had to leave was three o'clock I got two minutes to answer questions let's try like I like doing it with like a high learning rate you can see like it gets too good sort of see it bouncing maybe this learning rate is too high it's kind of get stuck it kind of can't get there and then if we could do we could do weird stuff like like what if I gave the hidden layer 16 units for like no reason like look how it is there's no correct answer there's only training data at the corners that learn to rainbow people for asking me questions and I are you going to do this convolutional 2t soon that's why I I hope to plan to yes all right everybody so this is my list Oh Thank You Joshua Myers that's very kind of you if I had my Philip's light bulb it would have flashed by the way oh let me just mention if any of you are if any of you are sponsors you can now sponsor the channel through the through the YouTube interface itself make sure you go and check the community tab and look for a post that links to a Google Form to enter your email so I can send you an invitation to the slack group I need a better system for doing that I have a thing set up that it actually like I have a I get alerts in a spreadsheet and I can look but I don't get your email address so there's no automatic way to invite you to slack other than by doing it manually yeah so this is where I'm this is where I am going to next I am going to do a I want to do the TF playground idea and the classification idea so if anybody has any ideas for really simple goofy just like basic numeric data sets probably going to do something where I'm going to look for some color data set which takes any RGB set of RGB numbers and like categorizes it it's like a few with a few different like labels that's one so if you have some suggestions for that please let me know at Schiffman on twitter is probably the best way all right everyone I don't know I had more layers sorry everybody I hope today I hope you enjoyed today somehow uh somehow I I just I don't know maybe I shouldn't be covered next week let me through really maybe I'll try to at least do a coding challenge that's not machine learning related I want to get thrown mean learning content but it is kind of overwhelming and taking over everything weather prediction but I need I want to do classification so I don't want to do time series great all these these things will come but I want to do a really basic classification I don't use images I don't want to use text data that's like I don't use time series sequential data I just want like you know like it's like house prediction the iris data say these are the kind of thing house price prediction wellthat's prediction that's not classification even but regression but something like that but I want it to feel goofy creative in the art world space that kind of thing alright tictactoe game okay goodbye everybody I'm gonna hit stop streaming I guess I will play you out with my weird trailer since that's what I do now while the trailer is playing I will attempt to explain why doesn't it dispose of tensors automatically s asks ray Jackson asks Arnab I don't actually know why it doesn't dispose of tensors automatically but it's this is the thing that there is no way to clean up the memory memory on the GPU with a garbage collector in the same way and this is like a lowerlevel question that I would be curious we have to investigate more about how tend to flow J s works you you
