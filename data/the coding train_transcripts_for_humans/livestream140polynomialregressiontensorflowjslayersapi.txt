With timestamps:

00:01 - [Music]
00:30 - [Music]
00:41 - sara is asking any luck with the lights
00:44 - no luck with the light but I will talk
00:46 - about oh and welcome to is it Thursday
01:03 - Thursday afternoon coding train you
01:05 - would think the fact that it's the
01:08 - summer my teaching schedule is freed up
01:11 - that I would be more consistent about
01:14 - the time less late
01:16 - strangely enough my life is just not
01:18 - working out that way so I am apparently
01:20 - less consistent about the time and more
01:22 - late than usual so I have to apologize
01:26 - for that so but the good thing is I'm
01:27 - here I mean I guess that's good you're
01:29 - watching so hopefully that's what you
01:31 - were hoping for I have I have all my
01:35 - supplies
01:36 - train whistle water warm beverage
01:44 - sometimes caffeinated sometimes not
01:46 - caffeinated you can try to guess back to
01:52 - the lapel mic which is here on my coding
01:55 - train branded hoodie and of course not
01:58 - of course but I have today I have a
02:00 - melon medley from an unnamed local shop
02:04 - that sells food items and I paid 359 for
02:10 - this we
02:11 - really bugs me you know I can never get
02:14 - myself to bite you know you go to the
02:15 - grocery store and they have the the bin
02:18 - of sliced melon and it's like $12 for
02:22 - like this large thing of sliced melon
02:24 - which is approximately a single large
02:25 - melon entirely sliced these single melon
02:29 - itself is usually like four or five
02:31 - bucks I can't pay six or seven bucks to
02:34 - have my melon sliced I have to slice it
02:36 - myself
02:38 - soft focus it might be that the focus is
02:40 - messed up there was some weird stuff
02:41 - going on in this room so if the focus is
02:44 - bad I I will fix that let me know if the
02:50 - focus is weird I will I will take a
02:52 - moment look at this this is regular
02:55 - non-space melon it is of the water
02:58 - variety which goes well with Cody
03:01 - Trent's main sponsor h2o a little bit
03:15 - upset today everything's off focus no
03:20 - but nobody saying anybody's voice now I
03:22 - also have the space Bella the mysterious
03:26 - magical space melon alright alright so
03:35 - this is gonna be tricky but let's see if
03:38 - we can figure this out the good news is
03:40 - I didn't actually put the cloaking
03:45 - device on this yet so this is
03:46 - approximately where I stand you know
03:51 - about here so if I focus on this right
03:55 - here I should be able to fix that focus
03:58 - so let's see if I can do that
04:07 - it's that better thank you for promise I
04:10 - can't see the output very easily oh that
04:12 - looks better let's see how that is have
04:21 - I improved the focus of course when I
04:23 - stand back here I'm a little bit out of
04:24 - focus but such is life
04:28 - I'm hoping this has improved things I'm
04:32 - gonna pop out this oh I don't even have
04:34 - the chat here oh yeah okay hold on hold
04:39 - on everybody
04:42 - pop out I'm sorry I really should get
04:45 - ready before I start but today I just
04:48 - didn't happen all right now oops let me
04:51 - go back to here let me get the chat on
04:55 - my monitor I have to do that um if I go
05:03 - to look at the live stream I'm live
05:07 - right now I go to that page I see myself
05:10 - in the past and then pop out chat so now
05:18 - I can see the chat all right burgerbob
05:24 - in the chat I just pulled the chest
05:26 - saying 20 seconds slow mode is too much
05:28 - please make it 10 seconds I have no
05:30 - issue with doing that whatsoever let me
05:33 - see if I can adjust that easily you know
05:38 - unless the chat is presenting a problem
05:40 - I'm happy to decrease the slow mode I'm
05:44 - just curious if anybody has a youtube
05:45 - sponsor does the slow mode apply to you
05:47 - or no maybe this is under advanced
05:50 - settings enable slow mode limit chat
05:53 - post to every 10 seconds per person
05:55 - let's try that all right so I switched
06:00 - it to 10 seconds let's see how that goes
06:01 - all right let me move this water over
06:04 - here now here's the thing uh one of the
06:09 - reasons we'll have late today it was
06:11 - actually early not really I was here at
06:13 - 1 o'clock to give myself a whole hour
06:15 - you would have thought that maybe I
06:17 - would use that time
06:18 - try to see if I could fix the whole
06:20 - thing with the camera shutting off but
06:22 - of course I did not now be much too
06:26 - practical and useful and logical for me
06:29 - so what I have done instead yes it
06:34 - applies to sponsors okay no who knows I
06:38 - don't know what this means so so I have
06:46 - this light oh we can't turn this off yet
06:50 - this is a Phillips Phillips hue light
06:57 - 600 lumens I have it clipped right here
07:03 - to my table
07:04 - welcome to sponsor Meyer Shaw see this
07:07 - is the thing I was so excited I almost
07:09 - want to tell you Meyer to unspun sir
07:11 - maybe sponsor again later because I also
07:15 - have over here this thing which is a
07:20 - Philips a hue bridge this is the thing
07:23 - that connects to the internet and
07:25 - controls the light it's easy for a
07:27 - second there it's like a six were warm
07:29 - in here and anyway okay
07:32 - I'm fine uh not sleeping well this week
07:34 - all right so what was I saying about
07:41 - this this I would connect to the
07:44 - internet it would control the light then
07:47 - I could use an app on the phone or I
07:48 - could write like a node program all
07:50 - sorts of ways I could control the light
07:52 - in this room and in fact I could use
07:53 - IFTTT which is if then this that what
07:59 - that stands for something something like
08:01 - that I could use that to every time
08:06 - somebody sponsors to blink the lights
08:09 - and possibly I could do something else
08:12 - like if you're in the slack channel and
08:13 - you need to alert me to a horrible bug
08:15 - in the code or the camera shutting off I
08:17 - could set up something like sort of
08:18 - slack bot messaging system to blink the
08:20 - lights but if this then that okay that
08:26 - makes sense if this then that
08:28 - thank you Chris Persian the problem is
08:37 - the problem is I'm starting reading this
08:41 - slack channel at the same time Simon I
08:42 - have seen your barns leaves fern message
08:44 - thank you
08:45 - I'm kind of right now I'm staying away
08:47 - from the matrix graphic stuff with Tesla
08:51 - - yes although I would like to return to
08:52 - that but the this is blocked on the NYU
08:57 - Network so I haven't figured out I was
08:59 - trying different ways of like setting up
09:00 - my own network or like getting a network
09:02 - from like on the laptop to give a
09:04 - network but none of that was working so
09:07 - I have to give up now here's the thing
09:10 - I'm thinking about when at least first
09:15 - as an experiment you know maybe I'm here
09:17 - till 4 o'clock which is like an hour and
09:19 - 45 minutes from now in this room
09:20 - streaming I can get this to work
09:22 - downstairs on the 4th floor of this
09:24 - building in my office down there because
09:26 - we have a special sandbox network that I
09:29 - can do connect to things in different
09:32 - ways that the larger NYU network does
09:35 - not work for I also was thinking the
09:36 - reason getting some of these from i/o no
09:39 - if I want people blinking my lights from
09:41 - the internet into my house but so if
09:45 - you're if after today if it's like 4:00
09:47 - and I'm kind of things have gone alright
09:49 - I might consider going downstairs and
09:51 - just live streaming from my laptop
09:53 - webcam just to test out the light thing
09:57 - so you might get that to work okay song
10:00 - that I sorted by sent me a super chat do
10:03 - not see that I see a sponsor I don't see
10:06 - that but I believe I believe that it did
10:08 - happen yeah so people I don't want to
10:12 - belabor the whole sponsorship thing but
10:13 - YouTube has a sponsorship feature you
10:15 - click sponsor you get an invitation to
10:18 - the slack channel and I'll mail you some
10:19 - stickers I also have a patreon which is
10:21 - basically the same exact thing you just
10:23 - give your money there instead of there
10:25 - but you don't get all of the the patreon
10:28 - thing has some benefits that YouTube
10:30 - doesn't have but YouTube has all the
10:31 - emoji integration with the chat stuff
10:33 - all right
10:34 - the Philips you bridge that is correct
10:39 - I've never used one of these before
10:41 - so what am I gonna do
10:42 - today what I really like to do is always
10:48 - like to know where I'm not gonna do this
10:49 - but this is what I would like to do I'd
10:51 - like to lie down on the floor I do
10:53 - something like meditation breathing and
10:55 - stretching exercise I feel like I have
10:59 - not found the right I have not found the
11:06 - groove
11:07 - Stella does not have her groove back yet
11:10 - but maybe it will happen today if I
11:13 - start doing some coding all right let's
11:17 - see if we can get the whiteboard camera
11:18 - going yeah yeah buyer it takes I have to
11:33 - send the slack invite which is not so
11:36 - easy for me to do actually I could
11:39 - actually do this during the livestream
11:40 - wakes people time too much but I the the
11:49 - the slack of Nations have to be done
11:51 - manually there's no automatic process
11:52 - for that so if you go to the community
11:55 - tab there is a post there that sponsors
11:58 - only and there's a Google Form putting
12:00 - your email address there and then I'll
12:01 - send you a slack invite if if I if
12:08 - anybody watching has the sort of power
12:10 - authority to figure that out in a
12:11 - different way and invite new sponsors of
12:13 - slack we could we could make that happen
12:15 - but I probably can't do that during the
12:16 - live stream all right there we go all
12:18 - right so I want to continue with
12:21 - tensorflow Jas today and I want to focus
12:25 - on something called the layers API so
12:29 - let's see where let's see what I have so
12:30 - far
12:42 - I'm going to go to this YouTube channel
12:44 - I heard of called the Coty train and I
12:48 - am going to go to neural networks and
12:52 - machine learning session six tensorflow
12:57 - je s and so this is what I have so far
13:03 - you know what I think this might be a
13:05 - t-shirt day I think that this hoodie is
13:10 - just it's extra warm in here if it's the
13:13 - New York City weather for what and I'm
13:15 - gonna switch to t-shirt that's gonna
13:18 - it's gonna make everything all right
13:20 - again Oh much better you know what it is
13:25 - I was rushing also and is I'm on the
13:29 - ninth floor now and I was down in the
13:31 - fourth floor and I'd go back and forth
13:32 - just took the stairs you know I made a
13:35 - half marathon a few months ago I I'm
13:37 - doing ok but somehow taking the stairs
13:39 - up and down a couple times I think maybe
13:41 - doing that right before a live stream
13:42 - isn't the best idea
13:46 - alright oh you can see this is the light
13:48 - right here see unfortunately I'm gonna
13:51 - just I'm gonna since it's not operable
13:54 - I'm gonna just put it down to the side
13:55 - ah polynomial regression we should
14:01 - change we should do that right I was
14:07 - gonna go right into the layers API and I
14:11 - was gonna go right into the layers API
14:14 - and then maybe do like XOR as a kind of
14:19 - hell open I don't want to use the word
14:20 - hello world but as a kind of basic
14:22 - example to see how the parts of tension
14:24 - flow digest the layers API works but I
14:26 - kind of like this idea of trying
14:28 - polynomial regression let's let's look
14:35 - where I last left off with the linear
14:37 - regression example and see how much that
14:39 - would take
14:47 - okay
14:53 - TF when your regression one of these
14:59 - days I want to do a live stream I say
15:01 - this every time where I want to like
15:02 - update my workflow
15:13 - okay so if we look at the code for this
15:15 - linear regression example
15:31 - so there's some interesting discussion
15:33 - going on in the slack Channel about
15:35 - there is a IFTTT app applet for
15:39 - triggering a light from a YouTube
15:42 - sponsorship so that exists everything
15:44 - else I'd have to do custom and I'd love
15:46 - to do some tutorials about how to do
15:47 - that but that's why I thought I could
15:49 - just get that out of the box but one
15:51 - limit one thing that you have to have
15:53 - working in order for that to work out of
15:55 - the box it's a working light all right
16:06 - informal poll let's see so what would I
16:09 - have to change here I would need some
16:13 - more per amp TF variables I would the
16:19 - loss function could be the same the
16:23 - predict function would be different oh
16:25 - and graphing it would be different so
16:28 - right here I would write the code for a
16:32 - polynomial equation so that would be
16:36 - like something like let's do it let's do
16:39 - that oh let's do that why not why not
16:49 - that's gonna be fun
16:52 - loads of good times for all let's go
16:56 - where's that what's that playground
17:01 - what's that tensorflow playground cuz I
17:05 - wonder if that has a yeah I mean
17:10 - basically what I'm doing is something
17:11 - like this what I want it's like creating
17:13 - an example like this okay
17:28 - so we are now let me just change a few
17:30 - things I don't know if this makes sense
17:32 - to be like a coding challenge part two
17:34 - of that but just go any important
17:51 - comments or feedback feedback before I
17:54 - start
18:14 - okay all right
18:22 - hmm why is this better
18:31 - all right Oh nope all right that's good
18:41 - this
18:57 - just one sec I think I need to read some
19:16 - random numbers it's been a while that's
19:18 - gonna help Randy numbers and melody
19:20 - those are the things you know I'm gonna
19:23 - I'm gonna look at my youtube analytics
19:24 - after this because this will be the
19:26 - point where the graph of the viewership
19:29 - [Music]
19:45 - 31,000 okay break album where's the
19:50 - sheet on this one eight hundred thirty
19:51 - seven seventeen thousand eight hundred
19:57 - thirteen whoo music ended anything I'm a
20:07 - person on the internet reading from a
20:09 - book of random numbers eating melon
20:11 - which nobody really likes to be
20:13 - perfectly honest even though I do and
20:15 - people the chakra talk about weather
20:18 - very tippy is like the best editor ever
20:21 - or something like that
20:22 - come on people get with the program free
20:25 - yourself from these worries about your
20:27 - text editor here's the thing this
20:31 - I like the honeydew I like the
20:33 - watermelon this is cantaloupe really
20:38 - it's part of the medley I'll eat it but
20:41 - 13,000
20:42 - I heard done mostly 1503 55000 I think I
20:55 - could actually read the whole book if I
20:56 - just had a melon although it's probably
20:59 - a little bit down which melon one should
21:00 - eat before you know take off your
21:03 - problems sleep four thousand four
21:12 - seventy I need to save some of this for
21:14 - later if I'm really gonna get to solving
21:20 - extrovert with Vincent Lopez eighty two
21:23 - thousand five hundred fifty six twenty
21:25 - two thousand six hundred ninety seven
21:27 - fifty four thousand nine hundred eighty
21:30 - five sixty four thousand eight hundred
21:32 - fifty two you know when I was in college
21:37 - right click this like performance art /
21:41 - dance class and I was taught by the
21:47 - founders of the dance troupe
21:48 - Pilobolus and the things I've ever heard
21:49 - of pilobolus and I had Joe performance
21:53 - for it and my performance involve a
21:58 - simple Huck bag full of rice rubber
22:02 - banded to my face and there was like a
22:04 - song I have rice on my face that I made
22:07 - up that went along with that so in a way
22:10 - it all led till now
22:11 - Oh welcome I can't believe you bought
22:14 - for sponsoring this nonsense you are all
22:17 - lunatics that's all I have to say to you
22:19 - you I know I am a lunatic but I thought
22:21 - you were very sensible all of you
22:23 - watching I was a little malfunction
22:29 - there's a little hole here that where
22:31 - the liquid comes out and if you don't
22:33 - have that aligned with your mouth
22:34 - perfectly kind of say but I gotta get a
22:37 - beer trimming you know so many things
22:38 - are wrong
22:41 - no the problem is the dance skills
22:44 - through here's the good news I'm having
22:45 - these back problems so I'm gonna just
22:47 - got an appointment for some physical
22:48 - therapy to strengthen core muscles stuff
22:51 - but the good news is standing is kind of
22:54 - the most comfortable
22:54 - for me so that's what I'm doing now all
22:58 - right
23:18 - cracking myself up hello I am in this
23:27 - video going to do something not
23:29 - advisable because really I should be
23:31 - moving on making some interesting weird
23:34 - creative projects that make use of
23:36 - machine learning in ways I could never
23:40 - possibly imagine but I'm not ready for
23:42 - that yet I'm still in the world of
23:43 - implementing kind of technical
23:46 - demonstrations and examples to
23:47 - understand how things work I kind of
23:49 - maybe I'm stuck in that place for a
23:51 - little too long and you should break
23:52 - free of that place and leave here and go
23:54 - somewhere else but if you want to stay
23:56 - that's what we're gonna do so what I'm
23:57 - gonna do in this video you might you
23:59 - might recall that I recently did a
24:01 - coding challenge where I implemented
24:03 - linear regression the idea of linear
24:06 - regression is to have a data set in this
24:08 - case my data set is a point a bunch of
24:10 - points that I'm clicking I'm just sort
24:12 - of adding arbitrary data to a
24:13 - two-dimensional plane by clicking the
24:15 - mouse but this could actually represent
24:17 - the x-axis could represent you know
24:19 - think about baseball pitching you know
24:21 - that the speed of a pitch and this could
24:24 - represent the amount of time it takes to
24:27 - get to home plate although that would be
24:28 - inverse anyway you get the point whose
24:30 - actions could represent data so the idea
24:33 - with linear regression is oh can we fit
24:35 - a line that can we can we make a line
24:37 - that fits that data and the formula for
24:40 - line is y equals MX plus B so the
24:43 - variables are of our system are M and B
24:45 - and we need to adjust those variables
24:47 - and we're going to use green a loss
24:49 - function that we try to minimize to
24:50 - follow blah blah blah that's all in the
24:52 - previous video so I thought well we're
24:55 - here why not just see if we can make a
24:58 - curve to fit that data this otherwise
25:01 - known as polynomial regression so what
25:03 - do I mean by polynomial agression
25:04 - regression it would be good if I had a
25:06 - marker this is definitely not the right
25:13 - marker
25:14 - I shouldn't mention that I'm really
25:20 - partial to berries like blueberries
25:22 - blueberries I'd love to happen they're
25:25 - like much more acidic so the belen is
25:27 - more soothing hey where where oh where
25:33 - is my marker gone now ah you know the
25:39 - other thing I purchased a ukulele and in
25:42 - the last week I've been teaching myself
25:43 - to play the ukulele but don't have it
25:44 - with me so stay tuned for by the way uh
25:49 - great YouTube videos for learning the
25:51 - ukulele um there's a woman I forgot her
25:54 - name already I think she's from Hawaii
25:55 - and she does videos on how to play the
25:57 - ukulele they were great Christina maybe
25:59 - I have to look that up um let's just see
26:01 - if this pen works oh this works fine I
26:05 - don't know what's wrong with me oh I
26:08 - didn't forget the cloth but look here's
26:12 - an old paper towel so at least I'm
26:13 - reusing paper towel sorry okay I got the
26:24 - marker so the formula for a polynomial
26:28 - equation might look something like this
26:29 - y equals a x squared plus BX plus C so
26:37 - in this case this is a polynomial
26:40 - equation of order do you say that too
26:43 - but I could also have a X cubed plus BX
26:47 - squared plus CX Plus D but let's just go
26:51 - with this right now so what does it mean
26:53 - actually we should do one with anyway
26:55 - let's see what we're gonna we're gonna
26:56 - see how this goes I'm gonna get
26:59 - somewhere so the point is now really
27:00 - I've already done this already the only
27:02 - I could keep the same loss function I
27:04 - can keep the same data set the same
27:06 - optimizer the predict function the only
27:08 - thing that's different is I have three
27:10 - variables now instead of two and when I
27:13 - write the predict function I'm taking
27:16 - the X values instead of plugging them
27:18 - into this form of a line I'm plugging
27:20 - them in form of this polynomial equation
27:23 - let's go let's go get started on that oh
27:26 - you know what I'm gonna have to do I'm
27:27 - the
27:27 - way I draw lines so simple because I
27:29 - only need two points to draw line but to
27:31 - draw like a curve that's going to
27:33 - require quite a bit more all right let's
27:34 - see what we can do do you know how
27:45 - expensive things are in New York City
27:47 - first of all yelling over crew thank you
27:48 - very much but Barry's one of the reasons
27:54 - why I got the melon this melon at the
27:56 - unnamed local food shop nearby
28:00 - this is $3.99 the berries $4.99 five
28:03 - dollars per berries that fit in here I
28:05 - don't even think they're maybe they're
28:06 - organic I don't even think they're
28:08 - necessary organic Trader Joe's that's
28:10 - the best place to get them at least a
28:12 - soggy vine in New York City to get the
28:14 - lowest price organic berries what did I
28:21 - um let me just check okay it's not a new
28:25 - new channel I'm sorry the new fruit I
28:28 - should totally do a new reviewing fruit
28:31 - like unboxing fruit let me just practice
28:36 - that for a second
28:50 - then what do I do I talk about it if
28:52 - juicy kind of sweet it's a little weird
28:54 - tasting prefer get five bits sitting it
28:57 - was like cut and refrigerated if I like
28:59 - a day old
28:59 - pretty good good all right linear
29:09 - quadratic cubic Simon thank you you're
29:15 - always so helpful no people are just
29:18 - like giving me money for berries I'm
29:20 - gonna go get some berries this evening
29:21 - thank you everybody
29:23 - whoops I had this go off oh I lost the
29:36 - whiteboard Cameron oh there it is
29:41 - so I want to so let me let me just do
29:45 - some quick hygiene here polynomial
29:50 - function quadratic cubic ya know let's
29:58 - go - here we go
30:09 - degree 3 degree 3 that's what I was
30:13 - looking for okay this is for the fruit
30:17 - Channel get some organics the coding
30:21 - fruit this is definitely like the coding
30:23 - this might as well the fruity coder I
30:25 - don't know that works for this channel
30:28 - as well the ratio of time I'm talking
30:37 - about fruit to time actually talking
30:38 - about code right now is about 1 to 1
30:45 - looking up on Wikipedia and thanks to
30:48 - Simon who's watching this live in the
30:50 - chat
30:50 - I will now reminded myself that the the
30:54 - the number here is off is referred to
30:57 - degree so this is a polynomial equation
30:59 - of degree 2 also known as quadratic 3
31:04 - would be cubic so on and so forth and
31:06 - linear is really I mean this is it's not
31:08 - a polynomial equation but it's a linear
31:10 - equation but it's of degree 1 and if you
31:12 - go and take a look at Wikipedia you'll
31:15 - see like okay well this is what its
31:17 - gonna look like if it's a degree 3 then
31:19 - the degree kind of matched to how many
31:21 - hills and valleys you have we'll see
31:23 - that we're gonna experiment with this
31:24 - we're gonna have some we're gonna do
31:25 - some fun stuff if you think huh you
31:28 - don't feel repression is fun stuff okay
31:31 - so now hmm look back to the code so
31:34 - looking at what I had before I'm gonna
31:37 - just change this to now a B and C and
31:42 - I'm gonna have three variables a B and C
31:47 - instead of M X and B then ah so now this
31:53 - the predict function so what do I need
31:57 - to do I need to stake a multiplied by x
32:03 - squared plus B multiplied by X plus C so
32:08 - how would I write that with tensorflow a
32:09 - is I'm writing y equals ax squared plus
32:17 - BX plus C so this would be constant Y's
32:22 - equal
32:23 - X's square multiplied by a adding X's
32:33 - multiplied by B add C what's the chance
32:41 - this is right x squared times a plus B X
32:48 - plus C I mean it looks kind of right to
32:50 - me
32:51 - I'm gonna wait for the chat to tell me
32:53 - why they think I made an error hills and
33:02 - valleys it is how many times it crosses
33:04 - the x-axis it's a much better way of
33:06 - putting it Thanks
33:09 - can I go back and restate this I need to
33:13 - declare C as a variable I'm pretty sure
33:15 - I did
33:18 - they're all declared as variables
33:27 - yeah all right I got a all right people
33:31 - in the chat are I'm so informal about my
33:34 - math my informal I mean it correct let's
33:37 - come back let's let's redo that
33:39 - explanation because I don't want it
33:42 - hills and valleys you're absolutely
33:43 - right it's the number of times it
33:45 - crosses the x-axis but you know it's got
33:48 - to go up and down to do that it's got a
33:51 - cross but that's a I let me go back to
33:55 - your the live viewers you're gonna have
33:57 - to bear with me today I'm gonna go back
33:59 - to explaining that again I cannot live I
34:12 - won't be able to live with myself if the
34:15 - live chat I mean the comments when the
34:19 - video gets published okay so so alright
34:26 - so let me so I think this is gonna be
34:31 - right Matz yeah oh but let me undo all
34:35 - my code stuff
34:48 - whoa-oh the atom editor really is good
34:52 - at remembering undo past when I closed
34:58 - okay all right good I think we're good
35:04 - let me just check here
35:13 - okay
35:21 - okay
35:26 - if we come here and look at the
35:27 - Wikipedia page you can see here this is
35:29 - a graph of a polynomial function of
35:31 - degree 3 meaning a x cubed cubic and
35:36 - notice if we look at this how many times
35:40 - does the graph across the x axis three
35:43 - times of degree one how many times do we
35:46 - cross that x-axis a line
35:48 - just once so squared twice all right so
35:51 - now I think we're ready to go and try to
35:54 - make some edits to our code so the main
35:57 - thing we need to do right is change M
36:00 - and B to a B and C M and B to a B and C
36:04 - so I am going to change this to a B and
36:08 - C then here I'm gonna make this a B and
36:14 - C just about everything else go remain
36:18 - the same of course there's a really
36:20 - really important there's two other
36:22 - things that I have to change one is this
36:24 - this is me writing the tensor dot
36:28 - tensorflow j/s code for y equals MX plus
36:32 - B now what I need to do is change that
36:35 - to y equals ax where's that little hat
36:39 - squared plus BX plus C so this should be
36:43 - let's see if I can figure this out
36:45 - constant y z-- equals X's square so
36:50 - that's the X is squared multiplied by a
36:56 - adding and then what I want to add to it
36:59 - is B times X so I have to add X is
37:04 - multiplied by B and then I also want to
37:08 - add C I think this is right a x squared
37:13 - plus BX plus C X's squared multiplied by
37:17 - a plus B both two x's multiplied by B
37:20 - plus C I don't know oh let's see maybe
37:24 - this is right maybe this is wrong but
37:26 - now I'm gonna get rid of this and
37:28 - interestingly enough I do in a weird
37:30 - sort of way I don't actually need to
37:32 - change anything else well I do so here's
37:34 - the main thing I mean I could run this
37:36 - right now just to sort of see if I have
37:37 - any errors
37:39 - now weirdly it's first of all I think
37:43 - it's actually just running the old code
37:46 - ooh what's this print line that I don't
37:48 - need I don't need that oh yeah Oh
37:51 - so let's oh I'm having some memory
37:54 - issues now but okay we're gonna figure
37:56 - that stuff out later save it's do we say
38:04 - we're now interestingly enough it's
38:06 - working I'm kind of surprised by this oh
38:10 - it's working of course it's working
38:13 - it's just but I'm drawing a line the
38:16 - carrot carrot stop a hat okay
38:38 - k week mon am i am i messing up did i
38:41 - miss something important
38:55 - so it's doing something but who knows
38:58 - what it's doing the problem is here I
39:00 - have this code here to draw a line
39:03 - between x1 x2 y1 between x1 y1 x2 y2 but
39:09 - now instead of just using two points to
39:11 - draw a line if I'm going to draw
39:15 - something that is a polynomial equation
39:18 - a curve I need to sample a lot of X
39:22 - points and get a bunch of Y points and
39:24 - connect those with begin shape and shape
39:26 - and vertex so let's see if I can put
39:29 - that in here so what I want to do here
39:32 - let's think about this line X
39:35 - interesting so I'm going to say call
39:38 - this curve X I'm gonna make it an empty
39:42 - array
39:42 - I'm gonna save for let I equals 0 I is
39:46 - less than let's do let's say x equals 0
39:50 - X is less than 1 X plus equal 0 point 1
39:59 - so we're just gonna I'm just going to
40:00 - example 10 points just for right now
40:04 - then what I'm going to do is I'm going
40:06 - to say curve curve X dot push X I'm sure
40:13 - there's a fill map quick way that I
40:16 - could do this but let's just list I'm
40:17 - putting a bunch of points 0.1 0.2 0.3
40:21 - 0.4 0.5 now I need to get all the Y's
40:25 - I'm still gonna use curve Y I'm still
40:29 - gonna use Data Sync and then now I'm
40:35 - going to say for let I equals 0 I is
40:42 - less than curve X dot length I plus plus
40:46 - and I need to say before I start this
40:50 - I'm gonna say begin shape then I'm gonna
40:53 - say n shape and then I need to get just
40:56 - it an X 1 and a y1 x1 is curve X a
41:05 - mapping curve X
41:08 - index I and y1 is mapping curve Y index
41:12 - I and then let's put the stroke weight
41:17 - here and then I'm going to say vertex x1
41:21 - y1 so before I was just calculating two
41:25 - points and x1 y1 x2 y2 now I have a
41:28 - whole bunch of X's and a whole bunch of
41:31 - wise and I need to set them all as
41:33 - vertices and then connect them with a
41:35 - line let's see what happens here whoa
41:41 - interesting so first of all something
41:43 - weird is happening it's getting a fill
41:45 - which I don't want so I want to say a no
41:49 - fill so this is kind of doing what I'm
41:57 - expecting right doing what I expected
42:03 - that one in the middle is really
42:05 - throwing it off let's see if I can
42:07 - actually try to draw a curve
42:16 - hmm
42:23 - oh so now I'm being told in the chat
42:27 - degree equals the number of crossing
42:29 - points explanation is a bad explanation
42:37 - uh-oh somebody's knocking at the door I
42:39 - wonder if I'm being too loud oh I did I
42:57 - was trying to see if I could find okay
43:03 - thank you yes yes that was me look why
43:12 - went answer the door
43:13 - it took a while it learned it first of
43:18 - all so if you're wondering what that was
43:20 - remember how I was talking about my be
43:27 - reported to NYU Internet services where
43:30 - is that thing where where where where
43:39 - where is my Philip's bridge where did I
43:44 - put it it's my prop that I need right
43:45 - now whoa boy that was really loud for
43:49 - you wasn't it where did I put that the
43:53 - lamp is there yeah if everybody was
43:58 - wondering if this really is live or not
44:00 - I guess now you know the answer of that
44:01 - question I can't find where that thing
44:04 - is anyway I was trying to find on the
44:06 - network to see if that big was on the
44:08 - network so I just did like a port scan
44:10 - of like the whole network which
44:12 - apparently you're not supposed to do so
44:20 - unfortunately also I did it from the
44:23 - computer that's live streaming live
44:27 - stream slowly goes down it's because I
44:29 - got shut down
44:31 - I got shut down but hopefully
44:36 - I should have muted my microphone poor
44:41 - okay all right
44:56 - the crossing point exhalation is wrong
44:58 - because a polynomial function can have
45:00 - no solutions so it's the number of real
45:03 - solutions
45:04 - how many complex solutions equals the
45:07 - degree I should never try to do any math
45:16 - let's read what it says here
45:34 - constant linear quadratic cubic that's
45:41 - quartic zero polynomial okay the
45:48 - commutative law a real polynomial is
45:50 - upon them with real coefficients and a
45:53 - complex polynomials apothem with complex
45:55 - coefficients the argument of the
45:57 - polynomial is not restricted it's plain
46:00 - bubble though degree is the maximum
46:08 - number of real roots okay okay max all
46:18 - right max you're gonna have to work your
46:20 - magic here
46:37 - so that wasn't exactly right see how I
46:40 - could miss it I guess I could do my
46:44 - green-screen thing concavity yeah that's
46:51 - what I'm that's what I was kind of
46:53 - thinking of all right so I know I did
46:57 - just say that the degree maps the number
47:00 - of times it crosses the x-axis you can
47:02 - think of that sort of like the number of
47:04 - actual real solutions there are to the
47:08 - equation and it's really just a Mac
47:11 - there could be no solutions to a
47:12 - particular polynomial equation with
47:15 - certain certain values for a B and C so
47:18 - uh-huh so it's really I don't I'm
47:21 - getting in the weeds here because this
47:23 - is not my forte and I'm always messing
47:24 - this up but you know the technically the
47:28 - degree is master the maximum number of
47:31 - real solutions to the particular
47:33 - polynomial equation now I'm gonna go on
47:47 - yes-no together and not and it cannot be
47:52 - flamed from the chat we'll just go all
47:58 - the way back again look this is one of
48:04 - those live streams where I get nothing
48:05 - done
48:24 - all right let's move on
48:43 - Wow people are really far behind in time
48:45 - so that's why no one's answering my
48:47 - questions all right right always has
48:53 - that many complex solutions oh that's
48:55 - interesting to note it's not relevant
49:00 - for this video yes
49:08 - all right I'm gonna fundamental theorem
49:15 - of algebra certainly is all right so I'm
49:18 - gonna keep going okay so now we've seen
49:21 - here that this actually does both let me
49:25 - run this sorry I had a little
49:27 - interruption there so there was a
49:29 - strange edit but I'm gonna now click
49:31 - with a bunch of points and let's see
49:35 - what happens over time so give it a
49:37 - little time here I'm gonna let the time
49:40 - speed up it's gonna be one of those fast
49:47 - forward things my tía
50:03 - Oh keep going keep going I'm so
50:06 - impatient right alright so uh you can
50:17 - see that it's starting to try to fit the
50:19 - curve to that Dana now there's a couple
50:21 - things here one is I could really play
50:22 - with the learning rate it's real
50:25 - incrementally very very slowly changing
50:28 - look at that it's moving but it's moving
50:29 - very very very slowly so the learning
50:31 - rate is something I could play with I
50:32 - also you can see that the way I'm
50:34 - drawing this is particularly jagged so
50:36 - what I'm gonna do also here is just go
50:39 - and like really increase the resolution
50:41 - of the number of points that I'm drawing
50:43 - and let's go look for the learning rate
50:45 - I have it point five Wow
50:48 - so I'm gonna leave it at point five and
50:50 - I'm just gonna hit refresh and you know
50:54 - I could also make it something easier
50:55 - for it to fit to which is just like this
50:58 - and so over time you can see that it is
51:02 - trying to fit this particular curve to
51:06 - this line now let's make sure we've done
51:08 - our memory management job I'm gonna go
51:10 - back and console.log the number of
51:13 - tensors alright so something went wrong
51:17 - here I'm creating more tensors than I'm
51:20 - disposing so we've got to go and look
51:23 - and see what I've done wrong there
51:28 - so what tents are what new tensors did I
51:31 - add the Y's this gets tidied the Y's are
51:39 - disposed so let's let me do my debugging
51:41 - where let me comment this out let me see
51:44 - if it's in the drawing nope is it in the
51:52 - oh you know what I started with an old
51:59 - version of my code so this just never
52:05 - got tidied so that
52:12 - really needed to be tidied I'm looking
52:22 - for another Oh missing a curly bracket
52:36 - nope 76
52:47 - and really let's do it this way
52:50 - ah hold on let me go back
53:02 - oh I'm using an old version of the code
53:04 - for some reason that what the coat that
53:06 - I started with doesn't actually have
53:08 - this section tidied so I'm gonna say TF
53:12 - tidy and then I'm going to make this
53:18 - into a function so I need to put a curly
53:21 - bracket here a close curly bracket there
53:24 - this there okay so now this should right
53:28 - I'm only using seven tensors I can put
53:31 - the drawing back in and there we go now
53:39 - what if I want to have it ah yes okay so
53:46 - this is actually another this is
53:48 - actually a good point we could use one
53:50 - thing that that I wasn't thinking about
53:51 - and the chat is suggesting is that I am
53:54 - using stochastic gradient descent as my
53:58 - optimizer but there are other options
54:00 - for other kinds of optimizers let's go
54:03 - look at the tensor flow API and see what
54:05 - out the documentation was I not the
54:09 - documentation and see there's another
54:17 - good suggestion
54:28 - there's another good city sorry
54:38 - everybody
54:39 - [Music]
54:43 - so that's one suggestion another idea
54:46 - that someone in the chat is for podcasts
54:48 - a gradient descent so TF train SG is
54:51 - here so we can see momentum a to graph
54:54 - which one I should try Oh cent let's try
55:03 - that
55:07 - does anybody have a craziness learning
55:10 - rates probably way too high learning
55:12 - rate is way too high for this
55:40 - yes yes I agree I agree looking at the
55:47 - chat
55:52 - still perhaps maybe too high oh I forgot
55:58 - to have it at Mouse drag now so I want
56:00 - to take out the I'm gonna just put this
56:03 - basket back to mousepressed interesting
56:12 - whoa look at that come on
56:14 - you know something else I don't love
56:15 - about what I've done is a eita grad and
56:19 - an atom work let's let's try using atom
56:25 - oh yeah that's what I was looking for I
56:32 - don't know I didn't see that how do you
56:37 - pronounce these by the way
56:38 - adda grad ada grad added Delta a 2 Delta
56:42 - atom let's look at the paper
56:47 - [Music]
57:05 - Oh fun times let's try I'm by the way
57:10 - I've just gone off the rails I'm
57:12 - forgetting about this really being like
57:13 - I feel like I need to do this again if
57:15 - I'm gonna do it as a video perhaps but
57:20 - yeah now this is working better buttery
57:26 - smooth learning you use a picture of a
57:28 - pencil all right
57:37 - adaptive Adam yeah right all right I
57:45 - know I think it's live stream was going
57:48 - better when I was just talking about
57:51 - melon and reading random numbers all
57:54 - right I'm gonna torture everybody
57:56 - because I think I'm gonna actually just
57:58 - go and do this whole thing from the
58:00 - beginning again because and it's it's
58:10 - gonna be much faster I'm not just not
58:13 - going to do any insane editing there
58:14 - won't be somebody I won't have I'm going
58:20 - to use the correct code and I'm gonna
58:23 - feel like today is a success so all of
58:25 - you watching this live stream I
58:27 - apologize to you you can all revoke your
58:29 - sponsorships right now but I I just got
58:33 - a I've got a I've got to start over
58:36 - so I'm looking down for TF linear
58:38 - regression sketch a yes I want to make
58:41 - sure I'm actually starting with the
58:42 - correct code so many things went wrong
58:44 - here okay
58:57 - all right everyone I should do more
59:01 - things to get banned from the NYU
59:02 - Network I should be so lucky as to get
59:05 - fired from NYU
59:11 - okay let's we're gonna travel back into
59:15 - time you and me together you know I just
59:18 - say doing these live streams is actually
59:20 - quite good for my back because I'm
59:22 - moving around a lot we're gonna race
59:27 - this here and this is now I'm gonna I'm
59:36 - just gonna blaze right through this okay
59:53 - let's cycle the camera hello coding
60:08 - challenge the last one part 2 of the
60:10 - last one something maybe I don't know
60:13 - this is actually my second attempt I
60:15 - started this earlier today and I'm now
60:17 - gonna try it again you should watch this
60:21 - video go something else but I'm making
60:23 - it because I want to make this video
60:25 - I've got to feel like I did something
60:28 - today so in the previous video I did a
60:31 - demonstration of linear regression with
60:35 - Tessa Farias and the idea of linear
60:38 - regression is I have some data set right
60:40 - my data set are a bunch I'm making up a
60:42 - data set just by clicking points but you
60:44 - could imagine the x-axis representing
60:46 - something in the y axis representing
60:48 - something and then I'm trying to fit a
60:49 - line I want my line to fit to that data
60:52 - set and I'm doing that by creating
60:57 - intention float yes these variables that
61:00 - represent the M and the B of the formula
61:03 - for a line and then I create an
61:04 - optimizer and I I try to like figure out
61:07 - the loss like well if I had a line there
61:09 - what's all the differences between all
61:11 - the actual data points and where the
61:12 - line is and minimize that all that stuff
61:14 - that I did in the previous video if you
61:16 - watch that but linear regression can
61:21 - only ever fit a line so what if for
61:24 - example my data looked something like
61:27 - this well you can see the best line you
61:32 - can figure out to fit that data is this
61:34 - line which is it really accurate but I
61:36 - could see pretty easily like oh I could
61:37 - create a polynomial function probably a
61:40 - quadratic function that is a curve that
61:43 - looks like this that fits all those
61:45 - points so this brings me to the topic of
61:53 - what everybody wants to do with their
61:55 - lives so what is a polynomial equation
61:59 - well in some sense this is a polynomial
62:01 - equation it's a degree
62:03 - one I could also have a polynomial
62:07 - equation that's like a constant of
62:08 - degree zero y equals five if I had one
62:12 - of degree two
62:13 - what I have is y equals a x squared plus
62:18 - BX plus C quadratic cubic would be y
62:23 - equals ax cubed plus BX squared plus C
62:26 - now what are these values what is this
62:29 - what does this mean so the degree has to
62:30 - do with the number of solutions whether
62:32 - they're real solutions or complex
62:34 - solutions and there are a bunch of
62:36 - solutions across that I'll always math
62:37 - stuff which is not my forte I'll try to
62:40 - find a good resource for you know the
62:42 - fundamental theory of algebra or
62:43 - whatever that you could read about but
62:45 - the point is if I just want to now
62:47 - adjust my code to use a polynomial
62:50 - equation instead of a linear equation
62:53 - alright so that's not going to be too
62:55 - hard there's actually very little I need
62:56 - to change in the code so let's go back
62:58 - here and let's go to the code and let's
63:02 - look so here's the thing now I'm not
63:05 - gonna do this but really what I want to
63:07 - I mentioned this at the end I have an
63:09 - exercise for you to do if you make it
63:10 - all the way into this video I've got an
63:12 - exercise for you to do which would be
63:13 - totally fun again in the sense in the
63:17 - world we're polynomial regression is fun
63:18 - alright so I had em in B as variables
63:22 - now I need a B and C okay so now I need
63:27 - instead of M and B a B and C a B you
63:34 - know one thing you don't like about this
63:36 - you know I'm gonna fix something first
63:38 - that I never liked about this example
63:40 - which is that my space my
63:44 - two-dimensional space goes between 0 and
63:46 - 1 I think it's better just for the world
63:48 - if it went between negative 1 and 1 so
63:51 - let me look at everywhere where I map
63:54 - and it's really gonna be the same
63:57 - negative 1 1 negative 1 and 1 probably
64:01 - gonna miss something negative 1 1
64:03 - negative 1 and 1 just think the I should
64:06 - have like a Cartesian plane where zero
64:08 - zeros in the middle as my data space
64:10 - just for what I'm doing here so I think
64:12 - and then this should be this so let's
64:15 - look at this
64:17 - whoops okay I missed something
64:21 - yes no shoot shoot I missed somewhere
64:30 - why did I decide to try to do this right
64:33 - now
64:34 - terrible idea was all working fine let
64:39 - me just look everywhere I have map
64:40 - between negative one and one negative
64:44 - one oh one made one oh one negative one
64:45 - one anyone yeah I think I got everywhere
64:59 - Oh such an interesting discussion going
65:03 - on in this black channel I totally
65:05 - totally wanna I'm happy to participate
65:08 - in this later but this is about
65:10 - youtubing and the like line 35 ah
65:21 - there's the mistake that you all saw
65:23 - negative 1 and 1 there we go ok ok so
65:27 - we're back my linear regression is
65:29 - working just to be really sure up into a
65:31 - point here oh no look at this is the
65:34 - quickness down there oh I have it
65:37 - backwards zero too high ah 1 2 negative
65:40 - 1 all right
65:41 - we're really gonna get this there we go
65:43 - ok there we go
65:45 - all right now everything's fine I can
65:47 - now go to changing these too sorry for
65:51 - that little digression a b and c a b c
65:58 - okay then what's the other thing that i
66:00 - need to change
66:01 - so clearly the thing that i need to
66:04 - change is my predict function wherever
66:06 - that is there it is right so this is the
66:10 - formula y equals MX plus b expressed
66:13 - with tensorflow j/s now I just need to
66:17 - express this formula also with
66:21 - tensorflow digest this I feel confident
66:24 - I know how to do cuz I did it earlier
66:25 - today y equals ax carrot or I like to
66:29 - say hat look at x squared plus BX plus C
66:34 - so I'm gonna say constant wise equals so
66:38 - X's squared right x squared multiplied
66:44 - all the stuff in tensorflow J's can be
66:46 - chained the mathematical operation so
66:48 - the X is squared multiplied by a adding
66:52 - the X is multiplied by B that's B X and
66:58 - then finally adding C so this you know
67:02 - takes some getting used to how to like
67:04 - put all this stuff together and I could
67:05 - put it in multiple steps to make it more
67:07 - clear but this is the kind of thing you
67:08 - want to if you want to get into little
67:10 - low-level tension flow digest if you
67:11 - want to practice
67:13 - so now I can get rid of I'm just gonna
67:15 - get rid of this this is the predict
67:17 - function now I have another really
67:19 - significant issue here so I'm not gonna
67:21 - let me just run the code to make sure
67:22 - there's no syntax errors but it's
67:24 - obviously not going to do anything that
67:25 - makes any sense because it is what it's
67:29 - that this code down here is designed to
67:33 - just draw a line by the way good see I
67:36 - was doing some earlier in other words
67:39 - I'm picking this point and this point
67:42 - and just drawing a line but in order for
67:45 - me to draw a curve I can't you know I
67:47 - have to sample a lot of points along the
67:49 - x-axis so I need to make a loop and I'm
67:51 - now going between negative 1 and 1 to
67:53 - sample all these points I need to say
67:54 - begin shape vertex to your vertex C a
67:57 - vertex here predicted and shape I'll see
68:00 - that line so instead of having X 1 and X
68:05 - 2 I want an array of X values and I'm
68:09 - going to call that instead of line X
68:12 - curve X and that's going to be an array
68:15 - and I am going to start X at negative 1
68:20 - go all the way up to 1 say X plus equals
68:24 - and let's use some increment like 0.05
68:26 - and then I'm gonna say curve X dot push
68:32 - X so I'm just trying to make let me just
68:34 - show you what I'm trying to do here
68:35 - console.log curve X and there's gonna be
68:40 - all sorts of errors here but let's see
68:41 - you can see here that I'm just trying to
68:43 - make an array that has lots of X values
68:47 - but between negative 1 all the way up to
68:50 - 1 so I can now I need to get the Y
68:52 - values that go with that so I can draw
68:54 - this curve and I have the predict
68:56 - function does that so predict curve X
69:01 - get all those wise then curve Y is then
69:06 - again maybe I shouldn't be using data
69:08 - sync here that's kind of gonna be an
69:10 - animation slow down a problem but
69:12 - hopefully it'll be fine
69:14 - curve y is now the regular number the
69:19 - floating point numbers not the tensor
69:21 - anymore version of the Y's then I can
69:23 - get rid of
69:24 - ten sir I'm done with it and instead of
69:26 - drawing a line what I'm gonna do is say
69:30 - now okay now I just need to go through
69:32 - and look at all of the X points and say
69:40 - the x value is map curve X index I which
69:45 - goes between negative 1 and 1 to 0 to
69:48 - width and Y map curve y between negative
69:54 - 1 to 0 to height and then I want to say
69:56 - before this begin shape no fill stroke
70:03 - 255 stroke wait for 2 or whatever I had
70:07 - it and then n shape oh I need to
70:11 - actually set the vertex vertex X comma Y
70:15 - so let's see let's see what happens here
70:20 - look at that so this by the way is the
70:23 - random curve that the coefficients and I
70:25 - should get rid of this console.log so
70:30 - what's interesting about this is that
70:32 - every time i refresh this I'm gonna get
70:34 - a new polynomial quadratic equation
70:38 - because it's picking random a B and C
70:40 - and you know what I didn't pick a random
70:41 - any negative numbers so this would also
70:44 - probably make more sense for it to write
70:47 - couldn't technically these be anywhere
70:51 - like I should start between like
70:52 - negative 1 and 1 does that make more
70:53 - sense right yes that makes more sense so
70:57 - you could see I'm getting all these
70:58 - random curves that's what it's starting
71:00 - with now when I click you can see it's
71:04 - trying to approximate the curve whoa
71:06 - weird huh that looks exactly right but
71:11 - backwards why because I made this
71:15 - mistake again height is flipped negative
71:17 - 1 to 1
71:18 - oh no no no not there this is hard to
71:22 - keep track of these pixel mappings
71:24 - between height and zero let's run this
71:30 - one more time there we go
71:34 - you can see now it's you can see it's
71:36 - taking a while let me let me be more
71:38 - methodical about this I'm gonna click
71:40 - like a bunch like this it's actually
71:44 - finding it quite nicely but you can see
71:46 - it's good that it's quite lovely let me
71:48 - do it the other way you know what's
71:53 - bothering me is that you can see how
71:55 - this is not getting all the way to the
71:56 - edge it's because when I created those
71:59 - points I really want to say less than or
72:01 - equal to I want to get the last point in
72:04 - there as well oh I don't do this that's
72:12 - a little bad trick there there we go let
72:17 - me let me write draw these now there we
72:21 - go so you can see this is working hooray
72:23 - so now we have polynomial regression now
72:26 - one thing I probably would want to do do
72:28 - is this you know now that I'm getting
72:30 - past just sort of like basic linear
72:34 - regression the optimizers the optimizer
72:38 - that I'm using if we look at the
72:39 - tensorflow
72:40 - code is trained SGD stochastic gradient
72:45 - descent let's just try for a second
72:48 - let's look at the tension flow jsapi
72:51 - and I'm actually already here because I
72:54 - was looking at this earlier you can see
72:56 - there's actually different optimizers
72:58 - that we could try a well-known one atom
73:01 - the a da here being for adaptive I
73:03 - believe like a de grad a dad a de Delta
73:07 - but if I look at this we can see oh this
73:09 - constructs an atom optimizer that uses
73:11 - the atom algorithm I could click on this
73:13 - link here and I could find this whole
73:15 - paper that explains it beyond the scope
73:17 - of what I'm doing right now but just out
73:19 - of curiosity all I would need to do it
73:21 - oh maybe stochastic gradient descent
73:23 - isn't the best algorithm for fitting
73:25 - this curve let's just change this to
73:27 - Adam and I kind of have a feeling that
73:29 - this learning rate being so high is
73:31 - going to not work forever let's leave it
73:33 - so warning it might flicker quite a bit
73:36 - let's just see what happens with like a
73:37 - very high learning rate yeah you can see
73:42 - look at it like whoa that is really fun
73:46 - actually I kind of like it with the high
73:47 - learning
73:48 - because look how quickly but you can see
73:50 - it's kind of like bouncing around is
73:51 - because it's high learning rate it's
73:52 - gonna overshoot the optimal spot so
73:55 - let's just make that point one and let's
73:57 - also I think it would be fun to change
74:00 - this to Mouse dragged although I'm
74:02 - worried I'm gonna guess at so many
74:04 - points that it's gonna really slow down
74:06 - yeah
74:07 - Oh while I'm adding the points that's
74:10 - interesting so while I'm adding the
74:15 - points let's see this is a little bit
74:18 - weird but let adding points equals false
74:23 - I'm just gonna have this is like a
74:26 - terrible idea adding points equals true
74:35 - adding points equals false this doesn't
74:38 - make any sense
74:39 - why would that be really slow while I'm
74:41 - adding the points because what I want to
74:47 - say is like if and if not adding I was
74:51 - thinking I could like don't run the
74:53 - training if I'm not adding the points
74:57 - only run the training if I'm not adding
74:59 - the points yeah that didn't really help
75:01 - why what am I missing here
75:13 - so do the chat is fascinating
75:27 - oh people are really complaining about
75:29 - my 1.01 fix why not Jim we're gonna this
75:48 - section is so irrelevant to the
75:51 - regression tutorial that is not work
75:54 - that was like a side that I'm not gonna
75:59 - let me think about this right while I'm
76:03 - adding the points it's slow and then it
76:11 - doesn't matter it's happy to have a lot
76:12 - of points Mouse drag gets called
76:15 - multiple times on Mouse oh right so what
76:20 - if I did yeah right I should do Mouse
76:29 - pressed yeah I know what to do
76:39 - all right I'm kind of off in the weeds
76:43 - here a little bit of something that's
76:44 - not irrelevant to this example but I've
76:45 - been noticing that it's like really slow
76:46 - as I'm like drawing and then as soon as
76:48 - I like let go it's perfectly happy to
76:51 - like animate very quickly I'm trying to
76:53 - figure out why this is so one thing I'm
76:55 - gonna try is I'm going to just change
76:57 - I'm going to add mousepressed and I'm
76:59 - gonna say let adding whoops I'm gonna
77:01 - create a new variable let adding points
77:08 - I'm interesting to call it let dragging
77:10 - equal the false and I'm going to say I'm
77:18 - gonna say my mouse press you're gonna
77:19 - say dragging dragging equals true and
77:23 - then I'm gonna add Mouse released and
77:28 - say dragging equals false and then in
77:32 - draw I'm gonna I'm gonna do this now in
77:37 - draw so this way I'm gonna kind of like
77:41 - really keep everything in draw and I'm
77:42 - gonna say if dragging then add points
77:46 - and if you're adding points maybe don't
77:49 - try minimizing the foot just wait till
77:53 - you're done adding points to minimize
77:55 - the function let's see how that works
77:57 - yeah there we go so it's not happening
78:01 - in real time in the same way but at
78:04 - least it's letting me add the points so
78:08 - I'm intrigued why I wonder if there's
78:10 - probably a different way to think about
78:11 - this I really love how you that
78:13 - animation of watching it fit oh it's
78:15 - really just very satisfying so in any
78:19 - case let's try I'm just you know let me
78:23 - just try a smaller learning rate just
78:25 - because I'm curious so you can see with
78:28 - a smaller learning rate it's moving much
78:30 - more slowly so the higher move learning
78:33 - rate it's gonna get there much more
78:35 - quickly if it's gonna bounce you could
78:36 - do something that's called annealing I
78:38 - believe of the learning rate meaning
78:39 - start with a high learning rate but
78:41 - lower it over time we have to
78:43 - potential flow das API to see how that's
78:45 - done but here's thing I really want to
78:47 - do this I want to do this with a
78:49 - equation of degree three just because so
78:53 - I'm gonna do that manually so I'm gonna
78:55 - add a D here and then I'm gonna add a D
78:59 - here and this is gonna lead to my
79:02 - exercise for you and then I'm gonna say
79:05 - ax cubed plus BX squared plus CX Plus D
79:10 - so I want
79:12 - oh is there a cube mathematical
79:15 - operation intend to float a s square
79:18 - operations operations there's a square
79:24 - POW I guess I could do just wondering if
79:26 - there's a TF square is there a cube no
79:32 - so I probably want to do power where is
79:36 - that it's funny how that's under it
79:41 - react so I want to do this POW base
79:45 - exponent so I would do X is POW 3 x a
79:55 - adding X's squared multiplied by B
79:59 - adding C X is multiplied by C and maybe
80:06 - I should start to do something where I
80:09 - put these on different lines so so let's
80:15 - do this X's pal 3 multiply adding X's
80:18 - squared x be adding X is multiplied by C
80:26 - adding D right did I get this right this
80:30 - is now of degree 3 X is power to the
80:34 - power of 3 multiplied by a plus X's
80:37 - squared multiplied by B plus X's plus CX
80:40 - Plus D ok and then I think actually I'm
80:45 - good because this everything else is the
80:48 - same so let's go check this ooh
80:51 - argument exponent past Oh must be a
80:54 - tensor but got a number of course
80:57 - so if I'm saying power 3 I need to say
81:00 - TF scaler 3 so everything's got to be
81:03 - everything's got to be a tensor I can't
81:05 - just use numbers like I'm used to so
81:06 - that's to be TFT scaler dot three and
81:12 - let me put the learning rate back up
81:14 - higher let me put it back up to like
81:17 - point - wow that is immensely satisfied
81:25 - so now you can see and by the way if I
81:29 - happen to draw a line it should still be
81:32 - really happy to sort of like fit a line
81:34 - there because it could just make those
81:36 - coefficients zero oh this is great oh hi
81:39 - Murphy glad I made this video for myself
81:41 - at least so here's the thing here's my
81:42 - exercise to you how can you make this so
81:47 - that the degree of the polynomial is
81:49 - like something that can be interactive
81:51 - as well so could I have a drop-down that
81:53 - allows me to try it with a degree 2
81:55 - degree 3 degree 4 or maybe a slider what
81:58 - other kinds of interactive features
82:00 - could I add to this - and how could I
82:03 - maybe like have the data set come into
82:05 - this in a more interesting way besides
82:06 - just kind of drawing with the mouse so
82:09 - um could you make what kinds of things
82:12 - could you could you do with this I think
82:16 - there's some interesting visual
82:17 - possibilities so I hope you enjoyed this
82:18 - video I'm going to publish this code now
82:20 - as a coding challenge called it's really
82:24 - part two of the linear regression now
82:25 - polynomial regression and I'll see you
82:27 - the future videos I'm going to work on
82:29 - that I'm going to look at that
82:30 - tensorflow digests layers API soon
82:32 - enough ok goodbye
82:45 - yes so Tobias in the chat is asking why
82:50 - didn't X less than Ernie let me dress
82:51 - this so this is really weird but I why
82:58 - did this not work why did this not work
83:07 - here's the thing floating point math on
83:09 - a computer is a weird thing and I've
83:12 - encountered this in many videos and
83:14 - tutorials but at least it's somewhere I
83:16 - cover this in an actual tutorial but
83:18 - like let's go to the JavaScript console
83:19 - for a second and I really should let
83:22 - this let me just shut this off so let me
83:30 - show you something
83:31 - oh it's being much too nice to me
83:38 - maybe it's like so there's all sorts of
83:49 - way of rounding stuff that can happen
83:51 - and floating-point rounding errors is
83:56 - like a thing because the computer's not
83:58 - actually storing this number as you see
84:01 - it it's storing a representation of this
84:03 - floating-point number with a certain
84:05 - number of bits and it has a limited
84:07 - number of bits so it can often kind of
84:09 - like get off by point one which doesn't
84:12 - really matter if you're just doing like
84:15 - a simple graphic simulation but if
84:16 - you're doing some like they're highly
84:18 - specific math that's why in certain
84:20 - programming languages and environments
84:22 - you'll find double which is more memory
84:25 - for decimal numbers you'll also have
84:28 - like kind of like specific math like I
84:30 - bet your tensor flow to deals with this
84:33 - stuff in a much more like lower level
84:35 - and accurate way probably 0.01 plus 0.02
84:40 - I'm being told no in short oh yes the
84:51 - dangers what are the dangers
85:00 - oh maybe I should try that 0.0100 no I
85:06 - don't know I don't know how to get it to
85:07 - show up the I don't know how to get it
85:12 - to show up it seems to be working
85:20 - perfectly there's nothing wrong at all
85:32 - let's Google that dangers of
85:35 - over-fitting polynomial curves
85:59 - all right so maybe I'll mention this
86:01 - real quick
86:29 - I forgot something important that I was
86:34 - told by the chat that I should mention
86:36 - and I'm referring of the dangers of
86:39 - overfitting of polynomial equation and
86:42 - that's what this being represented on
86:46 - this is a Wikipedia page for overfitting
86:48 - which is a term that you'll hear a lot
86:51 - in machine learning meaning what is
86:53 - overfitting so if my data set if those
86:55 - points are my data set a really fancy
86:58 - polynomial equation of some high degree
87:00 - is gonna be able to draw something like
87:02 - really accurately connecting all those
87:05 - but it doesn't actually have any real
87:07 - meaning as applied to the data the data
87:08 - can have a lot of noise in it and a line
87:10 - might actually be an appropriate way to
87:13 - make a prediction even though it doesn't
87:15 - fit it has on like a higher loss than
87:17 - the polynomial so so this idea of
87:20 - generalization you don't want your
87:22 - machine learning model to work so
87:26 - accurately with your known data set that
87:29 - it cannot make good predictions with an
87:31 - unknown data set and this is something
87:32 - that will come up more and more as I do
87:34 - more videos and tutorials with things
87:35 - here's the thing though I kind of like
87:38 - this example that I made for the fact
87:40 - that it does something kind of
87:41 - interesting visually in that it just
87:43 - looked figured out how to make this
87:45 - polynomial function fit this arbitrary
87:47 - set of points so I think we're sort of
87:49 - like some kind of artistic output there
87:51 - might be some value here but yes
87:53 - thinking about actual machine learning
87:56 - and how you want to make predictions is
87:57 - important so that's why I'm here at the
88:00 - with this public service announcement
88:02 - the more you know the more you know the
88:05 - music goodbye oh there we go
88:15 - CJ CJ got something here CJ you're you
88:19 - see J coding garden there we go there is
88:29 - our floating-point rounding error thingy
88:36 - what's this sound all awful in that and
88:39 - it won't be usable yes okay CJ yes DJ is
88:42 - rate coding guard of the CG
88:43 - guitar I love that you used the guitar
88:45 - CJ you see everyone should check out
88:48 - Odegard with CJ very I I wish I watched
88:53 - more but I've caught a few of the
88:55 - streams alright
89:00 - [Music]
89:05 - ken Haley writes only use higher order
89:07 - polynomials if you know there's a reason
89:10 - to do so not just the data yes will you
89:14 - keep the beard since when have I ever
89:16 - not kept the beard
89:18 - thank you Ken for that clarification
89:22 - alright 3:40 i could go downstairs and
89:32 - livestream be playing with the philips
89:36 - lamp or i could talk about the layers
89:38 - api i should probably talk about the
89:41 - layers api since that's what i said i
89:42 - was gonna do
89:48 - let me give me a minute here to figure
89:54 - out what I'm gonna do next I have one
89:55 - more piece of space melon the curves
89:59 - array has the mistake what mistake do I
90:01 - have
90:14 - one more piece of space melon I don't
90:18 - know how much longer I can do this we
90:20 - know all right can I stop the infinite
90:32 - loop what infinite loop all right I
90:34 - don't know what's going on let me just
90:36 - make sure NYU is not shutting me down
90:39 - ooh
90:41 - oh I got another sponsor that I missed
90:47 - because I'm getting now I'm getting so I
90:49 - know that Philips like thing will work
90:51 - because I'm getting email notifications
90:54 - thank you to Supriya okay I'm not seeing
91:03 - any emails from the internet services
91:07 - here telling me to stop doing what I'm
91:09 - doing okay all right I think I could
91:16 - move on yes slack channel any more
91:20 - things to say let's I think I'm gonna
91:22 - erase this I'm gonna reuse my paper
91:28 - towel
91:33 - right yeah
91:38 - it's probably what I should do is always
91:41 - do everything twice
91:44 - where is yeah I don't know why can't
91:49 - find that Phillips hue light bridge it's
91:54 - like completely oh there it is
91:57 - this is the prop I was looking to show
91:59 - ya we're over here this is the prop I
92:01 - was looking to show so I'm that's what
92:05 - caused me to get banned from the NYU
92:07 - Network
92:24 - Oh remember when I used different pen
92:29 - colors those were the days
92:41 - I want to do the XOR I'm going to do a
92:44 - video I want to do a tutorial of now
92:46 - about the the layers API and and try to
92:57 - train a model to learn XOR another thing
93:04 - that I'm thinking about to at the
93:06 - tensorflow
93:07 - one of the tensor flow videos is either
93:10 - at Google the Google i/o conference I
93:12 - think it was at the i/o conference not
93:15 - at not at the dev summit but I can't
93:19 - remember which one did and it's part of
93:21 - the TF chess examples did a
93:24 - classification example with Major League
93:26 - Baseball I'm using like pitching data
93:28 - and like sort of looked at the speed of
93:30 - the pitch and different things and then
93:31 - classified it as a I think it's a
93:33 - fastball or curveball or slider so I
93:36 - think that I would like to get to some
93:37 - more actual real world real data
93:41 - examples and then I'm gonna do the
93:43 - doodle classifier I should do all these
93:46 - things then eventually ml5 alright
94:07 - okay
94:20 - let's see here so now let's let's make
94:30 - some changes here
94:44 - I'm just gonna doing some housekeeping
94:46 - here
95:06 - okay
95:29 - would be nice if I kind of knew the
95:31 - lairs API who it there's a big thing of
95:33 - paper towels here that's funny
95:38 - zoomy asks does YouTube create a video
95:41 - on-demand once the livestream ends yes
95:43 - it does it sometimes takes a little
95:45 - while and it actually the way I have it
95:47 - set up is it automatically goes to
95:49 - unlisted but this URL that you're at
95:51 - right now it will be there and then I
95:53 - make it listed once I had a chance to
95:56 - work with matcha Chia to like write the
95:58 - description and all that kind of stuff
96:01 - alright so layers okay just gonna look
96:10 - at some examples that I was making
96:15 - working on should really pin some
96:26 - repositories
96:35 - so let me look at this oh yeah I had all
96:38 - this extra stuff okay right
96:40 - sequential well then you add and create
96:45 - layers create an optimizer and you have
96:49 - to compile it
96:52 - you can you can just set a loss function
96:58 - and then so that's pretty much so they
97:02 - could just call predict okay all right
97:18 - which deaath model that's different than
97:22 - the difference between TF model and TF
97:25 - sequential is that TF model is a more
97:27 - generic supporting an arbitrary graph
97:29 - without cycles of layers TF sequential
97:32 - is less generic and supports only a
97:33 - linear stack of errors Oh interesting
97:38 - how interesting so I'm used to just
97:46 - using TF sequential but I'm not going to
97:54 - worry about TF model right now yeah
98:02 - input what's this
98:07 - alright I haven't used that and then
98:15 - these are the different kind of layers
98:26 - and whoops look at my example Oh I
98:30 - closed that I just don't know any of
98:35 - this stuff off the top of my head units
98:39 - input shape so where I want to look up I
98:41 - just want to see what's in the API so I
98:44 - can things I want to look at our TF
98:48 - sequential I want to look at dents I
98:57 - already have then to this I need dents
99:02 - input shape config okay that I need why
99:09 - do I keep closing my thing that I want
99:15 - things I want to cover our dents a
99:21 - sequential compile so many sequential
99:32 - compile so that's under objects yep
99:43 - okay compile and I need this I think
99:50 - this is going to be enough and then
99:52 - predict I also want all right this
99:58 - should be a predict evaluates predict
100:01 - yep fit we're gonna get into also okay
100:17 - anyone who has experience with this
100:19 - stuff is there anything super important
100:22 - about anything important about TF
100:29 - sequential versus TF model that I should
100:31 - make sure to cover
100:46 - alright I think I'm gonna talk about the
100:49 - lairs API and
101:03 - I think I want to use the toy neural
101:06 - network
101:08 - [Music]
101:36 - okay
101:42 - [Music]
101:50 - actually just look at the library
101:59 - this make sense so okay to do all right
102:12 - all right I'm not gonna I will just okay
102:21 - yeah dense layers are what I used yeah
102:25 - yeah okay okay
102:38 - hello welcome to another tensorflow das
102:42 - tutorial now I'm very excited about this
102:47 - one I'm generally excited about a lot of
102:49 - things but in this tutorial everything
102:52 - that I've done so far has just used
102:54 - tensors operations to kind of create
102:58 - lists and matrices of numbers and
103:00 - multiply them and add them and optimize
103:02 - loss functions that kind of stuff now
103:06 - and I could keep going and there's a lot
103:08 - that I could do with just that alone but
103:10 - tensorflow digest I talked about this in
103:12 - the first video it has the sort of core
103:14 - API which has the tensors in it the
103:18 - operations and it's what I use to do a
103:21 - linear regression and a polynomial
103:23 - regression demonstration it also has
103:26 - something called the layers API and the
103:31 - layers API might look familiar to you if
103:34 - you've ever used something called Kerris
103:35 - because these are really Kerris layers
103:37 - caris is a lot machine learning library
103:40 - that is a higher level that allows you
103:42 - to create these machine learning models
103:44 - and underneath the hood a lower level
103:47 - code like tensorflow
103:48 - will be running so when tensorflow dot
103:52 - jeff's was created it was created with
103:53 - both the sort of lower level stuff and
103:55 - the slightly higher level stuff and I
103:58 - also am working on with a lot of
103:59 - collaborators here at NYU ITP an even
104:02 - higher level library that's built on top
104:06 - of the layers API called ml 5 I'll be
104:08 - getting to that eventually in this video
104:10 - I just want to talk about what the
104:12 - layers API is and its core features and
104:15 - so the way that I'm gonna do that is by
104:18 - looking at a sort of basic diagram of a
104:21 - neural network and how you would put
104:23 - together that neural network with the
104:25 - layers API and the kind of neural
104:28 - network that I'm going to diagram is the
104:30 - same exact one that I did in my very
104:32 - long tutorial series about Bill writing
104:34 - a neural network all from scratch so the
104:36 - point of this is you don't have to write
104:38 - it all from scratch you could just
104:40 - architect it with the layers API but if
104:43 - you want if you want to like get
104:44 - everything you possibly could ever get
104:45 - you could go back and look at some
104:47 - those videos if you want so what is so
104:50 - I'm gonna look at a simple well it's not
104:52 - simple but a basic feed-forward
104:54 - multi-layered perceptron it's gonna have
104:56 - just two layers so it often looks like
105:00 - three layers because there's also the
105:01 - inputs there's the inputs the hidden and
105:03 - the outputs that's three things but
105:06 - technically there's only two layers and
105:07 - you'll see why so let's consider that we
105:11 - have this neural network it's going to
105:13 - have inputs let's say that it has I
105:16 - don't know two inputs then it has a
105:23 - hidden layer how many nodes are in the
105:28 - hidden layer I don't know let's say four
105:34 - then it has an output layer how many
105:39 - outputs are there I don't know maybe
105:41 - we're doing some kind of classification
105:42 - task and there's three possibilities
105:44 - it's either a cat a dog or turtle so
105:47 - there will be three outputs you know I
105:50 - started diagramming the next the
105:52 - scenario that I'm gonna do in the next
105:54 - video maybe as a coding challenge is I'm
105:56 - going to solve again the XOR problem I
105:59 - really would like to get to some
106:00 - real-world applicable problems but I'm
106:03 - still here in the weeds of just like I
106:04 - want to see how things work and use kind
106:06 - of trivial and known problems just to
106:10 - see if I can get the solution that I
106:11 - know I'm supposed to get okay so if this
106:14 - is my diagram if you if you've never
106:17 - seen the neural network before again you
106:19 - could go back and look at to my other
106:20 - videos but the ideas are some data X we
106:24 - might call this like x0 and x1 and that
106:27 - data each each of those inputs gets sent
106:31 - to each hidden node there are weights
106:38 - here the inputs get multiplied by the
106:41 - weights added together and then pass
106:43 - through an activation function and then
106:46 - get sent out to each of the outputs and
106:49 - so that happens at each one of these
106:53 - it's very hard to draw this
106:55 - I'm sure I missed the connection by the
106:59 - ways that they called dropout so I've
107:01 - done that correct doesn't dropout
107:02 - caressing if I forgot to draw something
107:03 - but I'll get to that in another video so
107:05 - then then we have the outputs and those
107:07 - are again the weighted sum of all of the
107:10 - outputs of the hidden layer the hidden
107:12 - nodes comes into the output pass through
107:14 - an activation function and we see their
107:16 - result so the idea here is maybe that I
107:19 - have a image with just two pixels in it
107:21 - and two two pixels come in they get
107:26 - multiplied by all these weights
107:28 - activated of the activation function
107:30 - sent to the outputs and then I get some
107:31 - values that tell me the probability of
107:33 - those two pixels were a cat dog or a
107:34 - turtle that would be an image
107:35 - classification task so how do I use how
107:39 - do I use the layers API to create a
107:41 - neural network with this exact
107:44 - architecture so the first thing that I
107:46 - need to do is create something called TF
107:50 - sequential the TF sequential let's go
107:54 - look at that in the API Docs and I'm
107:57 - right here all right
107:58 - TF win is a secretes a sequential model
108:01 - is any model where the outputs of one
108:04 - layer are the inputs to the next layer
108:08 - that's what it says right there guess
108:10 - what and I know I'm sorry that I've kind
108:11 - of gone a little bit too high in my
108:13 - writing but the outputs of one layer are
108:16 - the inputs to the next layer the inputs
108:19 - go into the hidden the outputs of the
108:20 - hidden go down so that's exactly what I
108:22 - want I should say there is also
108:24 - something in the layers API called
108:28 - TF model which I'll click on here and
108:32 - the key difference is that TF model is
108:35 - more generic so there's kind of more
108:37 - possibilities there but if I really just
108:39 - want a simple basic feed-forward neural
108:41 - network or the data flows in one
108:43 - direction between layers TF sequential
108:45 - will work so if I'm writing some code I
108:48 - would say Const model equals T f dot
108:52 - sequential so there we go
108:57 - I'm done well not exactly so that's just
109:01 - creating a sort of empty so all I've
109:03 - done is created this kind of empty
109:05 - architecture so what I need to do
109:08 - now and by the way this is what makes
109:10 - working with something like the layers
109:12 - API really powerful if you watch my
109:14 - tutorials where I did the whole neural
109:15 - network from scratch it was so much
109:17 - easier just to have one layer and I
109:19 - never kind of got to like multiple acres
109:21 - cuz I have to rewrite the code and think
109:23 - about the layers and how they're
109:23 - connected and have a loop and all this
109:25 - layer object well guess what the layers
109:27 - API has this for you so I can actually
109:29 - just say TF now add layer so I can I can
109:34 - actually I can create a layer so the
109:36 - kind of layer that I want to create is
109:38 - known as dense so what is a dense layer
109:42 - a dense layer is the terminology for a
109:45 - fully connected layer meaning that every
109:48 - every node in that layer is fully
109:52 - connected to every node from the
109:54 - previous layer and that's exactly what I
109:56 - have here these are dense layers all of
109:59 - the connections are there so and you'll
110:01 - see you'll see there's like other kinds
110:03 - of layers there's a convolutional layer
110:04 - that i'll use one at some point when I
110:06 - talk about convolutional neural networks
110:08 - other things too but dense is where
110:09 - we're gonna get started so let's come
110:11 - back and look at now the API Docs for a
110:15 - dense and I think I've opened that up
110:17 - here yes TF layer sorry it's TF dot
110:20 - layers dense so I need to say Const and
110:25 - I'm gonna call this hidden equals TF
110:27 - layers dense Const output equals TF
110:34 - layers dense so I'm missing a lot of but
110:36 - this is the idea so like I will have a
110:39 - model which is it's a sequential and
110:40 - then I have a hidden layer and output
110:42 - layer we'll talk about the inputs in a
110:43 - second and then I would just say model
110:45 - dot add layer hidden model dot add layer
110:51 - output and I'm like guys like to say
110:55 - outputs I think is it's the output layer
110:58 - whatever nuts to say output so this is
111:00 - the idea this is how in theory simple it
111:03 - is to build your own model using the
111:06 - layers API now what's missing here like
111:08 - I could weirdly enough let's just like
111:10 - run this code and see if we get any
111:11 - errors so it cannot read property name
111:16 - of undefined so okay so so who knows
111:20 - what that error is but
111:21 - thing that I'm really missing here is I
111:22 - need to be more specific like I need to
111:25 - say when I make a layer what is the
111:28 - shape of that layer in other words how
111:31 - many nodes are there with the shape of
111:33 - the inputs how is it connected what
111:34 - activation function am I using those
111:36 - types of things so those are if we look
111:38 - at the API Docs the configuration of the
111:41 - layer so so I need to actually configure
111:43 - the model itself and configure each
111:46 - layer so let's go look at that and see
111:47 - if we can figure that out so let's go
111:49 - look again at TF sequential and you know
111:56 - actually I think we're going to be fine
111:57 - right now with there's an optional this
112:00 - question mark means optional there's an
112:01 - optional optional optional parameter
112:05 - config what I am going to skip that
112:08 - right now and yet it could be I could
112:14 - actually create it with a bunch of
112:15 - layers already and a name but I'm gonna
112:17 - skip that what's more important here is
112:19 - the sorry the the dense the config
112:25 - object for the dense layers which is
112:27 - required so that's why I'm getting error
112:29 - this is required so I need to specify
112:33 - some configuration options and this
112:37 - should be listed for me here units
112:39 - activation use bias kernel initializer
112:42 - bla bla bla bla bla bla bla bla and
112:44 - sorry for these markers here so let's
112:46 - start adding something what that means
112:47 - is I need to create an object I'm gonna
112:50 - call it config I could call it and here
112:53 - is where I'm gonna set up the
112:54 - configuration of this hidden layer so
112:58 - what I'm gonna do here is let's look at
113:00 - what some of these are so units so what
113:03 - I want is what's the unit so in this
113:04 - case I want to have the hidden layer has
113:07 - four units and so I'm gonna say units
113:12 - four maybe I want to specify the
113:15 - activation function
113:16 - so what activation function you use is a
113:19 - fascinating topic that you could go down
113:22 - many rabbit holes for different
113:24 - scenarios but and I'm just going to put
113:27 - sigmoid in there as kind of it for
113:29 - historical reasons and that's also the
113:32 - activation function you
113:33 - in my toy neural network JavaScript
113:35 - library but eventually as I start to
113:36 - build out examples I'm gonna be taking
113:38 - out that sigmoid and using other ones so
113:41 - I'm gonna say sigmoid and I'm gonna put
113:45 - config here and I'll just say config 1
113:47 - or config hidden again I could put the
113:50 - object itself directly in here there's
113:53 - lots of ways you could probably write
113:54 - this code in much shorter way I'm trying
113:56 - to write it as long away as possible to
113:58 - be most clear so let's do that and let's
114:01 - do config output and what did I say how
114:07 - many outputs do I have 3 and I'll also
114:10 - use sigmoid so I'm going to say 3 and
114:14 - I'm gonna say I'm gonna say config
114:19 - output ok so things are going pretty
114:23 - well now I'm gonna hit refresh here
114:27 - config is not defined sketch let's just
114:30 - line 7 oh right config hidden ooh oh
114:35 - interesting time out for a second I just
114:45 - need to just need to take a pause for a
114:50 - second I need some water
114:57 - how's this going so far is this like
114:59 - kind of helping people clear interesting
115:02 - useful to long-winded
115:07 - [Applause]
115:14 - because it's not add layer all right let
115:31 - me just take a peek once again at my
115:36 - example Oh input shape that's where I'm
115:42 - forgetting of course that's why I forget
115:49 - okay
116:05 - all right amusingly I got this weird
116:07 - error message which makes no sense at
116:09 - all it actually makes sense but it's
116:12 - because iíve got the p5 library involved
116:14 - here and p5 actually has a function
116:16 - called model so let me just write now p5
116:18 - is irrelevant for this discussion so I'm
116:20 - just going to comment out the p5 library
116:23 - and then hit refresh here now add layer
116:26 - is not a function so I must have
116:27 - imagined that this is how you add a
116:30 - layer to the model let's actually go and
116:32 - look and where would I find that out
116:34 - once again if I go to so here what I
116:38 - want to look at is all right hard for me
116:41 - to find things ok I know what I want to
116:43 - look for so I'm gonna just search for it
116:44 - I want to look for the TF sequential
116:47 - class so TF not sequential the function
116:50 - creates an object that is a TF not
116:53 - sequential so if I look at this we now
116:55 - look that the function is just add
116:57 - there's no add layer function is just
116:59 - add which is a lot nicer actually so
117:02 - this is meant to just be add and this is
117:04 - meant to just be add okay so now this is
117:08 - really bothered me that it's calling
117:10 - this polynomial regression so I'm going
117:13 - to have to change this to layers API
117:18 - explanation there we go okay uncaught
117:23 - error the first layer in a sequential
117:25 - model must get an input shape or a batch
117:28 - input shape argument all right so what
117:31 - did I miss this is what I was talking
117:32 - about so the inputs are technically not
117:34 - a layer themselves the inputs are in a
117:38 - way part of this hidden layer they are
117:40 - the inputs to that hidden layer the
117:42 - inputs to the output output layer are
117:45 - the outputs of the hidden layer so one
117:47 - of the things one of those properties
117:49 - that I have to specify is the input
117:51 - shape now interestingly enough it says I
117:54 - need an input shape or an input batch
117:56 - shape and what's interesting about this
117:58 - is what is the difference so here I can
118:00 - clearly say that the input shape is two
118:03 - there are two inputs it's just a no
118:06 - actually it's one well no no it's - it's
118:12 - an irregular number to it it's confusing
118:14 - this is what it is it's one dimensional
118:17 - it's a one dimensional
118:18 - ray with two spots in it but someday
118:22 - there might come a time where I have a
118:23 - data set that is just each each each
118:27 - each record of that data set is two
118:30 - numbers but I want to send in a hundred
118:32 - of them at once that's known as a batch
118:34 - so the shape might be something like 2
118:36 - comma 100 but this is not super relevant
118:39 - for right now
118:40 - this is what I need to specify so if we
118:41 - come back to the code
118:44 - I should here oops going back to sketch
118:47 - touch is what I need to specify here is
118:50 - input shape - that's it again
118:55 - I beat this up like I'm just saying my
118:57 - my bottle architecture has - so now I
119:00 - should be able to no errors
119:05 - so I created that model and I can even
119:07 - take a look at it here there it is you
119:09 - know all this stuff input layers we can
119:12 - see all sorts of stuff here now I
119:13 - there's not much there actually cuz I've
119:15 - forgotten a really crucial step but let
119:17 - me keep going there's some more stuff to
119:18 - discuss so the input shape is - what's
119:22 - the thing how come I don't need to
119:24 - specify an input shape here you would
119:27 - think that I might well I'm just doing
119:33 - it with some camera cycling after all I
119:41 - had to say that there were two inputs -
119:44 - hidden - I don't have to say that
119:46 - there's four inputs to outputs well the
119:49 - reason why I don't is because tensorflow
119:51 - has the layers API can infer the input
119:55 - shape of the outputs layer because it
119:58 - has to be the number of units in that
120:00 - hidden layer and by the way it's you
120:04 - know actually naming these things like
120:06 - hidden and output is almost less
120:08 - relevant now it's really kind of like
120:09 - layer 1 layer 2 that sort of thing so so
120:12 - I could put input shape here and I would
120:17 - put for now let's just see if I have any
120:21 - errors everything's fine now what if I
120:23 - put like a tear am I going to get an
120:26 - error no I didn't get an error that's
120:29 - weird now why didn't I get an error well
120:31 - maybe it should give me an error I don't
120:33 - think so though I should get an air
120:35 - though I'm missing a crucial step here I
120:37 - actually have just set up the idea of
120:40 - this sequential model I have the model
120:42 - object I have the hidden layer I have
120:44 - the output layer I've added them both in
120:46 - but I actually haven't like plugged all
120:48 - the pieces into each other yet and
120:50 - finished it off that has to have come as
120:52 - a separate it's not building up the
120:54 - model as you're creating it or
120:56 - configuring it it's at the layers API so
120:59 - you configure it and then call a
121:00 - function called compile so I know I'm
121:03 - making lists of and this by the way was
121:05 - in TF dot layers dot dense but another
121:08 - really important function here compile
121:11 - is part of a sequential object ad ad was
121:15 - the other one if I'm sure keeping track
121:16 - of the things that I'm looking at so I
121:18 - add I looked at now I also need compile
121:20 - so let's go take a look at that so if I
121:24 - go to the documentation here we can see
121:27 - there's ad evaluate there's a bunch of
121:29 - things I'm looking for compile wait
121:31 - maybe compile is not part of TF
121:34 - sequential oh it is it is it's just
121:38 - listed as part of TF model because TF
121:41 - sequential is based off of TF model so
121:45 - this is what I'm looking for this is me
121:48 - needing to compile the model now I need
121:50 - to compile it with an optimizer and a
121:54 - loss function aha so if you watched my
121:58 - linear regression with tension flow Jas
122:02 - videos you might remember that I had to
122:04 - create something called an optimizer and
122:07 - the optimizers job was to minimize a
122:13 - loss function so the same thing the idea
122:19 - you know I just had y equals MX plus B
122:22 - now I have a more complex architecture
122:24 - to learn about a data set so what I want
122:28 - to do with that architecture is the same
122:30 - though I want to feed it a lot of known
122:32 - data and have it optimize all of the
122:34 - weights of all these connections to fit
122:36 - and this gonna be a very fit that data
122:38 - so I need to specify those things so how
122:41 - do I do that so
122:42 - first I'm gonna make I'm gonna call this
122:48 - just um like config and I'm gonna say
122:54 - optimizer
122:55 - is and so some options here a string or
122:59 - a GF train optimizer so I think what I
123:03 - want actually want to create my own
123:04 - optimizer and I'm gonna go do that I'm
123:07 - by saying a constant optimizer
123:10 - equals TF train SGD zero point one so if
123:16 - you remember this is a way this is I
123:19 - mean this you might not have seen before
123:20 - if you're watching this video for the
123:21 - first time but this is exactly the same
123:23 - code that I had in my linear regression
123:28 - example I'm creating an optimizer from
123:31 - TF train the optimizer uses stochastic
123:34 - gradient descent and the learning rate
123:35 - is point one so this will be the
123:38 - optimizer so I'm gonna say SGD optimizer
123:41 - like that then what else do I need a
123:46 - loss function so the loss function can
123:49 - be I could probably to find my own loss
123:51 - function or I can use a string so I'm
123:55 - not seeing here the options for the loss
123:58 - function strings but let's see loss
124:01 - function like basically what I want to
124:04 - add here is like mean squared error or
124:07 - something
124:08 - that's my loss so I don't think that's
124:10 - right but let's just see now if what I
124:14 - would do is say my mean a model dot
124:16 - compile config so now right what is what
124:23 - does the Routh let's review make create
124:25 - the TF sequential object configure some
124:29 - layers add them to that model then
124:32 - configure the the Optima basically make
124:35 - an optimizer and a loss function to find
124:37 - those and then compile the model with
124:39 - those so I'm sure there's gonna be lots
124:41 - of errors here and there's things that
124:42 - I've done incorrectly let's so unknown
124:44 - loss mean squared error so let's figure
124:46 - out how do I look that up so let's see
124:50 - here means squared
124:52 - look I found it in an example
124:56 - ah so losses let's look at this I'm just
124:59 - looking for the list of them but I'm
125:02 - gonna I'm gonna come back I'm gonna find
125:04 - that and then to come back and show it
125:05 - to you but now that I see it's actually
125:07 - this is just lowercase M so somewhere
125:09 - the documentation I want to find what
125:10 - the options I could put here are let's
125:13 - run let's now hit refresh
125:14 - oh no errors interesting so this is
125:18 - weird I'm surprised know if this is a
125:20 - bug or not but I'm surprised that I
125:22 - didn't get an error for this input shape
125:24 - here for the output maybe once I started
125:27 - feeding it data again there maybe I
125:29 - never would but I'm just getting it
125:31 - refresh though so okay so hmm look pause
125:37 - for a second
125:45 - we were talking about the cameras of the
125:47 - chat so I want to pause here to find the
126:01 - to just look at my example that I made
126:05 - the other day
126:08 - what else did I put in compiled no I
126:10 - just have the optimizer in the loss and
126:16 - then predict can do predicts okay so
126:23 - some X's and I can do the training so I
126:26 - can do fit okay so I probably want to
126:32 - add fit so where do I find in the
126:35 - documentation the possible
126:57 - so my guess is that you can just use
127:05 - this any of these lost functions as a
127:08 - string right where did I write that Oh
127:14 - Simon it's not me I wrote dad where did
127:17 - I write dad compile oh here I know why
127:28 - did that okay there we go
127:33 - fix Y input shape of eight when we said
127:43 - to oh yeah no it shouldn't have an input
127:52 - shape of eight the into the correct
127:53 - input shape sure before so hopefully I
127:55 - didn't make that way too confusing okay
127:58 - let me come back here okay just to be
128:04 - clear the correct input shape here
128:07 - should be for the input shape should
128:11 - that of the next layer should match the
128:14 - number of outputs the number of units in
128:16 - the previous layer if you think of this
128:18 - as layer one and layer two but I don't
128:21 - actually have to include that because it
128:23 - can be inferred so I just want to be
128:24 - clear I was kind of experimenting to see
128:26 - if I could have it give me an error but
128:28 - I haven't figured that out yet okay so
128:30 - I'm actually in good shape here there's
128:33 - no errors here so let me see what can I
128:35 - do now so the two things that I can do
128:37 - now I have created my model I've created
128:41 - my model and I've compiled it and by the
128:43 - way I couldn't find in the documentation
128:44 - necessarily where there's a list of
128:46 - different loss songs as I could put here
128:48 - but my assumption is that these are a
128:52 - whole bunch of different loss functions
128:53 - I could probably just use any of these
128:55 - as a string so for example if I want to
128:57 - use softmax cross-entropy oh yeah that's
129:02 - one of my favorite loss functions it's
129:04 - gonna like Oh
129:06 - unknown loss softmax cross-entropy so
129:08 - meaning is
129:11 - Nikhil are you in the chat
129:14 - does anybody know I got a pause for a
129:16 - second I don't think it has to be for
129:22 - why because you could add it doesn't
129:25 - matter so why so let me explain this to
129:27 - me
129:28 - what am I why like if I suddenly wanted
129:38 - it in the case where I actually made
129:41 - like six the shape to here these are
129:44 - just some extra random data that I would
129:46 - be required to add in here
129:50 - [Applause]
129:58 - I think I'm gonna click on loss on top
130:00 - to see all losses so I'm gonna match you
130:09 - I'm gonna go all the way back to just
130:19 - I'm gonna I'm gonna try to explain where
130:21 - you can find the list of loss functions
130:23 - and that this input shape 8 thing was
130:26 - like very misleading
130:38 - [Music]
130:39 - so where somebody was saying that I
130:41 - could click on losses at the top I don't
130:45 - know where that is
130:45 - oh here yeah but it's not showing which
130:55 - ones are available as a string so let me
131:00 - just let's just try some of these other
131:02 - ones absolute difference yeah weird
131:15 - it definitely is happy with mean squared
131:20 - error so let me look again can any of
131:24 - these like cosine distance oh maybe you
131:28 - can't use those with this particular
131:30 - optimizer no all right all right so I'm
131:45 - not gonna worry about that too much
131:46 - let me see did anybody so there's a
131:52 - difference Jessa doe in the chat is
131:54 - asking don't you actually have 3 times 4
131:55 - equals 12 inputs there's a big
131:57 - difference between the weights the
132:00 - weight matrices have 4 times 3 but this
132:04 - is what the layers API is keeping track
132:06 - of for you I should mention that in the
132:08 - part just search the dock near mean
132:18 - squared mean square should be near I
132:22 - think it should be possible maybe a bug
132:24 - maybe that's a bug that I found so I'm
132:29 - just going to I'm gonna like gloss over
132:31 - that unfortunately
132:40 - object functions or names of object
132:42 - functions oh so in other words
132:59 - I think I found a bug oh you know what
133:02 - though
133:02 - I think there's a new version of it so
133:06 - let me make sure I'm at least in the
133:08 - current version somebody fact-checked me
133:09 - on my version what was it again shoot
133:19 - mean squared error so let's try this
133:33 - whoops sorry
133:35 - okay it likes that yeah so it's fine
133:45 - with yeah it's fine with it's fine with
133:49 - the other loss functions but not as a
133:52 - string so I think that's a bug yeah so
134:00 - interesting that's really interesting so
134:03 - I'm gonna okay so can anybody so I'm
134:09 - gonna go back again I'm going to redo
134:11 - this part of the tutorial from here and
134:20 - I know how to like fake it now again we
134:23 - can maybe add this as a bug report but
134:25 - what I want to clarify this so here and
134:30 - all right
134:37 - I want to understand is there a reason
134:40 - why you would ever do this or should it
134:42 - give me an error before I move on I need
134:44 - to feel it confident in that my
134:47 - understanding of it was this would just
134:49 - be a mistake and you don't actually put
134:51 - the input shape here because you you're
134:53 - gonna infer it from the units and if you
134:55 - put one there that's wrong it's gonna
134:56 - throw an error I'm waiting for K we
135:01 - c'mon maybe you guys are all um so yet
135:11 - tensorflow digest is brand new so I'm
135:14 - not surprised that there's things like
135:15 - that's always really tricky when you
135:16 - work with like Lauren even worked with
135:18 - p5 you're like oh did I make a mistake
135:19 - or a mistake in the library it's very
135:21 - hard to figure that out it helps when
135:22 - you're live streaming to an audience of
135:24 - lots of people who will help you I know
135:28 - it won't give me an error yeah it might
135:34 - mess with your network okay I think I'm
135:35 - gonna feel confident sort of in my
135:38 - thoughts here comedy I'm gonna go back
135:40 - sorry Matthew for the editing
135:42 - complication but hopefully this makes
135:43 - sense wait I would clarify something
135:49 - really important here because I I kind
135:51 - of put this in here as like a
135:52 - demonstration of some goofiness I was
135:53 - trying to like figure out but the input
135:55 - shape so a couple things let me first
135:58 - let me first mention something somebody
136:00 - in the chat just asked me oh isn't it 12
136:03 - cuz there's four here and there's 3 here
136:06 - and 4 times 3 equals 12 well the number
136:09 - 12 a just sort of wrote that up too high
136:11 - the number 12 is an important one there
136:13 - are 12 connections meaning 12 Waits
136:15 - but this is now we're in the place of
136:18 - using the larious api that is specified
136:20 - for us we just need to say there are
136:23 - four here there are three here and we
136:26 - need to say there are two coming into
136:29 - here and there are four going into there
136:31 - so two is the input shape two here four
136:35 - is the input shape two here now I have
136:37 - this weird eight in my code because I
136:39 - was like kind of messing around like
136:40 - what happens if you put like the quote
136:41 - unquote incorrect input shape and so
136:45 - this really should if I want my network
136:46 - to be to match what I'm drawing
136:49 - there this should be a four but I don't
136:51 - need it because it can be inferred by
136:53 - here and I think what I'm going to do
136:54 - actually now to make this a little more
136:56 - readable is I think it's as much as I
136:58 - wanted to try to write this in a
136:59 - long-winded way I think I'm gonna take
137:01 - the configuration and just put it right
137:03 - here inside the creation of this layer
137:07 - and then I'm going to take this object I
137:09 - mean I don't need to name these objects
137:11 - and do this I think this is actually
137:15 - easier to follow so now you can see I
137:19 - think this is easier to follow
137:21 - right there's and I you know so there's
137:24 - the model there's the hidden layer the
137:27 - output layer I need to specify the input
137:29 - shape always of the first layer and
137:31 - maybe what I want to do is actually say
137:33 - add that hidden layer then create the
137:35 - output it doesn't really matter what
137:36 - order because I'm gonna compile
137:37 - everything so I have the create that
137:39 - let's review did I already do this I
137:41 - might have to do it again I need to
137:43 - create the sequential object I need to
137:45 - make whatever layers I want configure
137:47 - them add them in and then I need to
137:49 - compile it and notice compiling it I
137:52 - need both an optimizer which I created
137:54 - one stochastic gradient descent I could
137:56 - use the add a blonde or any of the other
137:58 - ones again what these are and what the
138:00 - formulas are there's a lot we could go
138:01 - down many different paths and rabbit
138:03 - holes for a lot of depth here but I'm
138:05 - just kind of looking at the higher level
138:06 - point of view here and then I need to
138:08 - compile it and with a loss function so
138:10 - me and squared error being one of them
138:11 - and I actually the all of the loss
138:14 - functions are listed out here this is
138:16 - where they are so for example so I can
138:19 - name it by the string or I can actually
138:22 - just reference the function name
138:23 - directly like this so this should also
138:27 - not give me any errors and you could see
138:30 - now that if I wanted to use like Oh
138:33 - cosine distance I heard that that's the
138:35 - loss function I should be using I could
138:37 - put that in here and I could also hit
138:39 - refresh again and now I'm using cosine
138:41 - distance so again what the different
138:43 - loss functions are why you should use
138:44 - one versus the other hopefully I might
138:46 - get into these things as I start
138:47 - building examples and have to make those
138:50 - decisions but right now I'm just looking
138:51 - at how you put it together okay um so
138:54 - this is going to be the layers tutorial
138:56 - part one I'm going to do a second part
138:58 - to this because all I've done right now
139:01 - is I've cream
139:02 - all I've done right now is I have
139:04 - created the model and I have compiled it
139:07 - the two things that I need to do with
139:10 - this is I need to send data through it I
139:12 - want to put data through it and look at
139:14 - the outputs what are the things I might
139:16 - want to do the two things I might want
139:18 - to do is use predict predict is a
139:22 - function where I give the model inputs
139:24 - and I get out of predict the outputs
139:28 - presumably I would only be doing predict
139:30 - after I've trained the model so I want
139:33 - to train the bottle with some training
139:35 - data it's finished then I can make
139:37 - predictions with new unknown data so how
139:40 - do I train the model I use a function
139:43 - called fit fit is a function that I can
139:46 - basically say I want to fit just like we
139:48 - had to in the linear regression example
139:50 - I had a lot of points and I had to find
139:53 - the line that fits those points I need
139:56 - to find all the weights of this machine
139:59 - learning neural network model that fit
140:01 - the data and guess what this is what
140:03 - tensorflow digest is going to do behind
140:05 - the scenes it's gonna do all of the
140:07 - stochastic gradient descent math it's
140:09 - going to use its own loss function
140:11 - everything underneath the hood so all
140:13 - the stuff I did in that building neural
140:16 - network from scratch now is now done for
140:18 - us by attention flow J s and it has many
140:20 - more sophisticated options so that's
140:22 - what's coming in the next video
140:25 - looking at fit and predict okay thanks
140:33 - okay alright how are we doing now it is
140:40 - 4:30 boy I'm way so I'm a little bit a
140:42 - lot on a half an hour overtime if you
140:46 - will just humor me for a second I have
140:49 - got to send some messages to make sure
140:52 - everything okay
140:55 - livestreaming will be done within an
141:00 - hour okay
141:09 - I'm looking at my email I should be
141:15 - looking at my you know okay so I
141:18 - definitely want to try to do the fit and
141:28 - and predict so I'm weirdly gonna do
141:32 - pretty I assume I could just predict
141:36 - even before I do fit which is silly but
141:39 - just to see it give me something okay
141:48 - all right
141:50 - Thank You Carson in the chat always nice
141:55 - to hear possible I really started off
141:57 - was rough today at the start my melon is
142:00 - all gone I'm out of my caffeinated
142:05 - beverage am i there's some more in here
142:07 - whoo there's plenty more in there oh
142:09 - good this might be what I need
142:20 - okay alright alright let me check the
142:28 - slack Channel and let's go let's move on
142:33 - ah let's move on alright very loud in
142:45 - the hallway can you hear that oh my
142:48 - goodness I guess no one's complaining
142:51 - about the sound yeah so I think some
142:55 - people are maybe watching for the first
142:56 - time what you're watching is my almost
142:59 - my like tutorial recording sessions or a
143:01 - lot of a little bit live and so the
143:05 - there are two ways you'll be able to
143:07 - watch this later
143:08 - one is as soon as I finish this we'll
143:11 - get archived online the full what is now
143:13 - two and a half hour long made possibly
143:15 - three hour livestream and then within a
143:18 - week or so edited versions of these
143:21 - different pieces and tutorials will come
143:23 - out add some comments to your code that
143:26 - is a very good idea Chris ray alright
143:28 - let's do that that's what I'm doing
143:31 - alright welcome to tension flow yes the
143:36 - layers API tutorial part 2 so previous
143:39 - previously on its flow test layers API
143:42 - part 1 I created this model using TF dot
143:46 - sequential TF layers and model dot
143:49 - compile with with a training with that
143:51 - with a loss function and an optimizer
143:53 - now let's just review that really
143:57 - quickly before I go onto the next step
143:58 - which is looking at fit and predict so
144:01 - I'm gonna add some comments thank you to
144:02 - the chat to suggest that so this is the
144:04 - model create the hidden layer add the
144:16 - layer and the hidden layer has a number
144:23 - of nodes input shape and an activation
144:30 - function
144:31 - which I think it's probably pretty
144:32 - self-explanatory then the output
144:34 - layering is a dense layer so dense is a
144:38 - fully connected layer then the out
144:44 - create an output layer create another
144:47 - layer and here I'm gonna write here the
144:54 - input shape is inferred from the
144:57 - previous layer then an optimizer using
145:04 - gradient descent I must have a video
145:08 - somewhere that talks about what gradient
145:09 - descent is if that's not familiar to you
145:11 - and then I'm done configuring the model
145:17 - so compile it I don't know if this
145:20 - tutorial should include to be writing
145:21 - all these comments but it didn't so
145:23 - that's where we are
145:23 - so in a standard parser classic machine
145:27 - learning process I would configure my
145:30 - model well actually before I do any of
145:32 - this I should have like collected my
145:34 - data I've been really thoughtful about
145:36 - that and thought about the ethics of it
145:38 - and why am I doing this saying in the
145:39 - first place is this gonna help people or
145:41 - hurt people I should have been doing all
145:43 - of that but here I'm just looking at
145:45 - hedge fund is an API so I I'm kind of
145:48 - skipping those really fundamentals the
145:49 - most important parts I'm doing things
145:52 - backwards in a way and I'm just gonna
145:54 - make up data so what I want to do is
145:56 - there's two things one is I want to
145:58 - train the model I wanted to adjust all
145:59 - of its weights to fit my training data I
146:02 - have these inputs with these known
146:04 - outputs maybe I'm doing image
146:05 - classification I have all these labeled
146:07 - images cats dogs turtles and I want the
146:10 - model to output cats dogs or turtles
146:12 - some probability value is based on which
146:14 - things that thinks they are and then so
146:16 - that's what this fit function does I
146:18 - could also ask it to do predict which
146:21 - means just take this data I don't know
146:22 - what it is this is not part of my
146:24 - training data and just give me the
146:26 - output so let's do something weird I
146:28 - mean weirder then of what I'm already
146:30 - doing which is talking to myself in a
146:32 - room but the camera and some lights I
146:35 - have like an iPad weird stupid sound
146:37 - effects on it I don't know what's what's
146:38 - happened to me in my life anyway
146:42 - let's let's let's run predict without
146:45 - having trained the model will it
146:46 - actually just does it what does it start
146:48 - with it must have a whole bunch of
146:49 - randomly configured weights right must
146:51 - be configured itself randomly let's see
146:54 - if we can get some some output so what
146:56 - do I need to do to call predict let's go
146:59 - look at the API and let's look at I'm
147:02 - looking at TF sequential sorry I'm not
147:06 - in the right place hold on let's start
147:11 - this over let me get it
147:14 - sequential
147:21 - okay let's go look at the API and let's
147:26 - look at again TF sequential so I'm
147:28 - looking for the predict function now
147:30 - here's the thing to print out the
147:31 - predict function is there so it
147:33 - generates output predictions for the
147:35 - input samples model dot predict
147:38 - okay so config there are some
147:41 - configuration options like batch size
147:44 - and how verbose I want it to be but
147:46 - really all I need are the x's the x's
147:48 - are the inputs and i need two of them so
147:51 - what I'm going to do is I'm going to
147:52 - create a tensor I am going to say let
147:57 - input are abbess a Const inputs equals
148:01 - TF tensor one D and let's just give it
148:06 - some numbers like 0.5 0.25 sorry 0.25
148:15 - 0.93
148:16 - it's a tiny two so I just made up some
148:19 - input C and look all fake not real data
148:21 - at all I'm just trying to look at how
148:23 - tensorflow da chance the layers API
148:25 - works so now I should be able to say
148:27 - model dot predict inputs now let's go
148:33 - back and look at this model dot predict
148:38 - TF one so interesting dip dip I guess
148:44 - I'm good okay so hold on so let's say
148:47 - let outputs equal model dot predict
148:50 - inputs and outputs dot print I don't
148:56 - know could it really be as simple as
148:57 - that
148:58 - let's see
149:01 - and so now I'm gonna go back here
149:03 - uncaught expected dense input to have
149:07 - two dimensions but got array with shape
149:09 - too oh boy I think I might have a big
149:12 - major mistake here let's think about
149:17 - this the input shape
149:24 - I have to think about this dimensions of
149:34 - inputs should match what did I good
149:43 - timing for the cameras to go off
149:51 - wonder what have I done wrong here is it
149:56 - oh it has to be this no cuz it the first
150:05 - to mention is the batch dimension that
150:17 - worked so then I don't have to say this
150:21 - then
150:32 - the shape is supposed to be 2 D should
150:34 - be 2 1 no oh so it's ok because in
150:49 - theory yeah I see ok ok I got it
150:59 - you go back to this error interesting
151:19 - yeah Wow this is this is tricky okay
151:29 - if it's totally true okay sorry all
151:32 - right huh I got an error here that's
151:34 - weird right this is actually Barry this
151:36 - is the kind of error you're going to
151:37 - live with if you continue down this road
151:39 - a lot which is like requires shape blah
151:42 - blah blah shape oh wait hold on so
151:45 - that's not the right air Wow okay all
151:51 - right so what is this error hope oh boy
151:53 - this is the kind of error you're gonna
151:55 - get a lot which is something is wrong
151:57 - with my like shape shape errors error
152:00 - when checking expected dance tense want
152:03 - input to have two dimensions but got
152:04 - array was shaped to what is wrong here I
152:07 - mean after all there are just two inputs
152:10 - I said very specifically that the input
152:15 - shape is one-dimensional with two things
152:18 - in it so why is this wrong well it turns
152:20 - out I forgot about this idea of batching
152:23 - so it is quite uncommon or it's possible
152:27 - that you just want to send in a single
152:29 - data point like these two numbers in and
152:32 - get the prediction and so even though
152:34 - this is the array of the inputs that
152:37 - needs to live in an array itself because
152:40 - I might want to send in multiple sets of
152:42 - them and so actually the correct way for
152:45 - me to put this here is to actually have
152:47 - this be the first element in an array
152:49 - and actually this is not a 1d tensor
152:52 - then it's a 2d tensor so now if I run
152:56 - this we should see there we go this
152:58 - these are the outputs and we can see
153:01 - here every time I run it I'm gonna get
153:02 - different outputs because this
153:04 - particular model is initialized randomly
153:08 - now there's probably a way with the
153:10 - layers API that I could configure how
153:12 - the weights are initial initialize but
153:14 - it's using some default probably random
153:16 - maybe it's a normalized distribution of
153:18 - random numbers who knows we could look
153:19 - up the documentation somebody in the
153:21 - comments will tell me okay so but what's
153:24 - interesting about this is now right I
153:29 - could do I could have
153:37 - right I could now send in four inputs
153:42 - and what will I get out all of those
153:46 - results so these are the three output
153:48 - values for the first input the second
153:50 - input at third fourth so this is how the
153:52 - predict function works so I can create a
153:54 - model and I can start predictions now
153:56 - they're useless and pointless and random
153:58 - without me actually trading that model
154:00 - so that's what I need to do next
154:02 - fit there were some other things I
154:08 - wanted to say about this should I have
154:17 - said 1 comma 2 no 2 comma 1
154:32 - try input shapes feet Oh is the live
154:40 - closed captioning on I was hoping I
154:43 - would get that feature unlocked for my
154:45 - channel
154:45 - could somebody send me a screenshot at
154:47 - that I'm just curious because I guess I
154:49 - could look at it myself but I would love
154:51 - to see how that works or like a gift for
154:53 - a little quick oh great err hair bow
155:01 - hair man in the chat is looking up
155:08 - alright so I'm gonna move on to Fitz I
155:19 - see that K tweaked mine is typing though
155:29 - so I'm just gonna if you have a 2d input
155:32 - shape you need a 3d tensor for ya ya ya
155:34 - ya ya
155:36 - for predict so it's always one for
155:40 - predicting fit okay
155:47 - okay so let's now create a scenario
155:51 - where we have some training data right
155:53 - this is my and and by the way there is
155:55 - this really important piece of working
155:58 - with machine learning where you have
155:59 - both training data testing data you can
156:01 - even actually go on validation data and
156:04 - then you have the new data the stuff
156:05 - that you're making guesses and
156:06 - predictions with I'm not getting that
156:08 - far into it yet but let's just in this
156:10 - case I'm just gonna have some training
156:12 - data I'm not gonna have any testing data
156:13 - although we'll see I mean we're gonna
156:17 - look at fit we're gonna see see how this
156:18 - all works so let me go back now to the
156:23 - to the the looking for where I was ah
156:29 - predict fit here we go so now I want to
156:32 - look at fit so here I look at this oh
156:36 - wait oh wait oh wait oh wait oh wait oh
156:40 - wait
156:41 - I just made some video tutorials about
156:45 - wait so one of the reasons why so
156:47 - working with tensorflow digests natively
156:49 - your one gonna feel somewhat comfortable
156:53 - with the idea of the JavaScript promise
156:54 - and these new es 8 keywords a weight and
156:58 - a sink so I will I'm gonna use those
157:02 - concepts you might want to check my
157:03 - promises playlist if that's new to you
157:06 - okay so let's figure out how we're gonna
157:09 - do this so model dot fit the parameters
157:13 - are X's x and y so here's the thing
157:16 - unlike here and I really should call
157:18 - these X's and this is really the Y's
157:22 - right this is really what I'm doing
157:24 - these are the X's and now I'm getting
157:26 - the Y's to fit I want to do the same
157:29 - thing the difference is I'm gonna have
157:32 - some known outputs so in this case I
157:34 - might have like this is my training data
157:37 - these are the X's and now the Y's are I
157:46 - need to make you know dreamily this
157:48 - would come from a spreadsheet or some
157:50 - type of actual database that I've
157:52 - thoughtfully collect thought about how
157:54 - it to collect and the data and what I'm
157:56 - doing with it but right now I'm just
157:58 - making up dummy data so you
158:01 - one of these eye supplies just put in
158:02 - random numbers like weirdly sort of lazy
158:07 - about this I like to you just sort of
158:09 - see it so let's just pretend my training
158:11 - dataset just has instead of four let's
158:13 - just have three things in it and let's
158:21 - let's just make some arbitrary okay
158:24 - so this is now my training data I have
158:27 - the X's right the X's are there are only
158:32 - two of them again if I were doing my
158:34 - like image classification example that I
158:36 - did previously in a in the with the toy
158:38 - narrow Network library I might have 784
158:40 - inputs for 7 or 84 pixels but here it
158:43 - might made up scenarios two inputs and
158:45 - there's three outputs so the X's have
158:47 - two and the Y's have three and you can
158:52 - see that reflected here so now I should
158:54 - be saying model dot fit the x's and the
158:59 - Y's let's look here now here's the thing
159:05 - there are some there's this variable
159:09 - called history that's kind of
159:10 - interesting so let's say Const history
159:15 - equals model dot fit and I got to get
159:17 - into the weight and all that in a second
159:18 - so what the history is is that's an
159:21 - object that's returned that has lots of
159:24 - information about how the training is
159:25 - going like how accurate are things what
159:28 - happened there if I want to start like
159:29 - looking at the properties of the
159:30 - training what's the current lost that
159:32 - type of thing oh right these mismatch
159:36 - thank you so I just ran about three and
159:38 - three these have to match thank you to
159:41 - the chat for mentioning that to me okay
159:44 - so then I have this idea of batch size
159:47 - so batch size is let's look at that
159:52 - number of samples per gradient update if
159:55 - unspecified it will default to 32 so
159:57 - this has to do with the inner workings
160:00 - of how the gradient descent algorithm
160:02 - works right
160:03 - at some point gradient descent is going
160:05 - to look at the error and it's going to
160:07 - make all these adjustments to the
160:09 - weights and so does it does do that um
160:11 - does it do that after ten data points
160:13 - after 20 after 30
160:15 - - so I'm gonna ignore that and then
160:18 - epochs or epochs or I could never know
160:22 - how to know what to know how to
160:27 - pronounce that word is the number of
160:30 - times to iterate over the training data
160:32 - arrays it's optional I think the default
160:33 - is one so here's the thing I'm actually
160:35 - just gonna let this go without setting
160:38 - any of those those things are going to
160:39 - be certainly important hopefully as I
160:42 - get into future examples or as you have
160:44 - specific scenarios but basically I could
160:47 - add an object here that has things like
160:50 - that has various configuration
160:53 - properties and this would need to be a
160:54 - comma here but I'm going to skip that
160:57 - right now because I believe according to
161:00 - documentation config is completely
161:03 - optional so let's just run this and then
161:05 - I'm going to start talking about the
161:06 - asynchronous nature of this all right
161:11 - what happens if I say console dot log
161:16 - history let's just look at that so look
161:21 - at that
161:21 - I got a promise oh it's it's promising
161:25 - me something so what this means is fit
161:27 - is a function that executes
161:30 - asynchronously now it's possible I can
161:32 - do things with a kind of older style of
161:35 - JavaScript using callbacks because it
161:38 - looks like in the documentation here one
161:40 - of the one of the options I can specify
161:43 - as a callback but I'm gonna use promises
161:46 - so what I'm gonna do is I'm gonna say
161:48 - model dot fit then and what I want to
161:56 - look at is I don't know what I'm gonna
161:59 - just sort of see and in this case I
162:01 - think actually the way that I'm doing it
162:04 - I'm not I'm gonna look at what's sent
162:06 - into the promise so this should now this
162:12 - should be a way this is if it returns a
162:14 - promise I can figure out I can get the
162:18 - result of how its what it's done as an
162:20 - argument to a function that's executed
162:23 - when the promise is resolved
162:24 - so this is how this could look just to
162:26 - look at that history and you'll want to
162:29 - look at my promises videos if this
162:31 - syntax doesn't make sense to you and we
162:33 - can see here looks like oh I have a
162:36 - history object and I have a loss there
162:38 - we go so this is actually the response
162:41 - and what I want to look at is the
162:44 - response history dot loss index zero so
162:56 - looking at this there we go now what
163:00 - happens if I give it a lot of now what
163:04 - happens if I start to add in a
163:05 - configuration like because I'm really
163:08 - curious what happens if I say so let me
163:12 - let me actually use a variable call
163:17 - obviously they call it config and I'm
163:19 - gonna say it two things I want to add is
163:20 - I want to say verbose is true and I want
163:24 - to say a pox is five so because I don't
163:29 - see if it's verbose am I gonna get a lot
163:31 - of stuff in the console that's gonna
163:32 - tell me about what's going on so I want
163:33 - to add in some of those parameters
163:37 - verbose mode is not implemented yet oh
163:40 - okay so I guess I can't use it for Bost
163:44 - mode but I can add five and so this is
163:48 - just the loss after five let's add 100
163:55 - point to negative point two so you can
164:01 - see it's pretty consistent it's some
164:03 - kind of getting about the same being
164:04 - sort of an arbitrary data all right
164:07 - pause I lost the chat
164:28 - I'm sorry I'm looking at the chat see if
164:30 - I missed anything super important
164:33 - EEP ox he box that's all I like to say
164:36 - it okay alright so how if I put this in
164:47 - a loop so let me ask a question to the
164:50 - chat
164:59 - what I want to do is put this in a loop
165:02 - and show the loss changing over time and
165:06 - so oh I have a trailing camera oh yeah
165:12 - oh yeah I'll fix that in a second
165:20 - yeah right why is it negative oh I have
165:24 - a cosine distance all right
165:39 - all right actually something weird was
165:40 - bothering me there for a second which is
165:42 - why do I have a negative loss like you
165:44 - can't have a negative mean squared error
165:46 - right mean squared error has to be
165:48 - positive it's the difference squared
165:52 - averaged over all the data points so I
165:55 - forgot that I still had cosine distance
165:57 - so let me put it back to mean squared
166:00 - error and now we can see that I'm
166:04 - getting something that looks like it
166:07 - could be mean squared error yes
166:08 - so one thing about this okay so I'm
166:11 - gonna get rid of this config actually
166:15 - not you let me go let's try to go back
166:19 - to when I first run this Co no it's fine
166:23 - it's fine
166:26 - what's a good right I'm gonna get rid of
166:28 - this config whoops I'm gonna get rid of
166:33 - this config because what I want to do is
166:37 - I want to look at how what if I want to
166:40 - like run whatever I want to sort of like
166:42 - watch the loss over time and I'm trying
166:46 - to think about the best way to do that
166:47 - but what I think I'm gonna do is I'm
166:50 - going to write a function myself called
166:54 - training train and I'm gonna put this
166:57 - and I'm gonna say Const history equals
167:03 - model dot fit and then I'm gonna add a
167:06 - weight and I'm gonna say async so by the
167:10 - way if I wanted so even though I was
167:11 - using a promise here this is a way that
167:13 - I can put model dot fit in an async
167:17 - function that I defined myself to sort
167:19 - of clean up the syntax a little bit and
167:22 - then I can say return history I guess
167:27 - this is silly is this silly but because
167:29 - what I want to do is call I want to call
167:32 - train recursively I think so I want to
167:35 - say train then ah yes this is exactly
167:38 - what I want to do so I need to just not
167:44 - use the arrow syntax for a second to
167:47 - just sort of figure this out so I want
167:49 - to say train then
167:52 - once I'm done training execute this
167:54 - function that gets the history I want to
167:58 - say console dot log history dot loss
168:01 - index zero and then I want to call train
168:08 - again oh no no and then I want to say
168:12 - yeah then I want to say return huh train
168:17 - oh I'm lost here how do I do this
168:20 - recursively within a promise this is an
168:21 - interesting problem do you see where I'm
168:23 - trying to do when it's done I want to
168:26 - and I'm gonna just add the arrow syntax
168:28 - here now I want to kind of like run
168:31 - train again I could just say then hmm
168:37 - pause for a second kay week nine is
168:39 - pointing me to an example of model dot
168:41 - fit look at this
168:51 - oh of course of course
168:59 - what am i doing the fact the reason why
169:03 - I want to move into an async function is
169:07 - now I can actually put a loop in here
169:14 - so this right the fact that I'm using a
169:18 - weight means I can actually put this
169:22 - here I don't need to return anything
169:27 - right this now what I could do here is I
169:31 - can say I can actually just call train
169:33 - like I don't need anything to happen but
169:36 - I'm going to say train then I think I
169:40 - want to go back hold on I have to figure
169:44 - this maybe figure uh I'm torn whether
169:47 - the whole figuring it out is like super
169:49 - useful or it actually makes more sense
169:52 - for me to like just go back and do this
169:53 - again because it's sort of a mess to
169:54 - edit yeah
170:02 - yeah a loop instead of recursion the
170:05 - recursion idea was like let me go back
170:13 - okay I'm gonna go all the way back to
170:24 - before I added the async function
170:35 - and I'm gonna go okay so now I have loss
170:40 - values that make more sense here's the
170:44 - thing though what I want to do is I want
170:46 - to do this a bunch of times like there's
170:48 - gonna be a lot of scenarios where I want
170:49 - to animate something as it's training I
170:51 - want to graph the loss function over
170:53 - time so how do I do this multiple times
170:57 - well I want to have a loop right I want
171:00 - to be able to say for I mean I could
171:01 - have multiple I could have multiple
171:03 - epochs so I could like add the
171:07 - configuration that gives me multiple
171:09 - epochs oh do I need to good but when I
171:12 - had before sorry but what I really want
171:15 - to do is this I want to just do one
171:20 - epoch at a time so this is weirdly work
171:25 - with like a loop I want to it's really
171:29 - awkward the way that I wrote this I feel
171:31 - like hold on I'm sorry did I delete
171:32 - something back here yeah like let me
171:35 - just run this for a second oh I'm
171:39 - missing another parentheses here sorry
171:44 - about this everybody this tutorial is a
171:47 - mess and 48 oh I had him there already
171:53 - ah zoomed in hey hold on a second let me
171:58 - try this again this one more time
172:12 - yeah that's right model dot fit
172:18 - I'm toil and this is where my brain
172:21 - starts to die yeah there should be
172:24 - another parentheses here there we go
172:28 - okay go back to the chat
172:49 - all right so now I'm getting lost values
172:52 - that make more sense and but what I
172:54 - really want to do is I want to see the
172:57 - lost values over time like as I'm
172:59 - fitting the model I'm running the
173:01 - training data in multiple times maybe
173:03 - I'm shuffling the order there's all
173:04 - sorts of things that I've done in
173:05 - previous videos that I want to do with
173:07 - tenth afloat yes so how do I have this
173:09 - model that fit multiple times I mean I
173:11 - could just call it twice right
173:13 - ooh oh it did not like that whoa
173:18 - Oh fascinating so I can't do that I
173:20 - don't know what's wrong with that but
173:21 - that's not really what I want to do
173:23 - anyway I could try to add a little work
173:26 - by the way the chat is really upset that
173:28 - I have these extra commas here so I'm
173:30 - gonna remove them JavaScript doesn't
173:31 - care it's like put your commas wherever
173:33 - you want like I had a lot of commas
173:34 - right oh no no no it doesn't want extra
173:38 - commas because it has blank entries but
173:39 - okay so so what if what if I were to put
173:43 - a loop here say like oh I want to do
173:46 - this two times so I want to fit the
173:51 - model twice again I got an error oh it
173:56 - really doesn't like this so here's the
173:58 - thing I I'm not exactly sure what this
174:00 - error is but uh maybe I'll somebody will
174:02 - tell me in the comments but I was going
174:04 - down a road I didn't want to go down
174:05 - this is where using wherever it was here
174:11 - in the tensorflow Dutch ass the await
174:17 - keyword is crucial so what I want to be
174:20 - able to do is I want to be able to call
174:23 - this fit inside a loop but promises just
174:26 - like everything's happening
174:27 - asynchronously becomes very hard to
174:29 - follow
174:30 - so actually what I really want to do is
174:33 - I want to go back to that syntax of
174:35 - saying Const history equals model dot
174:40 - fit and then this is what I want to do I
174:46 - want to say then console dot log history
174:51 - plus okay this is what I want this is
174:54 - like synchronous code blocking code this
174:57 - is what I want I want to fit the model
174:59 - and then see the results
175:00 - it won't do this because that's not how
175:02 - JavaScript
175:03 - works asynchronously so there is the
175:05 - awake keyword which is new at ESA which
175:07 - says hey wait for this and then to the
175:09 - next line of code right shouldn't that
175:10 - work whoops and a weight is only valid
175:15 - in an async function so again if you
175:17 - watch my async and await tutorials you
175:19 - would know this but I cannot just do
175:21 - this anywhere in my code I have to
175:23 - create a function a function with the
175:28 - keyword async I'm gonna call it train
175:30 - and I have to do this inside that so
175:33 - this is now the correct syntax this
175:37 - actually is valid but it's in an
175:39 - asynchronous function I have to call
175:40 - that function so I could just call train
175:42 - let's just run this now and oh cannot
175:49 - reap repartee zero of undefined and
175:50 - train so what do I have wrong let's put
175:55 - the X's and Y's in that function
176:06 - oh that's not the problem history hold
176:09 - on let's just let's just look at the
176:12 - history oh yeah it's their history o dot
176:16 - history right I forget you forgetting
176:17 - this is the response so it was fine
176:20 - actually
176:21 - history dot loss index zero so I just
176:26 - want to look at just the loss so these
176:28 - don't need to be in here
176:29 - these I don't want these in here so now
176:33 - we run this again there's the loss now
176:37 - here's the thing I also just want I'm
176:39 - just gonna put it then in here because I
176:44 - also want to have some sort of event for
176:46 - when the training is complete so this
176:52 - function now kind of magically returns a
176:55 - promise because of the way that await
176:56 - and async work so I want to make sure
176:59 - this is working great and now training
177:01 - is complete now guess what I can now do
177:04 - this I can do this ten times so I can
177:13 - now loop can go in here because of the
177:15 - beauty of this await syntax this is now
177:18 - work functions as if it's blocking code
177:20 - when it's all complete it will return a
177:22 - promise there we go we can see this is
177:27 - what we should have the loss should be
177:29 - going down right let's change that
177:31 - learning rate somewhere I set up a
177:33 - learning rate let's make the learning
177:35 - rate 0.5 you can see the loss is going
177:41 - down and maybe let's go back to 21 let's
177:44 - give it like a thousand basically a
177:47 - thousand iterations and we can see the
177:51 - loss is going down over time so and now
177:54 - what I could do I mean a thousand is a
177:58 - lot the training is now complete
177:59 - now what I could do is I can call
178:02 - remember this I can now call predict now
178:09 - again I'm only working with one data set
178:12 - my training data is my testing data is
178:14 - my validation data is my out by regular
178:17 - data that I'm using in the future once
178:19 - I'm done
178:19 - the model so but we now see the full
178:22 - process
178:23 - I have X's and Y's I can call model dot
178:27 - fit again this is a really hairy stuff
178:29 - not for the faint of heart I'm in the
178:30 - weeds of those sort of like lower-level
178:32 - tensorflow dot J s stuff I mean even
178:34 - though I'm in the layers API but using
178:36 - yes eight syntax but now we should be
178:38 - able to see let's take a look at
178:43 - oh and it's I'm gonna have an issue here
178:45 - where I have these Y's so I'm gonna call
178:48 - this I'm gonna call this outputs so
178:52 - let's take a look let's train it just a
178:53 - hundred times whoops oh whoa
178:58 - silly me look what happened cipher
179:00 - totally forgot about the asynchronous
179:01 - nature of all this stuff and I got the
179:03 - predictions before the training finished
179:05 - right because this is happening
179:07 - asynchronously which means what I want
179:11 - to do is after the training is complete
179:14 - then then I want to do my prediction so
179:19 - I want to train using the asynchronous
179:20 - training a hundred times then I want to
179:22 - do my prediction train train train train
179:26 - the coding trade and then let's look at
179:29 - this so how do these numbers match with
179:31 - what I said the the outputs should be
179:35 - yeah not so great not so great huh but
179:39 - you know I did my best
179:40 - maybe oops
179:48 - but really the issue here is simply that
179:51 - I'm the I'm working with such like fake
179:53 - for ten small arbitrary data that I
179:56 - can't really do any training an
179:58 - inference here in any meaningful way by
180:00 - the way inference is another word for
180:01 - what's happening here with this predict
180:03 - function but I'm hoping I mean I just
180:05 - wanna while I'm talking I I'll let this
180:07 - run one more time with like you know
180:10 - let's try three thousand three that
180:13 - training it and and let's actually let's
180:17 - try it with three thousand so I'm gonna
180:22 - let this the loss function go down so
180:25 - much further so okay so hopefully now
180:28 - you've seen right the first app I can't
180:39 - refresh the page until it's finished
180:48 - so while this is training hopefully now
180:51 - you've seen here that I have now in
180:53 - these two tutorials covered the basics
180:56 - just the basics of the layers API the
180:58 - layers API you can create a model or TF
181:01 - sequential or TF dot model that has some
181:04 - amount of layers that can be dense
181:06 - layers or other kinds of layers I
181:07 - haven't explored yet I can configure
181:09 - those layers I can add them I could
181:11 - compile the model and once that's done
181:13 - if I have data I can fit the model with
181:17 - that data and then I can ask for
181:19 - predictions with new data and this is
181:22 - what I'm hoping to do so I've got to
181:23 - come up with some actual real-world
181:25 - scenarios and the first thing I'm gonna
181:27 - do is a coding challenge would basically
181:29 - redo all this again is I'm going to try
181:32 - to just do the XOR problem so I have an
181:35 - XOR coding challenge already and I'm
181:37 - gonna do that so that video will be out
181:39 - at some point in the future or right now
181:40 - depending on when you're watching this
181:41 - let's go back and check so the loss got
181:46 - all the way down to 0.5 and we can see
181:49 - the outputs here 0.23 the outputs are
181:52 - very similar for each inputs so I'm just
181:55 - going to assume that the problem I'm
181:58 - having is just the fact that I have just
182:00 - this tiny little bit of data and I also
182:02 - trained it for a very very little bit of
182:04 - time and I might not but actually I want
182:08 - to look for the shuffle option before I
182:10 - go so that's really important I don't
182:12 - know if it's doing it by default by
182:13 - default when you are yeah in this fit
182:19 - there is an option called shuffle
182:22 - whether to shuffle the training data has
182:24 - no effect optional so the question is is
182:27 - the training data shuffled by default
182:29 - when you're training with the same data
182:31 - over and over again if you keep the data
182:33 - in the same order that can really be
182:35 - problematic in terms of how things work
182:37 - how well the it performs so I want to I
182:41 - want to add that so where where do I add
182:44 - that that's real that's important so
182:46 - right here I'm gonna add shuffle true so
182:52 - it's basically just a parameter just a
182:56 - one one print one a field of this
182:58 - configure
182:59 - an object where I want to say shuffle is
183:00 - true and let's just let's just do it a
183:03 - thousand times and go back and let's see
183:09 - if I get that loss much further down
183:15 - yeah not really
183:22 - I'm gonna do one more thing which is I'm
183:27 - going to add a pox 100 so each time
183:34 - through this I'm gonna do it a hundred
183:35 - times and then I'm gonna do that a
183:37 - hundred times so that should give me ten
183:39 - thousand times just to get it going
183:42 - further yeah I think I think this is a
183:57 - fool's errand I'm never gonna get this
183:59 - to produce overfit let's overfit my data
184:05 - so uh mat-su we're gonna get rid of all
184:08 - this stuff from the end here
184:13 - [Applause]
184:21 - all right whoo okay
184:31 - yeah that's like kind of point four
184:33 - point three point four can't remember
184:38 - what I said it was yeah I mean how could
184:47 - you this makes notes okay my data makes
184:50 - no sense
185:03 - so this ended and you could see this as
185:06 - a fools I'm at a fool's errand here it
185:09 - makes us it I got these like little
185:10 - three data points with two numbers in
185:13 - them and I'm trying to get three numbers
185:14 - out of each of those and train the model
185:16 - and then I've looked it's this is this
185:17 - is never gonna go anywhere so I'm gonna
185:19 - do this again in the XOR example you
185:21 - have one more idea just to make it I
185:23 - wanted to get a result that makes sense
185:24 - one more idea just give me give me a
185:27 - second here let's just change this model
185:29 - to have the output just have one output
185:33 - and so now the wise like I'm gonna say
185:38 - 0.1 0.1 I want that to give me like 0.9
185:44 - and then I want to say like 0.9 0.9 I
185:48 - want that to give me point 1 and then
185:52 - I'm gonna say like 0.5 0.5 give me 0.25
185:59 - so this is now going to be my data but
186:02 - simpler like I is it it has it like a
186:05 - very simple linear relationship so I
186:07 - think that I should be able to now I'm
186:13 - going to go back to one Apoc I'm going
186:16 - to do this just 200 times and we're
186:19 - going to now fit this model and here we
186:24 - go
186:25 - oh I really got it let's let's let's
186:28 - increase the learning rate man let's
186:31 - increase the learning rate let's do it
186:34 - let's do it a thousand times
186:37 - Teddy box each all right I'll be back in
186:41 - a little bit
186:41 - oh look at this look at that loss I
186:43 - finally got somewhere see this is the
186:46 - machine learning is hard all these
186:47 - parameters of what you're doing but this
186:50 - is going to end in a second I'll be back
186:56 - this can be sped up this can be sped up
186:59 - as its training over and over again this
187:03 - is so silly I don't need to watch this
187:04 - for the whole time by the way I'm gonna
187:06 - be gone going I'm not going to do the
187:07 - XOR thing today because it's already
187:09 - 5:20 and I'm late
187:14 - anybody have any idea where I am in the
187:17 - number of I got what would you estimate
187:19 - now how long is it gonna take me to get
187:21 - to oh there we go
187:25 - yeah look I need it do something I made
187:29 - it match the exactly over fitted my
187:31 - model basically but I made it match the
187:33 - exact output so my issues were really
187:35 - learning rate and the amount of time I
187:38 - gave it to train and that sort of stuff
187:40 - so hopefully now I'm really at the end
187:42 - of this video
187:43 - apologies that this was maybe a bit
187:45 - convoluted in terms of how it came
187:47 - together if it came together at all
187:49 - but now I've really completed my two
187:52 - tutorials on the layers API and in the
187:54 - next video I will do some coding
187:57 - challenges and actually use the layers
187:58 - API for some hopefully some more
188:00 - practical examples all right see you
188:01 - later good bye yeah
188:16 - Carson is giving me a good some good
188:18 - feedback I mean let me I I have like me
188:24 - I'm gonna go back to I'm gonna do this
188:36 - whole last section again just in case so
188:51 - let me go back
188:57 - just cuz there's a lot of mess there how
189:00 - am i doing this stuff twice
189:10 - so at some point
189:21 - oh right
189:28 - I'm gonna go
189:53 - I'm gonna go from here okay all right
190:07 - Matt you what I'm going to do now is we
190:10 - go all the way back back back back back
190:12 - back back back back to basically the
190:15 - first time I ran it pudding predict
190:20 - inside of here okay
190:27 - so I didn't weigh back back back back so
190:31 - I'm gonna do that
190:38 - okay
190:43 - alright here we go alright so this is
190:48 - running we can see that the loss is
190:49 - going down it's training over time and
190:52 - when it gets to the end I've got the
190:55 - results of my predictions now you can
190:58 - see these don't look very good like
191:01 - first of all these don't resemble these
191:04 - outputs at all the main issue here is
191:06 - that nothing here makes sense I just
191:08 - have the skeleton of the story of data I
191:12 - can fit I can create the model I can fit
191:15 - it with some data and I can ask it to
191:17 - run a prediction with that same data but
191:18 - I'm ignoring like really important
191:20 - considerations and you can see that
191:22 - machine learning isn't just magic it
191:23 - doesn't just do the right I don't care
191:24 - that was the right answer so let me at
191:26 - least before I go add a couple things to
191:29 - so that we can actually see that it got
191:31 - somewhere so one thing that I want to do
191:33 - is I'm gonna just simplify what's going
191:35 - on here I'm going to make this data
191:38 - something really obvious like zero zero
191:41 - one one point five point five that's my
191:46 - X's and then my Y's I'm gonna I want to
191:49 - get I want to get one I'm gonna just
191:52 - change that to just one output point
191:54 - five and zero
191:58 - whoops what I missed here yeah
192:07 - oh boy syntax there we go so what I'm
192:10 - doing here is I just try to like come up
192:13 - with some ridiculous scheme that's like
192:15 - super simple to learn like they're sort
192:16 - of like an inverse linear relationship
192:18 - here zero goes to one it's like the map
192:20 - eventually let's see if there's a neural
192:21 - network can learn the map function but
192:24 - in my using two numbers and one numbers
192:26 - here but whatever so now I am going to I
192:30 - have to change the output to have just
192:32 - one output and now let me run this so we
192:39 - could see look at this the loss function
192:40 - is still pretty high so there is
192:42 - something that's kind of important here
192:44 - you can see it's going down and actually
192:46 - it like a thousand iterations it it kind
192:50 - of got something it got something pretty
192:51 - close so I would just need to give it
192:53 - more time to train to kind of reproduce
192:56 - the known results but I just want to
192:59 - show a couple important things here one
193:01 - thing that's important is I probably
193:03 - should always shuffle the training data
193:07 - so there is actually a kind of important
193:09 - one of the configuration parameters here
193:11 - for the fit function like if I shuffle
193:17 - is true what this will do is whatever
193:19 - each X's and Y's are each time it puts
193:22 - it through the training function they'll
193:24 - do it in a different order and this will
193:25 - really help things as you're trading
193:27 - with the same data over and over and
193:29 - over again so let me add that in and
193:32 - just see what happens there so I'm gonna
193:34 - add this config here to here so this is
193:38 - shuffle true that's the only thing I've
193:39 - turned on doing one epoch at a time
193:42 - still so let's also while I'm here just
193:46 - to make things go a little faster let's
193:48 - add let's do like ten a pox at a time
193:52 - once again total here a thousand times
193:55 - 10 is 10,000 epochs right but because I
193:58 - want to see it over time I'm not just
194:00 - putting not just doing this once with
194:02 - the number 10,000 there I'm doing it ten
194:04 - I'm doing it a thousand times with ten
194:06 - but this what I think will make things
194:07 - move a little faster and we could say I
194:09 - got even better here and so now we can
194:12 - see this over time we can see that the
194:17 - the loss is really going quite far down
194:20 - now
194:20 - have to sit here and wait for a minute
194:21 - which I'm gonna do but you're not gonna
194:23 - have to this video will now speed up but
194:32 - his discussion is going on in the chat
194:34 - right now I'm gonna check my email while
194:45 - this is going
194:54 - Thank You grant Roz Marin for sponsoring
194:57 - my channel alright I'm back so look at
195:09 - this I reproduce those results right at
195:12 - reproduce the same results that my
195:14 - training data produces should have
195:16 - produced and I got the loss down pretty
195:18 - low so you can again completely
195:20 - meaningless trivial nonsense nothing but
195:23 - hope I'm just I'm hopefully this is
195:25 - helper to you it's helpful to me because
195:27 - I'm trying to figure out just how the
195:28 - library actually works itself so let's
195:30 - review for a second okay let's review
195:33 - everything I have shown you now these
195:38 - two videos that I can create an empty
195:40 - model right the diagram of my neural
195:45 - network architecture is an empty model
195:47 - then what I could do is I can create
195:51 - layers there can be dense layers or
195:54 - other kinds of layers that I might look
195:56 - at in another time in the future and I
195:58 - can add them the order that I add them
196:01 - is very important because it is a
196:03 - feed-forward sequential model so this is
196:07 - first in this a second beef you know I'm
196:09 - thinking of this is hid in an output but
196:10 - they're really just layer 1 layer 2 ok
196:14 - once I've done that I have to define how
196:18 - what sort of mathematics are going to be
196:21 - used to train the model and there's a
196:24 - loss function mean squared errors what
196:27 - I'm choosing to use and opt an
196:28 - optimization function stochastic
196:30 - gradient descent with a learning rate of
196:32 - 0.1 that's everything from part one and
196:35 - now what I've done here is I've shown
196:37 - you okay if I have some data presumably
196:39 - coming in from a spreadsheet I turn
196:41 - those into tensors I can use the fit
196:44 - function to train the model I can say
196:48 - fit all of the weights with these X's in
196:50 - these Y's
196:51 - once that's complete I can then ask it
196:53 - to predict with presumably new data this
196:56 - skeleton has not used any meaningful
196:59 - data I'm also not really being
197:00 - thoughtful about like well what
197:02 - activation function makes sense or what
197:03 - optimization function makes sense
197:05 - different things work
197:06 - different scenarios so hopefully we'll
197:08 - see that more in the future as I make
197:11 - more videos I would encourage you to
197:12 - just try to do this maybe with some
197:14 - actual data play around with it try
197:17 - different input shapes activation
197:18 - functions play around a little bit see
197:20 - what you get
197:21 - let me know about the results in the
197:22 - comments let me know if this was helpful
197:24 - to you and I will see you next next
197:26 - video I'm gonna do I'm gonna look at
197:27 - that XOR problem again in the context of
197:29 - tensorflow doubts yes okay good bye
197:32 - [Music]
197:35 - alright everybody I don't know not sure
197:39 - if you're still watching thank you John
197:44 - you only look once if you were still
197:48 - watching this match yeah hopefully this
197:52 - could be put together and the fact that
197:53 - I did things that section and twenty one
197:56 - thing a little worried I did is I kind
197:57 - of recaps twice so he could cut out my
197:59 - earlier recap but uh you want to seem to
198:01 - figure this out so but if this is really
198:04 - problematic this last section I can
198:06 - always do that again alright everybody
198:12 - unfortunately because I have to go home
198:14 - I don't have time maybe I could bring
198:17 - the stuff home with me I don't have time
198:22 - to live stream the my experiments with
198:28 - these life I've my brain some so brain
198:31 - dead right now so so yeah Sarah much in
198:43 - the patron group and the slack asks who
198:46 - is matchy-matchy ax is math blank and
198:52 - match it is the coding train I don't
198:56 - know what the title should be but video
198:59 - editor coding train channel organizer
199:02 - manager extraordinaire oh the sound
199:11 - effect was glitching
199:17 - the sound effects are glitching again
199:18 - tell me if they are sorry this is might
199:20 - hurt your ears turn your volume down
199:22 - three two one
199:31 - so yeah is that gonna be a problem why
199:40 - does that happen after like a while I
199:42 - think it's to do it though where these
199:46 - cables are it sounded kind of find that
199:53 - alright
199:55 - you have nothing changes at all except
199:58 - you have consistent commas at the end oh
200:01 - shoot that's gonna drive people crazy
200:09 - yes choppy
200:15 - I'm so confused by all the chat stuff
200:18 - that's going on you know it sometimes
200:22 - the glitching is actually just in the
200:24 - live stream and the the video that I'm
200:28 - recording might not have the glitching
200:30 - that's a wish that might be wishful
200:32 - thinking but that's happened before
200:35 - that's probably wishful thinking because
200:37 - hold on this is gonna it's gonna glitch
200:40 - again okay but I'm gonna I'm gonna
200:42 - listen to my monitor three two one
200:49 - oh boy that I'm really loud I'm
200:52 - listening
200:53 - it's that well three two one
201:01 - yeah so I'm just gonna at the moment
201:04 - hope that the glitching is not in a
201:05 - recorded video so when this gets edited
201:07 - all right I have to go there's so much
201:11 - activity going on in all these different
201:12 - chats I can't keep track of anything
201:14 - I'm completely brain-dead this is
201:17 - suddenly but this isn't happening like a
201:18 - four hour livestream so you know how I
201:20 - said I was gonna do two live streams a
201:21 - week I want it I was doing that because
201:23 - I wanted to do two hours at a time twice
201:25 - instead of four hours because everything
201:27 - goes you know I really like this is uh I
201:36 - didn't do a good job of like this is how
201:39 - I feel right now so but this was to live
201:44 - streams this week because I did it for
201:46 - like four hours and I will see you all
201:51 - next week probably Friday not Tuesday
202:00 - possibly Wednesday not Thursday possibly
202:03 - Friday so so yeah okay I got to go I'm
202:13 - gonna play the outro and see you all
202:18 - well usually I try to answer some
202:20 - questions and stuff like that but it's
202:22 - already 5:30 I just can't I will train I
202:26 - wish I had that ukulele so I could sing
202:27 - you out
202:29 - but I will load the train whistle still
202:33 - waiting for the water ripples and tfj s
202:35 - that might never happen at this point
202:37 - but I really interested in that I will
202:39 - come back and do that
202:40 - I mean maybe I'll do that when I get to
202:43 - like convolutional neural networks
202:47 - [Applause]
202:53 - [Music]
203:03 - [Music]
203:14 - [Music]
203:27 - pick you on the fastest station will
203:29 - make things that are all creation
203:43 - [Music]
203:48 - [Music]
203:51 - you
204:00 - you

Cleaned transcript:

sara is asking any luck with the lights no luck with the light but I will talk about oh and welcome to is it Thursday Thursday afternoon coding train you would think the fact that it's the summer my teaching schedule is freed up that I would be more consistent about the time less late strangely enough my life is just not working out that way so I am apparently less consistent about the time and more late than usual so I have to apologize for that so but the good thing is I'm here I mean I guess that's good you're watching so hopefully that's what you were hoping for I have I have all my supplies train whistle water warm beverage sometimes caffeinated sometimes not caffeinated you can try to guess back to the lapel mic which is here on my coding train branded hoodie and of course not of course but I have today I have a melon medley from an unnamed local shop that sells food items and I paid 359 for this we really bugs me you know I can never get myself to bite you know you go to the grocery store and they have the the bin of sliced melon and it's like $12 for like this large thing of sliced melon which is approximately a single large melon entirely sliced these single melon itself is usually like four or five bucks I can't pay six or seven bucks to have my melon sliced I have to slice it myself soft focus it might be that the focus is messed up there was some weird stuff going on in this room so if the focus is bad I I will fix that let me know if the focus is weird I will I will take a moment look at this this is regular nonspace melon it is of the water variety which goes well with Cody Trent's main sponsor h2o a little bit upset today everything's off focus no but nobody saying anybody's voice now I also have the space Bella the mysterious magical space melon alright alright so this is gonna be tricky but let's see if we can figure this out the good news is I didn't actually put the cloaking device on this yet so this is approximately where I stand you know about here so if I focus on this right here I should be able to fix that focus so let's see if I can do that it's that better thank you for promise I can't see the output very easily oh that looks better let's see how that is have I improved the focus of course when I stand back here I'm a little bit out of focus but such is life I'm hoping this has improved things I'm gonna pop out this oh I don't even have the chat here oh yeah okay hold on hold on everybody pop out I'm sorry I really should get ready before I start but today I just didn't happen all right now oops let me go back to here let me get the chat on my monitor I have to do that um if I go to look at the live stream I'm live right now I go to that page I see myself in the past and then pop out chat so now I can see the chat all right burgerbob in the chat I just pulled the chest saying 20 seconds slow mode is too much please make it 10 seconds I have no issue with doing that whatsoever let me see if I can adjust that easily you know unless the chat is presenting a problem I'm happy to decrease the slow mode I'm just curious if anybody has a youtube sponsor does the slow mode apply to you or no maybe this is under advanced settings enable slow mode limit chat post to every 10 seconds per person let's try that all right so I switched it to 10 seconds let's see how that goes all right let me move this water over here now here's the thing uh one of the reasons we'll have late today it was actually early not really I was here at 1 o'clock to give myself a whole hour you would have thought that maybe I would use that time try to see if I could fix the whole thing with the camera shutting off but of course I did not now be much too practical and useful and logical for me so what I have done instead yes it applies to sponsors okay no who knows I don't know what this means so so I have this light oh we can't turn this off yet this is a Phillips Phillips hue light 600 lumens I have it clipped right here to my table welcome to sponsor Meyer Shaw see this is the thing I was so excited I almost want to tell you Meyer to unspun sir maybe sponsor again later because I also have over here this thing which is a Philips a hue bridge this is the thing that connects to the internet and controls the light it's easy for a second there it's like a six were warm in here and anyway okay I'm fine uh not sleeping well this week all right so what was I saying about this this I would connect to the internet it would control the light then I could use an app on the phone or I could write like a node program all sorts of ways I could control the light in this room and in fact I could use IFTTT which is if then this that what that stands for something something like that I could use that to every time somebody sponsors to blink the lights and possibly I could do something else like if you're in the slack channel and you need to alert me to a horrible bug in the code or the camera shutting off I could set up something like sort of slack bot messaging system to blink the lights but if this then that okay that makes sense if this then that thank you Chris Persian the problem is the problem is I'm starting reading this slack channel at the same time Simon I have seen your barns leaves fern message thank you I'm kind of right now I'm staying away from the matrix graphic stuff with Tesla yes although I would like to return to that but the this is blocked on the NYU Network so I haven't figured out I was trying different ways of like setting up my own network or like getting a network from like on the laptop to give a network but none of that was working so I have to give up now here's the thing I'm thinking about when at least first as an experiment you know maybe I'm here till 4 o'clock which is like an hour and 45 minutes from now in this room streaming I can get this to work downstairs on the 4th floor of this building in my office down there because we have a special sandbox network that I can do connect to things in different ways that the larger NYU network does not work for I also was thinking the reason getting some of these from i/o no if I want people blinking my lights from the internet into my house but so if you're if after today if it's like 400 and I'm kind of things have gone alright I might consider going downstairs and just live streaming from my laptop webcam just to test out the light thing so you might get that to work okay song that I sorted by sent me a super chat do not see that I see a sponsor I don't see that but I believe I believe that it did happen yeah so people I don't want to belabor the whole sponsorship thing but YouTube has a sponsorship feature you click sponsor you get an invitation to the slack channel and I'll mail you some stickers I also have a patreon which is basically the same exact thing you just give your money there instead of there but you don't get all of the the patreon thing has some benefits that YouTube doesn't have but YouTube has all the emoji integration with the chat stuff all right the Philips you bridge that is correct I've never used one of these before so what am I gonna do today what I really like to do is always like to know where I'm not gonna do this but this is what I would like to do I'd like to lie down on the floor I do something like meditation breathing and stretching exercise I feel like I have not found the right I have not found the groove Stella does not have her groove back yet but maybe it will happen today if I start doing some coding all right let's see if we can get the whiteboard camera going yeah yeah buyer it takes I have to send the slack invite which is not so easy for me to do actually I could actually do this during the livestream wakes people time too much but I the the the slack of Nations have to be done manually there's no automatic process for that so if you go to the community tab there is a post there that sponsors only and there's a Google Form putting your email address there and then I'll send you a slack invite if if I if anybody watching has the sort of power authority to figure that out in a different way and invite new sponsors of slack we could we could make that happen but I probably can't do that during the live stream all right there we go all right so I want to continue with tensorflow Jas today and I want to focus on something called the layers API so let's see where let's see what I have so far I'm going to go to this YouTube channel I heard of called the Coty train and I am going to go to neural networks and machine learning session six tensorflow je s and so this is what I have so far you know what I think this might be a tshirt day I think that this hoodie is just it's extra warm in here if it's the New York City weather for what and I'm gonna switch to tshirt that's gonna it's gonna make everything all right again Oh much better you know what it is I was rushing also and is I'm on the ninth floor now and I was down in the fourth floor and I'd go back and forth just took the stairs you know I made a half marathon a few months ago I I'm doing ok but somehow taking the stairs up and down a couple times I think maybe doing that right before a live stream isn't the best idea alright oh you can see this is the light right here see unfortunately I'm gonna just I'm gonna since it's not operable I'm gonna just put it down to the side ah polynomial regression we should change we should do that right I was gonna go right into the layers API and I was gonna go right into the layers API and then maybe do like XOR as a kind of hell open I don't want to use the word hello world but as a kind of basic example to see how the parts of tension flow digest the layers API works but I kind of like this idea of trying polynomial regression let's let's look where I last left off with the linear regression example and see how much that would take okay TF when your regression one of these days I want to do a live stream I say this every time where I want to like update my workflow okay so if we look at the code for this linear regression example so there's some interesting discussion going on in the slack Channel about there is a IFTTT app applet for triggering a light from a YouTube sponsorship so that exists everything else I'd have to do custom and I'd love to do some tutorials about how to do that but that's why I thought I could just get that out of the box but one limit one thing that you have to have working in order for that to work out of the box it's a working light all right informal poll let's see so what would I have to change here I would need some more per amp TF variables I would the loss function could be the same the predict function would be different oh and graphing it would be different so right here I would write the code for a polynomial equation so that would be like something like let's do it let's do that oh let's do that why not why not that's gonna be fun loads of good times for all let's go where's that what's that playground what's that tensorflow playground cuz I wonder if that has a yeah I mean basically what I'm doing is something like this what I want it's like creating an example like this okay so we are now let me just change a few things I don't know if this makes sense to be like a coding challenge part two of that but just go any important comments or feedback feedback before I start okay all right hmm why is this better all right Oh nope all right that's good this just one sec I think I need to read some random numbers it's been a while that's gonna help Randy numbers and melody those are the things you know I'm gonna I'm gonna look at my youtube analytics after this because this will be the point where the graph of the viewership 31,000 okay break album where's the sheet on this one eight hundred thirty seven seventeen thousand eight hundred thirteen whoo music ended anything I'm a person on the internet reading from a book of random numbers eating melon which nobody really likes to be perfectly honest even though I do and people the chakra talk about weather very tippy is like the best editor ever or something like that come on people get with the program free yourself from these worries about your text editor here's the thing this I like the honeydew I like the watermelon this is cantaloupe really it's part of the medley I'll eat it but 13,000 I heard done mostly 1503 55000 I think I could actually read the whole book if I just had a melon although it's probably a little bit down which melon one should eat before you know take off your problems sleep four thousand four seventy I need to save some of this for later if I'm really gonna get to solving extrovert with Vincent Lopez eighty two thousand five hundred fifty six twenty two thousand six hundred ninety seven fifty four thousand nine hundred eighty five sixty four thousand eight hundred fifty two you know when I was in college right click this like performance art / dance class and I was taught by the founders of the dance troupe Pilobolus and the things I've ever heard of pilobolus and I had Joe performance for it and my performance involve a simple Huck bag full of rice rubber banded to my face and there was like a song I have rice on my face that I made up that went along with that so in a way it all led till now Oh welcome I can't believe you bought for sponsoring this nonsense you are all lunatics that's all I have to say to you you I know I am a lunatic but I thought you were very sensible all of you watching I was a little malfunction there's a little hole here that where the liquid comes out and if you don't have that aligned with your mouth perfectly kind of say but I gotta get a beer trimming you know so many things are wrong no the problem is the dance skills through here's the good news I'm having these back problems so I'm gonna just got an appointment for some physical therapy to strengthen core muscles stuff but the good news is standing is kind of the most comfortable for me so that's what I'm doing now all right cracking myself up hello I am in this video going to do something not advisable because really I should be moving on making some interesting weird creative projects that make use of machine learning in ways I could never possibly imagine but I'm not ready for that yet I'm still in the world of implementing kind of technical demonstrations and examples to understand how things work I kind of maybe I'm stuck in that place for a little too long and you should break free of that place and leave here and go somewhere else but if you want to stay that's what we're gonna do so what I'm gonna do in this video you might you might recall that I recently did a coding challenge where I implemented linear regression the idea of linear regression is to have a data set in this case my data set is a point a bunch of points that I'm clicking I'm just sort of adding arbitrary data to a twodimensional plane by clicking the mouse but this could actually represent the xaxis could represent you know think about baseball pitching you know that the speed of a pitch and this could represent the amount of time it takes to get to home plate although that would be inverse anyway you get the point whose actions could represent data so the idea with linear regression is oh can we fit a line that can we can we make a line that fits that data and the formula for line is y equals MX plus B so the variables are of our system are M and B and we need to adjust those variables and we're going to use green a loss function that we try to minimize to follow blah blah blah that's all in the previous video so I thought well we're here why not just see if we can make a curve to fit that data this otherwise known as polynomial regression so what do I mean by polynomial agression regression it would be good if I had a marker this is definitely not the right marker I shouldn't mention that I'm really partial to berries like blueberries blueberries I'd love to happen they're like much more acidic so the belen is more soothing hey where where oh where is my marker gone now ah you know the other thing I purchased a ukulele and in the last week I've been teaching myself to play the ukulele but don't have it with me so stay tuned for by the way uh great YouTube videos for learning the ukulele um there's a woman I forgot her name already I think she's from Hawaii and she does videos on how to play the ukulele they were great Christina maybe I have to look that up um let's just see if this pen works oh this works fine I don't know what's wrong with me oh I didn't forget the cloth but look here's an old paper towel so at least I'm reusing paper towel sorry okay I got the marker so the formula for a polynomial equation might look something like this y equals a x squared plus BX plus C so in this case this is a polynomial equation of order do you say that too but I could also have a X cubed plus BX squared plus CX Plus D but let's just go with this right now so what does it mean actually we should do one with anyway let's see what we're gonna we're gonna see how this goes I'm gonna get somewhere so the point is now really I've already done this already the only I could keep the same loss function I can keep the same data set the same optimizer the predict function the only thing that's different is I have three variables now instead of two and when I write the predict function I'm taking the X values instead of plugging them into this form of a line I'm plugging them in form of this polynomial equation let's go let's go get started on that oh you know what I'm gonna have to do I'm the way I draw lines so simple because I only need two points to draw line but to draw like a curve that's going to require quite a bit more all right let's see what we can do do you know how expensive things are in New York City first of all yelling over crew thank you very much but Barry's one of the reasons why I got the melon this melon at the unnamed local food shop nearby this is $3.99 the berries $4.99 five dollars per berries that fit in here I don't even think they're maybe they're organic I don't even think they're necessary organic Trader Joe's that's the best place to get them at least a soggy vine in New York City to get the lowest price organic berries what did I um let me just check okay it's not a new new channel I'm sorry the new fruit I should totally do a new reviewing fruit like unboxing fruit let me just practice that for a second then what do I do I talk about it if juicy kind of sweet it's a little weird tasting prefer get five bits sitting it was like cut and refrigerated if I like a day old pretty good good all right linear quadratic cubic Simon thank you you're always so helpful no people are just like giving me money for berries I'm gonna go get some berries this evening thank you everybody whoops I had this go off oh I lost the whiteboard Cameron oh there it is so I want to so let me let me just do some quick hygiene here polynomial function quadratic cubic ya know let's go here we go degree 3 degree 3 that's what I was looking for okay this is for the fruit Channel get some organics the coding fruit this is definitely like the coding this might as well the fruity coder I don't know that works for this channel as well the ratio of time I'm talking about fruit to time actually talking about code right now is about 1 to 1 looking up on Wikipedia and thanks to Simon who's watching this live in the chat I will now reminded myself that the the the number here is off is referred to degree so this is a polynomial equation of degree 2 also known as quadratic 3 would be cubic so on and so forth and linear is really I mean this is it's not a polynomial equation but it's a linear equation but it's of degree 1 and if you go and take a look at Wikipedia you'll see like okay well this is what its gonna look like if it's a degree 3 then the degree kind of matched to how many hills and valleys you have we'll see that we're gonna experiment with this we're gonna have some we're gonna do some fun stuff if you think huh you don't feel repression is fun stuff okay so now hmm look back to the code so looking at what I had before I'm gonna just change this to now a B and C and I'm gonna have three variables a B and C instead of M X and B then ah so now this the predict function so what do I need to do I need to stake a multiplied by x squared plus B multiplied by X plus C so how would I write that with tensorflow a is I'm writing y equals ax squared plus BX plus C so this would be constant Y's equal X's square multiplied by a adding X's multiplied by B add C what's the chance this is right x squared times a plus B X plus C I mean it looks kind of right to me I'm gonna wait for the chat to tell me why they think I made an error hills and valleys it is how many times it crosses the xaxis it's a much better way of putting it Thanks can I go back and restate this I need to declare C as a variable I'm pretty sure I did they're all declared as variables yeah all right I got a all right people in the chat are I'm so informal about my math my informal I mean it correct let's come back let's let's redo that explanation because I don't want it hills and valleys you're absolutely right it's the number of times it crosses the xaxis but you know it's got to go up and down to do that it's got a cross but that's a I let me go back to your the live viewers you're gonna have to bear with me today I'm gonna go back to explaining that again I cannot live I won't be able to live with myself if the live chat I mean the comments when the video gets published okay so so alright so let me so I think this is gonna be right Matz yeah oh but let me undo all my code stuff whoaoh the atom editor really is good at remembering undo past when I closed okay all right good I think we're good let me just check here okay okay if we come here and look at the Wikipedia page you can see here this is a graph of a polynomial function of degree 3 meaning a x cubed cubic and notice if we look at this how many times does the graph across the x axis three times of degree one how many times do we cross that xaxis a line just once so squared twice all right so now I think we're ready to go and try to make some edits to our code so the main thing we need to do right is change M and B to a B and C M and B to a B and C so I am going to change this to a B and C then here I'm gonna make this a B and C just about everything else go remain the same of course there's a really really important there's two other things that I have to change one is this this is me writing the tensor dot tensorflow j/s code for y equals MX plus B now what I need to do is change that to y equals ax where's that little hat squared plus BX plus C so this should be let's see if I can figure this out constant y z equals X's square so that's the X is squared multiplied by a adding and then what I want to add to it is B times X so I have to add X is multiplied by B and then I also want to add C I think this is right a x squared plus BX plus C X's squared multiplied by a plus B both two x's multiplied by B plus C I don't know oh let's see maybe this is right maybe this is wrong but now I'm gonna get rid of this and interestingly enough I do in a weird sort of way I don't actually need to change anything else well I do so here's the main thing I mean I could run this right now just to sort of see if I have any errors now weirdly it's first of all I think it's actually just running the old code ooh what's this print line that I don't need I don't need that oh yeah Oh so let's oh I'm having some memory issues now but okay we're gonna figure that stuff out later save it's do we say we're now interestingly enough it's working I'm kind of surprised by this oh it's working of course it's working it's just but I'm drawing a line the carrot carrot stop a hat okay k week mon am i am i messing up did i miss something important so it's doing something but who knows what it's doing the problem is here I have this code here to draw a line between x1 x2 y1 between x1 y1 x2 y2 but now instead of just using two points to draw a line if I'm going to draw something that is a polynomial equation a curve I need to sample a lot of X points and get a bunch of Y points and connect those with begin shape and shape and vertex so let's see if I can put that in here so what I want to do here let's think about this line X interesting so I'm going to say call this curve X I'm gonna make it an empty array I'm gonna save for let I equals 0 I is less than let's do let's say x equals 0 X is less than 1 X plus equal 0 point 1 so we're just gonna I'm just going to example 10 points just for right now then what I'm going to do is I'm going to say curve curve X dot push X I'm sure there's a fill map quick way that I could do this but let's just list I'm putting a bunch of points 0.1 0.2 0.3 0.4 0.5 now I need to get all the Y's I'm still gonna use curve Y I'm still gonna use Data Sync and then now I'm going to say for let I equals 0 I is less than curve X dot length I plus plus and I need to say before I start this I'm gonna say begin shape then I'm gonna say n shape and then I need to get just it an X 1 and a y1 x1 is curve X a mapping curve X index I and y1 is mapping curve Y index I and then let's put the stroke weight here and then I'm going to say vertex x1 y1 so before I was just calculating two points and x1 y1 x2 y2 now I have a whole bunch of X's and a whole bunch of wise and I need to set them all as vertices and then connect them with a line let's see what happens here whoa interesting so first of all something weird is happening it's getting a fill which I don't want so I want to say a no fill so this is kind of doing what I'm expecting right doing what I expected that one in the middle is really throwing it off let's see if I can actually try to draw a curve hmm oh so now I'm being told in the chat degree equals the number of crossing points explanation is a bad explanation uhoh somebody's knocking at the door I wonder if I'm being too loud oh I did I was trying to see if I could find okay thank you yes yes that was me look why went answer the door it took a while it learned it first of all so if you're wondering what that was remember how I was talking about my be reported to NYU Internet services where is that thing where where where where where is my Philip's bridge where did I put it it's my prop that I need right now whoa boy that was really loud for you wasn't it where did I put that the lamp is there yeah if everybody was wondering if this really is live or not I guess now you know the answer of that question I can't find where that thing is anyway I was trying to find on the network to see if that big was on the network so I just did like a port scan of like the whole network which apparently you're not supposed to do so unfortunately also I did it from the computer that's live streaming live stream slowly goes down it's because I got shut down I got shut down but hopefully I should have muted my microphone poor okay all right the crossing point exhalation is wrong because a polynomial function can have no solutions so it's the number of real solutions how many complex solutions equals the degree I should never try to do any math let's read what it says here constant linear quadratic cubic that's quartic zero polynomial okay the commutative law a real polynomial is upon them with real coefficients and a complex polynomials apothem with complex coefficients the argument of the polynomial is not restricted it's plain bubble though degree is the maximum number of real roots okay okay max all right max you're gonna have to work your magic here so that wasn't exactly right see how I could miss it I guess I could do my greenscreen thing concavity yeah that's what I'm that's what I was kind of thinking of all right so I know I did just say that the degree maps the number of times it crosses the xaxis you can think of that sort of like the number of actual real solutions there are to the equation and it's really just a Mac there could be no solutions to a particular polynomial equation with certain certain values for a B and C so uhhuh so it's really I don't I'm getting in the weeds here because this is not my forte and I'm always messing this up but you know the technically the degree is master the maximum number of real solutions to the particular polynomial equation now I'm gonna go on yesno together and not and it cannot be flamed from the chat we'll just go all the way back again look this is one of those live streams where I get nothing done all right let's move on Wow people are really far behind in time so that's why no one's answering my questions all right right always has that many complex solutions oh that's interesting to note it's not relevant for this video yes all right I'm gonna fundamental theorem of algebra certainly is all right so I'm gonna keep going okay so now we've seen here that this actually does both let me run this sorry I had a little interruption there so there was a strange edit but I'm gonna now click with a bunch of points and let's see what happens over time so give it a little time here I'm gonna let the time speed up it's gonna be one of those fast forward things my tía Oh keep going keep going I'm so impatient right alright so uh you can see that it's starting to try to fit the curve to that Dana now there's a couple things here one is I could really play with the learning rate it's real incrementally very very slowly changing look at that it's moving but it's moving very very very slowly so the learning rate is something I could play with I also you can see that the way I'm drawing this is particularly jagged so what I'm gonna do also here is just go and like really increase the resolution of the number of points that I'm drawing and let's go look for the learning rate I have it point five Wow so I'm gonna leave it at point five and I'm just gonna hit refresh and you know I could also make it something easier for it to fit to which is just like this and so over time you can see that it is trying to fit this particular curve to this line now let's make sure we've done our memory management job I'm gonna go back and console.log the number of tensors alright so something went wrong here I'm creating more tensors than I'm disposing so we've got to go and look and see what I've done wrong there so what tents are what new tensors did I add the Y's this gets tidied the Y's are disposed so let's let me do my debugging where let me comment this out let me see if it's in the drawing nope is it in the oh you know what I started with an old version of my code so this just never got tidied so that really needed to be tidied I'm looking for another Oh missing a curly bracket nope 76 and really let's do it this way ah hold on let me go back oh I'm using an old version of the code for some reason that what the coat that I started with doesn't actually have this section tidied so I'm gonna say TF tidy and then I'm going to make this into a function so I need to put a curly bracket here a close curly bracket there this there okay so now this should right I'm only using seven tensors I can put the drawing back in and there we go now what if I want to have it ah yes okay so this is actually another this is actually a good point we could use one thing that that I wasn't thinking about and the chat is suggesting is that I am using stochastic gradient descent as my optimizer but there are other options for other kinds of optimizers let's go look at the tensor flow API and see what out the documentation was I not the documentation and see there's another good suggestion there's another good city sorry everybody so that's one suggestion another idea that someone in the chat is for podcasts a gradient descent so TF train SG is here so we can see momentum a to graph which one I should try Oh cent let's try that does anybody have a craziness learning rates probably way too high learning rate is way too high for this yes yes I agree I agree looking at the chat still perhaps maybe too high oh I forgot to have it at Mouse drag now so I want to take out the I'm gonna just put this basket back to mousepressed interesting whoa look at that come on you know something else I don't love about what I've done is a eita grad and an atom work let's let's try using atom oh yeah that's what I was looking for I don't know I didn't see that how do you pronounce these by the way adda grad ada grad added Delta a 2 Delta atom let's look at the paper Oh fun times let's try I'm by the way I've just gone off the rails I'm forgetting about this really being like I feel like I need to do this again if I'm gonna do it as a video perhaps but yeah now this is working better buttery smooth learning you use a picture of a pencil all right adaptive Adam yeah right all right I know I think it's live stream was going better when I was just talking about melon and reading random numbers all right I'm gonna torture everybody because I think I'm gonna actually just go and do this whole thing from the beginning again because and it's it's gonna be much faster I'm not just not going to do any insane editing there won't be somebody I won't have I'm going to use the correct code and I'm gonna feel like today is a success so all of you watching this live stream I apologize to you you can all revoke your sponsorships right now but I I just got a I've got a I've got to start over so I'm looking down for TF linear regression sketch a yes I want to make sure I'm actually starting with the correct code so many things went wrong here okay all right everyone I should do more things to get banned from the NYU Network I should be so lucky as to get fired from NYU okay let's we're gonna travel back into time you and me together you know I just say doing these live streams is actually quite good for my back because I'm moving around a lot we're gonna race this here and this is now I'm gonna I'm just gonna blaze right through this okay let's cycle the camera hello coding challenge the last one part 2 of the last one something maybe I don't know this is actually my second attempt I started this earlier today and I'm now gonna try it again you should watch this video go something else but I'm making it because I want to make this video I've got to feel like I did something today so in the previous video I did a demonstration of linear regression with Tessa Farias and the idea of linear regression is I have some data set right my data set are a bunch I'm making up a data set just by clicking points but you could imagine the xaxis representing something in the y axis representing something and then I'm trying to fit a line I want my line to fit to that data set and I'm doing that by creating intention float yes these variables that represent the M and the B of the formula for a line and then I create an optimizer and I I try to like figure out the loss like well if I had a line there what's all the differences between all the actual data points and where the line is and minimize that all that stuff that I did in the previous video if you watch that but linear regression can only ever fit a line so what if for example my data looked something like this well you can see the best line you can figure out to fit that data is this line which is it really accurate but I could see pretty easily like oh I could create a polynomial function probably a quadratic function that is a curve that looks like this that fits all those points so this brings me to the topic of what everybody wants to do with their lives so what is a polynomial equation well in some sense this is a polynomial equation it's a degree one I could also have a polynomial equation that's like a constant of degree zero y equals five if I had one of degree two what I have is y equals a x squared plus BX plus C quadratic cubic would be y equals ax cubed plus BX squared plus C now what are these values what is this what does this mean so the degree has to do with the number of solutions whether they're real solutions or complex solutions and there are a bunch of solutions across that I'll always math stuff which is not my forte I'll try to find a good resource for you know the fundamental theory of algebra or whatever that you could read about but the point is if I just want to now adjust my code to use a polynomial equation instead of a linear equation alright so that's not going to be too hard there's actually very little I need to change in the code so let's go back here and let's go to the code and let's look so here's the thing now I'm not gonna do this but really what I want to I mentioned this at the end I have an exercise for you to do if you make it all the way into this video I've got an exercise for you to do which would be totally fun again in the sense in the world we're polynomial regression is fun alright so I had em in B as variables now I need a B and C okay so now I need instead of M and B a B and C a B you know one thing you don't like about this you know I'm gonna fix something first that I never liked about this example which is that my space my twodimensional space goes between 0 and 1 I think it's better just for the world if it went between negative 1 and 1 so let me look at everywhere where I map and it's really gonna be the same negative 1 1 negative 1 and 1 probably gonna miss something negative 1 1 negative 1 and 1 just think the I should have like a Cartesian plane where zero zeros in the middle as my data space just for what I'm doing here so I think and then this should be this so let's look at this whoops okay I missed something yes no shoot shoot I missed somewhere why did I decide to try to do this right now terrible idea was all working fine let me just look everywhere I have map between negative one and one negative one oh one made one oh one negative one one anyone yeah I think I got everywhere Oh such an interesting discussion going on in this black channel I totally totally wanna I'm happy to participate in this later but this is about youtubing and the like line 35 ah there's the mistake that you all saw negative 1 and 1 there we go ok ok so we're back my linear regression is working just to be really sure up into a point here oh no look at this is the quickness down there oh I have it backwards zero too high ah 1 2 negative 1 all right we're really gonna get this there we go ok there we go all right now everything's fine I can now go to changing these too sorry for that little digression a b and c a b c okay then what's the other thing that i need to change so clearly the thing that i need to change is my predict function wherever that is there it is right so this is the formula y equals MX plus b expressed with tensorflow j/s now I just need to express this formula also with tensorflow digest this I feel confident I know how to do cuz I did it earlier today y equals ax carrot or I like to say hat look at x squared plus BX plus C so I'm gonna say constant wise equals so X's squared right x squared multiplied all the stuff in tensorflow J's can be chained the mathematical operation so the X is squared multiplied by a adding the X is multiplied by B that's B X and then finally adding C so this you know takes some getting used to how to like put all this stuff together and I could put it in multiple steps to make it more clear but this is the kind of thing you want to if you want to get into little lowlevel tension flow digest if you want to practice so now I can get rid of I'm just gonna get rid of this this is the predict function now I have another really significant issue here so I'm not gonna let me just run the code to make sure there's no syntax errors but it's obviously not going to do anything that makes any sense because it is what it's that this code down here is designed to just draw a line by the way good see I was doing some earlier in other words I'm picking this point and this point and just drawing a line but in order for me to draw a curve I can't you know I have to sample a lot of points along the xaxis so I need to make a loop and I'm now going between negative 1 and 1 to sample all these points I need to say begin shape vertex to your vertex C a vertex here predicted and shape I'll see that line so instead of having X 1 and X 2 I want an array of X values and I'm going to call that instead of line X curve X and that's going to be an array and I am going to start X at negative 1 go all the way up to 1 say X plus equals and let's use some increment like 0.05 and then I'm gonna say curve X dot push X so I'm just trying to make let me just show you what I'm trying to do here console.log curve X and there's gonna be all sorts of errors here but let's see you can see here that I'm just trying to make an array that has lots of X values but between negative 1 all the way up to 1 so I can now I need to get the Y values that go with that so I can draw this curve and I have the predict function does that so predict curve X get all those wise then curve Y is then again maybe I shouldn't be using data sync here that's kind of gonna be an animation slow down a problem but hopefully it'll be fine curve y is now the regular number the floating point numbers not the tensor anymore version of the Y's then I can get rid of ten sir I'm done with it and instead of drawing a line what I'm gonna do is say now okay now I just need to go through and look at all of the X points and say the x value is map curve X index I which goes between negative 1 and 1 to 0 to width and Y map curve y between negative 1 to 0 to height and then I want to say before this begin shape no fill stroke 255 stroke wait for 2 or whatever I had it and then n shape oh I need to actually set the vertex vertex X comma Y so let's see let's see what happens here look at that so this by the way is the random curve that the coefficients and I should get rid of this console.log so what's interesting about this is that every time i refresh this I'm gonna get a new polynomial quadratic equation because it's picking random a B and C and you know what I didn't pick a random any negative numbers so this would also probably make more sense for it to write couldn't technically these be anywhere like I should start between like negative 1 and 1 does that make more sense right yes that makes more sense so you could see I'm getting all these random curves that's what it's starting with now when I click you can see it's trying to approximate the curve whoa weird huh that looks exactly right but backwards why because I made this mistake again height is flipped negative 1 to 1 oh no no no not there this is hard to keep track of these pixel mappings between height and zero let's run this one more time there we go you can see now it's you can see it's taking a while let me let me be more methodical about this I'm gonna click like a bunch like this it's actually finding it quite nicely but you can see it's good that it's quite lovely let me do it the other way you know what's bothering me is that you can see how this is not getting all the way to the edge it's because when I created those points I really want to say less than or equal to I want to get the last point in there as well oh I don't do this that's a little bad trick there there we go let me let me write draw these now there we go so you can see this is working hooray so now we have polynomial regression now one thing I probably would want to do do is this you know now that I'm getting past just sort of like basic linear regression the optimizers the optimizer that I'm using if we look at the tensorflow code is trained SGD stochastic gradient descent let's just try for a second let's look at the tension flow jsapi and I'm actually already here because I was looking at this earlier you can see there's actually different optimizers that we could try a wellknown one atom the a da here being for adaptive I believe like a de grad a dad a de Delta but if I look at this we can see oh this constructs an atom optimizer that uses the atom algorithm I could click on this link here and I could find this whole paper that explains it beyond the scope of what I'm doing right now but just out of curiosity all I would need to do it oh maybe stochastic gradient descent isn't the best algorithm for fitting this curve let's just change this to Adam and I kind of have a feeling that this learning rate being so high is going to not work forever let's leave it so warning it might flicker quite a bit let's just see what happens with like a very high learning rate yeah you can see look at it like whoa that is really fun actually I kind of like it with the high learning because look how quickly but you can see it's kind of like bouncing around is because it's high learning rate it's gonna overshoot the optimal spot so let's just make that point one and let's also I think it would be fun to change this to Mouse dragged although I'm worried I'm gonna guess at so many points that it's gonna really slow down yeah Oh while I'm adding the points that's interesting so while I'm adding the points let's see this is a little bit weird but let adding points equals false I'm just gonna have this is like a terrible idea adding points equals true adding points equals false this doesn't make any sense why would that be really slow while I'm adding the points because what I want to say is like if and if not adding I was thinking I could like don't run the training if I'm not adding the points only run the training if I'm not adding the points yeah that didn't really help why what am I missing here so do the chat is fascinating oh people are really complaining about my 1.01 fix why not Jim we're gonna this section is so irrelevant to the regression tutorial that is not work that was like a side that I'm not gonna let me think about this right while I'm adding the points it's slow and then it doesn't matter it's happy to have a lot of points Mouse drag gets called multiple times on Mouse oh right so what if I did yeah right I should do Mouse pressed yeah I know what to do all right I'm kind of off in the weeds here a little bit of something that's not irrelevant to this example but I've been noticing that it's like really slow as I'm like drawing and then as soon as I like let go it's perfectly happy to like animate very quickly I'm trying to figure out why this is so one thing I'm gonna try is I'm going to just change I'm going to add mousepressed and I'm gonna say let adding whoops I'm gonna create a new variable let adding points I'm interesting to call it let dragging equal the false and I'm going to say I'm gonna say my mouse press you're gonna say dragging dragging equals true and then I'm gonna add Mouse released and say dragging equals false and then in draw I'm gonna I'm gonna do this now in draw so this way I'm gonna kind of like really keep everything in draw and I'm gonna say if dragging then add points and if you're adding points maybe don't try minimizing the foot just wait till you're done adding points to minimize the function let's see how that works yeah there we go so it's not happening in real time in the same way but at least it's letting me add the points so I'm intrigued why I wonder if there's probably a different way to think about this I really love how you that animation of watching it fit oh it's really just very satisfying so in any case let's try I'm just you know let me just try a smaller learning rate just because I'm curious so you can see with a smaller learning rate it's moving much more slowly so the higher move learning rate it's gonna get there much more quickly if it's gonna bounce you could do something that's called annealing I believe of the learning rate meaning start with a high learning rate but lower it over time we have to potential flow das API to see how that's done but here's thing I really want to do this I want to do this with a equation of degree three just because so I'm gonna do that manually so I'm gonna add a D here and then I'm gonna add a D here and this is gonna lead to my exercise for you and then I'm gonna say ax cubed plus BX squared plus CX Plus D so I want oh is there a cube mathematical operation intend to float a s square operations operations there's a square POW I guess I could do just wondering if there's a TF square is there a cube no so I probably want to do power where is that it's funny how that's under it react so I want to do this POW base exponent so I would do X is POW 3 x a adding X's squared multiplied by B adding C X is multiplied by C and maybe I should start to do something where I put these on different lines so so let's do this X's pal 3 multiply adding X's squared x be adding X is multiplied by C adding D right did I get this right this is now of degree 3 X is power to the power of 3 multiplied by a plus X's squared multiplied by B plus X's plus CX Plus D ok and then I think actually I'm good because this everything else is the same so let's go check this ooh argument exponent past Oh must be a tensor but got a number of course so if I'm saying power 3 I need to say TF scaler 3 so everything's got to be everything's got to be a tensor I can't just use numbers like I'm used to so that's to be TFT scaler dot three and let me put the learning rate back up higher let me put it back up to like point wow that is immensely satisfied so now you can see and by the way if I happen to draw a line it should still be really happy to sort of like fit a line there because it could just make those coefficients zero oh this is great oh hi Murphy glad I made this video for myself at least so here's the thing here's my exercise to you how can you make this so that the degree of the polynomial is like something that can be interactive as well so could I have a dropdown that allows me to try it with a degree 2 degree 3 degree 4 or maybe a slider what other kinds of interactive features could I add to this and how could I maybe like have the data set come into this in a more interesting way besides just kind of drawing with the mouse so um could you make what kinds of things could you could you do with this I think there's some interesting visual possibilities so I hope you enjoyed this video I'm going to publish this code now as a coding challenge called it's really part two of the linear regression now polynomial regression and I'll see you the future videos I'm going to work on that I'm going to look at that tensorflow digests layers API soon enough ok goodbye yes so Tobias in the chat is asking why didn't X less than Ernie let me dress this so this is really weird but I why did this not work why did this not work here's the thing floating point math on a computer is a weird thing and I've encountered this in many videos and tutorials but at least it's somewhere I cover this in an actual tutorial but like let's go to the JavaScript console for a second and I really should let this let me just shut this off so let me show you something oh it's being much too nice to me maybe it's like so there's all sorts of way of rounding stuff that can happen and floatingpoint rounding errors is like a thing because the computer's not actually storing this number as you see it it's storing a representation of this floatingpoint number with a certain number of bits and it has a limited number of bits so it can often kind of like get off by point one which doesn't really matter if you're just doing like a simple graphic simulation but if you're doing some like they're highly specific math that's why in certain programming languages and environments you'll find double which is more memory for decimal numbers you'll also have like kind of like specific math like I bet your tensor flow to deals with this stuff in a much more like lower level and accurate way probably 0.01 plus 0.02 I'm being told no in short oh yes the dangers what are the dangers oh maybe I should try that 0.0100 no I don't know I don't know how to get it to show up the I don't know how to get it to show up it seems to be working perfectly there's nothing wrong at all let's Google that dangers of overfitting polynomial curves all right so maybe I'll mention this real quick I forgot something important that I was told by the chat that I should mention and I'm referring of the dangers of overfitting of polynomial equation and that's what this being represented on this is a Wikipedia page for overfitting which is a term that you'll hear a lot in machine learning meaning what is overfitting so if my data set if those points are my data set a really fancy polynomial equation of some high degree is gonna be able to draw something like really accurately connecting all those but it doesn't actually have any real meaning as applied to the data the data can have a lot of noise in it and a line might actually be an appropriate way to make a prediction even though it doesn't fit it has on like a higher loss than the polynomial so so this idea of generalization you don't want your machine learning model to work so accurately with your known data set that it cannot make good predictions with an unknown data set and this is something that will come up more and more as I do more videos and tutorials with things here's the thing though I kind of like this example that I made for the fact that it does something kind of interesting visually in that it just looked figured out how to make this polynomial function fit this arbitrary set of points so I think we're sort of like some kind of artistic output there might be some value here but yes thinking about actual machine learning and how you want to make predictions is important so that's why I'm here at the with this public service announcement the more you know the more you know the music goodbye oh there we go CJ CJ got something here CJ you're you see J coding garden there we go there is our floatingpoint rounding error thingy what's this sound all awful in that and it won't be usable yes okay CJ yes DJ is rate coding guard of the CG guitar I love that you used the guitar CJ you see everyone should check out Odegard with CJ very I I wish I watched more but I've caught a few of the streams alright ken Haley writes only use higher order polynomials if you know there's a reason to do so not just the data yes will you keep the beard since when have I ever not kept the beard thank you Ken for that clarification alright 340 i could go downstairs and livestream be playing with the philips lamp or i could talk about the layers api i should probably talk about the layers api since that's what i said i was gonna do let me give me a minute here to figure out what I'm gonna do next I have one more piece of space melon the curves array has the mistake what mistake do I have one more piece of space melon I don't know how much longer I can do this we know all right can I stop the infinite loop what infinite loop all right I don't know what's going on let me just make sure NYU is not shutting me down ooh oh I got another sponsor that I missed because I'm getting now I'm getting so I know that Philips like thing will work because I'm getting email notifications thank you to Supriya okay I'm not seeing any emails from the internet services here telling me to stop doing what I'm doing okay all right I think I could move on yes slack channel any more things to say let's I think I'm gonna erase this I'm gonna reuse my paper towel right yeah it's probably what I should do is always do everything twice where is yeah I don't know why can't find that Phillips hue light bridge it's like completely oh there it is this is the prop I was looking to show ya we're over here this is the prop I was looking to show so I'm that's what caused me to get banned from the NYU Network Oh remember when I used different pen colors those were the days I want to do the XOR I'm going to do a video I want to do a tutorial of now about the the layers API and and try to train a model to learn XOR another thing that I'm thinking about to at the tensorflow one of the tensor flow videos is either at Google the Google i/o conference I think it was at the i/o conference not at not at the dev summit but I can't remember which one did and it's part of the TF chess examples did a classification example with Major League Baseball I'm using like pitching data and like sort of looked at the speed of the pitch and different things and then classified it as a I think it's a fastball or curveball or slider so I think that I would like to get to some more actual real world real data examples and then I'm gonna do the doodle classifier I should do all these things then eventually ml5 alright okay let's see here so now let's let's make some changes here I'm just gonna doing some housekeeping here okay would be nice if I kind of knew the lairs API who it there's a big thing of paper towels here that's funny zoomy asks does YouTube create a video ondemand once the livestream ends yes it does it sometimes takes a little while and it actually the way I have it set up is it automatically goes to unlisted but this URL that you're at right now it will be there and then I make it listed once I had a chance to work with matcha Chia to like write the description and all that kind of stuff alright so layers okay just gonna look at some examples that I was making working on should really pin some repositories so let me look at this oh yeah I had all this extra stuff okay right sequential well then you add and create layers create an optimizer and you have to compile it you can you can just set a loss function and then so that's pretty much so they could just call predict okay all right which deaath model that's different than the difference between TF model and TF sequential is that TF model is a more generic supporting an arbitrary graph without cycles of layers TF sequential is less generic and supports only a linear stack of errors Oh interesting how interesting so I'm used to just using TF sequential but I'm not going to worry about TF model right now yeah input what's this alright I haven't used that and then these are the different kind of layers and whoops look at my example Oh I closed that I just don't know any of this stuff off the top of my head units input shape so where I want to look up I just want to see what's in the API so I can things I want to look at our TF sequential I want to look at dents I already have then to this I need dents input shape config okay that I need why do I keep closing my thing that I want things I want to cover our dents a sequential compile so many sequential compile so that's under objects yep okay compile and I need this I think this is going to be enough and then predict I also want all right this should be a predict evaluates predict yep fit we're gonna get into also okay anyone who has experience with this stuff is there anything super important about anything important about TF sequential versus TF model that I should make sure to cover alright I think I'm gonna talk about the lairs API and I think I want to use the toy neural network okay actually just look at the library this make sense so okay to do all right all right I'm not gonna I will just okay yeah dense layers are what I used yeah yeah okay okay hello welcome to another tensorflow das tutorial now I'm very excited about this one I'm generally excited about a lot of things but in this tutorial everything that I've done so far has just used tensors operations to kind of create lists and matrices of numbers and multiply them and add them and optimize loss functions that kind of stuff now and I could keep going and there's a lot that I could do with just that alone but tensorflow digest I talked about this in the first video it has the sort of core API which has the tensors in it the operations and it's what I use to do a linear regression and a polynomial regression demonstration it also has something called the layers API and the layers API might look familiar to you if you've ever used something called Kerris because these are really Kerris layers caris is a lot machine learning library that is a higher level that allows you to create these machine learning models and underneath the hood a lower level code like tensorflow will be running so when tensorflow dot jeff's was created it was created with both the sort of lower level stuff and the slightly higher level stuff and I also am working on with a lot of collaborators here at NYU ITP an even higher level library that's built on top of the layers API called ml 5 I'll be getting to that eventually in this video I just want to talk about what the layers API is and its core features and so the way that I'm gonna do that is by looking at a sort of basic diagram of a neural network and how you would put together that neural network with the layers API and the kind of neural network that I'm going to diagram is the same exact one that I did in my very long tutorial series about Bill writing a neural network all from scratch so the point of this is you don't have to write it all from scratch you could just architect it with the layers API but if you want if you want to like get everything you possibly could ever get you could go back and look at some those videos if you want so what is so I'm gonna look at a simple well it's not simple but a basic feedforward multilayered perceptron it's gonna have just two layers so it often looks like three layers because there's also the inputs there's the inputs the hidden and the outputs that's three things but technically there's only two layers and you'll see why so let's consider that we have this neural network it's going to have inputs let's say that it has I don't know two inputs then it has a hidden layer how many nodes are in the hidden layer I don't know let's say four then it has an output layer how many outputs are there I don't know maybe we're doing some kind of classification task and there's three possibilities it's either a cat a dog or turtle so there will be three outputs you know I started diagramming the next the scenario that I'm gonna do in the next video maybe as a coding challenge is I'm going to solve again the XOR problem I really would like to get to some realworld applicable problems but I'm still here in the weeds of just like I want to see how things work and use kind of trivial and known problems just to see if I can get the solution that I know I'm supposed to get okay so if this is my diagram if you if you've never seen the neural network before again you could go back and look at to my other videos but the ideas are some data X we might call this like x0 and x1 and that data each each of those inputs gets sent to each hidden node there are weights here the inputs get multiplied by the weights added together and then pass through an activation function and then get sent out to each of the outputs and so that happens at each one of these it's very hard to draw this I'm sure I missed the connection by the ways that they called dropout so I've done that correct doesn't dropout caressing if I forgot to draw something but I'll get to that in another video so then then we have the outputs and those are again the weighted sum of all of the outputs of the hidden layer the hidden nodes comes into the output pass through an activation function and we see their result so the idea here is maybe that I have a image with just two pixels in it and two two pixels come in they get multiplied by all these weights activated of the activation function sent to the outputs and then I get some values that tell me the probability of those two pixels were a cat dog or a turtle that would be an image classification task so how do I use how do I use the layers API to create a neural network with this exact architecture so the first thing that I need to do is create something called TF sequential the TF sequential let's go look at that in the API Docs and I'm right here all right TF win is a secretes a sequential model is any model where the outputs of one layer are the inputs to the next layer that's what it says right there guess what and I know I'm sorry that I've kind of gone a little bit too high in my writing but the outputs of one layer are the inputs to the next layer the inputs go into the hidden the outputs of the hidden go down so that's exactly what I want I should say there is also something in the layers API called TF model which I'll click on here and the key difference is that TF model is more generic so there's kind of more possibilities there but if I really just want a simple basic feedforward neural network or the data flows in one direction between layers TF sequential will work so if I'm writing some code I would say Const model equals T f dot sequential so there we go I'm done well not exactly so that's just creating a sort of empty so all I've done is created this kind of empty architecture so what I need to do now and by the way this is what makes working with something like the layers API really powerful if you watch my tutorials where I did the whole neural network from scratch it was so much easier just to have one layer and I never kind of got to like multiple acres cuz I have to rewrite the code and think about the layers and how they're connected and have a loop and all this layer object well guess what the layers API has this for you so I can actually just say TF now add layer so I can I can actually I can create a layer so the kind of layer that I want to create is known as dense so what is a dense layer a dense layer is the terminology for a fully connected layer meaning that every every node in that layer is fully connected to every node from the previous layer and that's exactly what I have here these are dense layers all of the connections are there so and you'll see you'll see there's like other kinds of layers there's a convolutional layer that i'll use one at some point when I talk about convolutional neural networks other things too but dense is where we're gonna get started so let's come back and look at now the API Docs for a dense and I think I've opened that up here yes TF layer sorry it's TF dot layers dense so I need to say Const and I'm gonna call this hidden equals TF layers dense Const output equals TF layers dense so I'm missing a lot of but this is the idea so like I will have a model which is it's a sequential and then I have a hidden layer and output layer we'll talk about the inputs in a second and then I would just say model dot add layer hidden model dot add layer output and I'm like guys like to say outputs I think is it's the output layer whatever nuts to say output so this is the idea this is how in theory simple it is to build your own model using the layers API now what's missing here like I could weirdly enough let's just like run this code and see if we get any errors so it cannot read property name of undefined so okay so so who knows what that error is but thing that I'm really missing here is I need to be more specific like I need to say when I make a layer what is the shape of that layer in other words how many nodes are there with the shape of the inputs how is it connected what activation function am I using those types of things so those are if we look at the API Docs the configuration of the layer so so I need to actually configure the model itself and configure each layer so let's go look at that and see if we can figure that out so let's go look again at TF sequential and you know actually I think we're going to be fine right now with there's an optional this question mark means optional there's an optional optional optional parameter config what I am going to skip that right now and yet it could be I could actually create it with a bunch of layers already and a name but I'm gonna skip that what's more important here is the sorry the the dense the config object for the dense layers which is required so that's why I'm getting error this is required so I need to specify some configuration options and this should be listed for me here units activation use bias kernel initializer bla bla bla bla bla bla bla bla and sorry for these markers here so let's start adding something what that means is I need to create an object I'm gonna call it config I could call it and here is where I'm gonna set up the configuration of this hidden layer so what I'm gonna do here is let's look at what some of these are so units so what I want is what's the unit so in this case I want to have the hidden layer has four units and so I'm gonna say units four maybe I want to specify the activation function so what activation function you use is a fascinating topic that you could go down many rabbit holes for different scenarios but and I'm just going to put sigmoid in there as kind of it for historical reasons and that's also the activation function you in my toy neural network JavaScript library but eventually as I start to build out examples I'm gonna be taking out that sigmoid and using other ones so I'm gonna say sigmoid and I'm gonna put config here and I'll just say config 1 or config hidden again I could put the object itself directly in here there's lots of ways you could probably write this code in much shorter way I'm trying to write it as long away as possible to be most clear so let's do that and let's do config output and what did I say how many outputs do I have 3 and I'll also use sigmoid so I'm going to say 3 and I'm gonna say I'm gonna say config output ok so things are going pretty well now I'm gonna hit refresh here config is not defined sketch let's just line 7 oh right config hidden ooh oh interesting time out for a second I just need to just need to take a pause for a second I need some water how's this going so far is this like kind of helping people clear interesting useful to longwinded because it's not add layer all right let me just take a peek once again at my example Oh input shape that's where I'm forgetting of course that's why I forget okay all right amusingly I got this weird error message which makes no sense at all it actually makes sense but it's because iíve got the p5 library involved here and p5 actually has a function called model so let me just write now p5 is irrelevant for this discussion so I'm just going to comment out the p5 library and then hit refresh here now add layer is not a function so I must have imagined that this is how you add a layer to the model let's actually go and look and where would I find that out once again if I go to so here what I want to look at is all right hard for me to find things ok I know what I want to look for so I'm gonna just search for it I want to look for the TF sequential class so TF not sequential the function creates an object that is a TF not sequential so if I look at this we now look that the function is just add there's no add layer function is just add which is a lot nicer actually so this is meant to just be add and this is meant to just be add okay so now this is really bothered me that it's calling this polynomial regression so I'm going to have to change this to layers API explanation there we go okay uncaught error the first layer in a sequential model must get an input shape or a batch input shape argument all right so what did I miss this is what I was talking about so the inputs are technically not a layer themselves the inputs are in a way part of this hidden layer they are the inputs to that hidden layer the inputs to the output output layer are the outputs of the hidden layer so one of the things one of those properties that I have to specify is the input shape now interestingly enough it says I need an input shape or an input batch shape and what's interesting about this is what is the difference so here I can clearly say that the input shape is two there are two inputs it's just a no actually it's one well no no it's it's an irregular number to it it's confusing this is what it is it's one dimensional it's a one dimensional ray with two spots in it but someday there might come a time where I have a data set that is just each each each each record of that data set is two numbers but I want to send in a hundred of them at once that's known as a batch so the shape might be something like 2 comma 100 but this is not super relevant for right now this is what I need to specify so if we come back to the code I should here oops going back to sketch touch is what I need to specify here is input shape that's it again I beat this up like I'm just saying my my bottle architecture has so now I should be able to no errors so I created that model and I can even take a look at it here there it is you know all this stuff input layers we can see all sorts of stuff here now I there's not much there actually cuz I've forgotten a really crucial step but let me keep going there's some more stuff to discuss so the input shape is what's the thing how come I don't need to specify an input shape here you would think that I might well I'm just doing it with some camera cycling after all I had to say that there were two inputs hidden I don't have to say that there's four inputs to outputs well the reason why I don't is because tensorflow has the layers API can infer the input shape of the outputs layer because it has to be the number of units in that hidden layer and by the way it's you know actually naming these things like hidden and output is almost less relevant now it's really kind of like layer 1 layer 2 that sort of thing so so I could put input shape here and I would put for now let's just see if I have any errors everything's fine now what if I put like a tear am I going to get an error no I didn't get an error that's weird now why didn't I get an error well maybe it should give me an error I don't think so though I should get an air though I'm missing a crucial step here I actually have just set up the idea of this sequential model I have the model object I have the hidden layer I have the output layer I've added them both in but I actually haven't like plugged all the pieces into each other yet and finished it off that has to have come as a separate it's not building up the model as you're creating it or configuring it it's at the layers API so you configure it and then call a function called compile so I know I'm making lists of and this by the way was in TF dot layers dot dense but another really important function here compile is part of a sequential object ad ad was the other one if I'm sure keeping track of the things that I'm looking at so I add I looked at now I also need compile so let's go take a look at that so if I go to the documentation here we can see there's ad evaluate there's a bunch of things I'm looking for compile wait maybe compile is not part of TF sequential oh it is it is it's just listed as part of TF model because TF sequential is based off of TF model so this is what I'm looking for this is me needing to compile the model now I need to compile it with an optimizer and a loss function aha so if you watched my linear regression with tension flow Jas videos you might remember that I had to create something called an optimizer and the optimizers job was to minimize a loss function so the same thing the idea you know I just had y equals MX plus B now I have a more complex architecture to learn about a data set so what I want to do with that architecture is the same though I want to feed it a lot of known data and have it optimize all of the weights of all these connections to fit and this gonna be a very fit that data so I need to specify those things so how do I do that so first I'm gonna make I'm gonna call this just um like config and I'm gonna say optimizer is and so some options here a string or a GF train optimizer so I think what I want actually want to create my own optimizer and I'm gonna go do that I'm by saying a constant optimizer equals TF train SGD zero point one so if you remember this is a way this is I mean this you might not have seen before if you're watching this video for the first time but this is exactly the same code that I had in my linear regression example I'm creating an optimizer from TF train the optimizer uses stochastic gradient descent and the learning rate is point one so this will be the optimizer so I'm gonna say SGD optimizer like that then what else do I need a loss function so the loss function can be I could probably to find my own loss function or I can use a string so I'm not seeing here the options for the loss function strings but let's see loss function like basically what I want to add here is like mean squared error or something that's my loss so I don't think that's right but let's just see now if what I would do is say my mean a model dot compile config so now right what is what does the Routh let's review make create the TF sequential object configure some layers add them to that model then configure the the Optima basically make an optimizer and a loss function to find those and then compile the model with those so I'm sure there's gonna be lots of errors here and there's things that I've done incorrectly let's so unknown loss mean squared error so let's figure out how do I look that up so let's see here means squared look I found it in an example ah so losses let's look at this I'm just looking for the list of them but I'm gonna I'm gonna come back I'm gonna find that and then to come back and show it to you but now that I see it's actually this is just lowercase M so somewhere the documentation I want to find what the options I could put here are let's run let's now hit refresh oh no errors interesting so this is weird I'm surprised know if this is a bug or not but I'm surprised that I didn't get an error for this input shape here for the output maybe once I started feeding it data again there maybe I never would but I'm just getting it refresh though so okay so hmm look pause for a second we were talking about the cameras of the chat so I want to pause here to find the to just look at my example that I made the other day what else did I put in compiled no I just have the optimizer in the loss and then predict can do predicts okay so some X's and I can do the training so I can do fit okay so I probably want to add fit so where do I find in the documentation the possible so my guess is that you can just use this any of these lost functions as a string right where did I write that Oh Simon it's not me I wrote dad where did I write dad compile oh here I know why did that okay there we go fix Y input shape of eight when we said to oh yeah no it shouldn't have an input shape of eight the into the correct input shape sure before so hopefully I didn't make that way too confusing okay let me come back here okay just to be clear the correct input shape here should be for the input shape should that of the next layer should match the number of outputs the number of units in the previous layer if you think of this as layer one and layer two but I don't actually have to include that because it can be inferred so I just want to be clear I was kind of experimenting to see if I could have it give me an error but I haven't figured that out yet okay so I'm actually in good shape here there's no errors here so let me see what can I do now so the two things that I can do now I have created my model I've created my model and I've compiled it and by the way I couldn't find in the documentation necessarily where there's a list of different loss songs as I could put here but my assumption is that these are a whole bunch of different loss functions I could probably just use any of these as a string so for example if I want to use softmax crossentropy oh yeah that's one of my favorite loss functions it's gonna like Oh unknown loss softmax crossentropy so meaning is Nikhil are you in the chat does anybody know I got a pause for a second I don't think it has to be for why because you could add it doesn't matter so why so let me explain this to me what am I why like if I suddenly wanted it in the case where I actually made like six the shape to here these are just some extra random data that I would be required to add in here I think I'm gonna click on loss on top to see all losses so I'm gonna match you I'm gonna go all the way back to just I'm gonna I'm gonna try to explain where you can find the list of loss functions and that this input shape 8 thing was like very misleading so where somebody was saying that I could click on losses at the top I don't know where that is oh here yeah but it's not showing which ones are available as a string so let me just let's just try some of these other ones absolute difference yeah weird it definitely is happy with mean squared error so let me look again can any of these like cosine distance oh maybe you can't use those with this particular optimizer no all right all right so I'm not gonna worry about that too much let me see did anybody so there's a difference Jessa doe in the chat is asking don't you actually have 3 times 4 equals 12 inputs there's a big difference between the weights the weight matrices have 4 times 3 but this is what the layers API is keeping track of for you I should mention that in the part just search the dock near mean squared mean square should be near I think it should be possible maybe a bug maybe that's a bug that I found so I'm just going to I'm gonna like gloss over that unfortunately object functions or names of object functions oh so in other words I think I found a bug oh you know what though I think there's a new version of it so let me make sure I'm at least in the current version somebody factchecked me on my version what was it again shoot mean squared error so let's try this whoops sorry okay it likes that yeah so it's fine with yeah it's fine with it's fine with the other loss functions but not as a string so I think that's a bug yeah so interesting that's really interesting so I'm gonna okay so can anybody so I'm gonna go back again I'm going to redo this part of the tutorial from here and I know how to like fake it now again we can maybe add this as a bug report but what I want to clarify this so here and all right I want to understand is there a reason why you would ever do this or should it give me an error before I move on I need to feel it confident in that my understanding of it was this would just be a mistake and you don't actually put the input shape here because you you're gonna infer it from the units and if you put one there that's wrong it's gonna throw an error I'm waiting for K we c'mon maybe you guys are all um so yet tensorflow digest is brand new so I'm not surprised that there's things like that's always really tricky when you work with like Lauren even worked with p5 you're like oh did I make a mistake or a mistake in the library it's very hard to figure that out it helps when you're live streaming to an audience of lots of people who will help you I know it won't give me an error yeah it might mess with your network okay I think I'm gonna feel confident sort of in my thoughts here comedy I'm gonna go back sorry Matthew for the editing complication but hopefully this makes sense wait I would clarify something really important here because I I kind of put this in here as like a demonstration of some goofiness I was trying to like figure out but the input shape so a couple things let me first let me first mention something somebody in the chat just asked me oh isn't it 12 cuz there's four here and there's 3 here and 4 times 3 equals 12 well the number 12 a just sort of wrote that up too high the number 12 is an important one there are 12 connections meaning 12 Waits but this is now we're in the place of using the larious api that is specified for us we just need to say there are four here there are three here and we need to say there are two coming into here and there are four going into there so two is the input shape two here four is the input shape two here now I have this weird eight in my code because I was like kind of messing around like what happens if you put like the quote unquote incorrect input shape and so this really should if I want my network to be to match what I'm drawing there this should be a four but I don't need it because it can be inferred by here and I think what I'm going to do actually now to make this a little more readable is I think it's as much as I wanted to try to write this in a longwinded way I think I'm gonna take the configuration and just put it right here inside the creation of this layer and then I'm going to take this object I mean I don't need to name these objects and do this I think this is actually easier to follow so now you can see I think this is easier to follow right there's and I you know so there's the model there's the hidden layer the output layer I need to specify the input shape always of the first layer and maybe what I want to do is actually say add that hidden layer then create the output it doesn't really matter what order because I'm gonna compile everything so I have the create that let's review did I already do this I might have to do it again I need to create the sequential object I need to make whatever layers I want configure them add them in and then I need to compile it and notice compiling it I need both an optimizer which I created one stochastic gradient descent I could use the add a blonde or any of the other ones again what these are and what the formulas are there's a lot we could go down many different paths and rabbit holes for a lot of depth here but I'm just kind of looking at the higher level point of view here and then I need to compile it and with a loss function so me and squared error being one of them and I actually the all of the loss functions are listed out here this is where they are so for example so I can name it by the string or I can actually just reference the function name directly like this so this should also not give me any errors and you could see now that if I wanted to use like Oh cosine distance I heard that that's the loss function I should be using I could put that in here and I could also hit refresh again and now I'm using cosine distance so again what the different loss functions are why you should use one versus the other hopefully I might get into these things as I start building examples and have to make those decisions but right now I'm just looking at how you put it together okay um so this is going to be the layers tutorial part one I'm going to do a second part to this because all I've done right now is I've cream all I've done right now is I have created the model and I have compiled it the two things that I need to do with this is I need to send data through it I want to put data through it and look at the outputs what are the things I might want to do the two things I might want to do is use predict predict is a function where I give the model inputs and I get out of predict the outputs presumably I would only be doing predict after I've trained the model so I want to train the bottle with some training data it's finished then I can make predictions with new unknown data so how do I train the model I use a function called fit fit is a function that I can basically say I want to fit just like we had to in the linear regression example I had a lot of points and I had to find the line that fits those points I need to find all the weights of this machine learning neural network model that fit the data and guess what this is what tensorflow digest is going to do behind the scenes it's gonna do all of the stochastic gradient descent math it's going to use its own loss function everything underneath the hood so all the stuff I did in that building neural network from scratch now is now done for us by attention flow J s and it has many more sophisticated options so that's what's coming in the next video looking at fit and predict okay thanks okay alright how are we doing now it is 430 boy I'm way so I'm a little bit a lot on a half an hour overtime if you will just humor me for a second I have got to send some messages to make sure everything okay livestreaming will be done within an hour okay I'm looking at my email I should be looking at my you know okay so I definitely want to try to do the fit and and predict so I'm weirdly gonna do pretty I assume I could just predict even before I do fit which is silly but just to see it give me something okay all right Thank You Carson in the chat always nice to hear possible I really started off was rough today at the start my melon is all gone I'm out of my caffeinated beverage am i there's some more in here whoo there's plenty more in there oh good this might be what I need okay alright alright let me check the slack Channel and let's go let's move on ah let's move on alright very loud in the hallway can you hear that oh my goodness I guess no one's complaining about the sound yeah so I think some people are maybe watching for the first time what you're watching is my almost my like tutorial recording sessions or a lot of a little bit live and so the there are two ways you'll be able to watch this later one is as soon as I finish this we'll get archived online the full what is now two and a half hour long made possibly three hour livestream and then within a week or so edited versions of these different pieces and tutorials will come out add some comments to your code that is a very good idea Chris ray alright let's do that that's what I'm doing alright welcome to tension flow yes the layers API tutorial part 2 so previous previously on its flow test layers API part 1 I created this model using TF dot sequential TF layers and model dot compile with with a training with that with a loss function and an optimizer now let's just review that really quickly before I go onto the next step which is looking at fit and predict so I'm gonna add some comments thank you to the chat to suggest that so this is the model create the hidden layer add the layer and the hidden layer has a number of nodes input shape and an activation function which I think it's probably pretty selfexplanatory then the output layering is a dense layer so dense is a fully connected layer then the out create an output layer create another layer and here I'm gonna write here the input shape is inferred from the previous layer then an optimizer using gradient descent I must have a video somewhere that talks about what gradient descent is if that's not familiar to you and then I'm done configuring the model so compile it I don't know if this tutorial should include to be writing all these comments but it didn't so that's where we are so in a standard parser classic machine learning process I would configure my model well actually before I do any of this I should have like collected my data I've been really thoughtful about that and thought about the ethics of it and why am I doing this saying in the first place is this gonna help people or hurt people I should have been doing all of that but here I'm just looking at hedge fund is an API so I I'm kind of skipping those really fundamentals the most important parts I'm doing things backwards in a way and I'm just gonna make up data so what I want to do is there's two things one is I want to train the model I wanted to adjust all of its weights to fit my training data I have these inputs with these known outputs maybe I'm doing image classification I have all these labeled images cats dogs turtles and I want the model to output cats dogs or turtles some probability value is based on which things that thinks they are and then so that's what this fit function does I could also ask it to do predict which means just take this data I don't know what it is this is not part of my training data and just give me the output so let's do something weird I mean weirder then of what I'm already doing which is talking to myself in a room but the camera and some lights I have like an iPad weird stupid sound effects on it I don't know what's what's happened to me in my life anyway let's let's let's run predict without having trained the model will it actually just does it what does it start with it must have a whole bunch of randomly configured weights right must be configured itself randomly let's see if we can get some some output so what do I need to do to call predict let's go look at the API and let's look at I'm looking at TF sequential sorry I'm not in the right place hold on let's start this over let me get it sequential okay let's go look at the API and let's look at again TF sequential so I'm looking for the predict function now here's the thing to print out the predict function is there so it generates output predictions for the input samples model dot predict okay so config there are some configuration options like batch size and how verbose I want it to be but really all I need are the x's the x's are the inputs and i need two of them so what I'm going to do is I'm going to create a tensor I am going to say let input are abbess a Const inputs equals TF tensor one D and let's just give it some numbers like 0.5 0.25 sorry 0.25 0.93 it's a tiny two so I just made up some input C and look all fake not real data at all I'm just trying to look at how tensorflow da chance the layers API works so now I should be able to say model dot predict inputs now let's go back and look at this model dot predict TF one so interesting dip dip I guess I'm good okay so hold on so let's say let outputs equal model dot predict inputs and outputs dot print I don't know could it really be as simple as that let's see and so now I'm gonna go back here uncaught expected dense input to have two dimensions but got array with shape too oh boy I think I might have a big major mistake here let's think about this the input shape I have to think about this dimensions of inputs should match what did I good timing for the cameras to go off wonder what have I done wrong here is it oh it has to be this no cuz it the first to mention is the batch dimension that worked so then I don't have to say this then the shape is supposed to be 2 D should be 2 1 no oh so it's ok because in theory yeah I see ok ok I got it you go back to this error interesting yeah Wow this is this is tricky okay if it's totally true okay sorry all right huh I got an error here that's weird right this is actually Barry this is the kind of error you're going to live with if you continue down this road a lot which is like requires shape blah blah blah shape oh wait hold on so that's not the right air Wow okay all right so what is this error hope oh boy this is the kind of error you're gonna get a lot which is something is wrong with my like shape shape errors error when checking expected dance tense want input to have two dimensions but got array was shaped to what is wrong here I mean after all there are just two inputs I said very specifically that the input shape is onedimensional with two things in it so why is this wrong well it turns out I forgot about this idea of batching so it is quite uncommon or it's possible that you just want to send in a single data point like these two numbers in and get the prediction and so even though this is the array of the inputs that needs to live in an array itself because I might want to send in multiple sets of them and so actually the correct way for me to put this here is to actually have this be the first element in an array and actually this is not a 1d tensor then it's a 2d tensor so now if I run this we should see there we go this these are the outputs and we can see here every time I run it I'm gonna get different outputs because this particular model is initialized randomly now there's probably a way with the layers API that I could configure how the weights are initial initialize but it's using some default probably random maybe it's a normalized distribution of random numbers who knows we could look up the documentation somebody in the comments will tell me okay so but what's interesting about this is now right I could do I could have right I could now send in four inputs and what will I get out all of those results so these are the three output values for the first input the second input at third fourth so this is how the predict function works so I can create a model and I can start predictions now they're useless and pointless and random without me actually trading that model so that's what I need to do next fit there were some other things I wanted to say about this should I have said 1 comma 2 no 2 comma 1 try input shapes feet Oh is the live closed captioning on I was hoping I would get that feature unlocked for my channel could somebody send me a screenshot at that I'm just curious because I guess I could look at it myself but I would love to see how that works or like a gift for a little quick oh great err hair bow hair man in the chat is looking up alright so I'm gonna move on to Fitz I see that K tweaked mine is typing though so I'm just gonna if you have a 2d input shape you need a 3d tensor for ya ya ya ya ya for predict so it's always one for predicting fit okay okay so let's now create a scenario where we have some training data right this is my and and by the way there is this really important piece of working with machine learning where you have both training data testing data you can even actually go on validation data and then you have the new data the stuff that you're making guesses and predictions with I'm not getting that far into it yet but let's just in this case I'm just gonna have some training data I'm not gonna have any testing data although we'll see I mean we're gonna look at fit we're gonna see see how this all works so let me go back now to the to the the looking for where I was ah predict fit here we go so now I want to look at fit so here I look at this oh wait oh wait oh wait oh wait oh wait oh wait I just made some video tutorials about wait so one of the reasons why so working with tensorflow digests natively your one gonna feel somewhat comfortable with the idea of the JavaScript promise and these new es 8 keywords a weight and a sink so I will I'm gonna use those concepts you might want to check my promises playlist if that's new to you okay so let's figure out how we're gonna do this so model dot fit the parameters are X's x and y so here's the thing unlike here and I really should call these X's and this is really the Y's right this is really what I'm doing these are the X's and now I'm getting the Y's to fit I want to do the same thing the difference is I'm gonna have some known outputs so in this case I might have like this is my training data these are the X's and now the Y's are I need to make you know dreamily this would come from a spreadsheet or some type of actual database that I've thoughtfully collect thought about how it to collect and the data and what I'm doing with it but right now I'm just making up dummy data so you one of these eye supplies just put in random numbers like weirdly sort of lazy about this I like to you just sort of see it so let's just pretend my training dataset just has instead of four let's just have three things in it and let's let's just make some arbitrary okay so this is now my training data I have the X's right the X's are there are only two of them again if I were doing my like image classification example that I did previously in a in the with the toy narrow Network library I might have 784 inputs for 7 or 84 pixels but here it might made up scenarios two inputs and there's three outputs so the X's have two and the Y's have three and you can see that reflected here so now I should be saying model dot fit the x's and the Y's let's look here now here's the thing there are some there's this variable called history that's kind of interesting so let's say Const history equals model dot fit and I got to get into the weight and all that in a second so what the history is is that's an object that's returned that has lots of information about how the training is going like how accurate are things what happened there if I want to start like looking at the properties of the training what's the current lost that type of thing oh right these mismatch thank you so I just ran about three and three these have to match thank you to the chat for mentioning that to me okay so then I have this idea of batch size so batch size is let's look at that number of samples per gradient update if unspecified it will default to 32 so this has to do with the inner workings of how the gradient descent algorithm works right at some point gradient descent is going to look at the error and it's going to make all these adjustments to the weights and so does it does do that um does it do that after ten data points after 20 after 30 so I'm gonna ignore that and then epochs or epochs or I could never know how to know what to know how to pronounce that word is the number of times to iterate over the training data arrays it's optional I think the default is one so here's the thing I'm actually just gonna let this go without setting any of those those things are going to be certainly important hopefully as I get into future examples or as you have specific scenarios but basically I could add an object here that has things like that has various configuration properties and this would need to be a comma here but I'm going to skip that right now because I believe according to documentation config is completely optional so let's just run this and then I'm going to start talking about the asynchronous nature of this all right what happens if I say console dot log history let's just look at that so look at that I got a promise oh it's it's promising me something so what this means is fit is a function that executes asynchronously now it's possible I can do things with a kind of older style of JavaScript using callbacks because it looks like in the documentation here one of the one of the options I can specify as a callback but I'm gonna use promises so what I'm gonna do is I'm gonna say model dot fit then and what I want to look at is I don't know what I'm gonna just sort of see and in this case I think actually the way that I'm doing it I'm not I'm gonna look at what's sent into the promise so this should now this should be a way this is if it returns a promise I can figure out I can get the result of how its what it's done as an argument to a function that's executed when the promise is resolved so this is how this could look just to look at that history and you'll want to look at my promises videos if this syntax doesn't make sense to you and we can see here looks like oh I have a history object and I have a loss there we go so this is actually the response and what I want to look at is the response history dot loss index zero so looking at this there we go now what happens if I give it a lot of now what happens if I start to add in a configuration like because I'm really curious what happens if I say so let me let me actually use a variable call obviously they call it config and I'm gonna say it two things I want to add is I want to say verbose is true and I want to say a pox is five so because I don't see if it's verbose am I gonna get a lot of stuff in the console that's gonna tell me about what's going on so I want to add in some of those parameters verbose mode is not implemented yet oh okay so I guess I can't use it for Bost mode but I can add five and so this is just the loss after five let's add 100 point to negative point two so you can see it's pretty consistent it's some kind of getting about the same being sort of an arbitrary data all right pause I lost the chat I'm sorry I'm looking at the chat see if I missed anything super important EEP ox he box that's all I like to say it okay alright so how if I put this in a loop so let me ask a question to the chat what I want to do is put this in a loop and show the loss changing over time and so oh I have a trailing camera oh yeah oh yeah I'll fix that in a second yeah right why is it negative oh I have a cosine distance all right all right actually something weird was bothering me there for a second which is why do I have a negative loss like you can't have a negative mean squared error right mean squared error has to be positive it's the difference squared averaged over all the data points so I forgot that I still had cosine distance so let me put it back to mean squared error and now we can see that I'm getting something that looks like it could be mean squared error yes so one thing about this okay so I'm gonna get rid of this config actually not you let me go let's try to go back to when I first run this Co no it's fine it's fine what's a good right I'm gonna get rid of this config whoops I'm gonna get rid of this config because what I want to do is I want to look at how what if I want to like run whatever I want to sort of like watch the loss over time and I'm trying to think about the best way to do that but what I think I'm gonna do is I'm going to write a function myself called training train and I'm gonna put this and I'm gonna say Const history equals model dot fit and then I'm gonna add a weight and I'm gonna say async so by the way if I wanted so even though I was using a promise here this is a way that I can put model dot fit in an async function that I defined myself to sort of clean up the syntax a little bit and then I can say return history I guess this is silly is this silly but because what I want to do is call I want to call train recursively I think so I want to say train then ah yes this is exactly what I want to do so I need to just not use the arrow syntax for a second to just sort of figure this out so I want to say train then once I'm done training execute this function that gets the history I want to say console dot log history dot loss index zero and then I want to call train again oh no no and then I want to say yeah then I want to say return huh train oh I'm lost here how do I do this recursively within a promise this is an interesting problem do you see where I'm trying to do when it's done I want to and I'm gonna just add the arrow syntax here now I want to kind of like run train again I could just say then hmm pause for a second kay week nine is pointing me to an example of model dot fit look at this oh of course of course what am i doing the fact the reason why I want to move into an async function is now I can actually put a loop in here so this right the fact that I'm using a weight means I can actually put this here I don't need to return anything right this now what I could do here is I can say I can actually just call train like I don't need anything to happen but I'm going to say train then I think I want to go back hold on I have to figure this maybe figure uh I'm torn whether the whole figuring it out is like super useful or it actually makes more sense for me to like just go back and do this again because it's sort of a mess to edit yeah yeah a loop instead of recursion the recursion idea was like let me go back okay I'm gonna go all the way back to before I added the async function and I'm gonna go okay so now I have loss values that make more sense here's the thing though what I want to do is I want to do this a bunch of times like there's gonna be a lot of scenarios where I want to animate something as it's training I want to graph the loss function over time so how do I do this multiple times well I want to have a loop right I want to be able to say for I mean I could have multiple I could have multiple epochs so I could like add the configuration that gives me multiple epochs oh do I need to good but when I had before sorry but what I really want to do is this I want to just do one epoch at a time so this is weirdly work with like a loop I want to it's really awkward the way that I wrote this I feel like hold on I'm sorry did I delete something back here yeah like let me just run this for a second oh I'm missing another parentheses here sorry about this everybody this tutorial is a mess and 48 oh I had him there already ah zoomed in hey hold on a second let me try this again this one more time yeah that's right model dot fit I'm toil and this is where my brain starts to die yeah there should be another parentheses here there we go okay go back to the chat all right so now I'm getting lost values that make more sense and but what I really want to do is I want to see the lost values over time like as I'm fitting the model I'm running the training data in multiple times maybe I'm shuffling the order there's all sorts of things that I've done in previous videos that I want to do with tenth afloat yes so how do I have this model that fit multiple times I mean I could just call it twice right ooh oh it did not like that whoa Oh fascinating so I can't do that I don't know what's wrong with that but that's not really what I want to do anyway I could try to add a little work by the way the chat is really upset that I have these extra commas here so I'm gonna remove them JavaScript doesn't care it's like put your commas wherever you want like I had a lot of commas right oh no no no it doesn't want extra commas because it has blank entries but okay so so what if what if I were to put a loop here say like oh I want to do this two times so I want to fit the model twice again I got an error oh it really doesn't like this so here's the thing I I'm not exactly sure what this error is but uh maybe I'll somebody will tell me in the comments but I was going down a road I didn't want to go down this is where using wherever it was here in the tensorflow Dutch ass the await keyword is crucial so what I want to be able to do is I want to be able to call this fit inside a loop but promises just like everything's happening asynchronously becomes very hard to follow so actually what I really want to do is I want to go back to that syntax of saying Const history equals model dot fit and then this is what I want to do I want to say then console dot log history plus okay this is what I want this is like synchronous code blocking code this is what I want I want to fit the model and then see the results it won't do this because that's not how JavaScript works asynchronously so there is the awake keyword which is new at ESA which says hey wait for this and then to the next line of code right shouldn't that work whoops and a weight is only valid in an async function so again if you watch my async and await tutorials you would know this but I cannot just do this anywhere in my code I have to create a function a function with the keyword async I'm gonna call it train and I have to do this inside that so this is now the correct syntax this actually is valid but it's in an asynchronous function I have to call that function so I could just call train let's just run this now and oh cannot reap repartee zero of undefined and train so what do I have wrong let's put the X's and Y's in that function oh that's not the problem history hold on let's just let's just look at the history oh yeah it's their history o dot history right I forget you forgetting this is the response so it was fine actually history dot loss index zero so I just want to look at just the loss so these don't need to be in here these I don't want these in here so now we run this again there's the loss now here's the thing I also just want I'm just gonna put it then in here because I also want to have some sort of event for when the training is complete so this function now kind of magically returns a promise because of the way that await and async work so I want to make sure this is working great and now training is complete now guess what I can now do this I can do this ten times so I can now loop can go in here because of the beauty of this await syntax this is now work functions as if it's blocking code when it's all complete it will return a promise there we go we can see this is what we should have the loss should be going down right let's change that learning rate somewhere I set up a learning rate let's make the learning rate 0.5 you can see the loss is going down and maybe let's go back to 21 let's give it like a thousand basically a thousand iterations and we can see the loss is going down over time so and now what I could do I mean a thousand is a lot the training is now complete now what I could do is I can call remember this I can now call predict now again I'm only working with one data set my training data is my testing data is my validation data is my out by regular data that I'm using in the future once I'm done the model so but we now see the full process I have X's and Y's I can call model dot fit again this is a really hairy stuff not for the faint of heart I'm in the weeds of those sort of like lowerlevel tensorflow dot J s stuff I mean even though I'm in the layers API but using yes eight syntax but now we should be able to see let's take a look at oh and it's I'm gonna have an issue here where I have these Y's so I'm gonna call this I'm gonna call this outputs so let's take a look let's train it just a hundred times whoops oh whoa silly me look what happened cipher totally forgot about the asynchronous nature of all this stuff and I got the predictions before the training finished right because this is happening asynchronously which means what I want to do is after the training is complete then then I want to do my prediction so I want to train using the asynchronous training a hundred times then I want to do my prediction train train train train the coding trade and then let's look at this so how do these numbers match with what I said the the outputs should be yeah not so great not so great huh but you know I did my best maybe oops but really the issue here is simply that I'm the I'm working with such like fake for ten small arbitrary data that I can't really do any training an inference here in any meaningful way by the way inference is another word for what's happening here with this predict function but I'm hoping I mean I just wanna while I'm talking I I'll let this run one more time with like you know let's try three thousand three that training it and and let's actually let's try it with three thousand so I'm gonna let this the loss function go down so much further so okay so hopefully now you've seen right the first app I can't refresh the page until it's finished so while this is training hopefully now you've seen here that I have now in these two tutorials covered the basics just the basics of the layers API the layers API you can create a model or TF sequential or TF dot model that has some amount of layers that can be dense layers or other kinds of layers I haven't explored yet I can configure those layers I can add them I could compile the model and once that's done if I have data I can fit the model with that data and then I can ask for predictions with new data and this is what I'm hoping to do so I've got to come up with some actual realworld scenarios and the first thing I'm gonna do is a coding challenge would basically redo all this again is I'm going to try to just do the XOR problem so I have an XOR coding challenge already and I'm gonna do that so that video will be out at some point in the future or right now depending on when you're watching this let's go back and check so the loss got all the way down to 0.5 and we can see the outputs here 0.23 the outputs are very similar for each inputs so I'm just going to assume that the problem I'm having is just the fact that I have just this tiny little bit of data and I also trained it for a very very little bit of time and I might not but actually I want to look for the shuffle option before I go so that's really important I don't know if it's doing it by default by default when you are yeah in this fit there is an option called shuffle whether to shuffle the training data has no effect optional so the question is is the training data shuffled by default when you're training with the same data over and over again if you keep the data in the same order that can really be problematic in terms of how things work how well the it performs so I want to I want to add that so where where do I add that that's real that's important so right here I'm gonna add shuffle true so it's basically just a parameter just a one one print one a field of this configure an object where I want to say shuffle is true and let's just let's just do it a thousand times and go back and let's see if I get that loss much further down yeah not really I'm gonna do one more thing which is I'm going to add a pox 100 so each time through this I'm gonna do it a hundred times and then I'm gonna do that a hundred times so that should give me ten thousand times just to get it going further yeah I think I think this is a fool's errand I'm never gonna get this to produce overfit let's overfit my data so uh matsu we're gonna get rid of all this stuff from the end here all right whoo okay yeah that's like kind of point four point three point four can't remember what I said it was yeah I mean how could you this makes notes okay my data makes no sense so this ended and you could see this as a fools I'm at a fool's errand here it makes us it I got these like little three data points with two numbers in them and I'm trying to get three numbers out of each of those and train the model and then I've looked it's this is this is never gonna go anywhere so I'm gonna do this again in the XOR example you have one more idea just to make it I wanted to get a result that makes sense one more idea just give me give me a second here let's just change this model to have the output just have one output and so now the wise like I'm gonna say 0.1 0.1 I want that to give me like 0.9 and then I want to say like 0.9 0.9 I want that to give me point 1 and then I'm gonna say like 0.5 0.5 give me 0.25 so this is now going to be my data but simpler like I is it it has it like a very simple linear relationship so I think that I should be able to now I'm going to go back to one Apoc I'm going to do this just 200 times and we're going to now fit this model and here we go oh I really got it let's let's let's increase the learning rate man let's increase the learning rate let's do it let's do it a thousand times Teddy box each all right I'll be back in a little bit oh look at this look at that loss I finally got somewhere see this is the machine learning is hard all these parameters of what you're doing but this is going to end in a second I'll be back this can be sped up this can be sped up as its training over and over again this is so silly I don't need to watch this for the whole time by the way I'm gonna be gone going I'm not going to do the XOR thing today because it's already 520 and I'm late anybody have any idea where I am in the number of I got what would you estimate now how long is it gonna take me to get to oh there we go yeah look I need it do something I made it match the exactly over fitted my model basically but I made it match the exact output so my issues were really learning rate and the amount of time I gave it to train and that sort of stuff so hopefully now I'm really at the end of this video apologies that this was maybe a bit convoluted in terms of how it came together if it came together at all but now I've really completed my two tutorials on the layers API and in the next video I will do some coding challenges and actually use the layers API for some hopefully some more practical examples all right see you later good bye yeah Carson is giving me a good some good feedback I mean let me I I have like me I'm gonna go back to I'm gonna do this whole last section again just in case so let me go back just cuz there's a lot of mess there how am i doing this stuff twice so at some point oh right I'm gonna go I'm gonna go from here okay all right Matt you what I'm going to do now is we go all the way back back back back back back back back back to basically the first time I ran it pudding predict inside of here okay so I didn't weigh back back back back so I'm gonna do that okay alright here we go alright so this is running we can see that the loss is going down it's training over time and when it gets to the end I've got the results of my predictions now you can see these don't look very good like first of all these don't resemble these outputs at all the main issue here is that nothing here makes sense I just have the skeleton of the story of data I can fit I can create the model I can fit it with some data and I can ask it to run a prediction with that same data but I'm ignoring like really important considerations and you can see that machine learning isn't just magic it doesn't just do the right I don't care that was the right answer so let me at least before I go add a couple things to so that we can actually see that it got somewhere so one thing that I want to do is I'm gonna just simplify what's going on here I'm going to make this data something really obvious like zero zero one one point five point five that's my X's and then my Y's I'm gonna I want to get I want to get one I'm gonna just change that to just one output point five and zero whoops what I missed here yeah oh boy syntax there we go so what I'm doing here is I just try to like come up with some ridiculous scheme that's like super simple to learn like they're sort of like an inverse linear relationship here zero goes to one it's like the map eventually let's see if there's a neural network can learn the map function but in my using two numbers and one numbers here but whatever so now I am going to I have to change the output to have just one output and now let me run this so we could see look at this the loss function is still pretty high so there is something that's kind of important here you can see it's going down and actually it like a thousand iterations it it kind of got something it got something pretty close so I would just need to give it more time to train to kind of reproduce the known results but I just want to show a couple important things here one thing that's important is I probably should always shuffle the training data so there is actually a kind of important one of the configuration parameters here for the fit function like if I shuffle is true what this will do is whatever each X's and Y's are each time it puts it through the training function they'll do it in a different order and this will really help things as you're trading with the same data over and over and over again so let me add that in and just see what happens there so I'm gonna add this config here to here so this is shuffle true that's the only thing I've turned on doing one epoch at a time still so let's also while I'm here just to make things go a little faster let's add let's do like ten a pox at a time once again total here a thousand times 10 is 10,000 epochs right but because I want to see it over time I'm not just putting not just doing this once with the number 10,000 there I'm doing it ten I'm doing it a thousand times with ten but this what I think will make things move a little faster and we could say I got even better here and so now we can see this over time we can see that the the loss is really going quite far down now have to sit here and wait for a minute which I'm gonna do but you're not gonna have to this video will now speed up but his discussion is going on in the chat right now I'm gonna check my email while this is going Thank You grant Roz Marin for sponsoring my channel alright I'm back so look at this I reproduce those results right at reproduce the same results that my training data produces should have produced and I got the loss down pretty low so you can again completely meaningless trivial nonsense nothing but hope I'm just I'm hopefully this is helper to you it's helpful to me because I'm trying to figure out just how the library actually works itself so let's review for a second okay let's review everything I have shown you now these two videos that I can create an empty model right the diagram of my neural network architecture is an empty model then what I could do is I can create layers there can be dense layers or other kinds of layers that I might look at in another time in the future and I can add them the order that I add them is very important because it is a feedforward sequential model so this is first in this a second beef you know I'm thinking of this is hid in an output but they're really just layer 1 layer 2 ok once I've done that I have to define how what sort of mathematics are going to be used to train the model and there's a loss function mean squared errors what I'm choosing to use and opt an optimization function stochastic gradient descent with a learning rate of 0.1 that's everything from part one and now what I've done here is I've shown you okay if I have some data presumably coming in from a spreadsheet I turn those into tensors I can use the fit function to train the model I can say fit all of the weights with these X's in these Y's once that's complete I can then ask it to predict with presumably new data this skeleton has not used any meaningful data I'm also not really being thoughtful about like well what activation function makes sense or what optimization function makes sense different things work different scenarios so hopefully we'll see that more in the future as I make more videos I would encourage you to just try to do this maybe with some actual data play around with it try different input shapes activation functions play around a little bit see what you get let me know about the results in the comments let me know if this was helpful to you and I will see you next next video I'm gonna do I'm gonna look at that XOR problem again in the context of tensorflow doubts yes okay good bye alright everybody I don't know not sure if you're still watching thank you John you only look once if you were still watching this match yeah hopefully this could be put together and the fact that I did things that section and twenty one thing a little worried I did is I kind of recaps twice so he could cut out my earlier recap but uh you want to seem to figure this out so but if this is really problematic this last section I can always do that again alright everybody unfortunately because I have to go home I don't have time maybe I could bring the stuff home with me I don't have time to live stream the my experiments with these life I've my brain some so brain dead right now so so yeah Sarah much in the patron group and the slack asks who is matchymatchy ax is math blank and match it is the coding train I don't know what the title should be but video editor coding train channel organizer manager extraordinaire oh the sound effect was glitching the sound effects are glitching again tell me if they are sorry this is might hurt your ears turn your volume down three two one so yeah is that gonna be a problem why does that happen after like a while I think it's to do it though where these cables are it sounded kind of find that alright you have nothing changes at all except you have consistent commas at the end oh shoot that's gonna drive people crazy yes choppy I'm so confused by all the chat stuff that's going on you know it sometimes the glitching is actually just in the live stream and the the video that I'm recording might not have the glitching that's a wish that might be wishful thinking but that's happened before that's probably wishful thinking because hold on this is gonna it's gonna glitch again okay but I'm gonna I'm gonna listen to my monitor three two one oh boy that I'm really loud I'm listening it's that well three two one yeah so I'm just gonna at the moment hope that the glitching is not in a recorded video so when this gets edited all right I have to go there's so much activity going on in all these different chats I can't keep track of anything I'm completely braindead this is suddenly but this isn't happening like a four hour livestream so you know how I said I was gonna do two live streams a week I want it I was doing that because I wanted to do two hours at a time twice instead of four hours because everything goes you know I really like this is uh I didn't do a good job of like this is how I feel right now so but this was to live streams this week because I did it for like four hours and I will see you all next week probably Friday not Tuesday possibly Wednesday not Thursday possibly Friday so so yeah okay I got to go I'm gonna play the outro and see you all well usually I try to answer some questions and stuff like that but it's already 530 I just can't I will train I wish I had that ukulele so I could sing you out but I will load the train whistle still waiting for the water ripples and tfj s that might never happen at this point but I really interested in that I will come back and do that I mean maybe I'll do that when I get to like convolutional neural networks pick you on the fastest station will make things that are all creation you you
