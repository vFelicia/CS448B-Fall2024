With timestamps:

00:34 - oh here what a failure what a failure I
01:12 - know you hear me now I know I know I
01:14 - know I know you know i know i know i
01:16 - know i know i know i know i know i know
01:18 - i know i know i know i know i know we
01:21 - can i have we start this over again i'm
01:26 - waiting till the chat catches up to
01:28 - realize that my audio has come back on i
01:30 - always i haven't done this in such a
01:31 - long time i've become slightly more
01:34 - professional in my live streaming but
01:36 - yeah I went to go ah oh now I hear
01:41 - myself even coming out of here this
01:43 - computer I went to I went to go use the
01:49 - facilities which is something I'd like
01:50 - to do right before I start live
01:52 - streaming and when I do that even though
01:54 - I'm not we're streaming I'm actually not
01:55 - even recording so I'm gonna hit start
01:57 - recording or recording to disk I I like
02:00 - to just mute my microphone just in case
02:02 - you know anything happened by accident
02:04 - you know I don't want to confess to some
02:06 - crime in the bathroom by accident so and
02:12 - then I came back and I started and I
02:13 - forgot to turn the microphone back on
02:16 - anyway I was saying go Mexico scoop
02:19 - please give me score updates go Mexico
02:21 - sorry Sweden sorry sweden i love you'
02:23 - Sweden but go Mexico for what I
02:27 - understand if Mexico wins today Mexico
02:29 - would be the first World Cup team ever
02:31 - to win three games the first three games
02:32 - in a row I don't know if that's somebody
02:34 - out of fact check me on that but I think
02:35 - might be correct yes hey Zeus in the
02:39 - chat says Mexico is playing I can't
02:41 - watch both please go and watch Mexico
02:44 - this first of all this will be not
02:45 - nearly as interesting or exciting and
02:48 - also this you could watch later and you
02:51 - can watch the Mexico game later but
02:52 - that's you're gonna already know what's
02:54 - gonna happen and you know I can tell you
02:56 - what's gonna happen here I'm gonna try
02:58 - to code some stuff I'm gonna make lots
02:59 - of mistakes I'm gonna get stuck a lot
03:01 - I'm gonna waste a lot of time and
03:03 - eventually maybe at some point there
03:05 - will be a small useful nugget of
03:07 - educational material that comes out
03:09 - that's what's gonna happen okay anyway
03:13 - what was I saying
03:14 - good morning it is me the coding train
03:18 - person Schiffman thing human being and I
03:26 - have one job today my one job today is
03:30 - to complete the color classifier example
03:33 - with tensorflow Jas so the two things I
03:37 - would like from the chat i am i Yura I'm
03:42 - going to answer your question in a
03:43 - second the two things I would like from
03:44 - the chat well are one score updates so
03:48 - spoiler alert I want score updates the
03:50 - BIC okay Pat the chat so if you're
03:52 - watching this and you don't want to be
03:54 - spoiled I don't know too bad I guess
03:57 - - if this if slack comes back online
03:59 - somebody let me know because I can then
04:01 - look at the slack channel for the for
04:04 - the slack chat and actually since I
04:06 - don't have I'm gonna so I'm gonna be
04:11 - looking more at the YouTube chat than I
04:14 - typically do okay so I have to get
04:18 - myself organized and figure out where I
04:21 - last left off so let me open up terminal
04:25 - also I want to say thank you to the
04:28 - YouTube family and Learning Team I was
04:31 - able to attend something called YouTube
04:34 - EDU Khan last week so that's a move to a
04:38 - new apartment and so out of sorts I live
04:41 - in a sea of drowning in boxes going away
04:45 - on vacation this tomorrow
04:48 - two weeks all right so Wow
04:53 - oh no no no no no no no Sweden scored
04:55 - are you now I can't tell if the chat is
04:57 - trolling me I need somebody reliable
05:01 - nothing you're not reliable the dark
05:06 - hound who gave me the score of why look
05:10 - so unfortunately this is the the World
05:12 - Cup is on a lot this is the let me talk
05:14 - about schedule this is the only time I
05:16 - have right now to do a live stream this
05:18 - week and I have some bad news I suppose
05:21 - if you enjoy watching these live streams
05:23 - there won't be another one until the
05:24 - week of July 15th I'm gonna be away for
05:26 - two weeks the good news it's true okay
05:29 - that's too bad
05:31 - the good news is for though which isn't
05:36 - really good news for you presumably as a
05:38 - live stream viewer but for the non live
05:40 - stream viewers out there I have a
05:41 - backlog of about 8 edited video
05:44 - tutorials that have not been published
05:46 - to the channel and I'm gonna make a
05:47 - couple more today mostly there's the
05:50 - first seven parts of this color
05:51 - classifier thing if you can believe that
05:52 - the first that's right seven parts and
05:55 - then that one edited challenge of doing
05:57 - a subscriber visualization so those will
06:01 - come out be published as public edited
06:05 - videos once every few days while I'm
06:07 - away and I should also let you know that
06:10 - most likely even when one comes out and
06:13 - is public you can look in the
06:14 - description if there's a link that says
06:16 - next video you can go to the next video
06:18 - you might see that it's unlisted but I
06:20 - don't know if this is good practice but
06:21 - this is my the way that I approach if I
06:24 - have a series of like a 10 part series
06:26 - instead of just putting them all 10 live
06:29 - online at once I space them out every
06:32 - few days but I leave the other ones
06:34 - there unlisted so the viewer who really
06:37 - wants to keep watch all of them at once
06:38 - could do that ok
06:41 - oh Germany scored I'm not paying
06:42 - attention to that game I guess that's
06:44 - good though
06:44 - Chico Germany whose German I didn't know
06:46 - he's playing for Mexico I don't know
06:48 - like a huge Mexico football fan
06:56 - all right Oh Italy scored all right I
07:03 - really also I would like to do something
07:05 - with major I'm a little bit of a
07:07 - baseball nerd we'd like to do something
07:09 - with machine learning and Major League
07:11 - Baseball data something that I'm
07:12 - thinking about if people have ideas or
07:13 - suggestions there is a entire example
07:17 - created by the tension flow Jas team
07:19 - that classifies pitches into ball
07:22 - strikes you know curveballs sliders no
07:25 - no no no not ball strikes that
07:26 - classifies what was I saying
07:29 - classifies pitches into fastball
07:30 - curveball slider etc based on the pitch
07:34 - data all right wow we are really got it
07:41 - we've got a good World Cup discussion I
07:42 - love the international quality of this
07:44 - audience I love that there's a great
07:46 - World Cup discussion going on the chat
07:47 - is a wonderful break from the usual its
07:50 - programming language or which IDE is
07:53 - better than the other so if it's one
07:56 - thing that the World Cup has done it's
07:57 - made us forget about all of our quirky
07:59 - preferences for coding editors and the
08:03 - like sorry C are brown can't get behind
08:07 - that
08:08 - Red Sox thing but look congratulations
08:13 - to you on all the recent success
08:15 - interesting tidbit I did go to college
08:18 - with Theo Epstein I didn't really know
08:20 - him though I did not get a haircut as
08:25 - Shubra is mentioning in the chat but I
08:27 - did I could I meant to get a haircut but
08:29 - I did just use my home grooming device
08:32 - to trim this beard which had gotten out
08:34 - of control
08:34 - so no wilderness Dan today okay so I
08:36 - need to figure out where I am and I'm
08:39 - gonna go to the desktop what am I in p5
08:44 - tensorflow probably and I was so one
08:49 - thing I really messed up I think is so
08:56 - something I could use help with is it's
08:59 - gonna not be easy for you to help with
09:01 - it because I haven't published the
09:02 - videos yet but I was trying to keep the
09:05 - code that is at the end of
09:07 - each video separate and I erased one set
09:11 - of codes I need to recreate that which I
09:13 - will do myself but if someone wants to
09:14 - volunteer to do that
09:16 - it's the one I think where I was
09:17 - visualizing the grid of the colors he'd
09:22 - have to go back to the live stream at
09:23 - this point okay so I'm gonna create
09:27 - color classifier - I'm gonna run a
09:30 - server I'm gonna open up the browser and
09:41 - open up the console and go to color
09:44 - classifier - there we go this is where I
09:48 - left left off then I need to open up I'm
09:53 - gonna use the atom text editor
10:05 - happy birthday um I can't by the way
10:07 - people often ask for like shout out or
10:09 - you look at this or way it's impossible
10:11 - I think for me to catch every message
10:12 - and but you know hey happy birthday I
10:15 - care about your birthday hold on let's
10:22 - get this going here I don't want this
10:24 - YouTube this thing I do want to open up
10:30 - on the desktop oh oh computers are such
10:34 - a silly thing so let's close this and
10:45 - let's go to color classifier - that's
10:49 - where I want to be and Here I am and
10:55 - let's go here let's make this a little
10:59 - bit bigger and I think I am I am ready
11:10 - to go so it's 11:15 a.m. my goal today
11:16 - is to finish this example I really want
11:18 - this example to be finished I think this
11:19 - example is interesting enough or has
11:24 - enough to it that I can both publish the
11:29 - kind of fixed version that is the code
11:33 - that I'll leave with at the end of this
11:34 - tutorial as part of the coding train com
11:36 - website and then I'll also make a
11:38 - separate github repository with this
11:41 - color classifier that people can submit
11:45 - pull requests to like just improve the
11:46 - design and interface and that sort of
11:48 - thing I do have an issue where I don't
11:51 - have a good system for maintaining the
11:54 - various github projects that are
11:56 - associated with the coding train and so
11:58 - a lot of this stuff really looks like
11:59 - lingers and languages and so I am really
12:03 - thinking about that and how to mean how
12:06 - I can like manage the community better
12:08 - and so hopefully things will improve
12:18 - all right we need to find a tissue to
12:21 - blow my nose oh and I need to deal with
12:25 - this other camera so if you if you
12:32 - remember I did something really silly
12:34 - and ridiculous on the previous
12:36 - livestream where I didn't want to erase
12:41 - all of this over here I just turned the
12:45 - camera so that I could write some notes
12:48 - about a different project here so let me
12:50 - erase this this is nothing to do with
12:53 - the color classifier I'm just realizing
13:00 - now that I don't even need this so me
13:08 - not erasing it was kind of pointless but
13:10 - I want to turn the camera back uh-oh
13:12 - loose HDMI cable sorry about that okay
13:16 - no green all the way to like they're up
13:18 - loose HDMI cable so this should be good
13:22 - does this seem like focus and you're
13:26 - seeing what you're seeing Oh actually I
13:28 - don't usually have it turned all the way
13:29 - that way because that little smiley face
13:31 - that I drew was my like testing okay
13:34 - there we go
13:35 - so I think this seems good and this is
13:37 - where I last left off I know you can't
13:41 - read this tf1 hot thing there all right
13:45 - so that's good we come back over here
13:48 - and let me open up the tensorflow api
13:54 - reference we're definitely gonna need
13:56 - that a little bit okay my eyesight is
14:01 - more supports I bigger computer screen
14:06 - alright so now I am here and so I
14:18 - actually one thing that I have to talk
14:20 - about which I will get to is I I'm
14:23 - actually going to use validation data
14:25 - and not testing data as
14:28 - this tutorial here's some here's some
14:29 - news actually I took about two or three
14:32 - hours last night kind of building out
14:35 - the example I'll just show it to you
14:36 - right now so as just to prepare for
14:40 - today because as you know I typically
14:43 - don't prepare but I felt like this was I
14:46 - needed to sort of see how this was gonna
14:48 - work so if I go to a shipment github dot
14:53 - you color classifier I think this is it
15:01 - so this here is actually whoops this
15:10 - here is running a version of what I want
15:12 - to build today so behind the scenes its
15:15 - training the model with the training
15:18 - data and you and I'm and this is the
15:20 - number of epochs for now I'm gonna say
15:25 - epoch a lot pronounced epoch look
15:31 - YouTube thank you YouTube epic okay okay
15:35 - what about the British pronunciation is
15:37 - a difference epic British pronunciation
15:42 - epic how to pronounce well let's look at
15:46 - oxford dictionaries
16:03 - I guess you could say it either way
16:11 - don't die okay and yes I could watch
16:20 - Peppa Pig live right now all right
16:24 - oh I logged in to because I was watching
16:28 - the World Cup Mexico game I've logged
16:31 - into my daughter's the account that I
16:34 - have set up with YouTube red for my
16:36 - daughter to watch videos and we can see
16:38 - what she likes to watch here okay let's
16:42 - move away from that epic epoch I want to
16:46 - say Epoque all right second one is the
16:52 - only correct one I so agree me I am soo
16:56 - me as well as the only way to say maths
17:01 - is matched alright so this is what I'm
17:06 - going to look at this so I've trained it
17:08 - with the data and we should see now
17:10 - reddish yellowish greenish bluish stick
17:17 - it with bluish purplish purplish-pinkish
17:21 -  ish purplish so this does this will
17:25 - work and I've gone now for nine
17:28 - a pox I feel confident now it's if the
17:32 - key is saying it with confidence all
17:34 - right does anyone remember where last
17:39 - left up like what was the last thing
17:40 - that I did oh no Sweden scored again how
17:45 - much time is left
17:46 - oh I'm bad luck for sweet and I have to
17:49 - stop live-streaming I mean bad luck for
17:50 - Mexico come on Mexico I mean Mexico's
17:56 - gonna I think Mexico will advance anyway
17:58 - is that correct better be nice for them
18:01 - to win all right let me see hold on for
18:07 - a second I'm going to go somewhere that
18:10 - I don't want to show by accident so here
18:13 - we are for a second
18:15 - I'm going to I just want to login to my
18:18 - YouTube dashboard to find a particular
18:22 - video and I have to log in now as myself
18:24 - switch account another coding train and
18:34 - I'm gonna go to my creator studio
18:42 - creator studio videos color classifier
18:49 - part seven seven is the last one so this
18:53 - is an edited version of part seven that
18:56 - has not come out yet but I can watch it
18:59 - because it is private and I'm going to
19:02 - do that right now
19:03 - Oh should I put on the hoodie so that
19:10 - the matches continuity I know you can't
19:16 - hear the audio
19:34 - okay so let me see where what happens at
19:37 - the end do you want to hear this audio
19:46 - wait wait don't leave just wait don't
19:50 - leave just yet I do want to think about
19:54 - memory management and I'm maybe I'm
19:55 - going to think about memory management
19:57 - later but and the X's and Y's I'm gonna
19:59 - want to use in the next video but I
20:02 - probably should after I make the one
20:04 - last little tidbit here and then I'll
20:06 - move on in the next video I'm gonna
20:07 - start creating the architecture of the
20:10 - neural network model itself and oh I'm
20:12 - gonna introduce some new concepts soft
20:14 - backs and cross-entropy oh no I have to
20:21 - talk about softmax and cross-entropy
20:23 - okay all right that's fine that's fine
20:27 - okay good thing I watch that alright
20:34 - alright I'm ready ready everybody ready
20:38 - Freddy Roby we think of maybe I need to
20:48 - get into the mood a little bit
20:52 - [Music]
20:59 - oh I'm to the wrong string like I
21:09 - [Music]
21:37 - actually have left this ukulele here and
21:40 - I haven't been here for a while so I
21:41 - haven't had a chance to play I need to
21:42 - get a second one so I can have one here
21:44 - one at home so I can actually practice
21:45 - and learn some new songs
21:46 - [Music]
21:56 - [Music]
22:11 - all right here we go
22:21 - stretch my stretch I'm gonna get up and
22:25 - stretch stretch your hamstring muscles
22:31 - okay there we go let's cycle those
22:33 - cameras yes
22:42 - Warner says stick decoding it totally
22:44 - totally okay hello I am back I have
22:54 - returned to finally build the
22:58 - architecture to take this training data
23:01 - these RGB values that are matched with
23:04 - these one hot encoded labels if you
23:07 - don't know what I'm talking about you
23:09 - should probably go back and watch the
23:10 - first seven parts of this series but I'm
23:12 - now going to create the neural network
23:14 - architecture to train a model with this
23:17 - data and I'm going to use continue to
23:22 - use tensorflow j s in particular the
23:25 - layers API to do this so let's review
23:27 - over here a bit
23:31 - let's review over here a bit where I am
23:35 - alright so we've talked about all these
23:38 - things you know it's really funny I just
23:45 - want to erase I want to erase all that I
23:49 - can't like I can't I can't
23:53 - I apologize here sorry everybody go go
23:55 - watch the go go route for Mexico for a
23:58 - little bit
24:02 - this is just - it doesn't help me to
24:05 - have this here it's like two out of my
24:07 - head and so I need to erase all of it
24:12 - what oh how ridiculous mi then I so
24:17 - desperately jumping through various
24:21 - hoops to not erase this it's like I
24:25 - needed it and now that I'm here yes
24:38 - you haven't switched to watching the
24:40 - World Cup once again you were watching a
24:43 - livestream of a person quietly are not
24:49 - so quietly
24:51 - erasing I really need a cloth so I don't
25:09 - kill the earth by the way we have a
25:28 - little family World Cup pool going and
25:33 - the person in the family who happens to
25:36 - be leading with the best picks is my
25:40 - six-year-old far and away running off
25:47 - with
25:51 - [Music]
26:07 - [Music]
26:17 - [Music]
26:19 - watch this one of those glass boards
26:42 - that I could put right here drawn this
26:50 - before it started but don't worry I'm
26:53 - gonna get the coating is it noon already
26:57 - if it's noon time to be a knight I
26:59 - couldn't my watch my watch battery died
27:02 - and my Fitbit why and I didn't know
27:04 - packing and moving a hip behind the
27:06 - charger
27:12 - [Music]
27:29 - don't get your sips of tea or coffee or
27:32 - do some more stretching I'm almost ready
27:35 - I think I should probably just start
27:37 - over all right oh you weren't even watch
27:53 - I didn't even have the camera on sorry
28:09 - everybody are you ready oh it's only
28:21 - 11:30 okay whoo Oh slack is still down
28:29 - is that correct
28:33 - all right Oh No
28:39 - Sweden scored again I mean I love Sweden
28:45 - and everything but got to go all right
29:01 - hello I have returned once again to make
29:06 - a color classifier machine learning well
29:11 - by my first intro is much better okay
29:16 - alright hold on I think my belt isn't
29:21 - tight enough my pants feel like they're
29:23 - falling down okay now everything's gonna
29:25 - be right my brain is gonna work
29:27 - correctly
29:29 - hello I am back in this video I am
29:32 - finally going to start to build the
29:35 - neural network architecture to make this
29:37 - color classifier I am going to take this
29:40 - data over here which is a long array of
29:43 - many many RGB values normalized to
29:46 - arrange there to one which matches with
29:48 - all of these one hot encoded labels and
29:50 - if you don't know what I'm talking about
29:52 - then you might want to go back and watch
29:54 - the first seven yes that's right seven
29:56 - parts of this tutorial series that's
30:00 - getting very very long but this I think
30:01 - is I'm really getting to the good stuff
30:03 - I don't know maybe it was good stuff
30:05 - before maybe this is bad stuff I don't
30:06 - really know but this I'm really excited
30:08 - I'm excited because now what I'm gonna
30:10 - do and I'm gonna use tension flow yes
30:12 - but I'm going to create the neural
30:13 - network architecture so let's just
30:14 - remind ourselves what we have we have a
30:19 - data set most of the first seven videos
30:21 - of the series was all just about
30:23 - collecting and cleaning that data set
30:25 - and that data set is many many RGB
30:29 - values I think I have like five thousand
30:33 - which is actually is kind of very very
30:35 - small for a data set but it's fine for
30:37 - this particular demonstration I have
30:39 - five thousand RGB values each one is
30:41 - labeled with something like blueish or
30:43 - reddish or purplish these were
30:46 - crowd-sourced but those got converted to
30:49 - one hot encoded vectors meaning if there
30:53 - are nine if there are nine labels well
31:00 - let's see then I have a vector that
31:03 - looks like this one two three four five
31:05 - six seven times ten
31:06 - nice night and maybe this one refers to
31:10 - purplish if this particular element of
31:14 - this array of numbers has a 1 in it it
31:17 - is that that and that one is for a
31:19 - particular label this one sort of label
31:21 - okay so that's what I have
31:22 - so what I I know that I need to have
31:24 - some kind of neural network and the
31:27 - inputs has have a shape of 3 there are 3
31:32 - inputs are G B the outputs have a shape
31:39 - of 9 1 2 3 4 5 6 7 8 9 this is the
31:45 - output layer this has a shape of 9
31:49 - inputs of a shape of three outputs have
31:52 - a shape of nine because the goal of this
31:55 - is by what once this whole thing is
31:57 - trained and finished if I send in some
31:59 - RGB values what I'm gonna get is a bunch
32:02 - of numbers all between 0 & 1 and I'm
32:04 - gonna find the one that's the highest
32:05 - and and those numbers are gonna be the
32:08 - probability of this particular data
32:10 - point being a particular label and I'm
32:12 - gonna find the one that's highest in
32:13 - front of sign at that label who
32:14 - classification we're doing
32:16 - classification so now what goes in
32:18 - between all this now this is a big
32:21 - question and many different scenarios
32:23 - might call for multiple layers different
32:26 - kinds of layers there's something called
32:28 - a convolutional layer which I'll get to
32:31 - but I'm gonna do something really simple
32:33 - I'm gonna have a basic dense layer which
32:36 - is kind of the standard building block
32:40 - of neural network systems and I'm gonna
32:42 - give it some number of nodes so for the
32:45 - sake of our even right now let's pretend
32:46 - that I just gave it 4 nodes and a dense
32:48 - layer this output is also going to be a
32:50 - dense layer dense layer means fully
32:53 - connected meaning that every input is
32:55 - connected to every node and then every
33:01 - node in the hidden layer this dense
33:04 - layer is connected to every output now
33:06 - I'm going to let your imagination draw
33:08 - the rest of all these connections but so
33:10 - this is what I want to architect so
33:12 - let's now go on
33:13 - detectives now I'm going to do this
33:15 - using tensorflow digest and the layers
33:17 - API if you don't know about the layers
33:19 - API you're going to watch my three or
33:22 - four part series about the layers API
33:23 - tutorial but I'm bookin I sort of talk
33:24 - you through it while we're doing it here
33:25 - so you don't necessarily have to watch
33:26 - that okay so if I come back again this
33:29 - is what I built so far I have all of the
33:32 - training data and tensors and you can
33:34 - see the shape of it I have 5643 RGB
33:38 - values and 5643 labels nine with nine
33:42 - possibilities okay so the first thing
33:44 - that I want to do is and I'm gonna do
33:46 - some goofy stuff with some global
33:48 - variables that I might not know that you
33:51 - know just to make my life kind of easier
33:53 - I'm going to create a variable called
33:55 - model and my model which I'm going to
33:58 - create in setup at the end after I've
34:00 - prepared all the data I'm going to say
34:02 - model equals t f dot sequential TF dot
34:10 - sequential so that now that's that's me
34:13 - creating a sequential neural network
34:15 - model it's sequential because it's a
34:17 - feed-forward the layers go in this order
34:20 - so now what I need to do is create some
34:22 - layers so the first thing I want to do
34:24 - is make the hidden let's make the output
34:26 - layer now let's make this we should do
34:28 - it in order we have to do it in order
34:29 - and to make the hidden layer hidden
34:32 - equals TF layers dense and then I put
34:40 - some configuration stuff so I make a
34:42 - layer by calling TF dot layers and then
34:45 - I specify the kind of layer this is
34:47 - gonna be a dense layer and then I can
34:49 - pass an object in as an argument and
34:51 - that's where I can configure things like
34:54 - input I don't remember any of this let's
34:58 - go look it up so let's go to the
35:02 - documentation let's go to TF TF layers
35:07 - and let's go to dense where do we see
35:12 - that hold on
35:19 - time out for a second
35:23 - let me look at my cheat sheet which I
35:25 - didn't want to do but I'm going to
35:27 - totally do
35:40 - TF layers dense I got that right so
35:46 - where did I miss it
35:48 - oh it's right there under basic sorry
35:53 - I'm looking around for it and it's right
35:54 - there in front of my face under basic so
35:56 - I'm gonna make a TF layers of dents I'm
35:58 - gonna click on that and now I'm gonna
36:00 - see these are all of the things that I
36:05 - can pass into the configuration so I
36:07 - need to specify the number of units the
36:10 - number of units is like the number of
36:11 - nodes and I made up four right here
36:13 - maybe let's try sixteen maybe we want to
36:15 - have some more than four whatever we can
36:17 - make up anything we want so I'm going to
36:21 - now say units sixteen one thing that I
36:24 - know I need there's an activation
36:26 - function again I can't cover everything
36:28 - in this video I have an other videos
36:30 - where I've talked about what an
36:31 - activation function is and how it works
36:32 - but the idea is the activation function
36:35 - is the function that takes all the sum
36:38 - of all of the things passing through the
36:41 - network being multiplied by the weights
36:43 - and kind of squashes them into some
36:45 - range and so there probably is a really
36:49 - useful interesting discussion about we
36:51 - could have about what would be the best
36:53 - activation function to use right here
36:55 - right now maybe later
36:57 - he'll try some different ones but just
36:58 - for simplicity I'm gonna use I'm gonna
37:01 - make a bad decision and just use sigmoid
37:04 - this sort of like historically original
37:06 - activation function of neural networks
37:09 - I'm gonna use the activation function
37:10 - sigmoid let's see what else do I want
37:12 - input dimensions so this is something
37:15 - that I definitely need to do here
37:17 - because remember this this this these
37:20 - inputs this is not actually a layer this
37:22 - is a two layer network it looks like
37:24 - there's three but I'm just drawing it
37:26 - with three things and the inputs being
37:27 - but that's not a layer but I do need to
37:29 - specify that three things are coming in
37:32 - so I need to come here and say the input
37:37 - dimensions input dimensions is three
37:41 - because I have an RGB value this should
37:43 - do me just fine for right now so then I
37:47 - want to also create the output layer
37:53 - output TF that's gonna be dense that's
37:56 - going to have nine units because they're
37:58 - nine labels again that's completely
37:59 - arbitrary that's just how I happen to
38:01 - prepare my dataset now I don't need the
38:05 - input dimensions because the input
38:07 - dimensions can be inferred by the
38:09 - previous one the input dimensions to the
38:11 - output or the number of units of the
38:13 - hidden so I don't need that but I do
38:14 - need to specify an activation function
38:15 - and guess what I am going to use a
38:20 - different activation function softmax so
38:24 - I'm just gonna type that in right now I
38:26 - will come back and explain what softmax
38:28 - is in a separate video which i think
38:30 - will be the next video of this series
38:32 - just gonna push this a little bit
38:33 - further now I'm gonna say model dot add
38:39 - the hidden and then model dot add the
38:43 - output so this is now me this is now the
38:48 - code for exactly what I diagrammed right
38:53 - here three inputs into a hidden layer
38:56 - with some number of units with some
38:58 - activation function into an output layer
39:01 - with some number of units and an
39:02 - activation function timeout for a second
39:07 - I just want to see do I want to do the
39:10 - optimizer yes I think that I do okay
39:13 - okay okay
39:19 - oh yes so we have now built the model
39:24 - here's the thing the next thing that I
39:26 - need to do and I'm gonna do this in the
39:28 - next video what I need to do is create
39:30 - an optimizer so let's just put this in
39:33 - comments create an optimizer and I need
39:36 - an optimization function which typically
39:38 - in the past I've used mean squared error
39:40 - but I'm gonna use something called
39:43 - categorical cross true
39:48 - I don't know why it sounds really scary
39:52 - but it's not and I can't its I also
39:54 - can't spell it so I'm gonna create the
39:57 - optimizer and then I'm going to compile
40:00 - the model and then I'm going to train
40:03 - the model these are the next step so
40:04 - they need to do this is the architecture
40:06 - for the model people telling me I have
40:09 - an error oh yeah I have something extra
40:10 - extra comma here but so this one do the
40:15 - next video and so what I need to do in
40:17 - the next video this is like just a few
40:18 - lines of code but I need to I mean I
40:21 - could just add them but I would like to
40:23 - try to understand a bit more about what
40:25 - why am i have softmax here instead of
40:28 - sigmoid or new or any of the other
40:30 - activation functions and why I might
40:33 - choose categorical cross-entropy
40:35 - instead of mean squared error which is
40:37 - if you have happened to watch my ex or
40:40 - tensorflow TAS coding challenge or some
40:42 - of my other layers tutorials
40:45 - I always just use mean squared error so
40:48 - that's what's coming the next video I'm
40:49 - going to create the optimizer I'm gonna
40:51 - compile the model and I'm going to talk
40:53 - about softmax and categorical cross
40:56 - entropy oh wait wait wait wait let's
41:01 - actually run with this and see if
41:02 - there's a syntax errors no okay and if I
41:07 - just say if I if I look in the console
41:10 - here at model we can see there it is
41:12 - this is the object and it's got all this
41:15 - stuff in it alright see you in the next
41:16 - video
41:24 - okay now is the moment where people can
41:29 - ask some questions or offer some
41:32 - comments before I move on to the next so
41:37 - I I so I need to open up a Wikipedia
41:43 - page for softmax
41:46 - and then why did why does everybody make
41:51 - everything look so insanely insane
42:00 - and then categorical cross-entropy one
42:10 - thing I want to know is what's the
42:11 - difference between categorical
42:12 - cross-entropy and because I think in if
42:16 - I go to loss functions here but it's
42:24 - actually not here so this is I don't
42:27 - know if this is a bit of a point of
42:29 - confusion for me there's the loss
42:31 - function here is written as softmax
42:33 - cross-entropy
42:34 - but I know for a fact because I was
42:36 - building this example last night I'm
42:42 - looking at the chat so hard to follow
42:43 - the chat without the /o Caddick so
42:48 - here's something I would love so this is
42:56 - what I'm building I have it in here
42:57 - already right so this is what I did I
43:02 - use categorical cross entropy because
43:05 - this I have found in this is what I
43:10 - found that examples of tension flow jas
43:13 - however as the loss function and then
43:18 - but however if I go here and look at the
43:21 - API Docs
43:22 - it says softmax cross entropy are those
43:25 - the same thing
43:27 - no metrics oh it's a mint interesting
43:38 - it's here under metrics
43:50 - [Music]
43:53 - I'm looking at the chat I guess I'm
43:57 - gonna just kind of gloss over this right
44:03 - now and this should be a lowercase e by
44:09 - the way I mean I'll leave that there
44:12 - since all right oh I
44:18 - I also messed up which is that the
44:26 - optimizer I totally I totally misspoke
44:30 - at the end of the last video oh yeah
44:32 - okay thank you about for the the battery
44:40 - the battery notes I only have one plug
44:43 - today so let me plug this in the first
44:54 - thing I totally messed up the
44:55 - optimization function is stochastic
44:57 - gradient descent or adaptive something
45:01 - the loss function the loss function is
45:05 - mean squared error or in this case I'm
45:08 - going to use categorical cross entropy
45:10 - instead okay so that I messed up okay
45:19 - all right so I got to just move on I
45:21 - guess
45:29 - cycle the camera
45:55 - the chat has more entropy than the
45:58 - program itself all right
46:02 - I am going to move on and let's look at
46:31 - why is the word entropy use that's what
46:34 - I would like to understand it's between
46:41 - two probability distributions P and Q
46:44 - over the same underlying set of events
46:46 - measures the average number of bits
46:47 - needed to identify an event drawn from
46:50 - the set coding scheme so basically let
46:56 - me see if I get this right before I put
46:58 - it in the tutorial where did I put my
47:01 - marker where's that marker here it is so
47:08 - this this output layer ultimately with
47:12 - softmax gives us a probability
47:13 - distribution and we could absolutely
47:16 - look at the correct distribution wipe
47:20 - the thing that we're training against
47:22 - the calculating an error is this right
47:25 - the one hot encoded vector so we could
47:28 - use mean squared error it would give us
47:30 - a loss but cross-entropy is a formula
47:34 - for looking at the difference between
47:37 - this output vector and this vector if
47:40 - we're considering these probabilities
47:42 - because it looks it does use a different
47:44 - formula to compute the loss but the
47:47 - error between two probability
47:49 - distributions it's optimized for that
47:52 - and in particular but not and and it's
47:57 - linked to using cross entropy with soft
48:00 - max because soft max is an activation
48:02 - function that generates a probability
48:04 - distribution from any just vector of
48:07 - numbers that seemed about right without
48:11 - going into the actual maths the formulas
48:13 - for these two things although I I might
48:15 - describe the formula for a soft Mac so I
48:17 - think that's worth doing because it's
48:18 - much simpler than it looks like on
48:19 - Wikipedia but does that seem like a
48:22 - fairly accurate explanation before I get
48:24 - into the next video
48:25 - Oh Korea scored that's exciting
48:34 - I'm gonna have to wait a minute for
48:36 - people to catch up to me I'm gonna look
48:38 - at the formula here so uh so it's just
48:41 - the sum of probability
48:43 - P Times log of probability Q that's
48:46 - pretty simple actually H of P is the
48:49 - entropy of P the cross entropy for
48:52 - distributions P and Q is this and for
48:57 - discrete P and Q that's what we have
48:59 - here it means this okay I'm not gonna
49:02 - get into the mass of cross entropy the
49:10 - slack still that's not everyone just
49:13 - talking about Mac versus Windows so I'm
49:15 - just gonna quit slack because it's
49:17 - causing my computer to freeze up here
49:19 - how am i doing timewise it's only noon
49:22 - okay I think we're pretty good
49:28 - that's a nice definition of entropy like
49:32 - you know lack of order or predictability
49:39 - all right I I'm not seeing anyone
49:42 - complaining about my explanation like no
49:45 - there's like a completely separate
49:48 - discussion going on in the chat that has
49:50 - nothing to do with what I'm talking
49:51 - about so I'm just gonna assume that I'm
49:53 - gonna move forward Oh
49:58 - how much time is left in the Mexica game
50:00 - okay thanks Dan I'll take that as a yes
50:05 - all right
50:11 - Oh Korea's beating Germany and realize
50:15 - Korea and Germany were playing entropy
50:17 - is a mathematical way to determine how
50:19 - think Thank You Jo two micron who writes
50:21 - entropy is a mathematical way to
50:23 - determine how chaotic something is in
50:25 - this case it's a way to measure how two
50:26 - different how different two tensors are
50:29 - to one another that is a great way of
50:30 - explaining it so let me let me say that
50:32 - again in my own words so in other words
50:34 - entropy as a measurement of how chaotic
50:39 - something is so cross entropy is
50:43 - basically looking at this probability
50:45 - distribution that the neural network
50:47 - generated and the sort of known correct
50:49 - one and what's the sort of chaos that
50:51 - exists in between coats what's the cross
50:54 - entropy the error between those two
50:56 - probability distributions how if we are
50:58 - using them as problems we're generating
51:00 - an outcome like how chaotic is that
51:02 - outcome going to be well with the 100
51:05 - vector it won't be chaotic at all it
51:06 - will always be just the one thing
51:08 - because it's a hundred percent this
51:10 - label but with this what's the cross
51:12 - entropy there I think that's a good way
51:13 - of describing it all right let's move on
51:21 - all right okay I can't look at the chat
51:29 - now
51:41 - okay okay I'm back hi I'm back to create
51:50 - an optimizer and a loss function and
51:52 - compile my model for this color class
51:54 - fired another thing at the end of the
51:56 - previous video I was talking about oh I
51:58 - want to use I gonna use the thing called
52:01 - softmax
52:02 - and I want to create an optimizer I want
52:04 - to use mean squared I will use
52:05 - categorical cross entropy instead of
52:06 - mean squared error boy did I really kind
52:08 - of box that the optimization function I
52:11 - want to use this is something I'm
52:13 - choosing from for example what I'm going
52:14 - to use is stochastic gradient descent
52:17 - this is the optimization function that
52:24 - whole let me let me start this over I
52:32 - need to get my head straight here
52:45 - okay all right here we go I'm actually
52:53 - going to I think while I'm recording I
52:55 - don't really need this laptop even open
52:59 - because I have to there's no slack
53:01 - channel and I have the YouTube chat over
53:02 - here although actually I just realized
53:05 - it could be useful because I do have my
53:09 - pre-made code which I rarely do in
53:12 - advance circuit I do have that over here
53:16 - so I think that would be useful to have
53:18 - available to me okay alright I'm back in
53:26 - part 471 of building a color classifier
53:30 - now what am I gonna do here in the
53:32 - previous video I created the
53:34 - architecture of my model a hidden layer
53:37 - and output layer a sixtieth 10 ths
53:41 - sequential model to dense layers
53:43 - activation functions units etc now at
53:46 - the end of the last video said only the
53:48 - next thing I need to do is define an
53:49 - optimization function and then compile
53:53 - the model well I really botched that is
53:55 - what there's three things I need to do
53:56 - optimization function loss function and
53:59 - compile the model and so I kind of
54:02 - conflated optimization and loss I'm
54:04 - optimizing against the loss but the
54:06 - optimizer that I want to make is I can
54:11 - use Const I guess here I get a very
54:14 - inconsistent about winning using
54:15 - converses let maybe I'll go back and
54:17 - clean up that code at some point I'm
54:18 - gonna say I can get it from TF train
54:23 - stochastic gradient descent and I can
54:26 - create a learning rate which I'm going
54:27 - to say is like 0.2 so this so one thing
54:31 - to do is create an optimization function
54:32 - right there are different options and we
54:34 - can try other options stochastic
54:36 - gradient descent is the one that I
54:37 - basically used in almost all of my
54:39 - examples and covered in detailed in my
54:41 - how to build a neural network from
54:42 - scratch series and the idea of created
54:46 - descent is walking along trying to go
54:48 - down the graph of the loss function to
54:51 - minimize that loss so what is the loss
54:54 - function that I want well if I'm just
54:57 - say model
54:58 - compile I believe this is a whoops this
55:03 - is a function that I'm going to write
55:04 - with a configuration option and one of
55:10 - the things when I compile the model I
55:12 - need to specify up to optimizer
55:15 - optimizer ena this is very awkward that
55:17 - I just called this up here but that's
55:22 - fine and then the other thing I just
55:24 - specify is a loss function mean squared
55:27 - error so this is typically what I have
55:31 - done in previous examples if you look at
55:34 - my X or coding challenge but this is now
55:38 - going to change and the reason is
55:40 - because I am using an activation
55:42 - function called softmax so let's talk
55:44 - about what softmax
55:46 - is softmax
55:50 - question mark okay so remember the
55:54 - output that we want from the neural
55:58 - network is a probability distribution
56:00 - right what's an example of what an
56:03 - output might look like it might look
56:05 - like this there's nine values 0.1 0.1
56:08 - 0.2 0 zero zero zero point seven zero
56:13 - zero right
56:15 - oh-ho my math is off zero point six
56:18 - right these all add up to a hundred
56:21 - percent this is the idea we're what this
56:25 - is saying is this particular RGB color
56:28 - has a 60% chance of being you know
56:31 - bluish if that's the particularly ball
56:33 - that matches with zero one two three
56:35 - four five index number six a 10% chance
56:38 - of being reddish a 10% chance of being
56:41 - purplish and a 2% chance of being
56:42 - greenish this is what we want now the
56:45 - training data is encoded like this and
56:48 - maybe we can actually look at it right
56:52 - next to it maybe this is what the
56:53 - training data looks like zero zero one
56:55 - zero zero zero zero zero zero a one hot
56:59 - encoded vector because actually the
57:02 - correct label for that color is greenish
57:05 - so I need a loss function sorry
57:11 - let's good cat across entropy and soft
57:15 - backs are linked together they're used
57:17 - together so that's why I just can't
57:19 - remember which one I'm explaining but I
57:21 - need a loss function to give me the
57:24 - error between this probability
57:26 - distribution and this probability
57:27 - distribution but I need my neural
57:29 - network to generate a probability
57:31 - distribution in the first place
57:33 - activation function as you might recall
57:35 - is something that squashes any number
57:38 - into some range it's one way of thinking
57:40 - about it the sigmoid function if we were
57:42 - to graph that sigmoid function it looks
57:45 - like a boy can never do this something
57:49 - like this oh boy that's a terrible graph
57:50 - of it you look it up on Wikipedia
57:52 - something more like this right and this
57:54 - the top is one the bottom zero so any
57:57 - number given to sigmoid results in a
57:59 - number between zero and one softmax is
58:01 - an activation function that not only
58:03 - squashes the values that are coming in
58:07 - to these outputs between zero and one
58:08 - but guarantees that they all add up to
58:11 - one
58:11 - now you might say to yourself that's
58:14 - easy that's very easy to do we do this
58:16 - all the time with normalizing data I
58:18 - could just find I could just take all of
58:21 - the outputs add them all up and then
58:23 - divide each one by the sum of the total
58:25 - right because let's say somewhere I have
58:28 - these numbers two two one five right I
58:35 - can add all these up and they're going
58:39 - to add up well look at that they added
58:40 - up to ten let me divide by ten I have
58:43 - 0.2 0.2 0.1 0.5 so this we could do this
58:49 - sort of like divided by the sum as our
58:52 - activation function in but that's but
58:53 - but this is not going to give us an AK
58:55 - an accurate probability distribution
58:57 - that we want for this scenario and
58:59 - softmax is another way of doing the same
59:01 - thing with more that that sort of
59:04 - expands the difference this one makes
59:06 - this one much more likely expands the
59:09 - difference between these different
59:10 - values so the way that softmax works is
59:13 - we actually do the following you know
59:16 - that
59:17 - I gotta find it Eraser know that natural
59:20 - number e for natural log to point
59:25 - seven something I think well what if I
59:27 - said and took E squared e squared e to
59:31 - the 1 power e to the 5th power what if I
59:34 - took all of these what if I took all of
59:39 - these and then added them all up and
59:44 - made that I'll call that the e sum and
59:47 - then just took each one of these values
59:49 - and divided by E song that is softmax in
59:53 - a nutshell
59:53 - you'd only think I'm gonna do I'm gonna
59:54 - have a like a tan tangent video that you
59:56 - can go and watch now where I'm actually
59:58 - gonna write the code for the softmax
59:59 - function we'll explain it better
60:01 - only it's worth doing that in this video
60:02 - but I'm gonna I'm going to do that in a
60:05 - separate video so look for that in the
60:06 - delete it look for a link to that in
60:08 - this video's description just to like
60:14 - just to be sure that I'm right about
60:17 - this we can now go here and this makes
60:19 - it look like oh my god this is like the
60:21 - craziest scariest thing in the world but
60:24 - you could see it right here the softmax
60:26 - function for a vector of values z means
60:30 - take every value e to that z index J
60:37 - power divided by the sum of all of those
60:40 - values and so that and you can see here
60:43 - the probability theory the output of the
60:45 - softmax function can be used to
60:47 - represent a categorical distribution a
60:49 - probably miss tribution over k different
60:51 - possible outcomes time out for a second
60:59 - so what i want to do is you know i'm
61:03 - going to do that in a different video
61:05 - okay yeah
61:15 - okay
61:32 - okay okay
61:41 - alright so again in a separate video I'm
61:43 - gonna write the code for softmax and
61:45 - actually it's right there intensive load
61:47 - is also as functions for doing it and
61:49 - I'm gonna compare what those outcomes
61:51 - look like versus just summing and
61:53 - dividing but I'm gonna move on and say
61:56 - so if I've established that softmax is
62:01 - what I'm using as the activation
62:03 - function for the last layer the output
62:05 - layer the question then becomes what
62:09 - loss function should I use how do I
62:11 - calculate the error between the node the
62:15 - target outputs with the training data
62:17 - and what the what the model generated
62:19 - during the training process
62:20 - so again mean squared error would work
62:22 - here but I am gonna change that to
62:29 - categorical cross-entropy why am i using
62:33 - that so first of all what is entropy
62:37 - entropy is a term that refers to like
62:39 - the chaos associated with the system so
62:42 - you can think of a probability
62:43 - distribution is like being very chaotic
62:45 - or more or less chaotic so what the
62:48 - cross entropy function is a loss
62:51 - function designed to compare to
62:53 - probability distributions and look at
62:55 - how much chaos there is in between that
62:57 - the cross entropy between them and the
63:00 - math of it is you know mean squared
63:03 - errors like subtract take this one -
63:05 - this one and then do like the square
63:08 - root square it then do the square root
63:10 - or make it don't do the square root then
63:11 - add them all together me and squared
63:13 - error I've talked about that you can
63:15 - look it up it's a pretty simple
63:16 - mathematical function cross entropy if
63:19 - we look at it we get we could build that
63:22 - I could build this in a separate video
63:23 - which might be worth doing as well is
63:25 - really just the if if I have two
63:27 - probability distributions P and Q I'm
63:30 - looking at the mine- the sum of one
63:33 - probability distribution times the log
63:35 - of the other probability distribution so
63:37 - again you can research what cross
63:39 - entropy how the math behind it works
63:42 - more in more detail and maybe I'll do a
63:44 - video about that for those who are
63:45 - interested but at the moment the
63:48 - important thing to do where am i over
63:50 - here
63:51 - thing to realize is that softmax is an
63:53 - activation function for generating a
63:55 - probability distribution and
63:56 - cross-entropy
63:58 - is a loss function that works well for
64:01 - comparing to probability distributions
64:03 - so for a classification problem those
64:06 - are the two things we want to use pause
64:12 - wait if it's a binary classifier it's
64:17 - better if Dan use binary cross-entropy
64:19 - yeah yeah yeah that's a good point
64:22 - okay um in the chat someone just
64:26 - mentioned that you know if it was a
64:28 - binary classifier there's only two
64:29 - possible outcomes hot dog or not hot dog
64:32 - I think is the classic example now then
64:34 - I would use binary cross-entropy
64:37 - so again there is no be all end all for
64:41 - the loss function you choose I'm just
64:43 - showing you one scenario with the idea
64:45 - of and locking your mind to think about
64:47 - well let me research all these other
64:48 - loss functions and why would he use one
64:50 - with its the other and what's available
64:51 - as part of that I get for free as part
64:53 - of tensorflow digest okay so we've done
64:56 - that oh I think I'm done with this video
64:59 - let me just uh let me just kind of like
65:02 - run this code oh wait we got it unknown
65:04 - loss ah okay I think this is lowercase e
65:08 - okay there we go
65:11 - so so now we're done what is the next
65:14 - step what am I gonna do in the next
65:15 - video it is now time for me to call
65:19 - model dot fit model dot fit is actually
65:24 - the function I will call with the X's
65:27 - and the Y's that I've prepared in a
65:29 - previous video to train the model right
65:32 - I really only got two steps left and I'm
65:33 - sure there's gonna be lots other stuff
65:34 - that are forgetting about right now I
65:36 - want to train the model then I want to
65:38 - use the model to give me a label for a
65:41 - new color that the user is going to
65:43 - specify okay so in the next video I'm
65:45 - going to actually add model dot fit see
65:49 - you then
65:53 - oh please talk about the difference
65:57 - between stochastic gradient descent and
65:59 - gradient descent that's a good question
66:07 - I should talk about that somewhere maybe
66:09 - cannot I think I need to do like little
66:11 - aside videos let me see if I can answer
66:14 - this down so we can tell me if I'm
66:16 - correct stochastic rate I always forget
66:18 - which ones which but stochastic gradient
66:20 - descent is the one where you run you
66:26 - some a whole bunch of like loss function
66:29 - values and then compute the gradients as
66:31 - opposed to computing the gradients for
66:33 - each data point one at a time
66:35 - it has to do with went to you when do
66:38 - you tune the mod when do you tune the
66:39 - weights like here's a data point what's
66:42 - the error here's a data point what's the
66:43 - error here's a data point what's there
66:44 - do I do that a hundred times
66:45 - then tune some weights or do I some
66:47 - weights in between each data point I can
66:49 - never finish
66:51 - doing a bit stochastic means doing it in
66:54 - batches okay there we go all right how
66:58 - am I on time twelve ten that's pretty
67:00 - good I kind of want to be done by one if
67:02 - I can so we're gonna do model dot fit in
67:05 - the next video all right
67:15 - all right
67:26 - whoa binary cross-entropy is for
67:28 - multi-label classification x' whereas
67:30 - categorical cross-entropy is for
67:32 - multi-class classification where each
67:34 - example begun belongs to a single class
67:36 - oh I messed up so I totally miss explain
67:44 - that match I wonder if you can cut out
67:46 - the thing where I mentioned binary
67:48 - cross-entropy no people are asking I
67:56 - guess about my this is a google Summer
67:58 - of Code t-shirt yes so I do I've for the
68:01 - lot I think since 2011 I missed one year
68:03 - I have done I'm the organ the org
68:08 - administrator and sometimes a mentor for
68:10 - the processing foundation google Summer
68:11 - of Code shoot so Matthew let's figure
68:19 - out what to do about that binary
68:21 - cross-entropy thing I guess it's not the
68:24 - worst thing that I mentioned in there
68:27 - but I got it slightly wrong by the way
68:31 - this is something that came up in that
68:32 - YouTube conference that I went to like
68:34 - there's no way to correct a video there
68:38 - used to be those annotations and you
68:39 - could add there's no way to correct a
68:41 - video without once it's been published
68:44 - without like just removing it and
68:45 - uploading a whole new video binary
68:48 - cross-entropy
68:48 - look at this
69:02 - anyway I'm not gonna worry about it too
69:03 - much okay so let me get to let me uh
69:13 - cycle the cameras
69:28 - okay slack come back I'm out of water I
69:35 - may need to go get some water
69:40 - all right
69:46 - okay
69:50 - good the chat all right it's time it's
69:56 - time to fit our model here we go so so
69:59 - far you know hopefully you've watched
70:01 - all the previous parts of this series if
70:03 - you haven't that's fine too but what
70:05 - what I have so far is I prepared my data
70:08 - set
70:08 - loaded it from a JSON file I've turned
70:10 - everything into tensors and then I
70:12 - created a Model T F using touchflo data
70:16 - has a TF sequential model which is
70:20 - designed to receive RGB inputs and
70:23 - output a probability distribution for
70:26 - color labels and you know again this is
70:28 - somewhat of a trivial scenario but I'm
70:30 - classifying data simple data with just
70:33 - three values all between zero one and
70:35 - nine possible categories or labels okay
70:38 - so that's what I've done so far so now
70:42 - that I have this this is actually like
70:44 - it's always the feeders gonna be over in
70:45 - like two seconds not really all I need
70:47 - to do is call model dot fit so modeled
70:51 - outfit now what do I need to pass to
70:53 - model dot fit well the idea of model dot
70:55 - fit is that I'm saying hey here's the
70:57 - training data here are all the inputs
70:59 - and their associated target outputs
71:02 - which I have called X's and Y's now I
71:05 - think I'm gonna get an error right now
71:06 - let me just actually run this and I'm
71:09 - going to up so let me run this and see
71:11 - if I get the error that I'm expecting
71:12 - yeah so look at this Oh okay so a couple
71:15 - things
71:18 - welcome to your life doing machine
71:21 - learning shaped mismatching I didn't
71:23 - even expect this error so I have to
71:24 - think about this one error when checking
71:26 - input expected dense input to have shake
71:29 - three but got array with shape 5643
71:34 - three so I guess right I'm sending in
71:37 - not just three inputs the shape of my
71:40 - inputs is many so I think if I just do
71:48 - let me look at what an example that I
71:50 - made previously that's weird I didn't
71:53 - have this issue what I'm thinking about
71:55 - this where is this error coming from
71:59 - we'll know to think about this for a
72:00 - second
72:04 - huh I know that this batching thing is
72:09 - always an issue I thought I took care of
72:13 - that I'm just looking at the code I
72:17 - wrote yesterday but that's so weird the
72:26 - code that I wrote yesterday did is there
72:30 - something I did to it
72:39 - X's colors yeah
72:48 - Oh input shape got it that's what I
72:56 - that's the difference between the code
72:57 - that I wrote yesterday all right
73:09 - aha so I made a mistake and I used input
73:13 - dimensions where what I really meant was
73:16 - input shape let's see if we can actually
73:18 - look in the documentation here input dim
73:24 - with it for the dense layer what's the
73:27 - difference
73:28 - Oh input shape isn't actually listed
73:31 - here ah if specified defines input shape
73:36 - as is has input dimensions inside of
73:39 - brackets so actually I think this would
73:42 - probably fix it let's try this right
73:45 - really what I want is this like I'm
73:47 - going to have an array of batches of
73:50 - data each one with three values in it so
73:53 - I think no but probably what I would
73:57 - need to do is say five because I know I
74:03 - have this much data maybe I need to do
74:06 - this this is actually not the way I want
74:09 - to do it let's try this
74:11 - No so weird I know what I want is in I
74:16 - know what's going to work is this okay
74:26 - let me let me go let me go redo that
74:27 - explanation I have to think about this
74:30 - more later I went down a little bit of a
74:32 - rabbit hole though that's unnecessary
74:40 - all right let's go look at the
74:42 - documentation and see what it says there
74:46 - and I actually I've got it pulled up
74:48 - already okay so you can see what I
74:51 - specified was input dimensions if
74:54 - specified defines input shape as bracket
74:58 - input dimensions oh so actually I don't
75:02 - even need those that those array
75:06 - brackets there and that should fix it
75:09 - there we go
75:10 - but if I wanted to use those array
75:12 - brackets because I'm sending in many
75:14 - data points I could actually just
75:15 - specify the input shape directly and
75:18 - this would then have the array brackets
75:20 - around it so it's a subtle distinction I
75:22 - think because only input dimensions is
75:25 - documented let's use that one and let's
75:28 - put a 3 here okay so no peeps we've got
75:32 - that I wonder why that didn't know
75:33 - because I didn't call fit before okay so
75:35 - now I'm fitting the model I don't see an
75:38 - error I expected an error let's so what
75:41 - happens when I fit the model well it
75:43 - returns a promise model dot fit returns
75:45 - a promise if you don't know what a
75:47 - promise is guess what I have a whole set
75:49 - of videos about what a promises and I'm
75:52 - also going to be using eventually a
75:53 - weight and a sync which I also have
75:54 - videos about but right now I can just
75:56 - write the dot then the prompt fit
76:00 - returns a promise which I can then call
76:02 - a function called then to where the
76:05 - results will be passed in and I'm just
76:08 - gonna say and I'm gonna use this arrow
76:11 - syntax this es6 arrow syntax console dot
76:14 - log results and eventually I might want
76:17 - to do more with this so I'm actually
76:18 - gonna make it a full function so this is
76:20 - what I'm saying is once you fit the
76:22 - model then log the results let's see
76:27 - what happens
76:30 - waitingwaiting
76:32 - ah ok great look at this history loss
76:36 - and there's my loss so it fit that model
76:38 - it did one epoch and gave me a loss
76:45 - great so done train the model here's the
76:48 - thing I want what I want to do
76:51 - ultimately so this is actually
76:52 - way done what I want to do is first of
76:54 - all I want to train the model for more
76:56 - than just one epoch so one thing that I
76:59 - need to do here is pass in some options
77:02 - so I'm gonna create a variable called
77:04 - options and one thing I can specify is
77:08 - like epochs I'm gonna say do it for 10
77:11 - and then I'm gonna say and let's
77:13 - actually let's just say 2 right now
77:16 - because it's gonna take a while so the
77:18 - third argument to model dot fit is
77:20 - options and if I go into tension flow
77:22 - yes and I look for a model dot fit oops
77:25 - I was right there already we can see now
77:28 - these are the various options and I'm
77:29 - gonna be using a bunch of these but
77:30 - epochs is one of them the number of
77:32 - times to iterate over the training data
77:35 - so let's rub this down and you don't
77:40 - have to do I'm gonna I don't think we
77:42 - need all of this printing stuff so I'm
77:44 - gonna get rid of some of the earlier
77:45 - printing things because I don't need to
77:47 - look at all of that so much so let's run
77:50 - this whoops
77:52 - options is not defined I spelled that
77:55 - wrong I guess I still have 44 and 45
78:00 - console logging stuff which I don't need
78:03 - I didn't get an error that I expected
78:06 - yet which is kind of interesting and oh
78:10 - you know why one thing that I want to do
78:14 - is I want to update you know at the time
78:16 - of this recording I think the most
78:18 - recent version of tension flow chess is
78:20 - zero point eleven point seven well and I
78:25 - when I was previously recording I was
78:27 - using 0.4 and I think some things have
78:29 - changed so it was alright so let's let
78:31 - this run it's it's running for to epochs
78:33 - right now it's finished and I can look
78:36 - at the history and I can see both lost
78:38 - so we can see the loss went down for the
78:40 - second époque that's great now let's run
78:45 - this over ten a pox and let's run this
78:51 - and let's just console log results dot
78:54 - lost by the way or what was it is it
78:56 - results dot history dot loss might be
78:59 - that now let's look at what it is a
79:01 - history history dot loss okay so let's
79:06 - do this
79:06 - whoops I don't need that let's go back
79:09 - here hit refresh and waiting I'm gonna
79:12 - edit out this waiting part or speed up
79:16 - we're doing for ten a pox I'm have a lot
79:19 - I gotta fix I can fix this here while
79:22 - I'm here this is awful
79:28 - cloaking device oh it's back okay great
79:32 - so look at this over ten a pox the loss
79:35 - is going down this is good this is what
79:37 - we want to see now here's the thing
79:39 - what's it using to calculate that law
79:42 - huh oh there's so much to discuss I
79:45 - gotta get myself organized my thoughts
79:47 - here I want to here's I think maybe
79:49 - maybe I've done this video I'm really
79:52 - really drugged breaking this into lots
79:54 - of small parts and really what I've done
79:56 - now is call model dot fit with one
79:59 - single option the two things I need to
80:01 - do that are next one is I need to figure
80:05 - out what's getting that law like what
80:07 - data is it using to calculate that loss
80:08 - is it the training data didn't I talk
80:11 - about testing data and validation data
80:12 - should I be thinking about that it's a
80:14 - point so I've got to deal with that
80:15 - number two is I would like to I the
80:19 - point of this is I'm in a p5 sketch and
80:22 - I could say function draw background
80:25 - zero and I can run this but look at this
80:28 - it just is loading up there all the
80:30 - while while it's training I'm locked I
80:33 - don't have any ability to run an
80:34 - animation I want once it finishes I see
80:37 - the canvas I want the canvas to animate
80:39 - while it's training and I want to see
80:41 - the loss over time I want to have that
80:43 - reported back to me so those are the two
80:46 - things that I need to do I think I can
80:48 - tackle the training the testing and
80:51 - validation data thing right now because
80:53 - let's do that in this video and I'm
80:55 - going to add the animation stuff in the
80:57 - next video so first of all okay so I
81:01 - have my data set my data set has I think
81:06 - it was five thousand six hundred and
81:08 - forty three elements data points in it I
81:12 - said at the very beginning of this
81:14 - series runs preparing the data set that
81:16 - a typical thing to do is divide the data
81:19 - and again this is
81:19 - really small for proper machine learning
81:23 - model robust I probably want to have a
81:25 - much larger data set but this will
81:27 - actually kind of work just fine as we'll
81:28 - see I want to use probably the 80/20
81:32 - rule saying that 80% is actually the
81:35 - training data so I want to just only use
81:41 - why does it it's because the keyboard is
81:43 - next to this it's going sound I want to
81:48 - I want these X's and Y's to only
81:51 - actually be 80% of that original data so
81:55 - I'm not doing that I'll maybe I'll add
81:56 - that in another point that can be an
81:58 - exercise for you of the view for you as
82:00 - the viewer to take out 20% or maybe
82:02 - because my data centers are small just
82:04 - take out 10% of the data so that's what
82:07 - would be used to test the model after I
82:09 - finish training it but while I'm
82:11 - training it while I'm actually training
82:13 - it figuring out well how many input
82:16 - notes do I want what learning rate do I
82:17 - want
82:19 - what are these sort of taper parameters
82:21 - what are the parameters of this system
82:23 - that I want to try different things how
82:25 - many pucks do I want to train the model
82:26 - for what batch size do I want to use all
82:30 - these things are known as hyper
82:31 - parameters the parameters of the during
82:33 - the training process if I want to be
82:35 - playing around with those I need a
82:37 - separate data set to compute a loss
82:39 - that's not part of the training data but
82:42 - also is not part of my testing data that
82:44 - if you use when I'm completely done
82:45 - training that's what the validation data
82:48 - is the validation data is basically a
82:50 - test dataset but it's not your test data
82:53 - set when you're done and you're ready to
82:55 - publish your model it's your test dates
82:57 - that while you're doing all the training
82:58 - intensive like that jazz has a
83:01 - configuration option for model dot fit
83:03 - that just says hey use this much as the
83:06 - validation data so let's go back over
83:08 - here let's go back to the documentation
83:11 - and we can see here now I could specify
83:14 - the validation data or I could just
83:16 - specify validation split which is a
83:19 - float between 0 & 1 it's the fraction of
83:21 - the training data to be used as the
83:23 - validation data so if I come back here
83:25 - and I just add an option validation data
83:28 - and I say 0.1 I want to use 10%
83:31 - of my training data as the validation
83:35 - data that's what's going to be used to
83:37 - calculate the loss but it's not part of
83:40 - the Train down now there might be an
83:41 - issue I also want to make sure I have
83:43 - shuffle on shuffle is a parameter that
83:48 - shuffles the training data at each epoch
83:50 - because you don't always want to train
83:52 - with the data in the same order as
83:54 - you're tweaking all the weights and
83:55 - stuff as it's doing its training if it's
83:57 - in a different order it's gonna help it
83:58 - out but the validation data I think I
84:02 - looked at this before is before selected
84:06 - before shuffling so it's selected from
84:08 - the last sample so I might have a slight
84:10 - issue or if for some reason the order my
84:13 - data is in there's something weird about
84:15 - the end of it is all one label or
84:17 - something I probably won't like shuffle
84:18 - it myself manually but let's not worry
84:19 - about that right now
84:20 - but that's something definitely to be
84:21 - cautious of well this is so much to
84:24 - think about
84:25 - all right now so now that we've added
84:27 - shuffle and we've added 10% as
84:32 - validation data
84:33 - let me now run this again it's danced
84:37 - around
84:44 - oh it's finished already ten tiny boxes
84:51 - not very long so here we go
84:54 - and we can see this is good we still
84:57 - have a loss that's going down over ten a
85:00 - box maybe we can get this lost even less
85:01 - maybe one train for more epochs maybe we
85:03 - want a different learning rate we need
85:05 - to tweak all that stuff but I want to be
85:07 - least see it and see an animation going
85:09 - while I'm doing that so that's what I'm
85:12 - going to look at in the next video I'm
85:13 - gonna look at how to do an animation by
85:16 - also adding callbacks to these options
85:20 - here okay so ello and async you know I
85:23 - need to make this happen in an async
85:25 - function that's actually I think that it
85:26 - really helped me out okay I'll come back
85:28 - to that in next video thanks for
85:29 - watching so far so we train the model
85:31 - I'll be back soon oh shoot shoot I wrote
85:40 - the wrong thing hold on
85:46 - not you let's go back to where this
85:50 - finished we just run this again
85:55 - unnecessarily oh shoot and I'll fix that
86:00 - okay so I'm gonna do redo the end of
86:03 - this trip okay uh so we finished it
86:07 - trained now with the validation splits
86:10 - and OH breaking news breaking news
86:12 - getting information from the chat that I
86:15 - wrote validation data here interesting
86:17 - give me an error so if I wanted to give
86:19 - it specific validation data that's what
86:21 - I would use but I want to use validation
86:22 - split thank you for to the chat for
86:24 - correcting me there let's try running
86:26 - this again let's give it just more
86:28 - epochs a little bit more time to wait
86:30 - let's give it 50 all right all right so
86:34 - this is gonna be the sped up portion
86:35 - where I could play the ukulele but
86:38 - nobody will hear the ukulele in the
86:39 - edited version of this video
86:42 - why isn't still printing out this one
86:44 - tensor actually where is that there must
86:48 - be a labels tensor dot print alright
86:54 - [Music]
87:07 - alright so it's back let's take a look
87:13 - it's ok it's back let's take a look at
87:17 - our loss function over 50 epochs and we
87:20 - can see it's going way down to 0.75 you
87:23 - can see it's kind of stopped actually we
87:24 - kind of accidentally might have you
87:27 - could see how it kind of goes up now we
87:28 - can see like it's not able to get any
87:30 - better so we might not even need 50
87:32 - epochs but we might want to tune various
87:34 - parameters to see but I'm not going to
87:35 - worry about all that right now the point
87:37 - is I have now trained the model using
87:40 - model dot fit shuffling the data with a
87:44 - certain validation saving 10% for
87:46 - validation I'm not doing proper testing
87:48 - data yet that would come later
87:50 - and 50 ybox okay so in the next video
87:55 - what I want to do is make it so that I
87:59 - can run an animation I can graph the
88:01 - loss function over time all that sort of
88:03 - stuff and not have it kind of like
88:05 - blocking right the way it's doing right
88:07 - now the animation thread and then of
88:11 - course I also need to allow the user to
88:13 - specify a color and get a label for that
88:15 - so those are the next two steps I need
88:16 - to do see you in those videos
88:20 - okay sorry I'm my nose is running and my
88:26 - tissue box is empty I'm out of water I
88:28 - might need a little break let me see if
88:31 - I can find oh I have oh you know what
88:32 - look at me here prepared prepared with
88:36 - the Kleenex uh alright okay how are we
88:52 - doing everything everybody wait wait
88:59 - crazy teenager rights lol I was watching
89:03 - roblox and this on my recommended so did
89:06 - you are you were you watching just like
89:07 - some gaming and this was like arbitrary
89:09 - recommended to you and you just pops
89:11 - here welcome now if you don't know about
89:13 - coding what I'm doing here is probably
89:17 - like the most advanced stuff that I do
89:19 - on my channel and you might want to go
89:21 - back and watch a lot of the beginner
89:22 - tutorials if you're interested certainly
89:24 - don't have to slack is back okay let me
89:28 - see if I can get the slack channel going
89:34 - so for those of you might be wondering
89:36 - the slack channel is for sponsors of the
89:39 - of the YouTube channel or patreon
89:41 - patrons we have patreon it's the same
89:43 - thing there should be a sponsor button
89:45 - you can see I mean nobody eats the
89:46 - sponsor only if you feel so inclined
89:49 - you'll be able to watch all the content
89:51 - but I use that to have a smaller
89:53 - community people asking questions and
89:54 - discussing the stuff I'm trying to
89:56 - connect right now okay looks like it's
90:04 - coming back to me what time is it 12:30
90:07 - I'm okay on time it's gonna be way worse
90:10 - Josh in the chat asked do tutorials on
90:13 - slack BOTS no I would love to though
90:16 - okay okay let me close this thread okay
90:26 - I am now got the slack channel in
90:29 - welcome new sponsor ex-miss
90:32 - drink X so I don't know if my kids are
90:34 - at home they don't of school today pay
90:37 - for a babysitter so I could be here to
90:39 - do this live stream by the way so but I
90:43 - set it up my Phillips hue lights at home
90:44 - they blink some sponsors the channel I
90:47 - can't do that here because I'm in a
90:49 - lockdown in the NYU Wi-Fi I new sponsor
90:52 - Kyle right because I should mention the
90:53 - whole sponsorship if you're there thank
90:57 - you thank you to those very kind of you
90:59 - to sponsor it does really help me
91:01 - allocate time to doing these videos and
91:07 - algas thank you for thanking people and
91:10 - I really one thing I really need to do I
91:12 - need to do some maintenance in terms of
91:14 - like getting all the emojis stuff set up
91:18 - on the channel okay you have World Cup
91:23 - winner with prediction with tens of Lodi
91:24 - Jess ok alright I got to move on
91:29 - let's cycle these cameras cuz I got to
91:33 - get out of here
91:33 - I'm leaving for as I mentioned I'm
91:35 - leaving for a trip tomorrow for two
91:38 - weeks do my kids know how to code so my
91:41 - kids are 6 and 9 so I would say my
91:45 - younger daughter I talk and I show her
91:48 - things sometimes but she's really too
91:50 - young to do text based coding certainly
91:53 - and my son who actually is not 9 he just
91:55 - turned 10 was his birthday over the
91:57 - weekend he has done he's taking scratch
91:59 - classes and he's attended actually some
92:02 - p5.js workshops that I taught for fourth
92:04 - and fifth graders so he doesn't know how
92:06 - to do it unfortunately he fortnight is a
92:11 - thing it's very distracting anyway so
92:16 - let me having to get our gaming channel
92:17 - set up all right sorry ok ok all right
92:24 - let's move on now to the next video ok
92:34 - all right okay this is really let me see
92:40 - if I can do some where is it it's over
92:45 - here that it's like if I fold it I think
92:52 - that's better now
92:54 - the Kubo robot that sounds interesting
93:07 - okay
93:13 - okay I don't know about this fortnight
93:16 - thing I don't know if I should be
93:19 - allowing it okay all right all right
93:31 - videos seven thousand two hundred and
93:34 - sixty three of my making your own color
93:37 - classifier with previous video
93:42 - previously on making your own color
93:44 - classifier intense photo yes I worked in
93:46 - the model dot fit function so I'm
93:48 - fitting the model according to my
93:49 - training data with these options now
93:52 - what I want to do is I want to be able
93:55 - to basically see an animation graphing
93:58 - the loss function while it's doing the
94:00 - training so right now I just get a
94:01 - report when it's done so there's a few
94:03 - steps that I want to take to do this the
94:05 - first step that I want to do is I
94:06 - actually want to move this into a
94:08 - separate function so I'm gonna just
94:10 - write a function I'm gonna just make it
94:11 - a global function called train nice
94:15 - train and I'm gonna put model dot fit
94:20 - there then I'm gonna call train here so
94:23 - that's after one and let's let's uh oh
94:25 - and I'm gonna put the options here in
94:27 - this function and I'm gonna just go back
94:29 - to to a pox and I'm gonna run this Oh X
94:32 - is not defined oh boy I did all sorts of
94:35 - goofy stuff here so let's let's make
94:37 - these global variables X's and Y's I'm
94:41 - gonna need to do again just could use
94:43 - some refactoring but now it's training
94:45 - and to epochs done well you can see the
94:49 - loss functions great but still I don't
94:51 - have an animation so what I want to do
94:53 - is I want this to be an asynchronous
94:56 - function I want this function to be an
95:00 - asynchronous function to happen and let
95:02 - things keep going and guess what I have
95:04 - a video series about I do that with the
95:06 - keyword async and then if I say viii if
95:14 - I make a function async I can use the
95:16 - keyword await meaning this function will
95:20 - wait for model that fit to finish before
95:23 - it's done in
95:24 - turns a promise by the way so I can
95:27 - actually take this now I could say
95:32 - return a weight and then I can put my
95:36 - then up here right because it's gonna
95:38 - return that same promise but it will
95:43 - happen asynchronously meaning it will
95:46 - the code up here will be allowed to move
95:48 - on while this is happening in the
95:50 - background in theory but I've got to do
95:53 - more here it's the same behavior hmm
95:59 - so why is it the same behavior well I've
96:01 - set myself up for success but I don't
96:03 - have success yet and the reason why is
96:06 - that tensorflow das is using something
96:11 - called WebGL to do all of the
96:13 - calculations and it's taking over
96:15 - basically your animation or drawing
96:17 - capabilities while you're fitting the
96:19 - model however tensorflow J s comes with
96:23 - a function called next frame which
96:28 - returns a promise that resolves when a
96:30 - request animation frame has been
96:33 - completed it's simply a sugar method so
96:36 - that users can do the following a weight
96:38 - TF next frame so what I can actually do
96:41 - is kind of trigger the animation letting
96:44 - drew the draw loop go the p5.js draw
96:46 - loop is just using requestanimationframe
96:48 - itself by adding a weight TF next frame
96:53 - somehow in this async function so where
96:57 - do I add it so I have an idea I'm going
97:01 - to add something to this called
97:03 - callbacks so and I got a spell callbacks
97:07 - correctly for this to work so let's go
97:09 - back to look at model dot fit model dot
97:14 - fit and we can see that oh look at this
97:19 - this is like a list I was looking in the
97:23 - wrong place but last night when I was
97:24 - looking this up it wasn't actually here
97:27 - these are optional callbacks that can be
97:31 - called during training for example on
97:33 - train begin on train end
97:35 - EPOC begin on epoch and on back begin on
97:39 - batch end so let's just let's add on
97:42 - train begin on train end just for real
97:45 - quick so I'm gonna say I'm gonna have a
97:47 - callback on train begin and this needs
97:51 - to be a function it's gonna my life's
97:52 - gonna be easier if I just use this yes a
97:56 - six arrow notation and then I'm gonna
98:02 - have another callback called on train
98:04 - end and I'm gonna say training complete
98:09 - so I'm gonna just add these two
98:12 - callbacks so these are functions that
98:14 - are going to be executed during the
98:16 - training process let's see if I did that
98:19 - right training start and try to complete
98:24 - and I see the results wonderful so what
98:27 - if I in train begin just make it a
98:31 - slightly longer function which also has
98:34 - a weight TF next frame in it what
98:38 - happens if I await the next frame in it
98:43 - no oh huh
98:47 - do I have to say a sink here wait where
98:52 - do I put that
99:02 - all right that's not what I wanted to do
99:12 - let me go back
99:18 - we're gonna look at the chat while I'm
99:20 - taking a little okay alright alright
99:26 - nobody's discussing anything about like
99:30 - does anybody actually watch the stream
99:31 - or you just come to talk about like
99:33 - different programming languages and why
99:34 - why why why this is making me crazy okay
99:44 - all right training start training
99:48 - complete okay now let's try a different
99:51 - callback let's try on epoch end and on
99:58 - epoch and well it takes two arguments
100:03 - I'm looking over here on this computer
100:04 - because I I have some notes there which
100:07 - I don't typically do but it's the
100:09 - documentation here doesn't actually if
100:12 - we look here it's not telling you what
100:15 - the arguments are for these callback
100:17 - functions but I looked them up and so
100:21 - the arguments are the number of epochs
100:23 - so I can say num and then a log which is
100:27 - like a report so I'm gonna say num and
100:29 - logs and then what I can do so what I'm
100:33 - gonna do is I'm gonna do console I'm
100:35 - gonna write a function here with
100:36 - multiple lines of code I'm gonna say
100:40 - console.log epoch num and then I'm gonna
100:46 - say console log loss a logs dot loss
100:52 - loss so there's a property of loss
100:56 - that's in that logs object so these are
100:58 - the arguments do every time it finishes
101:00 - an epoch so I'm gonna now give it 10
101:03 - epochs let's see what happens if I add
101:05 - that call back alright
101:10 - epoch zero epoch one epoch to epoch
101:13 - three epoch four look at that so I am
101:16 - now getting I'm getting a call back for
101:19 - every one of those individual epochs and
101:21 - we can see the loss going down and then
101:23 - of course we see all of the lost values
101:24 - when we're done but now if I want to let
101:28 - it draw something
101:29 - I believe I can say oh wait TF dot next
101:34 - frame hold on
101:35 - timeout did I talk about what I'm so
101:40 - lost in to what I'm so lost like did I
101:49 - explain what TF not next frame is
101:51 - already cuz I went back and forth and I
101:53 - like went down a route and then I didn't
101:55 - want to I think I did explain it
102:04 - shoot I'm confused I'm just gonna have
102:06 - to do my best
102:07 - Matthew I'm sorry hopefully this will
102:09 - come together I might end up explaining
102:11 - the same thing twice okay if I want to
102:17 - draw something at the end of each epoch
102:20 - I want to allow the animation to proceed
102:23 - I can go and use that function TF next
102:28 - frame its whoops TF next frame which
102:36 - allows me to which allows me to sort of
102:39 - unlock the drawing thread and and let
102:43 - draw update itself so I'm gonna go and
102:45 - I'm gonna say a wait TF next frame right
102:52 - here at the end of each epoch and then
102:55 - this is also an async function so this
102:58 - now has to be an async function as well
103:03 - is that gonna allow me to do that I
103:05 - think so yes okay let's try this
103:14 - oh yeah look it's drawing now let's
103:20 - actually add an animation so let's do
103:22 - something like stroke 255 stroke wait
103:26 - for line a frame count modulus with zero
103:31 - frame count modulus with height so I
103:34 - just want to draw a line that is that is
103:38 - moving across so for example if I don't
103:41 - bother calling this train function
103:42 - all we can see here I have an animation
103:48 - that's running okay so now let's call
103:52 - the train function and see if that
103:55 - animation runs waiting waiting waiting
103:57 - waiting waiting waiting let's get to
103:59 - epoch zero I'll look at it so the
104:02 - animation is going but it's only able to
104:04 - draw once at the end of each epoch so
104:08 - while it's training if I want to let it
104:11 - unlock that drawing more often maybe a
104:14 - different callback would work better and
104:16 - in fact one something that tension photo
104:18 - jess is doing behind the scenes and
104:23 - model dot fit sorry is in the callbacks
104:29 - right it's actually batching the data so
104:32 - I have five thousand six hundred data
104:34 - points it's actually running the
104:37 - gradient descent algorithm in batches
104:40 - that's what stochastic gradient descent
104:41 - means and there are also on batch begin
104:44 - on batch ends and I could sort of
104:46 - specify the batch size I'm letting it
104:47 - use a default so what I actually think
104:50 - that I want to use it's gonna be able to
104:51 - it does a batch pretty quickly a full
104:53 - epoch takes quite a bit of time so I can
104:55 - actually do on batch end what I'm going
104:58 - to do here is I'm gonna add one more
105:01 - callback on batch end and I'm gonna make
105:05 - this the async one so it also has a
105:09 - batch num number and a number of logs so
105:13 - it's the like epoch end but I'm gonna
105:16 - put the await next frame in there
105:20 - this one no long as it needs to be an
105:22 - async function so this should unlock the
105:24 - animation much more quickly because it
105:26 - lets it draw every at the end of every
105:29 - batch so let's go to this now and we
105:33 - should see yeah look at this so the
105:34 - animation is running just fine and we
105:37 - should see now it got a little glitch
105:41 - there when I got to the end of epoch 0
105:43 - let's see if it does that again no I
105:45 - don't know what so the first epoch they
105:47 - must have had to do some copying onto
105:49 - the GPU I'm not sure why but you can see
105:51 - the animation is no longer study
105:52 - stuttering from epoch to epoch
105:55 - okay so now we have a trading the model
106:00 - with an animation going let's at least
106:03 - so what I really should do is graph the
106:06 - loss function and by the way I can look
106:08 - at the loss function at the end of each
106:09 - batch so I can get a much more quickly
106:11 - updated loss function so I'm gonna leave
106:13 - that as an exercise to the viewer but
106:15 - I'm gonna just what I'm gonna do is I'm
106:17 - gonna say let loss P and I'm gonna
106:24 - create a paragraph element again I'm not
106:25 - really being thoughtful about design and
106:28 - interface here loss so what I'm going to
106:31 - do here is it's just going to have a
106:33 - paragraph element that says loss in it
106:35 - and what I'm going to do is instead of
106:40 - logging the loss to the console I'm
106:42 - gonna say loss P dot HTML and this is
106:46 - using the p5.js Dom library I give use
106:48 - native JavaScript or jQuery I'm going to
106:50 - put this loss information into that
106:53 - paragraph element so now I have an
106:58 - animation going and then as soon as I
107:00 - get to the end of the first epoch I have
107:02 - to talk for a bit here I see the loss
107:05 - function so now I'm training and getting
107:07 - a report of the loss function so for you
107:09 - I'm in the next video what I'm gonna add
107:11 - is inference or prediction I'm gonna
107:14 - allow the user with sliders to specify a
107:16 - color and have the label returned to me
107:18 - and what I would say to you as an
107:20 - exercise is see what happens if you can
107:22 - query the loss function with the batches
107:25 - and graph it over time and so you see
107:27 - that would be an exercise to you as the
107:29 - viewer and I'm gonna publish a github
107:32 - repo with this finished project so you
107:35 - can look for the code this is very
107:36 - confusing but you can look for the code
107:37 - it'll be linked in the description in
107:39 - two different places there'll be the
107:40 - code that matches exactly this video and
107:43 - then there will be the code that's in a
107:45 - separate github repo that and someone in
107:47 - the future people will be contributing
107:48 - to that will have maybe the graph and
107:50 - other kind of designing things that
107:51 - people have from the community have
107:52 - added okay great so one more video to go
107:56 - I think and and then some other
107:58 - ancillary ones that I forgotten about
107:59 - but one more core video to this tutorial
108:02 - series which is adding the
108:04 - and I will see you if you're really
108:06 - gonna watch all of these I will see you
108:08 - in the next video you can check the
108:10 - video's description for the next video
108:11 - link okay
108:32 - okay so that's interesting I should have
108:34 - been looking at the chat maybe I'll
108:35 - cover this in the next video let me try
108:37 - some things so you're saying if I just
108:41 - say return TF next frame I don't need
108:46 - the await an async thing I like this
108:57 - better and then in fact I can just say
109:01 - TF next frame that's the callback I want
109:04 - to happen oh that's way nicer so I'm
109:12 - gonna explain this at the beginning of
109:14 - the next video yeah all right great
109:18 - Oh Dave into the chat let's see where
109:24 - can I find the code to the 500k
109:26 - subscribers map Doozers I'm Way behind I
109:28 - haven't published it yet I you know once
109:31 - the video gets publishes or like a
109:32 - coding challenge then I publish the code
109:35 - if you hit me up on Twitter at Schiffman
109:37 - I can probably it'll be a reminder to me
109:39 - and then I'll reply to you there and
109:40 - upload that code okay okay you want
109:47 - validation loss not training loss donut
109:49 - aren't I getting oh are the do the logs
109:52 - have different loss
109:55 - I thought the logs got lost is the
109:58 - validation lost let me look at this
110:10 - oh that's the validation loss is
110:13 - different I didn't realize that okay
110:20 - great
110:22 - did I say video subscription I meant to
110:25 - say description
110:26 - did I say subscription shoot brain is
110:31 - melting I need water today wouldn't mind
110:34 - if I go get some water I gotta stay
110:40 - hydrated on the coding train as you know
110:48 - welcome new sponsor Thank You FD God my
110:52 - lights are blinking at home if I could
110:53 - be home right now I would see the
110:55 - blinking lights probably off all right
111:04 - okay
111:05 - so thank you so the things I need to
111:07 - cover at the next video are the
111:10 - validation loss versus the loss thank
111:12 - you for that and interestingly enough I
111:15 - think there is a problem with my
111:17 - validation data
111:18 - I really should I need you need to
111:20 - shuffle it and I just have very little
111:22 - data so that's not great and I need to
111:34 - talk about this simplification here of
111:36 - TF next frame
111:45 - okay
112:01 - yeah okay
112:11 - all right everyone I'm back start over
112:25 - all right oh this is getting tiring but
112:28 - I am back and I have yet another in this
112:33 - building your own custom color
112:35 - classifier with 1000 GS series now the
112:38 - thing that I want to add to this video
112:40 - and by the way this line moving across
112:42 - is pointless I just have it there so
112:43 - that I could see that the draw loop is
112:45 - animating that I haven't blocked it
112:47 - there's two things that I missed that
112:49 - are kind of important from the previous
112:51 - video um one is this is actually not the
112:55 - validation data loss I didn't realize
112:58 - this but I'm going to I'm gonna change
113:00 - this here I'm gonna I'm gonna
113:02 - console.log the full logs object so
113:07 - right what I'm putting on to the screen
113:08 - is logs dot loss let me come to the
113:12 - console log what's there so again we
113:14 - have to wait a minute for the first
113:15 - epoch to finish apologies for that
113:18 - okay there are actually two lost values
113:22 - there's the loss function can be
113:24 - computed against the training data and
113:26 - there's the loss function computed
113:28 - against the validation data now to do
113:30 - this properly I really should be using
113:32 - the validation loss because that data
113:35 - that hasn't been done with the training
113:38 - that's that's that's that's protect
113:40 - against overfitting having my model work
113:42 - really well with the training data only
113:44 - the thing is I have a very small data
113:46 - set 5,000 data points I'm just using 10%
113:50 - as the validation data and the weight to
113:52 - the footage s works it also takes that
113:54 - 10% from the end and I didn't wasn't
113:57 - careful about shuffling the data around
113:59 - so this is something that I should come
114:00 - back to I don't know maybe this series
114:02 - will go on to infinity but if I were
114:05 - doing this properly I would actually
114:07 - want to show the validation loss here
114:11 - like this log stop validation loss maybe
114:14 - I want to show both and maybe I want to
114:16 - be more thoughtful about shuffling the
114:17 - data first in advance but I that's not
114:20 - what I said I was going to do in the
114:21 - next video so I'm again leaving that ten
114:23 - rarely as an exercise to the viewer or
114:25 - I'll come back and do it in a future
114:27 - video I don't know yet
114:28 - that's item number one item number two
114:31 - thank you to me I am so me and others in
114:33 - the coding train sponsor patron group
114:36 - I made this way more complicated than it
114:39 - needs by trying to make this an async
114:41 - function in here actually this does not
114:43 - need to be an async function if I just
114:46 - return TF dot next frame so if I just
114:49 - return TF next frame it's actually
114:51 - returning the promise and unlocking the
114:53 - draw loop so that makes it simpler
114:55 - actually I couldn't make this so simpler
115:00 - what am I doing here at the end of every
115:02 - batch I want TF next frame to be
115:04 - executed and so I actually don't need to
115:07 - write a wrapper function to execute see
115:09 - if that next frame what I could just do
115:12 - is set that as the callback the callback
115:15 - again if I wanted to do more with on
115:17 - batch and look at the loss and the logs
115:20 - but really what I want is at the end of
115:22 - every batch to draw a new frame of
115:23 - animation I can just put TF knocks next
115:26 - frame as the function which is the
115:27 - callback there
115:28 - okay so let's this is still working that
115:32 - simplifies the code makes it a little
115:34 - nicer to look at I don't even really
115:35 - need this on on begin and on end but
115:40 - I'll leave those in there just so you
115:42 - see them okay so now I'm ready for what
115:44 - is the purpose of this video the purpose
115:46 - of this video is while I'm training the
115:49 - model I couldn't wait till I finish
115:50 - training the model but I've actually it
115:52 - allowed this to happen while I'm
115:53 - training the model I want to be able to
115:55 - specify a color and see what the neural
115:58 - network thinks that color is so very
116:01 - quickly to do this what I'm going to do
116:03 - is I'm going to create our slider G
116:05 - slider B slider I'm gonna make three
116:08 - sliders again this could use a lot of
116:11 - improvements and I'm gonna use the p5
116:14 - Dom library create slider function so
116:16 - the slider is a range between 0 and 255
116:18 - and let's start with like what's this
116:22 - red and green make yellow let's start
116:26 - with a yellow and so the G's the B
116:32 - slider should be on 0 and
116:37 - I want the background color too and I
116:41 - don't know what that's doing there I
116:42 - don't need this line anymore it's
116:44 - distracting I want to say our slider
116:47 - well let's actually let's so I want to
116:50 - say let our equal our slider value so I
116:55 - want to get the values from the sliders
116:56 - I want G and I won't be eventually I'm
117:01 - gonna send these as inputs into the
117:03 - neural network but right now I just want
117:04 - to be able to see that color our G B
117:07 - okay so here we go so now we should see
117:10 - there are three sliders and as I adjust
117:12 - these sliders I can change the color and
117:15 - so what I want
117:16 - whoops what I want is to be able to and
117:20 - I see though what I want is now to see
117:22 - the neural networks prediction down here
117:23 - so how do I do that okay time to use
117:28 - tensorflow touch yes again whoo
117:30 - so I need to make some input data so the
117:35 - input X's are tensor T F dot tensor 2d
117:43 - and an array with RGB in it now in
117:47 - theory I could be running prediction
117:50 - with multiple RG B's right but I'm not
117:55 - so I need an array of arrays in here so
117:59 - this is my input data then what I want
118:03 - to do I want to say model dot predict
118:06 - with those X's feel like you know what I
118:10 - need to normalize those right because
118:15 - the it expects to have normalized values
118:17 - between 0 & 1 so I need to divide each
118:19 - of those by 255 then I need to call
118:21 - model dot predict and then look at the
118:26 - results and that hat oh you know what
118:32 - this doesn't actually happen
118:34 - asynchronously
118:38 - it's the because the data is still on
118:41 - the GPU this is a confusing thing I have
118:43 - to pull I'm gonna use that date I have
118:45 - to pull it out but let's just look at
118:46 - the results of pure results so I should
118:49 - then be able to say
118:50 - results dot print okay so I think this
118:55 - is knee just creating the inputs getting
118:58 - the prediction and then I should be able
118:59 - to see that in the console syntax error
119:04 - who I have an extra extra curly bracket
119:08 - alright okay so we can see this and this
119:11 - is exactly what I should be getting
119:13 - right it is a probability distribution
119:15 - over nine labels now whether it's giving
119:19 - me correct ones who knows but look at
119:21 - that so now how do I get the label out
119:24 - of there well remember that what I'm
119:27 - looking for is I'm looking for which
119:30 - probability is at them at the highest
119:33 - level is it a ninety percent chance of
119:35 - it being yellowish and point zero one
119:38 - point zero two point zero three you know
119:40 - 1% 2% 3% of being the other ones and
119:42 - there actually is a function intention
119:44 - flow Jas that will pull out the index of
119:48 - the highest probability value that's
119:51 - called Arg max right I could write a
119:53 - little for loop or or some kind of
119:56 - function to do that but if I look for
119:57 - Arg max TF dot Arg max returns the
120:01 - indices of the maximum values along an
120:04 - axis so this is can be quite more
120:06 - complex because I could have
120:08 - multi-dimensional data but I actually
120:09 - get to do this in a really simple way I
120:11 - just want to say let index equal results
120:17 - dot r DX oh and if there's an access of
120:22 - AK c access of one the first there's a
120:24 - one-dimensional here so now let me say
120:27 - index dot print and so let me run this
120:32 - and we can see it's just giving me whom
120:36 - is that right is that a coincidence so I
120:39 - should get some different values yes
120:42 - okay so it's actually changing so that
120:45 - that's giving me that maximum index so
120:48 - as I change so so this is my label
120:51 - here's the thing though that's my label
120:53 - but I need to convert that to one of
120:57 - these so 0 means reddish one means
120:59 - greenish two beads bluish 3 means
121:02 - orangish so
121:04 - have this label list already I should be
121:08 - able to just say let label equal label
121:12 - list index the only thing is I can't do
121:18 - that because this is a tensor
121:20 - that's a tensor and what I want I need
121:24 - to pull that the tensor is the numbers
121:26 - the data that lives on the GPU the WebGL
121:28 - fancy thing that TouchWiz has
121:30 - implemented I need to pull that off and
121:33 - normally I would pull that off with an
121:35 - asynchronous function shoot normally
121:46 - normally I would pull that off with an
121:47 - asynchronous function but the thing is
121:50 - here it's such a little tiny bit of data
121:54 - I think I can pull it off synchronously
121:56 - and not slow down my program from
121:58 - running so actually what I want to say
122:00 - here is Data Sync and then which is a
122:05 - dot data would pull it off
122:06 - asynchronously so let's look at and
122:09 - let's let's say I'm a console dot log
122:13 - index and let me get rid of my other
122:15 - console logs that I don't really want to
122:19 - look at right now so okay so I got an
122:25 - array with the number in it I pulled it
122:28 - off and so then I just want to say index
122:33 - 0 so I only need that first value index
122:36 - 0 and so there we go
122:40 - that's the label number I now have the
122:42 - label number and so now I can say this
122:44 - and I can say and let's put it out on a
122:48 - paragraph element so let's say let label
122:52 - P let's have that B first and so now I
122:57 - want to say label P equals creepy and
123:02 - then I should say all the way back down
123:06 - here label P dot HTML label
123:09 - ok ready for this here we go I've
123:13 - started my training oh wait why this is
123:16 - so silly but I want the labels
123:18 - of it I really should not be changing
123:20 - this right now so let me just put it
123:24 - here okay so here we go it thinks that's
123:31 - greenish right well it hasn't gotten
123:33 - very far with the training I would
123:34 - imagine that once we train further and
123:36 - the law starts going down it's going to
123:39 - recognize that as yellowish so here I'm
123:41 - gonna just wait a little bit and I'll be
123:43 - back in a minute
123:44 - let it rain there's some a pox and I'll
123:47 - look at the chat has Schiffman open the
123:58 - slack yet no no I still have open yes I
124:00 - do have a slack open I'm gonna let this
124:04 - run for a little bit I kind of wanted
124:06 - this to appear as yellowish okay I only
124:12 - do 10 epochs I don't know I should be
124:15 - console longing the epoch there we go
124:18 - look look look look look it's learning
124:23 - [Music]
124:24 - all right back
124:26 - so he trained over ten a box and you can
124:28 - see now it's saying this is yellowish
124:30 - let me tune this down that's greenish
124:32 - turn this up
124:34 - that's bluish we've still got blueish
124:36 - can we get some purple purplish can we
124:39 - get some pink oh it didn't get pink
124:41 - maybe if I add a little more brightness
124:43 - how it thinks that's pink so I have now
124:47 - trained the neural network to recognize
124:49 - and let's see if it can get red reddish
124:52 - so we could see I could play with this
124:53 - all day long this is now going to
124:55 - classify the color based on that
124:58 - particular model so in a way I'm done I
125:00 - probably want to train it for more
125:03 - epochs what are some things that I want
125:05 - to do so one is I would want to be more
125:08 - thoughtful get more data I would want to
125:10 - be more thoughtful about the validation
125:12 - data and then other thing I would want
125:14 - to start doing is thinking about well
125:15 - does it actually work better what are
125:17 - the hyper parameters that I can play
125:18 - with for example the hidden layer I put
125:23 - sixteen units in it well or what happens
125:25 - if I use a different activation function
125:27 - for the
125:28 - lare what happens I use more nodes or
125:30 - less nodes what if I change the learning
125:32 - rate what if I change the optimization
125:35 - function if I use like the atom
125:36 - optimization function so these are
125:39 - things that all these things are things
125:43 - that I could play with and research and
125:45 - think about an experiment with to try to
125:48 - tune the model really well then at some
125:50 - point I also would want to save that
125:52 - model right save it to a JSON file so
125:55 - the trained model somehow so that I
125:58 - could load it back in without having to
126:00 - run through the training process again
126:01 - maybe I'd even want a larger dataset I
126:03 - don't to train it over a long time me
126:05 - but I want to port this code to node so
126:07 - I could let it train-like server-side
126:09 - without having to train in the client
126:10 - there's so many possibilities but I have
126:12 - now built a machine learning model with
126:17 - tensor photo how many videos this took
126:22 - that trains a model based on crowdsource
126:25 - color data and if you want if you just a
126:28 - humor me for a second if you remember if
126:32 - I go here this is the system right this
126:35 - system was used to allow people from the
126:39 - internet to click on and say that's like
126:41 - pinkish that's greenish that's blueish
126:45 - tag a whole bunch of colors save all
126:48 - that data in a firebase database
126:49 - retrieve all that data clean that data
126:52 - put it into JSON file load that JSON
126:55 - file here into this sketch build a model
127:00 - train the model with that data and then
127:04 - pulls a new color from a slider oh and I
127:06 - forgotten something
127:08 - memory management oh I knew there was a
127:10 - step that I'm missing estimating what
127:14 - category out of the fixed set of labels
127:16 - this that color is but I did forget
127:19 - something really quite important which
127:23 - is memory management let's look at this
127:25 - num memory of TF memory dot num tensors
127:35 - so again when I create tensors that are
127:38 - allocated to memory on the GPU to store
127:41 - numbers
127:41 - those don't get cleaned up automatically
127:43 - there's no garbage collector like in
127:45 - kind of regular JavaScript programming
127:47 - so 15,000 for 85 tensors now one thing
127:50 - and there's still even more and more and
127:53 - more it's growing this is a memory leak
127:55 - so one place where I didn't clean up any
127:58 - of the tensors is right here and there's
128:00 - a easy way I can clean this up
128:02 - by adding in the TF tidy function so
128:08 - what TF tidy does is it says just put
128:11 - all of this code that's inside of this
128:14 - function passed into TF tidy clean up
128:17 - any tensors that are made there so this
128:19 - will clean up everything for me so now
128:23 - let's run this again and we're gonna
128:25 - take a look at the tensors there's
128:27 - thirty one seventy three it's kind of
128:30 - leaking right well let's let it get all
128:32 - the way through ten EPOC s-- I'll be
128:34 - back in a minute when that finishes okay
128:47 - it's taking I should add that I should
128:49 - have something that's reporting the
128:50 - epochs
128:53 - [Music]
129:09 - oh it finished okay okay so the training
129:36 - is complete and we can see now ah there
129:41 - we go
129:41 - I am no longer leaking tensors now the
129:45 - thing is do I really did I really need
129:47 - 1628 tensors I don't think that I did I
129:50 - think there is also there is also a leak
129:53 - going on inside of this train function
129:57 - and I think there's an issue with this
130:01 - and so I'm gonna I might have to do a
130:03 - follow-up video about this because at
130:05 - the moment if I go to github.com TF oh
130:11 - oh hold on let's go from here too
130:17 - I should have had this prepared where do
130:20 - I go github and issues and I'm gonna
130:26 - look for fit memory leak this one so I
130:33 - believe there is at present a memory
130:37 - leak in model dot fit with callbacks and
130:40 - you can see that's exactly what I'm
130:42 - doing right where model dot fits with
130:47 - callbacks
130:48 - so I'm gonna not worry about that
130:50 - particular memory leak right now I'm
130:52 - gonna wait for us to see if that gets
130:54 - corrected by the time you're watching
130:55 - this that might already be corrected and
130:57 - this code might have no more memory
130:59 - leaks in it just by updating the version
131:01 - of tensorflow Tijs or I might still be
131:03 - missing something in here to do a memory
131:05 - leak so you know if you don't want any
131:07 - spoilers and/or the following videos
131:10 - the fallout videos have not been
131:12 - published yet you could kind of kind of
131:13 - like sort that out yourself but I will
131:15 - come back at some point and talk about
131:16 - that okay so thank you for watching I
131:20 - wish you many
131:23 - purplish and pinkish and bluish and
131:27 - greenish days all the colors of the
131:30 - rainbow may they fill your days with joy
131:33 - may you make your own classifier with
131:35 - your own data please share with me I
131:37 - don't know it has this helped the world
131:39 - this tutorial series I've missed so much
131:41 - about data and data collection and
131:43 - machine learning and bottles and
131:44 - algorithms but hopefully I've got done
131:46 - something this is not the end
131:47 - it's only the beginning well I'll see
131:49 - you soon in future tutorial videos that
131:52 - up because this playlist probably has
131:53 - about 300 ok good bye alright everybody
132:11 - ok it is 1:15 I have done what I set out
132:15 - to do today which is finish this example
132:19 - I am now going to I'm gonna leave within
132:23 - 15 minutes at 1:30 because I because I
132:27 - because I need to do that I'm gonna
132:28 - check some messages here because I have
132:30 - all these texts mission to make sure
132:32 - yeah but 11:40 somebody spoke so that my
132:39 - kids are at home with a babysitter they
132:41 - hadn't left the house yet at 11:40 the
132:43 - lights blinking so they must have left
132:46 - because I'd get a so that did get a text
132:48 - message saying that the lights blinked
132:49 - at home who was that who sponsored that
132:51 - was awesome ok so that was all right
133:04 - great
133:07 - ok so I'm going on vacation tomorrow for
133:12 - two weeks
133:14 - and so I will be online here and there I
133:18 - let's see there are currently 7 videos
133:23 - edited versions of the color classifier
133:25 - series that have yet to be published I
133:27 - could use help keeping the code getting
133:31 - the code that goes with those videos
133:32 - online if anyone is like coding along or
133:35 - I have all the stuff it
133:36 - Google Drive and wants to help with that
133:38 - you know open an issue on coda train
133:41 - calm no no github slash Cody train slash
133:44 - website could you just help with that
133:46 - but I will hopefully keep up with that
133:48 - matsya who also who is does the editing
133:53 - will be uploading and editing and these
133:55 - videos will get released so the channel
133:57 - won't appear dead but there won't be any
133:59 - live streams until the week of July 15th
134:01 - I have something that I'm really trying
134:03 - to figure out how to do and so I'm
134:05 - looking this can be discussed in the
134:07 - slack Channel I really really really
134:11 - want to get the updated version of this
134:14 - book out I have a new chapter I want to
134:17 - write about neuro evolution I want to
134:20 - fix all the mistakes update all the code
134:22 - and get a p5.js version of this book out
134:25 - this summer is the time for me to do it
134:27 - now when I get back in July I've two or
134:29 - three weeks of being in New York City
134:31 - before I'm out of town again and I was
134:34 - planning to do to live streams per week
134:36 - to keep up with the YouTube channel but
134:39 - I might need to take a more of a break
134:40 - from that so I can focus on the writing
134:42 - of this so I guess I'm curious to hear
134:44 - from people you know the you know in
134:47 - terms of like keeping a youtube channel
134:48 - going you should never what what
134:50 - everyone says is don't stop making
134:52 - content but that's not realistic people
134:53 - have to go on vacation they have to take
134:55 - breaks so I don't believe in that you
134:56 - know the views go down subscribers go
134:58 - down such is life
135:00 - but I do know that there are people who
135:03 - are funding the work that I'm doing
135:05 - through patreon and the YouTube
135:06 - sponsorships so what I want to do is
135:09 - take the temperature how people feel
135:10 - about kind of that funding essentially
135:13 - going towards the time of working on
135:15 - this book taking away some of the time
135:18 - of doing live streaming so we can
135:21 - discuss that in the slack channel I'll
135:23 - be curious for people's thoughts but so
135:25 - I don't know to what my my goal is
135:28 - always for live streams per month that's
135:31 - what I'm kind of committed to and I've
135:33 - actually done five or six the last few
135:35 - months and so July if I did to the weeks
135:38 - that I'm back if I did two per week I'd
135:39 - get to four and August I could get to
135:41 - four but I'm thinking of actually doing
135:42 - fewer and and focusing on the book so
135:46 - I'll be curious for your thoughts about
135:47 - it and me I am so me
135:49 - says that I should work on it livestream
135:53 - working on it pardon there is a way that
135:55 - I could possibly do that I'm gonna have
135:58 - to that's an interesting idea okay so
136:05 - any questions anyone wants to ask I'm
136:08 - very I also really want to get stuff in
136:11 - the shape for the channel but can you
136:15 - have guests fill in I so that's a great
136:17 - suggestion and I do want to have more
136:19 - guests than I've had guests it's it's
136:24 - the same or it's a different kind of
136:26 - work to have the guests it's not the
136:28 - same amount it's not the same work but
136:31 - having guests is would not open up more
136:34 - time to work on the writing of the book
136:36 - it would be a good thing for the channel
136:39 - and for the world I would hope having
136:42 - more guests I would like to do that and
136:44 - I definitely planning to do that but I
136:46 - don't that doesn't solve the problem of
136:48 - allocating more time to work on the book
136:52 - okay any questions from anyone
136:57 - BRIC the world asks what programming
137:00 - languages do you know I would say the
137:03 - one the language that I probably know
137:04 - the best even though I don't know any of
137:06 - them that well is Java with I don't know
137:10 - dare I say second is now JavaScript I
137:12 - mean I only started programming
137:13 - JavaScript like two year three years ago
137:15 - probably longer at this point I'm sort
137:17 - of forgetting how much time has passed I
137:19 - to program in C and C++
137:22 - I used to I mean like 15 years ago so I
137:24 - guess I know those languages and I've
137:26 - hacked around PHP and Python and Perl as
137:30 - well next time use atom with helu it's
137:36 - very fast let's try that really quickly
137:38 - so I should do a fall and want someone
137:40 - help me keep track of follow-up videos
137:41 - so let me just try that real quick if I
137:44 - put atom here and then put this here
137:53 - know how do is it just lowercase
137:59 - oh look at that hold on a sec
138:17 - it's I don't know that it's necessarily
138:20 - happening faster but looks like that
138:22 - loss went down quite a bit and it got
138:25 - too yellowish very quickly you know
138:29 - interesting
138:30 - so it's interesting again to try these
138:31 - different architectures I'm gonna put it
138:34 - back because when I published this code
138:43 - alright this is the stream over yes it
138:48 - is over any classifier ideas that we can
138:51 - practice huh interesting question well
138:55 - the next thing that I was planning to do
138:57 - I based on how long this took I'm not so
139:00 - sure about any more of us to do image
139:01 - classification and use convolutional
139:04 - layer so that's something you could sort
139:05 - of try amazingly everything this here is
139:08 - the same just the data the input data is
139:11 - two-dimensional and you'd want to add a
139:13 - convolutional layer but to hit two two
139:16 - hidden layers one being a convolutional
139:17 - layer the learning rate should be around
139:20 - 0.0001 probably yeah I you could train
139:28 - it to use the yeah what is your so show
139:30 - hum is asking where's the slack
139:32 - workspace the slack right now is open
139:34 - only to patrons or sponsors of the
139:37 - channel and you will get an invitation
139:39 - if you're a YouTube spot I'm using both
139:41 - systems right now I should probably just
139:42 - pick one at some point but if you're a
139:44 - YouTube sponsor I don't get your email
139:46 - to send you an invite automatically so
139:48 - look through that look at the community
139:50 - tab there's a post there that you'll be
139:51 - able to see where you can find out how
139:53 - to submit your email okay
140:02 - are we going to train or get the Train
140:05 - miles from Python I actually do have a
140:07 - plan to train to do a video about
140:09 - training and LS TM model with Python and
140:14 - then have that train model coming to
140:16 - JavaScript I'm actually there's a great
140:18 - post about how to do this by paper space
140:25 - well let's see if I can find this paper
140:27 - space LS TM
140:30 - Khris Khris no so we could find this
140:39 - paper space LS TN blog yep here it is so
140:47 - this is a post just recently came out by
140:52 - Cristobal who is one of the main
140:54 - maintainer x' and contributors to the
140:56 - ml5 library which and so this tutorial
140:59 - goes through how to work with paper
141:03 - space and train the model with your own
141:05 - data and then run the model in using ml
141:11 - 5 j s which is a higher-level library on
141:13 - top of tensor photo jazz so that's the
141:14 - other thing that's just too much to do
141:15 - because my other priority is to make a
141:18 - lot of video tutorials with ml 5 but
141:21 - I'll just you know um go as I go and you
141:27 - can see it running here this is so you
141:32 - can see it's doing it's predicting doing
141:34 - predictive text based on the train model
141:38 - okay well I forgot to change atom I
141:44 - thought I didn't didn't I do that right
141:47 - here people are telling me I forgot to
141:51 - change atom but I thought I did
142:02 - and I also being told me I should have a
142:04 - much lower learning rate for Adam oh
142:09 - yeah Riaan for me I would love to do all
142:11 - the things reinforcement learning is
142:13 - actually something that I am most
142:15 - interested in more so in a way than this
142:17 - sort of classification stuff which is
142:19 - I'm also interested in but I haven't had
142:21 - the time to dig into it yet i've been
142:24 - doing neuro evolution and that i have a
142:27 - whole set about that which is kind of
142:29 - reinforcement learning style thing
142:30 - oh is it possible to code the
142:34 - approximation from tau Oh June 28th I
142:38 - don't have a child a challenge oh I mean
142:45 - it should be SGD oh it should be SGD
142:50 - with oh boy the point is you to let
142:57 - everybody else play with this stuff yes
143:02 - I did end the video with SGD oh I see
143:07 - it's fine now okay I don't know what I
143:10 - got it I got it I got it all right
143:22 - because today is coming up I mean this
143:27 - is just though me this is kind of like
143:31 - where is my there it is
143:54 - if I had an idea for a really short
143:57 - coding challenge with Tao I could do I
143:59 - would but let's at least look at the
144:02 - approximating PI code
144:15 - what was it 90 something I don't think
144:24 - we're gonna get a towel day challenge
144:26 - this summer
144:43 - so what do I change here this is the one
144:46 - that's approximating pi so if I wanted
144:55 - this to approximate tau I'm doing the
145:04 - circle probability over the total x for
145:20 - don't I just double it
145:28 - I'm brain is tired
145:41 - oh no I don't I don't
145:47 - although I have to answer this for
145:48 - myself right now so
146:04 - area of the circle is PI R squared
146:12 - that's our in the area of the square is
146:18 - 2 2 times R squared which is 4 R squared
146:23 - so so so this prop this ratio which is R
146:32 - no no sorry this ratio check on
146:35 - probability a pi equals 4 times that
146:44 - probability so if I wanted to do it with
146:47 - tau what is the area of a circle with
146:52 - tau PI
147:05 - tau divided by 2r squared right
147:11 - so wouldn't that be eight we're gonna
147:18 - just be eight looks well the camera went
147:21 - off oh you can't see me sorry everybody
147:29 - sorry everybody this is what I was
147:32 - drawing I was working out the
147:34 - approximation plan I can't do the towel
147:36 - thing now your record PI things and
147:42 - means doubling - oh the learning rate
147:46 - has changed oh my god I think I had a
147:51 - point - I'm not I don't remember what's
147:59 - in this code x times x plus y times y
148:08 - that's the distance well that's the is
148:15 - less than the radius squares in the
148:17 - circle
148:18 - oh the record PI no oh I'm printing
148:29 - record PI I see oh right of course
148:36 - that's not something available
149:12 - there's no math doubt towel
149:27 - I have to do that
149:49 - oh shoot there we go there we go
150:01 - all right I don't think this is worth me
150:03 - doing as a video coding challenge just
150:05 - to update this one for Talde I guess I
150:10 - could the difference yeah you know
150:14 - you're behind I correct that in the
150:16 - future me I am so me all right so I
150:20 - could I don't think this is worth making
150:24 - into a video but just for fun cuz I'm
150:38 - here and I'm leaving and why not oh here
150:43 - you can't see me let's let me erase all
150:46 - this
151:21 - okay
151:31 - okay here we go everybody maybe I can
151:35 - figure out a way to make this totally
151:36 - ridiculous
151:37 - and it'll be worse it'll be worth having
151:46 - done this close this
152:12 - oh but of course by heart has a Tao song
152:17 - Tao
152:24 - I'll to here I just want to find the
152:29 - digits
152:45 - what do you think accurate
153:01 - [Music]
153:10 - happy holiday everybody take point two
153:15 - point eight no there's no point there
153:18 - six point two eight three one eight five
153:24 - three oh seven one seven nine five okay
153:34 - welcome to a special just me wasting
153:43 - time on the internet but you know it's
153:45 - worth noting the day we I noted Pi Day
153:49 - and I made a coding challenge which was
153:52 - an approximating PI by using a kind of
153:56 - dart throwing technique let me close all
153:58 - this stuff up we can run the example
154:00 - right here the idea is I'm throwing
154:02 - darts into my processing canvas and the
154:06 - ratio of the number of darts that land
154:09 - outside of the circle versus inside the
154:11 - circle give me the value of pi now one
154:13 - of the comments I got so much on this
154:16 - video was but you're using PI to
154:21 - calculate pi well no I'm not and I want
154:24 - to be clear about this I'm going to
154:26 - comment out all of these lines of code
154:30 - right here and I'm gonna say print line
154:34 - pi so there we go we can see my
154:38 - approximation of pi right here now I am
154:41 - sort of using PI in the sense that
154:45 - probably the value of pi is used where
154:50 - is their lips function somewhere in here
154:52 - no wait I thought maybe I use the
154:55 - ellipse function oh I do uh in where
154:57 - this ellipse function is drama let me
155:00 - comment this out so now I am not using
155:04 - the ellipse function which probably
155:06 - behind the scenes is using pi and I am I
155:09 - am only you
155:10 - the reason why I'm seeing a circle is
155:12 - because I am calculating the distance of
155:15 - each point to the center and checking if
155:18 - it's below a certain amount and if it's
155:20 - below a certain amount it means it's
155:21 - inside a circle but let's just look here
155:23 - the only place where I might be calling
155:26 - math pie is now commented out I was just
155:30 - using that to compare okay so we can see
155:33 - here that this is in fact not using pi
155:36 - but giving me this amount now why does
155:38 - this work now I already covered this in
155:40 - the previous video so you totally could
155:42 - just go away and do something else right
155:43 - now but if you don't want to go watch
155:45 - the previous video the reason why this
155:47 - works just to talk about it again is if
155:50 - this is a square and this is a circle
155:56 - right the area and this is a value
156:00 - called R the value of the sorry the area
156:08 - of the square is each side of this
156:12 - square is to R so the area of that
156:14 - square is 2 R squared - R squared or 4 R
156:20 - the area of the circle as we know maybe
156:26 - from some math class is PI R squared so
156:30 - here I am using the idea of pi but only
156:33 - only because I know that that's the
156:35 - definition of the area of a circle now
156:38 - what if I were to just throw darts at
156:40 - the wall and I would count how many
156:42 - darts landed in the circle versus how
156:45 - many darts landed overall that would be
156:48 - the same so the total darts divided by
156:52 - the circle only should be equal to well
156:56 - this before our squared for R squared
156:59 - divided by PI R squared so now what I
157:02 - want to do is solve for pi ok well first
157:07 - of all the R squared can be is gone so
157:11 - then I could say 4 times total divided
157:16 - by circle total equals 1 divided by PI
157:20 - okay I just flipped those right
157:24 - so pi equals the circle total divided by
157:32 - four times the total total that seems
157:35 - kind of right maybe I got that wrong did
157:40 - I get that wrong because four times
157:56 - circle divided by total well yeah why is
158:00 - the four on the bottom why is the four
158:07 - on the top okay so now if I reverse
158:33 - these I could say pi divided by four
158:35 - equals the circle this should say Circle
158:38 - T like the circle total divided by the
158:41 - total now I can just multiply each side
158:44 - by four and I could say four times
158:46 - circle T divided by total equals PI or
158:48 - PI equals this and we go back to my code
158:51 - we have four times the total number that
158:56 - landed in the circle x divided by the
159:00 - total overall so that's there so now
159:03 - what if I want to approximate oh well
159:09 - what is tau tau by the way is okay so
159:14 - what's the circumference of this circle
159:17 - meaning what's the length of the arc all
159:21 - the way around to PI R well that seems
159:29 - like an awkward way of writing it why do
159:31 - I have to have this to here what if I
159:33 - just had a Val
159:35 - you that instead of being PI was twice
159:38 - the value of pi then I could say the
159:40 - circumference is just that value times R
159:42 - and that's what tau is uh something it's
159:46 - another Greek letter that I've totally
159:47 - botched let me go look up how to draw
159:49 - that better
160:09 - so here's the Greek letter write
160:11 - uppercase to lowercase towel so I can go
160:14 - back and son so I didn't do the worst
160:15 - job here I guess it yeah that kind of
160:19 - that'll be so I could say it's a tau R
160:20 - and guess what if R is one the
160:23 - circumference of the unit circle is tau
160:26 - the circumference of the unit circle is
160:30 - tau naught to PI I don't know people now
160:36 - enjoy the comment section of this video
160:38 - everybody so here's the thing what if I
160:42 - want to approximate tau okay oh boy oh
160:46 - boy okay
160:50 - well the area of this circle is still 4r
160:53 - squared and the area of the - I'm sorry
160:59 - area of the square is still 4r squared
161:01 - but the area of the circle pi no no no
161:04 - PI divided cow what is it with town tau
161:07 - tau divided by 2 tau divided by 2 R
161:10 - squared ok so yeah I love this is super
161:13 - nice I gotta say it's tau divided by 2 5
161:18 - - 2 where do I want the - well welcome
161:20 - to here but do I want the 2 there but
161:28 - let's say I do this then the total
161:33 - weight for R squared or tau divided by 2
161:41 - R squared so the R's get cancelled out
161:45 - that this becomes an 8 and so now the
161:49 - value of tau guess what you just double
161:52 - it it's just double it's just double pi
161:55 - pi is just 1/2 towel whatever you want
161:58 - just live your life tau and I just ate
162:04 - and then this is tau so we now can go
162:07 - and revise my coding job that's really
162:11 - your so watching this video apparently
162:13 - I'm gonna stop this and we're going to
162:16 - we're gonna make one of these for all of
162:18 - you
162:19 - tau lovers out there this one goes out
162:21 - to all you towel
162:22 - out there from me to you we're gonna
162:25 - change this to record towel we're gonna
162:31 - draw that we don't need to draw that
162:32 - this is unnecessary code just save this
162:36 - before I forget this approximating towel
162:39 - and obviously put this on the desktop of
162:41 - my computer
162:42 - this is processing its java processing
162:44 - org everybody always asks then i'm gonna
162:47 - come down here i don't need to do so
162:49 - many each frame my formula here is now 8
162:55 - tau is 8 and I can now what I want to do
163:01 - is I need to check the record tau if I
163:05 - get one closest because redo the record
163:10 - and then now and this is now tau and
163:12 - then ah this is just math tau and math
163:15 - dot tau all right oh and this is record
163:17 - tau
163:20 - ok we're good
163:22 - I did it I redid my coding talk what
163:31 - what Java dot math hmm where are the
163:44 - constants where are the constants hold
163:48 - on math dot PI Java hmm yep pi oh there
163:57 - we go here are the constants e we get e
164:00 - we get PI I don't see tau I don't see
164:04 - tell this video is over forget it
164:08 - all right fine I'll devote to it anyway
164:11 - anyway but this is so sad this is what I
164:14 - have to do look away everybody look away
164:17 - don't look don't look
164:22 - and now we are now approximating tau
164:30 - happy towel Day everybody Oh see ya okay
164:37 - there we go
164:40 - that's an extra bonus for all of you
164:43 - people out there on the internet
164:45 - watching this now I really got to go
164:47 - almost two o'clock so again I'm gonna be
164:49 - out of town for I'm gonna be out of town
164:55 - for did I miss something
164:59 - hope not I'm gonna be out of town for a
165:02 - couple of weeks as I mentioned all these
165:03 - edited versions of these videos will all
165:05 - become live well YouTube has this new
165:08 - feature like this new premiere thing so
165:10 - I kind of want to I don't know if that's
165:11 - unlocked for me yet I don't know if
165:13 - you've heard about this but I'm
165:14 - definitely an experiment with that and I
165:20 - will see you all in the future okay I'm
165:23 - gonna I'm gonna end this I'm gonna play
165:25 - you out with my ridiculous thing that I
165:27 - always do goodbye yes so Muhammad is
165:34 - asking you didn't use PI but used the
165:36 - formula for the circles area but that's
165:38 - okay because I mean I have to use that
165:42 - but if I were just using it with dart
165:45 - throwing I wouldn't have to I just have
165:46 - the circle draw and I would see which
165:48 - ones are in and out of the circle right
165:50 - visually okay goodbye everybody
165:54 - Oh dev new sponsor thank you so much all
165:57 - right here goes something we wanted to
166:02 - make some crazy idea
166:06 - to make
166:11 - [Music]
166:22 - [Music]
166:37 - [Music]
166:51 - [Music]
166:56 - [Music]
167:00 - you
167:08 - you

Cleaned transcript:

oh here what a failure what a failure I know you hear me now I know I know I know I know you know i know i know i know i know i know i know i know i know i know i know i know i know i know we can i have we start this over again i'm waiting till the chat catches up to realize that my audio has come back on i always i haven't done this in such a long time i've become slightly more professional in my live streaming but yeah I went to go ah oh now I hear myself even coming out of here this computer I went to I went to go use the facilities which is something I'd like to do right before I start live streaming and when I do that even though I'm not we're streaming I'm actually not even recording so I'm gonna hit start recording or recording to disk I I like to just mute my microphone just in case you know anything happened by accident you know I don't want to confess to some crime in the bathroom by accident so and then I came back and I started and I forgot to turn the microphone back on anyway I was saying go Mexico scoop please give me score updates go Mexico sorry Sweden sorry sweden i love you' Sweden but go Mexico for what I understand if Mexico wins today Mexico would be the first World Cup team ever to win three games the first three games in a row I don't know if that's somebody out of fact check me on that but I think might be correct yes hey Zeus in the chat says Mexico is playing I can't watch both please go and watch Mexico this first of all this will be not nearly as interesting or exciting and also this you could watch later and you can watch the Mexico game later but that's you're gonna already know what's gonna happen and you know I can tell you what's gonna happen here I'm gonna try to code some stuff I'm gonna make lots of mistakes I'm gonna get stuck a lot I'm gonna waste a lot of time and eventually maybe at some point there will be a small useful nugget of educational material that comes out that's what's gonna happen okay anyway what was I saying good morning it is me the coding train person Schiffman thing human being and I have one job today my one job today is to complete the color classifier example with tensorflow Jas so the two things I would like from the chat i am i Yura I'm going to answer your question in a second the two things I would like from the chat well are one score updates so spoiler alert I want score updates the BIC okay Pat the chat so if you're watching this and you don't want to be spoiled I don't know too bad I guess if this if slack comes back online somebody let me know because I can then look at the slack channel for the for the slack chat and actually since I don't have I'm gonna so I'm gonna be looking more at the YouTube chat than I typically do okay so I have to get myself organized and figure out where I last left off so let me open up terminal also I want to say thank you to the YouTube family and Learning Team I was able to attend something called YouTube EDU Khan last week so that's a move to a new apartment and so out of sorts I live in a sea of drowning in boxes going away on vacation this tomorrow two weeks all right so Wow oh no no no no no no no Sweden scored are you now I can't tell if the chat is trolling me I need somebody reliable nothing you're not reliable the dark hound who gave me the score of why look so unfortunately this is the the World Cup is on a lot this is the let me talk about schedule this is the only time I have right now to do a live stream this week and I have some bad news I suppose if you enjoy watching these live streams there won't be another one until the week of July 15th I'm gonna be away for two weeks the good news it's true okay that's too bad the good news is for though which isn't really good news for you presumably as a live stream viewer but for the non live stream viewers out there I have a backlog of about 8 edited video tutorials that have not been published to the channel and I'm gonna make a couple more today mostly there's the first seven parts of this color classifier thing if you can believe that the first that's right seven parts and then that one edited challenge of doing a subscriber visualization so those will come out be published as public edited videos once every few days while I'm away and I should also let you know that most likely even when one comes out and is public you can look in the description if there's a link that says next video you can go to the next video you might see that it's unlisted but I don't know if this is good practice but this is my the way that I approach if I have a series of like a 10 part series instead of just putting them all 10 live online at once I space them out every few days but I leave the other ones there unlisted so the viewer who really wants to keep watch all of them at once could do that ok oh Germany scored I'm not paying attention to that game I guess that's good though Chico Germany whose German I didn't know he's playing for Mexico I don't know like a huge Mexico football fan all right Oh Italy scored all right I really also I would like to do something with major I'm a little bit of a baseball nerd we'd like to do something with machine learning and Major League Baseball data something that I'm thinking about if people have ideas or suggestions there is a entire example created by the tension flow Jas team that classifies pitches into ball strikes you know curveballs sliders no no no no not ball strikes that classifies what was I saying classifies pitches into fastball curveball slider etc based on the pitch data all right wow we are really got it we've got a good World Cup discussion I love the international quality of this audience I love that there's a great World Cup discussion going on the chat is a wonderful break from the usual its programming language or which IDE is better than the other so if it's one thing that the World Cup has done it's made us forget about all of our quirky preferences for coding editors and the like sorry C are brown can't get behind that Red Sox thing but look congratulations to you on all the recent success interesting tidbit I did go to college with Theo Epstein I didn't really know him though I did not get a haircut as Shubra is mentioning in the chat but I did I could I meant to get a haircut but I did just use my home grooming device to trim this beard which had gotten out of control so no wilderness Dan today okay so I need to figure out where I am and I'm gonna go to the desktop what am I in p5 tensorflow probably and I was so one thing I really messed up I think is so something I could use help with is it's gonna not be easy for you to help with it because I haven't published the videos yet but I was trying to keep the code that is at the end of each video separate and I erased one set of codes I need to recreate that which I will do myself but if someone wants to volunteer to do that it's the one I think where I was visualizing the grid of the colors he'd have to go back to the live stream at this point okay so I'm gonna create color classifier I'm gonna run a server I'm gonna open up the browser and open up the console and go to color classifier there we go this is where I left left off then I need to open up I'm gonna use the atom text editor happy birthday um I can't by the way people often ask for like shout out or you look at this or way it's impossible I think for me to catch every message and but you know hey happy birthday I care about your birthday hold on let's get this going here I don't want this YouTube this thing I do want to open up on the desktop oh oh computers are such a silly thing so let's close this and let's go to color classifier that's where I want to be and Here I am and let's go here let's make this a little bit bigger and I think I am I am ready to go so it's 1115 a.m. my goal today is to finish this example I really want this example to be finished I think this example is interesting enough or has enough to it that I can both publish the kind of fixed version that is the code that I'll leave with at the end of this tutorial as part of the coding train com website and then I'll also make a separate github repository with this color classifier that people can submit pull requests to like just improve the design and interface and that sort of thing I do have an issue where I don't have a good system for maintaining the various github projects that are associated with the coding train and so a lot of this stuff really looks like lingers and languages and so I am really thinking about that and how to mean how I can like manage the community better and so hopefully things will improve all right we need to find a tissue to blow my nose oh and I need to deal with this other camera so if you if you remember I did something really silly and ridiculous on the previous livestream where I didn't want to erase all of this over here I just turned the camera so that I could write some notes about a different project here so let me erase this this is nothing to do with the color classifier I'm just realizing now that I don't even need this so me not erasing it was kind of pointless but I want to turn the camera back uhoh loose HDMI cable sorry about that okay no green all the way to like they're up loose HDMI cable so this should be good does this seem like focus and you're seeing what you're seeing Oh actually I don't usually have it turned all the way that way because that little smiley face that I drew was my like testing okay there we go so I think this seems good and this is where I last left off I know you can't read this tf1 hot thing there all right so that's good we come back over here and let me open up the tensorflow api reference we're definitely gonna need that a little bit okay my eyesight is more supports I bigger computer screen alright so now I am here and so I actually one thing that I have to talk about which I will get to is I I'm actually going to use validation data and not testing data as this tutorial here's some here's some news actually I took about two or three hours last night kind of building out the example I'll just show it to you right now so as just to prepare for today because as you know I typically don't prepare but I felt like this was I needed to sort of see how this was gonna work so if I go to a shipment github dot you color classifier I think this is it so this here is actually whoops this here is running a version of what I want to build today so behind the scenes its training the model with the training data and you and I'm and this is the number of epochs for now I'm gonna say epoch a lot pronounced epoch look YouTube thank you YouTube epic okay okay what about the British pronunciation is a difference epic British pronunciation epic how to pronounce well let's look at oxford dictionaries I guess you could say it either way don't die okay and yes I could watch Peppa Pig live right now all right oh I logged in to because I was watching the World Cup Mexico game I've logged into my daughter's the account that I have set up with YouTube red for my daughter to watch videos and we can see what she likes to watch here okay let's move away from that epic epoch I want to say Epoque all right second one is the only correct one I so agree me I am soo me as well as the only way to say maths is matched alright so this is what I'm going to look at this so I've trained it with the data and we should see now reddish yellowish greenish bluish stick it with bluish purplish purplishpinkish ish purplish so this does this will work and I've gone now for nine a pox I feel confident now it's if the key is saying it with confidence all right does anyone remember where last left up like what was the last thing that I did oh no Sweden scored again how much time is left oh I'm bad luck for sweet and I have to stop livestreaming I mean bad luck for Mexico come on Mexico I mean Mexico's gonna I think Mexico will advance anyway is that correct better be nice for them to win all right let me see hold on for a second I'm going to go somewhere that I don't want to show by accident so here we are for a second I'm going to I just want to login to my YouTube dashboard to find a particular video and I have to log in now as myself switch account another coding train and I'm gonna go to my creator studio creator studio videos color classifier part seven seven is the last one so this is an edited version of part seven that has not come out yet but I can watch it because it is private and I'm going to do that right now Oh should I put on the hoodie so that the matches continuity I know you can't hear the audio okay so let me see where what happens at the end do you want to hear this audio wait wait don't leave just wait don't leave just yet I do want to think about memory management and I'm maybe I'm going to think about memory management later but and the X's and Y's I'm gonna want to use in the next video but I probably should after I make the one last little tidbit here and then I'll move on in the next video I'm gonna start creating the architecture of the neural network model itself and oh I'm gonna introduce some new concepts soft backs and crossentropy oh no I have to talk about softmax and crossentropy okay all right that's fine that's fine okay good thing I watch that alright alright I'm ready ready everybody ready Freddy Roby we think of maybe I need to get into the mood a little bit oh I'm to the wrong string like I actually have left this ukulele here and I haven't been here for a while so I haven't had a chance to play I need to get a second one so I can have one here one at home so I can actually practice and learn some new songs all right here we go stretch my stretch I'm gonna get up and stretch stretch your hamstring muscles okay there we go let's cycle those cameras yes Warner says stick decoding it totally totally okay hello I am back I have returned to finally build the architecture to take this training data these RGB values that are matched with these one hot encoded labels if you don't know what I'm talking about you should probably go back and watch the first seven parts of this series but I'm now going to create the neural network architecture to train a model with this data and I'm going to use continue to use tensorflow j s in particular the layers API to do this so let's review over here a bit let's review over here a bit where I am alright so we've talked about all these things you know it's really funny I just want to erase I want to erase all that I can't like I can't I can't I apologize here sorry everybody go go watch the go go route for Mexico for a little bit this is just it doesn't help me to have this here it's like two out of my head and so I need to erase all of it what oh how ridiculous mi then I so desperately jumping through various hoops to not erase this it's like I needed it and now that I'm here yes you haven't switched to watching the World Cup once again you were watching a livestream of a person quietly are not so quietly erasing I really need a cloth so I don't kill the earth by the way we have a little family World Cup pool going and the person in the family who happens to be leading with the best picks is my sixyearold far and away running off with watch this one of those glass boards that I could put right here drawn this before it started but don't worry I'm gonna get the coating is it noon already if it's noon time to be a knight I couldn't my watch my watch battery died and my Fitbit why and I didn't know packing and moving a hip behind the charger don't get your sips of tea or coffee or do some more stretching I'm almost ready I think I should probably just start over all right oh you weren't even watch I didn't even have the camera on sorry everybody are you ready oh it's only 1130 okay whoo Oh slack is still down is that correct all right Oh No Sweden scored again I mean I love Sweden and everything but got to go all right hello I have returned once again to make a color classifier machine learning well by my first intro is much better okay alright hold on I think my belt isn't tight enough my pants feel like they're falling down okay now everything's gonna be right my brain is gonna work correctly hello I am back in this video I am finally going to start to build the neural network architecture to make this color classifier I am going to take this data over here which is a long array of many many RGB values normalized to arrange there to one which matches with all of these one hot encoded labels and if you don't know what I'm talking about then you might want to go back and watch the first seven yes that's right seven parts of this tutorial series that's getting very very long but this I think is I'm really getting to the good stuff I don't know maybe it was good stuff before maybe this is bad stuff I don't really know but this I'm really excited I'm excited because now what I'm gonna do and I'm gonna use tension flow yes but I'm going to create the neural network architecture so let's just remind ourselves what we have we have a data set most of the first seven videos of the series was all just about collecting and cleaning that data set and that data set is many many RGB values I think I have like five thousand which is actually is kind of very very small for a data set but it's fine for this particular demonstration I have five thousand RGB values each one is labeled with something like blueish or reddish or purplish these were crowdsourced but those got converted to one hot encoded vectors meaning if there are nine if there are nine labels well let's see then I have a vector that looks like this one two three four five six seven times ten nice night and maybe this one refers to purplish if this particular element of this array of numbers has a 1 in it it is that that and that one is for a particular label this one sort of label okay so that's what I have so what I I know that I need to have some kind of neural network and the inputs has have a shape of 3 there are 3 inputs are G B the outputs have a shape of 9 1 2 3 4 5 6 7 8 9 this is the output layer this has a shape of 9 inputs of a shape of three outputs have a shape of nine because the goal of this is by what once this whole thing is trained and finished if I send in some RGB values what I'm gonna get is a bunch of numbers all between 0 & 1 and I'm gonna find the one that's the highest and and those numbers are gonna be the probability of this particular data point being a particular label and I'm gonna find the one that's highest in front of sign at that label who classification we're doing classification so now what goes in between all this now this is a big question and many different scenarios might call for multiple layers different kinds of layers there's something called a convolutional layer which I'll get to but I'm gonna do something really simple I'm gonna have a basic dense layer which is kind of the standard building block of neural network systems and I'm gonna give it some number of nodes so for the sake of our even right now let's pretend that I just gave it 4 nodes and a dense layer this output is also going to be a dense layer dense layer means fully connected meaning that every input is connected to every node and then every node in the hidden layer this dense layer is connected to every output now I'm going to let your imagination draw the rest of all these connections but so this is what I want to architect so let's now go on detectives now I'm going to do this using tensorflow digest and the layers API if you don't know about the layers API you're going to watch my three or four part series about the layers API tutorial but I'm bookin I sort of talk you through it while we're doing it here so you don't necessarily have to watch that okay so if I come back again this is what I built so far I have all of the training data and tensors and you can see the shape of it I have 5643 RGB values and 5643 labels nine with nine possibilities okay so the first thing that I want to do is and I'm gonna do some goofy stuff with some global variables that I might not know that you know just to make my life kind of easier I'm going to create a variable called model and my model which I'm going to create in setup at the end after I've prepared all the data I'm going to say model equals t f dot sequential TF dot sequential so that now that's that's me creating a sequential neural network model it's sequential because it's a feedforward the layers go in this order so now what I need to do is create some layers so the first thing I want to do is make the hidden let's make the output layer now let's make this we should do it in order we have to do it in order and to make the hidden layer hidden equals TF layers dense and then I put some configuration stuff so I make a layer by calling TF dot layers and then I specify the kind of layer this is gonna be a dense layer and then I can pass an object in as an argument and that's where I can configure things like input I don't remember any of this let's go look it up so let's go to the documentation let's go to TF TF layers and let's go to dense where do we see that hold on time out for a second let me look at my cheat sheet which I didn't want to do but I'm going to totally do TF layers dense I got that right so where did I miss it oh it's right there under basic sorry I'm looking around for it and it's right there in front of my face under basic so I'm gonna make a TF layers of dents I'm gonna click on that and now I'm gonna see these are all of the things that I can pass into the configuration so I need to specify the number of units the number of units is like the number of nodes and I made up four right here maybe let's try sixteen maybe we want to have some more than four whatever we can make up anything we want so I'm going to now say units sixteen one thing that I know I need there's an activation function again I can't cover everything in this video I have an other videos where I've talked about what an activation function is and how it works but the idea is the activation function is the function that takes all the sum of all of the things passing through the network being multiplied by the weights and kind of squashes them into some range and so there probably is a really useful interesting discussion about we could have about what would be the best activation function to use right here right now maybe later he'll try some different ones but just for simplicity I'm gonna use I'm gonna make a bad decision and just use sigmoid this sort of like historically original activation function of neural networks I'm gonna use the activation function sigmoid let's see what else do I want input dimensions so this is something that I definitely need to do here because remember this this this these inputs this is not actually a layer this is a two layer network it looks like there's three but I'm just drawing it with three things and the inputs being but that's not a layer but I do need to specify that three things are coming in so I need to come here and say the input dimensions input dimensions is three because I have an RGB value this should do me just fine for right now so then I want to also create the output layer output TF that's gonna be dense that's going to have nine units because they're nine labels again that's completely arbitrary that's just how I happen to prepare my dataset now I don't need the input dimensions because the input dimensions can be inferred by the previous one the input dimensions to the output or the number of units of the hidden so I don't need that but I do need to specify an activation function and guess what I am going to use a different activation function softmax so I'm just gonna type that in right now I will come back and explain what softmax is in a separate video which i think will be the next video of this series just gonna push this a little bit further now I'm gonna say model dot add the hidden and then model dot add the output so this is now me this is now the code for exactly what I diagrammed right here three inputs into a hidden layer with some number of units with some activation function into an output layer with some number of units and an activation function timeout for a second I just want to see do I want to do the optimizer yes I think that I do okay okay okay oh yes so we have now built the model here's the thing the next thing that I need to do and I'm gonna do this in the next video what I need to do is create an optimizer so let's just put this in comments create an optimizer and I need an optimization function which typically in the past I've used mean squared error but I'm gonna use something called categorical cross true I don't know why it sounds really scary but it's not and I can't its I also can't spell it so I'm gonna create the optimizer and then I'm going to compile the model and then I'm going to train the model these are the next step so they need to do this is the architecture for the model people telling me I have an error oh yeah I have something extra extra comma here but so this one do the next video and so what I need to do in the next video this is like just a few lines of code but I need to I mean I could just add them but I would like to try to understand a bit more about what why am i have softmax here instead of sigmoid or new or any of the other activation functions and why I might choose categorical crossentropy instead of mean squared error which is if you have happened to watch my ex or tensorflow TAS coding challenge or some of my other layers tutorials I always just use mean squared error so that's what's coming the next video I'm going to create the optimizer I'm gonna compile the model and I'm going to talk about softmax and categorical cross entropy oh wait wait wait wait let's actually run with this and see if there's a syntax errors no okay and if I just say if I if I look in the console here at model we can see there it is this is the object and it's got all this stuff in it alright see you in the next video okay now is the moment where people can ask some questions or offer some comments before I move on to the next so I I so I need to open up a Wikipedia page for softmax and then why did why does everybody make everything look so insanely insane and then categorical crossentropy one thing I want to know is what's the difference between categorical crossentropy and because I think in if I go to loss functions here but it's actually not here so this is I don't know if this is a bit of a point of confusion for me there's the loss function here is written as softmax crossentropy but I know for a fact because I was building this example last night I'm looking at the chat so hard to follow the chat without the /o Caddick so here's something I would love so this is what I'm building I have it in here already right so this is what I did I use categorical cross entropy because this I have found in this is what I found that examples of tension flow jas however as the loss function and then but however if I go here and look at the API Docs it says softmax cross entropy are those the same thing no metrics oh it's a mint interesting it's here under metrics I'm looking at the chat I guess I'm gonna just kind of gloss over this right now and this should be a lowercase e by the way I mean I'll leave that there since all right oh I I also messed up which is that the optimizer I totally I totally misspoke at the end of the last video oh yeah okay thank you about for the the battery the battery notes I only have one plug today so let me plug this in the first thing I totally messed up the optimization function is stochastic gradient descent or adaptive something the loss function the loss function is mean squared error or in this case I'm going to use categorical cross entropy instead okay so that I messed up okay all right so I got to just move on I guess cycle the camera the chat has more entropy than the program itself all right I am going to move on and let's look at why is the word entropy use that's what I would like to understand it's between two probability distributions P and Q over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set coding scheme so basically let me see if I get this right before I put it in the tutorial where did I put my marker where's that marker here it is so this this output layer ultimately with softmax gives us a probability distribution and we could absolutely look at the correct distribution wipe the thing that we're training against the calculating an error is this right the one hot encoded vector so we could use mean squared error it would give us a loss but crossentropy is a formula for looking at the difference between this output vector and this vector if we're considering these probabilities because it looks it does use a different formula to compute the loss but the error between two probability distributions it's optimized for that and in particular but not and and it's linked to using cross entropy with soft max because soft max is an activation function that generates a probability distribution from any just vector of numbers that seemed about right without going into the actual maths the formulas for these two things although I I might describe the formula for a soft Mac so I think that's worth doing because it's much simpler than it looks like on Wikipedia but does that seem like a fairly accurate explanation before I get into the next video Oh Korea scored that's exciting I'm gonna have to wait a minute for people to catch up to me I'm gonna look at the formula here so uh so it's just the sum of probability P Times log of probability Q that's pretty simple actually H of P is the entropy of P the cross entropy for distributions P and Q is this and for discrete P and Q that's what we have here it means this okay I'm not gonna get into the mass of cross entropy the slack still that's not everyone just talking about Mac versus Windows so I'm just gonna quit slack because it's causing my computer to freeze up here how am i doing timewise it's only noon okay I think we're pretty good that's a nice definition of entropy like you know lack of order or predictability all right I I'm not seeing anyone complaining about my explanation like no there's like a completely separate discussion going on in the chat that has nothing to do with what I'm talking about so I'm just gonna assume that I'm gonna move forward Oh how much time is left in the Mexica game okay thanks Dan I'll take that as a yes all right Oh Korea's beating Germany and realize Korea and Germany were playing entropy is a mathematical way to determine how think Thank You Jo two micron who writes entropy is a mathematical way to determine how chaotic something is in this case it's a way to measure how two different how different two tensors are to one another that is a great way of explaining it so let me let me say that again in my own words so in other words entropy as a measurement of how chaotic something is so cross entropy is basically looking at this probability distribution that the neural network generated and the sort of known correct one and what's the sort of chaos that exists in between coats what's the cross entropy the error between those two probability distributions how if we are using them as problems we're generating an outcome like how chaotic is that outcome going to be well with the 100 vector it won't be chaotic at all it will always be just the one thing because it's a hundred percent this label but with this what's the cross entropy there I think that's a good way of describing it all right let's move on all right okay I can't look at the chat now okay okay I'm back hi I'm back to create an optimizer and a loss function and compile my model for this color class fired another thing at the end of the previous video I was talking about oh I want to use I gonna use the thing called softmax and I want to create an optimizer I want to use mean squared I will use categorical cross entropy instead of mean squared error boy did I really kind of box that the optimization function I want to use this is something I'm choosing from for example what I'm going to use is stochastic gradient descent this is the optimization function that whole let me let me start this over I need to get my head straight here okay all right here we go I'm actually going to I think while I'm recording I don't really need this laptop even open because I have to there's no slack channel and I have the YouTube chat over here although actually I just realized it could be useful because I do have my premade code which I rarely do in advance circuit I do have that over here so I think that would be useful to have available to me okay alright I'm back in part 471 of building a color classifier now what am I gonna do here in the previous video I created the architecture of my model a hidden layer and output layer a sixtieth 10 ths sequential model to dense layers activation functions units etc now at the end of the last video said only the next thing I need to do is define an optimization function and then compile the model well I really botched that is what there's three things I need to do optimization function loss function and compile the model and so I kind of conflated optimization and loss I'm optimizing against the loss but the optimizer that I want to make is I can use Const I guess here I get a very inconsistent about winning using converses let maybe I'll go back and clean up that code at some point I'm gonna say I can get it from TF train stochastic gradient descent and I can create a learning rate which I'm going to say is like 0.2 so this so one thing to do is create an optimization function right there are different options and we can try other options stochastic gradient descent is the one that I basically used in almost all of my examples and covered in detailed in my how to build a neural network from scratch series and the idea of created descent is walking along trying to go down the graph of the loss function to minimize that loss so what is the loss function that I want well if I'm just say model compile I believe this is a whoops this is a function that I'm going to write with a configuration option and one of the things when I compile the model I need to specify up to optimizer optimizer ena this is very awkward that I just called this up here but that's fine and then the other thing I just specify is a loss function mean squared error so this is typically what I have done in previous examples if you look at my X or coding challenge but this is now going to change and the reason is because I am using an activation function called softmax so let's talk about what softmax is softmax question mark okay so remember the output that we want from the neural network is a probability distribution right what's an example of what an output might look like it might look like this there's nine values 0.1 0.1 0.2 0 zero zero zero point seven zero zero right ohho my math is off zero point six right these all add up to a hundred percent this is the idea we're what this is saying is this particular RGB color has a 60% chance of being you know bluish if that's the particularly ball that matches with zero one two three four five index number six a 10% chance of being reddish a 10% chance of being purplish and a 2% chance of being greenish this is what we want now the training data is encoded like this and maybe we can actually look at it right next to it maybe this is what the training data looks like zero zero one zero zero zero zero zero zero a one hot encoded vector because actually the correct label for that color is greenish so I need a loss function sorry let's good cat across entropy and soft backs are linked together they're used together so that's why I just can't remember which one I'm explaining but I need a loss function to give me the error between this probability distribution and this probability distribution but I need my neural network to generate a probability distribution in the first place activation function as you might recall is something that squashes any number into some range it's one way of thinking about it the sigmoid function if we were to graph that sigmoid function it looks like a boy can never do this something like this oh boy that's a terrible graph of it you look it up on Wikipedia something more like this right and this the top is one the bottom zero so any number given to sigmoid results in a number between zero and one softmax is an activation function that not only squashes the values that are coming in to these outputs between zero and one but guarantees that they all add up to one now you might say to yourself that's easy that's very easy to do we do this all the time with normalizing data I could just find I could just take all of the outputs add them all up and then divide each one by the sum of the total right because let's say somewhere I have these numbers two two one five right I can add all these up and they're going to add up well look at that they added up to ten let me divide by ten I have 0.2 0.2 0.1 0.5 so this we could do this sort of like divided by the sum as our activation function in but that's but but this is not going to give us an AK an accurate probability distribution that we want for this scenario and softmax is another way of doing the same thing with more that that sort of expands the difference this one makes this one much more likely expands the difference between these different values so the way that softmax works is we actually do the following you know that I gotta find it Eraser know that natural number e for natural log to point seven something I think well what if I said and took E squared e squared e to the 1 power e to the 5th power what if I took all of these what if I took all of these and then added them all up and made that I'll call that the e sum and then just took each one of these values and divided by E song that is softmax in a nutshell you'd only think I'm gonna do I'm gonna have a like a tan tangent video that you can go and watch now where I'm actually gonna write the code for the softmax function we'll explain it better only it's worth doing that in this video but I'm gonna I'm going to do that in a separate video so look for that in the delete it look for a link to that in this video's description just to like just to be sure that I'm right about this we can now go here and this makes it look like oh my god this is like the craziest scariest thing in the world but you could see it right here the softmax function for a vector of values z means take every value e to that z index J power divided by the sum of all of those values and so that and you can see here the probability theory the output of the softmax function can be used to represent a categorical distribution a probably miss tribution over k different possible outcomes time out for a second so what i want to do is you know i'm going to do that in a different video okay yeah okay okay okay alright so again in a separate video I'm gonna write the code for softmax and actually it's right there intensive load is also as functions for doing it and I'm gonna compare what those outcomes look like versus just summing and dividing but I'm gonna move on and say so if I've established that softmax is what I'm using as the activation function for the last layer the output layer the question then becomes what loss function should I use how do I calculate the error between the node the target outputs with the training data and what the what the model generated during the training process so again mean squared error would work here but I am gonna change that to categorical crossentropy why am i using that so first of all what is entropy entropy is a term that refers to like the chaos associated with the system so you can think of a probability distribution is like being very chaotic or more or less chaotic so what the cross entropy function is a loss function designed to compare to probability distributions and look at how much chaos there is in between that the cross entropy between them and the math of it is you know mean squared errors like subtract take this one this one and then do like the square root square it then do the square root or make it don't do the square root then add them all together me and squared error I've talked about that you can look it up it's a pretty simple mathematical function cross entropy if we look at it we get we could build that I could build this in a separate video which might be worth doing as well is really just the if if I have two probability distributions P and Q I'm looking at the mine the sum of one probability distribution times the log of the other probability distribution so again you can research what cross entropy how the math behind it works more in more detail and maybe I'll do a video about that for those who are interested but at the moment the important thing to do where am i over here thing to realize is that softmax is an activation function for generating a probability distribution and crossentropy is a loss function that works well for comparing to probability distributions so for a classification problem those are the two things we want to use pause wait if it's a binary classifier it's better if Dan use binary crossentropy yeah yeah yeah that's a good point okay um in the chat someone just mentioned that you know if it was a binary classifier there's only two possible outcomes hot dog or not hot dog I think is the classic example now then I would use binary crossentropy so again there is no be all end all for the loss function you choose I'm just showing you one scenario with the idea of and locking your mind to think about well let me research all these other loss functions and why would he use one with its the other and what's available as part of that I get for free as part of tensorflow digest okay so we've done that oh I think I'm done with this video let me just uh let me just kind of like run this code oh wait we got it unknown loss ah okay I think this is lowercase e okay there we go so so now we're done what is the next step what am I gonna do in the next video it is now time for me to call model dot fit model dot fit is actually the function I will call with the X's and the Y's that I've prepared in a previous video to train the model right I really only got two steps left and I'm sure there's gonna be lots other stuff that are forgetting about right now I want to train the model then I want to use the model to give me a label for a new color that the user is going to specify okay so in the next video I'm going to actually add model dot fit see you then oh please talk about the difference between stochastic gradient descent and gradient descent that's a good question I should talk about that somewhere maybe cannot I think I need to do like little aside videos let me see if I can answer this down so we can tell me if I'm correct stochastic rate I always forget which ones which but stochastic gradient descent is the one where you run you some a whole bunch of like loss function values and then compute the gradients as opposed to computing the gradients for each data point one at a time it has to do with went to you when do you tune the mod when do you tune the weights like here's a data point what's the error here's a data point what's the error here's a data point what's there do I do that a hundred times then tune some weights or do I some weights in between each data point I can never finish doing a bit stochastic means doing it in batches okay there we go all right how am I on time twelve ten that's pretty good I kind of want to be done by one if I can so we're gonna do model dot fit in the next video all right all right whoa binary crossentropy is for multilabel classification x' whereas categorical crossentropy is for multiclass classification where each example begun belongs to a single class oh I messed up so I totally miss explain that match I wonder if you can cut out the thing where I mentioned binary crossentropy no people are asking I guess about my this is a google Summer of Code tshirt yes so I do I've for the lot I think since 2011 I missed one year I have done I'm the organ the org administrator and sometimes a mentor for the processing foundation google Summer of Code shoot so Matthew let's figure out what to do about that binary crossentropy thing I guess it's not the worst thing that I mentioned in there but I got it slightly wrong by the way this is something that came up in that YouTube conference that I went to like there's no way to correct a video there used to be those annotations and you could add there's no way to correct a video without once it's been published without like just removing it and uploading a whole new video binary crossentropy look at this anyway I'm not gonna worry about it too much okay so let me get to let me uh cycle the cameras okay slack come back I'm out of water I may need to go get some water all right okay good the chat all right it's time it's time to fit our model here we go so so far you know hopefully you've watched all the previous parts of this series if you haven't that's fine too but what what I have so far is I prepared my data set loaded it from a JSON file I've turned everything into tensors and then I created a Model T F using touchflo data has a TF sequential model which is designed to receive RGB inputs and output a probability distribution for color labels and you know again this is somewhat of a trivial scenario but I'm classifying data simple data with just three values all between zero one and nine possible categories or labels okay so that's what I've done so far so now that I have this this is actually like it's always the feeders gonna be over in like two seconds not really all I need to do is call model dot fit so modeled outfit now what do I need to pass to model dot fit well the idea of model dot fit is that I'm saying hey here's the training data here are all the inputs and their associated target outputs which I have called X's and Y's now I think I'm gonna get an error right now let me just actually run this and I'm going to up so let me run this and see if I get the error that I'm expecting yeah so look at this Oh okay so a couple things welcome to your life doing machine learning shaped mismatching I didn't even expect this error so I have to think about this one error when checking input expected dense input to have shake three but got array with shape 5643 three so I guess right I'm sending in not just three inputs the shape of my inputs is many so I think if I just do let me look at what an example that I made previously that's weird I didn't have this issue what I'm thinking about this where is this error coming from we'll know to think about this for a second huh I know that this batching thing is always an issue I thought I took care of that I'm just looking at the code I wrote yesterday but that's so weird the code that I wrote yesterday did is there something I did to it X's colors yeah Oh input shape got it that's what I that's the difference between the code that I wrote yesterday all right aha so I made a mistake and I used input dimensions where what I really meant was input shape let's see if we can actually look in the documentation here input dim with it for the dense layer what's the difference Oh input shape isn't actually listed here ah if specified defines input shape as is has input dimensions inside of brackets so actually I think this would probably fix it let's try this right really what I want is this like I'm going to have an array of batches of data each one with three values in it so I think no but probably what I would need to do is say five because I know I have this much data maybe I need to do this this is actually not the way I want to do it let's try this No so weird I know what I want is in I know what's going to work is this okay let me let me go let me go redo that explanation I have to think about this more later I went down a little bit of a rabbit hole though that's unnecessary all right let's go look at the documentation and see what it says there and I actually I've got it pulled up already okay so you can see what I specified was input dimensions if specified defines input shape as bracket input dimensions oh so actually I don't even need those that those array brackets there and that should fix it there we go but if I wanted to use those array brackets because I'm sending in many data points I could actually just specify the input shape directly and this would then have the array brackets around it so it's a subtle distinction I think because only input dimensions is documented let's use that one and let's put a 3 here okay so no peeps we've got that I wonder why that didn't know because I didn't call fit before okay so now I'm fitting the model I don't see an error I expected an error let's so what happens when I fit the model well it returns a promise model dot fit returns a promise if you don't know what a promise is guess what I have a whole set of videos about what a promises and I'm also going to be using eventually a weight and a sync which I also have videos about but right now I can just write the dot then the prompt fit returns a promise which I can then call a function called then to where the results will be passed in and I'm just gonna say and I'm gonna use this arrow syntax this es6 arrow syntax console dot log results and eventually I might want to do more with this so I'm actually gonna make it a full function so this is what I'm saying is once you fit the model then log the results let's see what happens waitingwaiting ah ok great look at this history loss and there's my loss so it fit that model it did one epoch and gave me a loss great so done train the model here's the thing I want what I want to do ultimately so this is actually way done what I want to do is first of all I want to train the model for more than just one epoch so one thing that I need to do here is pass in some options so I'm gonna create a variable called options and one thing I can specify is like epochs I'm gonna say do it for 10 and then I'm gonna say and let's actually let's just say 2 right now because it's gonna take a while so the third argument to model dot fit is options and if I go into tension flow yes and I look for a model dot fit oops I was right there already we can see now these are the various options and I'm gonna be using a bunch of these but epochs is one of them the number of times to iterate over the training data so let's rub this down and you don't have to do I'm gonna I don't think we need all of this printing stuff so I'm gonna get rid of some of the earlier printing things because I don't need to look at all of that so much so let's run this whoops options is not defined I spelled that wrong I guess I still have 44 and 45 console logging stuff which I don't need I didn't get an error that I expected yet which is kind of interesting and oh you know why one thing that I want to do is I want to update you know at the time of this recording I think the most recent version of tension flow chess is zero point eleven point seven well and I when I was previously recording I was using 0.4 and I think some things have changed so it was alright so let's let this run it's it's running for to epochs right now it's finished and I can look at the history and I can see both lost so we can see the loss went down for the second époque that's great now let's run this over ten a pox and let's run this and let's just console log results dot lost by the way or what was it is it results dot history dot loss might be that now let's look at what it is a history history dot loss okay so let's do this whoops I don't need that let's go back here hit refresh and waiting I'm gonna edit out this waiting part or speed up we're doing for ten a pox I'm have a lot I gotta fix I can fix this here while I'm here this is awful cloaking device oh it's back okay great so look at this over ten a pox the loss is going down this is good this is what we want to see now here's the thing what's it using to calculate that law huh oh there's so much to discuss I gotta get myself organized my thoughts here I want to here's I think maybe maybe I've done this video I'm really really drugged breaking this into lots of small parts and really what I've done now is call model dot fit with one single option the two things I need to do that are next one is I need to figure out what's getting that law like what data is it using to calculate that loss is it the training data didn't I talk about testing data and validation data should I be thinking about that it's a point so I've got to deal with that number two is I would like to I the point of this is I'm in a p5 sketch and I could say function draw background zero and I can run this but look at this it just is loading up there all the while while it's training I'm locked I don't have any ability to run an animation I want once it finishes I see the canvas I want the canvas to animate while it's training and I want to see the loss over time I want to have that reported back to me so those are the two things that I need to do I think I can tackle the training the testing and validation data thing right now because let's do that in this video and I'm going to add the animation stuff in the next video so first of all okay so I have my data set my data set has I think it was five thousand six hundred and forty three elements data points in it I said at the very beginning of this series runs preparing the data set that a typical thing to do is divide the data and again this is really small for proper machine learning model robust I probably want to have a much larger data set but this will actually kind of work just fine as we'll see I want to use probably the 80/20 rule saying that 80% is actually the training data so I want to just only use why does it it's because the keyboard is next to this it's going sound I want to I want these X's and Y's to only actually be 80% of that original data so I'm not doing that I'll maybe I'll add that in another point that can be an exercise for you of the view for you as the viewer to take out 20% or maybe because my data centers are small just take out 10% of the data so that's what would be used to test the model after I finish training it but while I'm training it while I'm actually training it figuring out well how many input notes do I want what learning rate do I want what are these sort of taper parameters what are the parameters of this system that I want to try different things how many pucks do I want to train the model for what batch size do I want to use all these things are known as hyper parameters the parameters of the during the training process if I want to be playing around with those I need a separate data set to compute a loss that's not part of the training data but also is not part of my testing data that if you use when I'm completely done training that's what the validation data is the validation data is basically a test dataset but it's not your test data set when you're done and you're ready to publish your model it's your test dates that while you're doing all the training intensive like that jazz has a configuration option for model dot fit that just says hey use this much as the validation data so let's go back over here let's go back to the documentation and we can see here now I could specify the validation data or I could just specify validation split which is a float between 0 & 1 it's the fraction of the training data to be used as the validation data so if I come back here and I just add an option validation data and I say 0.1 I want to use 10% of my training data as the validation data that's what's going to be used to calculate the loss but it's not part of the Train down now there might be an issue I also want to make sure I have shuffle on shuffle is a parameter that shuffles the training data at each epoch because you don't always want to train with the data in the same order as you're tweaking all the weights and stuff as it's doing its training if it's in a different order it's gonna help it out but the validation data I think I looked at this before is before selected before shuffling so it's selected from the last sample so I might have a slight issue or if for some reason the order my data is in there's something weird about the end of it is all one label or something I probably won't like shuffle it myself manually but let's not worry about that right now but that's something definitely to be cautious of well this is so much to think about all right now so now that we've added shuffle and we've added 10% as validation data let me now run this again it's danced around oh it's finished already ten tiny boxes not very long so here we go and we can see this is good we still have a loss that's going down over ten a box maybe we can get this lost even less maybe one train for more epochs maybe we want a different learning rate we need to tweak all that stuff but I want to be least see it and see an animation going while I'm doing that so that's what I'm going to look at in the next video I'm gonna look at how to do an animation by also adding callbacks to these options here okay so ello and async you know I need to make this happen in an async function that's actually I think that it really helped me out okay I'll come back to that in next video thanks for watching so far so we train the model I'll be back soon oh shoot shoot I wrote the wrong thing hold on not you let's go back to where this finished we just run this again unnecessarily oh shoot and I'll fix that okay so I'm gonna do redo the end of this trip okay uh so we finished it trained now with the validation splits and OH breaking news breaking news getting information from the chat that I wrote validation data here interesting give me an error so if I wanted to give it specific validation data that's what I would use but I want to use validation split thank you for to the chat for correcting me there let's try running this again let's give it just more epochs a little bit more time to wait let's give it 50 all right all right so this is gonna be the sped up portion where I could play the ukulele but nobody will hear the ukulele in the edited version of this video why isn't still printing out this one tensor actually where is that there must be a labels tensor dot print alright alright so it's back let's take a look it's ok it's back let's take a look at our loss function over 50 epochs and we can see it's going way down to 0.75 you can see it's kind of stopped actually we kind of accidentally might have you could see how it kind of goes up now we can see like it's not able to get any better so we might not even need 50 epochs but we might want to tune various parameters to see but I'm not going to worry about all that right now the point is I have now trained the model using model dot fit shuffling the data with a certain validation saving 10% for validation I'm not doing proper testing data yet that would come later and 50 ybox okay so in the next video what I want to do is make it so that I can run an animation I can graph the loss function over time all that sort of stuff and not have it kind of like blocking right the way it's doing right now the animation thread and then of course I also need to allow the user to specify a color and get a label for that so those are the next two steps I need to do see you in those videos okay sorry I'm my nose is running and my tissue box is empty I'm out of water I might need a little break let me see if I can find oh I have oh you know what look at me here prepared prepared with the Kleenex uh alright okay how are we doing everything everybody wait wait crazy teenager rights lol I was watching roblox and this on my recommended so did you are you were you watching just like some gaming and this was like arbitrary recommended to you and you just pops here welcome now if you don't know about coding what I'm doing here is probably like the most advanced stuff that I do on my channel and you might want to go back and watch a lot of the beginner tutorials if you're interested certainly don't have to slack is back okay let me see if I can get the slack channel going so for those of you might be wondering the slack channel is for sponsors of the of the YouTube channel or patreon patrons we have patreon it's the same thing there should be a sponsor button you can see I mean nobody eats the sponsor only if you feel so inclined you'll be able to watch all the content but I use that to have a smaller community people asking questions and discussing the stuff I'm trying to connect right now okay looks like it's coming back to me what time is it 1230 I'm okay on time it's gonna be way worse Josh in the chat asked do tutorials on slack BOTS no I would love to though okay okay let me close this thread okay I am now got the slack channel in welcome new sponsor exmiss drink X so I don't know if my kids are at home they don't of school today pay for a babysitter so I could be here to do this live stream by the way so but I set it up my Phillips hue lights at home they blink some sponsors the channel I can't do that here because I'm in a lockdown in the NYU WiFi I new sponsor Kyle right because I should mention the whole sponsorship if you're there thank you thank you to those very kind of you to sponsor it does really help me allocate time to doing these videos and algas thank you for thanking people and I really one thing I really need to do I need to do some maintenance in terms of like getting all the emojis stuff set up on the channel okay you have World Cup winner with prediction with tens of Lodi Jess ok alright I got to move on let's cycle these cameras cuz I got to get out of here I'm leaving for as I mentioned I'm leaving for a trip tomorrow for two weeks do my kids know how to code so my kids are 6 and 9 so I would say my younger daughter I talk and I show her things sometimes but she's really too young to do text based coding certainly and my son who actually is not 9 he just turned 10 was his birthday over the weekend he has done he's taking scratch classes and he's attended actually some p5.js workshops that I taught for fourth and fifth graders so he doesn't know how to do it unfortunately he fortnight is a thing it's very distracting anyway so let me having to get our gaming channel set up all right sorry ok ok all right let's move on now to the next video ok all right okay this is really let me see if I can do some where is it it's over here that it's like if I fold it I think that's better now the Kubo robot that sounds interesting okay okay I don't know about this fortnight thing I don't know if I should be allowing it okay all right all right videos seven thousand two hundred and sixty three of my making your own color classifier with previous video previously on making your own color classifier intense photo yes I worked in the model dot fit function so I'm fitting the model according to my training data with these options now what I want to do is I want to be able to basically see an animation graphing the loss function while it's doing the training so right now I just get a report when it's done so there's a few steps that I want to take to do this the first step that I want to do is I actually want to move this into a separate function so I'm gonna just write a function I'm gonna just make it a global function called train nice train and I'm gonna put model dot fit there then I'm gonna call train here so that's after one and let's let's uh oh and I'm gonna put the options here in this function and I'm gonna just go back to to a pox and I'm gonna run this Oh X is not defined oh boy I did all sorts of goofy stuff here so let's let's make these global variables X's and Y's I'm gonna need to do again just could use some refactoring but now it's training and to epochs done well you can see the loss functions great but still I don't have an animation so what I want to do is I want this to be an asynchronous function I want this function to be an asynchronous function to happen and let things keep going and guess what I have a video series about I do that with the keyword async and then if I say viii if I make a function async I can use the keyword await meaning this function will wait for model that fit to finish before it's done in turns a promise by the way so I can actually take this now I could say return a weight and then I can put my then up here right because it's gonna return that same promise but it will happen asynchronously meaning it will the code up here will be allowed to move on while this is happening in the background in theory but I've got to do more here it's the same behavior hmm so why is it the same behavior well I've set myself up for success but I don't have success yet and the reason why is that tensorflow das is using something called WebGL to do all of the calculations and it's taking over basically your animation or drawing capabilities while you're fitting the model however tensorflow J s comes with a function called next frame which returns a promise that resolves when a request animation frame has been completed it's simply a sugar method so that users can do the following a weight TF next frame so what I can actually do is kind of trigger the animation letting drew the draw loop go the p5.js draw loop is just using requestanimationframe itself by adding a weight TF next frame somehow in this async function so where do I add it so I have an idea I'm going to add something to this called callbacks so and I got a spell callbacks correctly for this to work so let's go back to look at model dot fit model dot fit and we can see that oh look at this this is like a list I was looking in the wrong place but last night when I was looking this up it wasn't actually here these are optional callbacks that can be called during training for example on train begin on train end EPOC begin on epoch and on back begin on batch end so let's just let's add on train begin on train end just for real quick so I'm gonna say I'm gonna have a callback on train begin and this needs to be a function it's gonna my life's gonna be easier if I just use this yes a six arrow notation and then I'm gonna have another callback called on train end and I'm gonna say training complete so I'm gonna just add these two callbacks so these are functions that are going to be executed during the training process let's see if I did that right training start and try to complete and I see the results wonderful so what if I in train begin just make it a slightly longer function which also has a weight TF next frame in it what happens if I await the next frame in it no oh huh do I have to say a sink here wait where do I put that all right that's not what I wanted to do let me go back we're gonna look at the chat while I'm taking a little okay alright alright nobody's discussing anything about like does anybody actually watch the stream or you just come to talk about like different programming languages and why why why why this is making me crazy okay all right training start training complete okay now let's try a different callback let's try on epoch end and on epoch and well it takes two arguments I'm looking over here on this computer because I I have some notes there which I don't typically do but it's the documentation here doesn't actually if we look here it's not telling you what the arguments are for these callback functions but I looked them up and so the arguments are the number of epochs so I can say num and then a log which is like a report so I'm gonna say num and logs and then what I can do so what I'm gonna do is I'm gonna do console I'm gonna write a function here with multiple lines of code I'm gonna say console.log epoch num and then I'm gonna say console log loss a logs dot loss loss so there's a property of loss that's in that logs object so these are the arguments do every time it finishes an epoch so I'm gonna now give it 10 epochs let's see what happens if I add that call back alright epoch zero epoch one epoch to epoch three epoch four look at that so I am now getting I'm getting a call back for every one of those individual epochs and we can see the loss going down and then of course we see all of the lost values when we're done but now if I want to let it draw something I believe I can say oh wait TF dot next frame hold on timeout did I talk about what I'm so lost in to what I'm so lost like did I explain what TF not next frame is already cuz I went back and forth and I like went down a route and then I didn't want to I think I did explain it shoot I'm confused I'm just gonna have to do my best Matthew I'm sorry hopefully this will come together I might end up explaining the same thing twice okay if I want to draw something at the end of each epoch I want to allow the animation to proceed I can go and use that function TF next frame its whoops TF next frame which allows me to which allows me to sort of unlock the drawing thread and and let draw update itself so I'm gonna go and I'm gonna say a wait TF next frame right here at the end of each epoch and then this is also an async function so this now has to be an async function as well is that gonna allow me to do that I think so yes okay let's try this oh yeah look it's drawing now let's actually add an animation so let's do something like stroke 255 stroke wait for line a frame count modulus with zero frame count modulus with height so I just want to draw a line that is that is moving across so for example if I don't bother calling this train function all we can see here I have an animation that's running okay so now let's call the train function and see if that animation runs waiting waiting waiting waiting waiting waiting let's get to epoch zero I'll look at it so the animation is going but it's only able to draw once at the end of each epoch so while it's training if I want to let it unlock that drawing more often maybe a different callback would work better and in fact one something that tension photo jess is doing behind the scenes and model dot fit sorry is in the callbacks right it's actually batching the data so I have five thousand six hundred data points it's actually running the gradient descent algorithm in batches that's what stochastic gradient descent means and there are also on batch begin on batch ends and I could sort of specify the batch size I'm letting it use a default so what I actually think that I want to use it's gonna be able to it does a batch pretty quickly a full epoch takes quite a bit of time so I can actually do on batch end what I'm going to do here is I'm gonna add one more callback on batch end and I'm gonna make this the async one so it also has a batch num number and a number of logs so it's the like epoch end but I'm gonna put the await next frame in there this one no long as it needs to be an async function so this should unlock the animation much more quickly because it lets it draw every at the end of every batch so let's go to this now and we should see yeah look at this so the animation is running just fine and we should see now it got a little glitch there when I got to the end of epoch 0 let's see if it does that again no I don't know what so the first epoch they must have had to do some copying onto the GPU I'm not sure why but you can see the animation is no longer study stuttering from epoch to epoch okay so now we have a trading the model with an animation going let's at least so what I really should do is graph the loss function and by the way I can look at the loss function at the end of each batch so I can get a much more quickly updated loss function so I'm gonna leave that as an exercise to the viewer but I'm gonna just what I'm gonna do is I'm gonna say let loss P and I'm gonna create a paragraph element again I'm not really being thoughtful about design and interface here loss so what I'm going to do here is it's just going to have a paragraph element that says loss in it and what I'm going to do is instead of logging the loss to the console I'm gonna say loss P dot HTML and this is using the p5.js Dom library I give use native JavaScript or jQuery I'm going to put this loss information into that paragraph element so now I have an animation going and then as soon as I get to the end of the first epoch I have to talk for a bit here I see the loss function so now I'm training and getting a report of the loss function so for you I'm in the next video what I'm gonna add is inference or prediction I'm gonna allow the user with sliders to specify a color and have the label returned to me and what I would say to you as an exercise is see what happens if you can query the loss function with the batches and graph it over time and so you see that would be an exercise to you as the viewer and I'm gonna publish a github repo with this finished project so you can look for the code this is very confusing but you can look for the code it'll be linked in the description in two different places there'll be the code that matches exactly this video and then there will be the code that's in a separate github repo that and someone in the future people will be contributing to that will have maybe the graph and other kind of designing things that people have from the community have added okay great so one more video to go I think and and then some other ancillary ones that I forgotten about but one more core video to this tutorial series which is adding the and I will see you if you're really gonna watch all of these I will see you in the next video you can check the video's description for the next video link okay okay so that's interesting I should have been looking at the chat maybe I'll cover this in the next video let me try some things so you're saying if I just say return TF next frame I don't need the await an async thing I like this better and then in fact I can just say TF next frame that's the callback I want to happen oh that's way nicer so I'm gonna explain this at the beginning of the next video yeah all right great Oh Dave into the chat let's see where can I find the code to the 500k subscribers map Doozers I'm Way behind I haven't published it yet I you know once the video gets publishes or like a coding challenge then I publish the code if you hit me up on Twitter at Schiffman I can probably it'll be a reminder to me and then I'll reply to you there and upload that code okay okay you want validation loss not training loss donut aren't I getting oh are the do the logs have different loss I thought the logs got lost is the validation lost let me look at this oh that's the validation loss is different I didn't realize that okay great did I say video subscription I meant to say description did I say subscription shoot brain is melting I need water today wouldn't mind if I go get some water I gotta stay hydrated on the coding train as you know welcome new sponsor Thank You FD God my lights are blinking at home if I could be home right now I would see the blinking lights probably off all right okay so thank you so the things I need to cover at the next video are the validation loss versus the loss thank you for that and interestingly enough I think there is a problem with my validation data I really should I need you need to shuffle it and I just have very little data so that's not great and I need to talk about this simplification here of TF next frame okay yeah okay all right everyone I'm back start over all right oh this is getting tiring but I am back and I have yet another in this building your own custom color classifier with 1000 GS series now the thing that I want to add to this video and by the way this line moving across is pointless I just have it there so that I could see that the draw loop is animating that I haven't blocked it there's two things that I missed that are kind of important from the previous video um one is this is actually not the validation data loss I didn't realize this but I'm going to I'm gonna change this here I'm gonna I'm gonna console.log the full logs object so right what I'm putting on to the screen is logs dot loss let me come to the console log what's there so again we have to wait a minute for the first epoch to finish apologies for that okay there are actually two lost values there's the loss function can be computed against the training data and there's the loss function computed against the validation data now to do this properly I really should be using the validation loss because that data that hasn't been done with the training that's that's that's that's protect against overfitting having my model work really well with the training data only the thing is I have a very small data set 5,000 data points I'm just using 10% as the validation data and the weight to the footage s works it also takes that 10% from the end and I didn't wasn't careful about shuffling the data around so this is something that I should come back to I don't know maybe this series will go on to infinity but if I were doing this properly I would actually want to show the validation loss here like this log stop validation loss maybe I want to show both and maybe I want to be more thoughtful about shuffling the data first in advance but I that's not what I said I was going to do in the next video so I'm again leaving that ten rarely as an exercise to the viewer or I'll come back and do it in a future video I don't know yet that's item number one item number two thank you to me I am so me and others in the coding train sponsor patron group I made this way more complicated than it needs by trying to make this an async function in here actually this does not need to be an async function if I just return TF dot next frame so if I just return TF next frame it's actually returning the promise and unlocking the draw loop so that makes it simpler actually I couldn't make this so simpler what am I doing here at the end of every batch I want TF next frame to be executed and so I actually don't need to write a wrapper function to execute see if that next frame what I could just do is set that as the callback the callback again if I wanted to do more with on batch and look at the loss and the logs but really what I want is at the end of every batch to draw a new frame of animation I can just put TF knocks next frame as the function which is the callback there okay so let's this is still working that simplifies the code makes it a little nicer to look at I don't even really need this on on begin and on end but I'll leave those in there just so you see them okay so now I'm ready for what is the purpose of this video the purpose of this video is while I'm training the model I couldn't wait till I finish training the model but I've actually it allowed this to happen while I'm training the model I want to be able to specify a color and see what the neural network thinks that color is so very quickly to do this what I'm going to do is I'm going to create our slider G slider B slider I'm gonna make three sliders again this could use a lot of improvements and I'm gonna use the p5 Dom library create slider function so the slider is a range between 0 and 255 and let's start with like what's this red and green make yellow let's start with a yellow and so the G's the B slider should be on 0 and I want the background color too and I don't know what that's doing there I don't need this line anymore it's distracting I want to say our slider well let's actually let's so I want to say let our equal our slider value so I want to get the values from the sliders I want G and I won't be eventually I'm gonna send these as inputs into the neural network but right now I just want to be able to see that color our G B okay so here we go so now we should see there are three sliders and as I adjust these sliders I can change the color and so what I want whoops what I want is to be able to and I see though what I want is now to see the neural networks prediction down here so how do I do that okay time to use tensorflow touch yes again whoo so I need to make some input data so the input X's are tensor T F dot tensor 2d and an array with RGB in it now in theory I could be running prediction with multiple RG B's right but I'm not so I need an array of arrays in here so this is my input data then what I want to do I want to say model dot predict with those X's feel like you know what I need to normalize those right because the it expects to have normalized values between 0 & 1 so I need to divide each of those by 255 then I need to call model dot predict and then look at the results and that hat oh you know what this doesn't actually happen asynchronously it's the because the data is still on the GPU this is a confusing thing I have to pull I'm gonna use that date I have to pull it out but let's just look at the results of pure results so I should then be able to say results dot print okay so I think this is knee just creating the inputs getting the prediction and then I should be able to see that in the console syntax error who I have an extra extra curly bracket alright okay so we can see this and this is exactly what I should be getting right it is a probability distribution over nine labels now whether it's giving me correct ones who knows but look at that so now how do I get the label out of there well remember that what I'm looking for is I'm looking for which probability is at them at the highest level is it a ninety percent chance of it being yellowish and point zero one point zero two point zero three you know 1% 2% 3% of being the other ones and there actually is a function intention flow Jas that will pull out the index of the highest probability value that's called Arg max right I could write a little for loop or or some kind of function to do that but if I look for Arg max TF dot Arg max returns the indices of the maximum values along an axis so this is can be quite more complex because I could have multidimensional data but I actually get to do this in a really simple way I just want to say let index equal results dot r DX oh and if there's an access of AK c access of one the first there's a onedimensional here so now let me say index dot print and so let me run this and we can see it's just giving me whom is that right is that a coincidence so I should get some different values yes okay so it's actually changing so that that's giving me that maximum index so as I change so so this is my label here's the thing though that's my label but I need to convert that to one of these so 0 means reddish one means greenish two beads bluish 3 means orangish so have this label list already I should be able to just say let label equal label list index the only thing is I can't do that because this is a tensor that's a tensor and what I want I need to pull that the tensor is the numbers the data that lives on the GPU the WebGL fancy thing that TouchWiz has implemented I need to pull that off and normally I would pull that off with an asynchronous function shoot normally normally I would pull that off with an asynchronous function but the thing is here it's such a little tiny bit of data I think I can pull it off synchronously and not slow down my program from running so actually what I want to say here is Data Sync and then which is a dot data would pull it off asynchronously so let's look at and let's let's say I'm a console dot log index and let me get rid of my other console logs that I don't really want to look at right now so okay so I got an array with the number in it I pulled it off and so then I just want to say index 0 so I only need that first value index 0 and so there we go that's the label number I now have the label number and so now I can say this and I can say and let's put it out on a paragraph element so let's say let label P let's have that B first and so now I want to say label P equals creepy and then I should say all the way back down here label P dot HTML label ok ready for this here we go I've started my training oh wait why this is so silly but I want the labels of it I really should not be changing this right now so let me just put it here okay so here we go it thinks that's greenish right well it hasn't gotten very far with the training I would imagine that once we train further and the law starts going down it's going to recognize that as yellowish so here I'm gonna just wait a little bit and I'll be back in a minute let it rain there's some a pox and I'll look at the chat has Schiffman open the slack yet no no I still have open yes I do have a slack open I'm gonna let this run for a little bit I kind of wanted this to appear as yellowish okay I only do 10 epochs I don't know I should be console longing the epoch there we go look look look look look it's learning all right back so he trained over ten a box and you can see now it's saying this is yellowish let me tune this down that's greenish turn this up that's bluish we've still got blueish can we get some purple purplish can we get some pink oh it didn't get pink maybe if I add a little more brightness how it thinks that's pink so I have now trained the neural network to recognize and let's see if it can get red reddish so we could see I could play with this all day long this is now going to classify the color based on that particular model so in a way I'm done I probably want to train it for more epochs what are some things that I want to do so one is I would want to be more thoughtful get more data I would want to be more thoughtful about the validation data and then other thing I would want to start doing is thinking about well does it actually work better what are the hyper parameters that I can play with for example the hidden layer I put sixteen units in it well or what happens if I use a different activation function for the lare what happens I use more nodes or less nodes what if I change the learning rate what if I change the optimization function if I use like the atom optimization function so these are things that all these things are things that I could play with and research and think about an experiment with to try to tune the model really well then at some point I also would want to save that model right save it to a JSON file so the trained model somehow so that I could load it back in without having to run through the training process again maybe I'd even want a larger dataset I don't to train it over a long time me but I want to port this code to node so I could let it trainlike serverside without having to train in the client there's so many possibilities but I have now built a machine learning model with tensor photo how many videos this took that trains a model based on crowdsource color data and if you want if you just a humor me for a second if you remember if I go here this is the system right this system was used to allow people from the internet to click on and say that's like pinkish that's greenish that's blueish tag a whole bunch of colors save all that data in a firebase database retrieve all that data clean that data put it into JSON file load that JSON file here into this sketch build a model train the model with that data and then pulls a new color from a slider oh and I forgotten something memory management oh I knew there was a step that I'm missing estimating what category out of the fixed set of labels this that color is but I did forget something really quite important which is memory management let's look at this num memory of TF memory dot num tensors so again when I create tensors that are allocated to memory on the GPU to store numbers those don't get cleaned up automatically there's no garbage collector like in kind of regular JavaScript programming so 15,000 for 85 tensors now one thing and there's still even more and more and more it's growing this is a memory leak so one place where I didn't clean up any of the tensors is right here and there's a easy way I can clean this up by adding in the TF tidy function so what TF tidy does is it says just put all of this code that's inside of this function passed into TF tidy clean up any tensors that are made there so this will clean up everything for me so now let's run this again and we're gonna take a look at the tensors there's thirty one seventy three it's kind of leaking right well let's let it get all the way through ten EPOC s I'll be back in a minute when that finishes okay it's taking I should add that I should have something that's reporting the epochs oh it finished okay okay so the training is complete and we can see now ah there we go I am no longer leaking tensors now the thing is do I really did I really need 1628 tensors I don't think that I did I think there is also there is also a leak going on inside of this train function and I think there's an issue with this and so I'm gonna I might have to do a followup video about this because at the moment if I go to github.com TF oh oh hold on let's go from here too I should have had this prepared where do I go github and issues and I'm gonna look for fit memory leak this one so I believe there is at present a memory leak in model dot fit with callbacks and you can see that's exactly what I'm doing right where model dot fits with callbacks so I'm gonna not worry about that particular memory leak right now I'm gonna wait for us to see if that gets corrected by the time you're watching this that might already be corrected and this code might have no more memory leaks in it just by updating the version of tensorflow Tijs or I might still be missing something in here to do a memory leak so you know if you don't want any spoilers and/or the following videos the fallout videos have not been published yet you could kind of kind of like sort that out yourself but I will come back at some point and talk about that okay so thank you for watching I wish you many purplish and pinkish and bluish and greenish days all the colors of the rainbow may they fill your days with joy may you make your own classifier with your own data please share with me I don't know it has this helped the world this tutorial series I've missed so much about data and data collection and machine learning and bottles and algorithms but hopefully I've got done something this is not the end it's only the beginning well I'll see you soon in future tutorial videos that up because this playlist probably has about 300 ok good bye alright everybody ok it is 115 I have done what I set out to do today which is finish this example I am now going to I'm gonna leave within 15 minutes at 130 because I because I because I need to do that I'm gonna check some messages here because I have all these texts mission to make sure yeah but 1140 somebody spoke so that my kids are at home with a babysitter they hadn't left the house yet at 1140 the lights blinking so they must have left because I'd get a so that did get a text message saying that the lights blinked at home who was that who sponsored that was awesome ok so that was all right great ok so I'm going on vacation tomorrow for two weeks and so I will be online here and there I let's see there are currently 7 videos edited versions of the color classifier series that have yet to be published I could use help keeping the code getting the code that goes with those videos online if anyone is like coding along or I have all the stuff it Google Drive and wants to help with that you know open an issue on coda train calm no no github slash Cody train slash website could you just help with that but I will hopefully keep up with that matsya who also who is does the editing will be uploading and editing and these videos will get released so the channel won't appear dead but there won't be any live streams until the week of July 15th I have something that I'm really trying to figure out how to do and so I'm looking this can be discussed in the slack Channel I really really really want to get the updated version of this book out I have a new chapter I want to write about neuro evolution I want to fix all the mistakes update all the code and get a p5.js version of this book out this summer is the time for me to do it now when I get back in July I've two or three weeks of being in New York City before I'm out of town again and I was planning to do to live streams per week to keep up with the YouTube channel but I might need to take a more of a break from that so I can focus on the writing of this so I guess I'm curious to hear from people you know the you know in terms of like keeping a youtube channel going you should never what what everyone says is don't stop making content but that's not realistic people have to go on vacation they have to take breaks so I don't believe in that you know the views go down subscribers go down such is life but I do know that there are people who are funding the work that I'm doing through patreon and the YouTube sponsorships so what I want to do is take the temperature how people feel about kind of that funding essentially going towards the time of working on this book taking away some of the time of doing live streaming so we can discuss that in the slack channel I'll be curious for people's thoughts but so I don't know to what my my goal is always for live streams per month that's what I'm kind of committed to and I've actually done five or six the last few months and so July if I did to the weeks that I'm back if I did two per week I'd get to four and August I could get to four but I'm thinking of actually doing fewer and and focusing on the book so I'll be curious for your thoughts about it and me I am so me says that I should work on it livestream working on it pardon there is a way that I could possibly do that I'm gonna have to that's an interesting idea okay so any questions anyone wants to ask I'm very I also really want to get stuff in the shape for the channel but can you have guests fill in I so that's a great suggestion and I do want to have more guests than I've had guests it's it's the same or it's a different kind of work to have the guests it's not the same amount it's not the same work but having guests is would not open up more time to work on the writing of the book it would be a good thing for the channel and for the world I would hope having more guests I would like to do that and I definitely planning to do that but I don't that doesn't solve the problem of allocating more time to work on the book okay any questions from anyone BRIC the world asks what programming languages do you know I would say the one the language that I probably know the best even though I don't know any of them that well is Java with I don't know dare I say second is now JavaScript I mean I only started programming JavaScript like two year three years ago probably longer at this point I'm sort of forgetting how much time has passed I to program in C and C++ I used to I mean like 15 years ago so I guess I know those languages and I've hacked around PHP and Python and Perl as well next time use atom with helu it's very fast let's try that really quickly so I should do a fall and want someone help me keep track of followup videos so let me just try that real quick if I put atom here and then put this here know how do is it just lowercase oh look at that hold on a sec it's I don't know that it's necessarily happening faster but looks like that loss went down quite a bit and it got too yellowish very quickly you know interesting so it's interesting again to try these different architectures I'm gonna put it back because when I published this code alright this is the stream over yes it is over any classifier ideas that we can practice huh interesting question well the next thing that I was planning to do I based on how long this took I'm not so sure about any more of us to do image classification and use convolutional layer so that's something you could sort of try amazingly everything this here is the same just the data the input data is twodimensional and you'd want to add a convolutional layer but to hit two two hidden layers one being a convolutional layer the learning rate should be around 0.0001 probably yeah I you could train it to use the yeah what is your so show hum is asking where's the slack workspace the slack right now is open only to patrons or sponsors of the channel and you will get an invitation if you're a YouTube spot I'm using both systems right now I should probably just pick one at some point but if you're a YouTube sponsor I don't get your email to send you an invite automatically so look through that look at the community tab there's a post there that you'll be able to see where you can find out how to submit your email okay are we going to train or get the Train miles from Python I actually do have a plan to train to do a video about training and LS TM model with Python and then have that train model coming to JavaScript I'm actually there's a great post about how to do this by paper space well let's see if I can find this paper space LS TM Khris Khris no so we could find this paper space LS TN blog yep here it is so this is a post just recently came out by Cristobal who is one of the main maintainer x' and contributors to the ml5 library which and so this tutorial goes through how to work with paper space and train the model with your own data and then run the model in using ml 5 j s which is a higherlevel library on top of tensor photo jazz so that's the other thing that's just too much to do because my other priority is to make a lot of video tutorials with ml 5 but I'll just you know um go as I go and you can see it running here this is so you can see it's doing it's predicting doing predictive text based on the train model okay well I forgot to change atom I thought I didn't didn't I do that right here people are telling me I forgot to change atom but I thought I did and I also being told me I should have a much lower learning rate for Adam oh yeah Riaan for me I would love to do all the things reinforcement learning is actually something that I am most interested in more so in a way than this sort of classification stuff which is I'm also interested in but I haven't had the time to dig into it yet i've been doing neuro evolution and that i have a whole set about that which is kind of reinforcement learning style thing oh is it possible to code the approximation from tau Oh June 28th I don't have a child a challenge oh I mean it should be SGD oh it should be SGD with oh boy the point is you to let everybody else play with this stuff yes I did end the video with SGD oh I see it's fine now okay I don't know what I got it I got it I got it all right because today is coming up I mean this is just though me this is kind of like where is my there it is if I had an idea for a really short coding challenge with Tao I could do I would but let's at least look at the approximating PI code what was it 90 something I don't think we're gonna get a towel day challenge this summer so what do I change here this is the one that's approximating pi so if I wanted this to approximate tau I'm doing the circle probability over the total x for don't I just double it I'm brain is tired oh no I don't I don't although I have to answer this for myself right now so area of the circle is PI R squared that's our in the area of the square is 2 2 times R squared which is 4 R squared so so so this prop this ratio which is R no no sorry this ratio check on probability a pi equals 4 times that probability so if I wanted to do it with tau what is the area of a circle with tau PI tau divided by 2r squared right so wouldn't that be eight we're gonna just be eight looks well the camera went off oh you can't see me sorry everybody sorry everybody this is what I was drawing I was working out the approximation plan I can't do the towel thing now your record PI things and means doubling oh the learning rate has changed oh my god I think I had a point I'm not I don't remember what's in this code x times x plus y times y that's the distance well that's the is less than the radius squares in the circle oh the record PI no oh I'm printing record PI I see oh right of course that's not something available there's no math doubt towel I have to do that oh shoot there we go there we go all right I don't think this is worth me doing as a video coding challenge just to update this one for Talde I guess I could the difference yeah you know you're behind I correct that in the future me I am so me all right so I could I don't think this is worth making into a video but just for fun cuz I'm here and I'm leaving and why not oh here you can't see me let's let me erase all this okay okay here we go everybody maybe I can figure out a way to make this totally ridiculous and it'll be worse it'll be worth having done this close this oh but of course by heart has a Tao song Tao I'll to here I just want to find the digits what do you think accurate happy holiday everybody take point two point eight no there's no point there six point two eight three one eight five three oh seven one seven nine five okay welcome to a special just me wasting time on the internet but you know it's worth noting the day we I noted Pi Day and I made a coding challenge which was an approximating PI by using a kind of dart throwing technique let me close all this stuff up we can run the example right here the idea is I'm throwing darts into my processing canvas and the ratio of the number of darts that land outside of the circle versus inside the circle give me the value of pi now one of the comments I got so much on this video was but you're using PI to calculate pi well no I'm not and I want to be clear about this I'm going to comment out all of these lines of code right here and I'm gonna say print line pi so there we go we can see my approximation of pi right here now I am sort of using PI in the sense that probably the value of pi is used where is their lips function somewhere in here no wait I thought maybe I use the ellipse function oh I do uh in where this ellipse function is drama let me comment this out so now I am not using the ellipse function which probably behind the scenes is using pi and I am I am only you the reason why I'm seeing a circle is because I am calculating the distance of each point to the center and checking if it's below a certain amount and if it's below a certain amount it means it's inside a circle but let's just look here the only place where I might be calling math pie is now commented out I was just using that to compare okay so we can see here that this is in fact not using pi but giving me this amount now why does this work now I already covered this in the previous video so you totally could just go away and do something else right now but if you don't want to go watch the previous video the reason why this works just to talk about it again is if this is a square and this is a circle right the area and this is a value called R the value of the sorry the area of the square is each side of this square is to R so the area of that square is 2 R squared R squared or 4 R the area of the circle as we know maybe from some math class is PI R squared so here I am using the idea of pi but only only because I know that that's the definition of the area of a circle now what if I were to just throw darts at the wall and I would count how many darts landed in the circle versus how many darts landed overall that would be the same so the total darts divided by the circle only should be equal to well this before our squared for R squared divided by PI R squared so now what I want to do is solve for pi ok well first of all the R squared can be is gone so then I could say 4 times total divided by circle total equals 1 divided by PI okay I just flipped those right so pi equals the circle total divided by four times the total total that seems kind of right maybe I got that wrong did I get that wrong because four times circle divided by total well yeah why is the four on the bottom why is the four on the top okay so now if I reverse these I could say pi divided by four equals the circle this should say Circle T like the circle total divided by the total now I can just multiply each side by four and I could say four times circle T divided by total equals PI or PI equals this and we go back to my code we have four times the total number that landed in the circle x divided by the total overall so that's there so now what if I want to approximate oh well what is tau tau by the way is okay so what's the circumference of this circle meaning what's the length of the arc all the way around to PI R well that seems like an awkward way of writing it why do I have to have this to here what if I just had a Val you that instead of being PI was twice the value of pi then I could say the circumference is just that value times R and that's what tau is uh something it's another Greek letter that I've totally botched let me go look up how to draw that better so here's the Greek letter write uppercase to lowercase towel so I can go back and son so I didn't do the worst job here I guess it yeah that kind of that'll be so I could say it's a tau R and guess what if R is one the circumference of the unit circle is tau the circumference of the unit circle is tau naught to PI I don't know people now enjoy the comment section of this video everybody so here's the thing what if I want to approximate tau okay oh boy oh boy okay well the area of this circle is still 4r squared and the area of the I'm sorry area of the square is still 4r squared but the area of the circle pi no no no PI divided cow what is it with town tau tau divided by 2 tau divided by 2 R squared ok so yeah I love this is super nice I gotta say it's tau divided by 2 5 2 where do I want the well welcome to here but do I want the 2 there but let's say I do this then the total weight for R squared or tau divided by 2 R squared so the R's get cancelled out that this becomes an 8 and so now the value of tau guess what you just double it it's just double it's just double pi pi is just 1/2 towel whatever you want just live your life tau and I just ate and then this is tau so we now can go and revise my coding job that's really your so watching this video apparently I'm gonna stop this and we're going to we're gonna make one of these for all of you tau lovers out there this one goes out to all you towel out there from me to you we're gonna change this to record towel we're gonna draw that we don't need to draw that this is unnecessary code just save this before I forget this approximating towel and obviously put this on the desktop of my computer this is processing its java processing org everybody always asks then i'm gonna come down here i don't need to do so many each frame my formula here is now 8 tau is 8 and I can now what I want to do is I need to check the record tau if I get one closest because redo the record and then now and this is now tau and then ah this is just math tau and math dot tau all right oh and this is record tau ok we're good I did it I redid my coding talk what what Java dot math hmm where are the constants where are the constants hold on math dot PI Java hmm yep pi oh there we go here are the constants e we get e we get PI I don't see tau I don't see tell this video is over forget it all right fine I'll devote to it anyway anyway but this is so sad this is what I have to do look away everybody look away don't look don't look and now we are now approximating tau happy towel Day everybody Oh see ya okay there we go that's an extra bonus for all of you people out there on the internet watching this now I really got to go almost two o'clock so again I'm gonna be out of town for I'm gonna be out of town for did I miss something hope not I'm gonna be out of town for a couple of weeks as I mentioned all these edited versions of these videos will all become live well YouTube has this new feature like this new premiere thing so I kind of want to I don't know if that's unlocked for me yet I don't know if you've heard about this but I'm definitely an experiment with that and I will see you all in the future okay I'm gonna I'm gonna end this I'm gonna play you out with my ridiculous thing that I always do goodbye yes so Muhammad is asking you didn't use PI but used the formula for the circles area but that's okay because I mean I have to use that but if I were just using it with dart throwing I wouldn't have to I just have the circle draw and I would see which ones are in and out of the circle right visually okay goodbye everybody Oh dev new sponsor thank you so much all right here goes something we wanted to make some crazy idea to make you you
