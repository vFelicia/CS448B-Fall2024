With timestamps:

00:00 - [TRAIN WHISTLE]
00:00 - Hello, and welcome to
another Teachable Machine
00:03 - video tutorial.
00:04 - This time, I'm going to look
at sound classification.
00:06 - So I have to say goodbye
to my friend the unicorn
00:09 - but hello, again, to the
ukulele, hello to the bell,
00:12 - and the train whistle.
00:13 - I am going to look
at how I can use
00:15 - Teachable Machine
to train a model
00:18 - to recognize different sounds.
00:20 - In the previous video, I
looked at image classification.
00:22 - So you can see
here, this example
00:23 - is recognizing the guitar.
00:25 - And now I want to make this
exact same example but have
00:28 - it show the guitar when
I play the ukulele,
00:31 - have it show the train whistle
when I play the train whistle.
00:35 - To do this, instead of
starting with an image project,
00:37 - I'm going to start
with an audio project.
00:40 - Now, you might remember,
in my previous video
00:42 - where I made an image
classifier, I talked
00:44 - about the process of
transfer learning,
00:46 - how Teachable Machine
starts with a base model,
00:48 - in this case, MobileNet, which
knows how to classify images
00:51 - into 1,000 categories,
removes all those categories,
00:55 - replaces them with
those of our own--
00:57 - unicorns, train whistles,
rainbows, and a ukulele--
01:02 - and then retrains the
model with new images.
01:05 - The sound classifier is going
to do exactly the same thing.
01:09 - This time, however,
the base model
01:11 - is something called
speech commands 18w.
01:14 - The speech commands model is
pre-trained to recognize 18
01:17 - words in English that a person
might say so the digits 0
01:21 - through 9--
01:22 - that's 10-- up, down,
left, right-- up to 14--
01:25 - stop, go, yes, no-- that's 18.
01:28 - It also has a category for
unknown word and background
01:31 - noise, so really there's 20
but 18w for those 18 words.
01:35 - So when I make an audio project,
I'm going to use that model,
01:38 - remove all of those
labels, and put in my own
01:42 - with my own training sounds
instead of images this time.
01:45 - And my labels will be train
whistle, bell, and ukulele.
01:50 - So let's get started and
make an audio project.
01:53 - The very first
thing you need to do
01:54 - when making an audio
classifier in Teachable Machine
01:57 - is record some samples or audio
examples of background noise.
02:02 - The model-- we'll need
this during the training
02:04 - process to have something to
compare the other classes to.
02:07 - So let's do that.
02:08 - I'm going to attempt
to be very quiet, which
02:10 - is quite hard for me, and
record background samples.
02:15 - Add Samples, Use
Mic, and then I'm
02:18 - going to record a
20-second sample
02:21 - and attempt to meditate
during that time.
02:24 - [MUSIC PLAYING]
02:27 - All right.
02:28 - I've got the 20 seconds
of background noise.
02:30 - Now the model as
its input doesn't
02:33 - take 20 seconds of audio.
02:35 - Any given example
presented to the model
02:38 - during the training process
has to be 1 second of audio.
02:42 - But I just recorded 20
seconds, so Teachable Machine
02:45 - required one additional step
to prepare that training data.
02:49 - And that is through this
button, Extract Sample.
02:52 - So if I click the
Extract Sample button,
02:54 - it's going to take that
20 seconds of audio
02:57 - and convert it into 20 samples.
02:59 - Let's just record 20 more
seconds of background audio
03:02 - just so I have 40
samples because I
03:03 - want to have a little bit more
than the minimum required.
03:06 - [MUSIC PLAYING]
03:08 - Now I can start adding
some of my own audio.
03:10 - Let's begin with
the train whistle.
03:12 - So I'm going to change
class 2 to train.
03:15 - I'm going to click Use Mic.
03:17 - Now you might notice
that my browser is not
03:19 - asking me for any
permission to use the mic.
03:21 - Most likely, you're
going to see that.
03:23 - I just have it set
already to allow that.
03:25 - So Use Mic and record
two seconds of samples.
03:31 - [TRAIN WHISTLE]
03:35 - Once again, I need to extract
those, and I have two samples.
03:40 - Now I need at least
eight minimum,
03:42 - so let me add a whole bunch.
03:43 - And I'll speed
through this for you.
03:46 - [TRAIN WHISTLE]
03:49 - And I've got 16 audio samples
of train whistle sounds.
03:53 - Now we move on and try the bell.
03:55 - Add a class, bell,
and Add Samples.
04:00 - [BELL RINGING]
04:03 - Two samples.
04:04 - Here's the thing.
04:05 - I know that I'm going to need--
04:07 - maybe I want to have
16 bell samples.
04:09 - You know what?
04:09 - I'm going to just
record for 16 seconds.
04:12 - [BELL RINGING]
04:14 - In some cases, I might want
to consider when I'm actually
04:17 - hitting the bell in relationship
to how it's chopping it
04:21 - up into 1-second increments.
04:22 - But let's just see
if it works even
04:24 - without me being
thoughtful about this.
04:25 - I'm just ringing this bell
and recording 16 seconds of it
04:29 - and letting Teachable Machine
do its extraction however
04:32 - it's going to do it.
04:33 - And there we go, 18 samples.
04:36 - I'm well above the minimum.
04:37 - I can move on to the ukulele.
04:39 - Another feature under
the Settings is a delay.
04:42 - And I want to give myself
two seconds from when
04:43 - I hit that Record button to
when it starts recording,
04:47 - so it can give me a minute to
get set up with the ukulele.
04:49 - Save Settings, record
16 seconds, 2, 1.
04:53 - [UKULELE PLAYING]
04:57 - Now I can extract
those, and I've
05:00 - got background noise,
train whistle, the bell,
05:05 - and the ukulele.
05:07 - And I'm ready to
train the model.
05:09 - Before I start the
training process,
05:11 - let me address something.
05:12 - What are these images?
05:14 - And wait a second.
05:15 - Is this actually an
image classifier?
05:17 - Because this kind of looks
like what we had before.
05:19 - Only the images aren't
things from the camera.
05:21 - They are these
pictures that somehow
05:23 - appear based on the sound.
05:24 - And what they are-- in a
way, this is kind of true--
05:27 - because what these
are visualizations
05:29 - of the audio signal,
specifically the spectrogram.
05:33 - What are the various amplitudes
of the different frequencies
05:37 - of the sound?
05:37 - Is it a very high pitch sound?
05:39 - Is it a very low pitch sound?
05:40 - So that's the actual data.
05:42 - That spectrogram of
1 second of audio
05:45 - is what is being sent into the
machine learning model itself.
05:50 - Let's train the model.
05:54 - (SINGING) Don't switch the tabs.
05:59 - Don't switch the tabs.
06:03 - [INAUDIBLE]
06:06 - [BELL RINGS]
06:07 - Bell.
06:08 - [TRAIN WHISTLE]
06:09 - Train whistle.
06:10 - It works.
06:11 - And now we can take this model
and follow the same steps we
06:14 - did with the image classifier.
06:15 - Step 1, export the model.
06:17 - I want to upload it.
06:19 - I can copy that URL.
06:20 - Click.
06:21 - Switch over to my p5.js sketch.
06:24 - In my code example from
the previous video,
06:26 - which was trained to recognize
a train whistle or a rainbow
06:29 - image--
06:29 - train, rainbow, train, rainbow--
06:32 - and I can switch it to
instead of having an image
06:34 - classifier, a sound classifier.
06:37 - I can change the model
URL to my new model URL.
06:43 - I don't need the video
anymore, so I can delete that.
06:46 - I'm going to change
this to Classify Audio.
06:50 - Unlike with the
image classifier,
06:51 - the audio classifier
doesn't need
06:54 - you to specifically say which
sound you want to link it to.
06:58 - It's going to default
to the microphone.
07:00 - So I can remove this video here.
07:02 - Keep this as gotResults.
07:04 - There's no video to draw.
07:07 - The categories, instead of
train, rainbow, unicorn,
07:10 - and ukulele here are, well,
train, then I've got bell,
07:14 - no unicorn-- it's so sad--
07:16 - and ukulele.
07:18 - And in the audio case as well,
something that's different
07:20 - is, instead of having to
explicitly say now go ahead
07:23 - and classify the video again,
the audio engine is going
07:26 - to just continue listening.
07:28 - So I can get rid of this
classifyVideo function.
07:31 - And I can run this sketch.
07:34 - A train already?
07:35 - Wait, wait, wait, wait, wait.
07:36 - So I don't want--
07:37 - I want to not make
the same mistake I
07:39 - made in the first video.
07:40 - Let's consider what
it should display
07:43 - if it doesn't hear anything.
07:45 - I'll just use headphones, so
let me put some headphones in.
07:48 - Then, I'm going to actually
say if the label is train,
07:54 - put in the train emoji.
07:58 - Now I'm going to
start the scratch,
08:00 - and I'm going to attempt to
be very quiet while I do so.
08:07 - [BELL RINGS]
08:11 - [UKULELE PLAYING]
08:12 - Ukulele.
08:13 - [TRAIN WHISTLE]
08:14 - [BELL RINGS]
08:17 - [UKULELE PLAYING]
08:19 - That works.
08:20 - Oh, that's so exciting.
08:22 - Interestingly, I wonder if I'm
talking, what it thinks it is.
08:25 - [BELL RINGS]
08:27 - Hello.
08:28 - This is me talking.
08:29 - I probably was saying
things while I was recording
08:31 - those ukulele sounds, or it
just matches the most closely
08:34 - because me talking is
not background noise.
08:36 - So I could have put another
category of just me talking
08:39 - or specific words, oh,
so many possibilities.
08:41 - Would it even work to train
the model on different chords
08:45 - of the ukulele?
08:47 - My suspicion is that's not
going to work particularly
08:49 - well because the quality of
the sound of those chords,
08:53 - particularly if they
share some of the notes,
08:55 - some of the frequencies is
going to be quite similar.
08:57 - And the base model was
trained on human speech.
09:01 - But let's give it
a try, and maybe we
09:02 - can control the snake game
with different ukulele chords.
09:06 - So I actually just went
and trained a model
09:07 - with this idea
with four chords--
09:09 - C, G7, F, and A. And you
could see it mostly works,
09:14 - or it kind of works.
09:15 - [UKULELE PLAYING]
09:16 - Give me a C. But it's
really not getting
09:19 - as clearly distinct
high confidence
09:21 - scores for what I want.
09:23 - Maybe if I try individual
notes, it'll work better.
09:26 - The first note on the
ukulele is A, the A string.
09:29 - [UKULELE PLAYING]
09:32 - E.
09:32 - [UKULELE PLAYING]
09:35 - C.
09:36 - [UKULELE PLAYING]
09:40 - And finally G.
09:41 - [UKULELE PLAYING]
09:44 - The images of the
spectrogram look
09:46 - kind of distinct
and different to me,
09:48 - so I view that as a good sign.
09:49 - Let's try training the model.
09:53 - And let's see how it performs.
09:55 - A. Yeah.
09:59 - We got a big bump there
in the A confidence score.
10:01 - That's good.
10:02 - Let's try G. Big bump there
in the G confidence score.
10:06 - Let's try C, E. So you can
see this isn't perfect.
10:15 - These sounds are maybe not as
distinct to this model that's
10:19 - based on how the pre-trained
model what kinds of audio
10:22 - it was trained on, but I'm
getting something there.
10:25 - Let's see how well I can
control that snake game
10:27 - with just these four notes.
10:29 - If you remember from before--
10:33 - left, right, up,
down-- still working.
10:38 - Go this way, left, left.
10:39 - Get that food.
10:40 - Let's try changing it to
use the audio classifier.
10:44 - I forgot to export the model.
10:46 - Export, Upload, Copy.
10:50 - [MUSIC PLAYING]
10:56 - Now you need to decide which
notes go with which movement.
11:01 - A is left.
11:03 - E is right.
11:05 - C is down.
11:06 - G is up.
11:08 - I made all the same
exact changes just
11:10 - to convert this from classifying
a video to classifying
11:13 - audio from the mic.
11:14 - Let's see what happens.
11:16 - For whatever reason, I
typed audioClassifier
11:18 - when it's actually
soundClassifier.
11:20 - And you can actually
look at the ml5 website
11:22 - to find all the documentation
for the soundClassifier.
11:27 - [UKULELE PLAYING]
11:34 - I was so close.
11:35 - I think I have a way of making
this work better for my brain.
11:39 - Up is A. Down is g.
11:42 - So those are the outer strings.
11:44 - Then right and left
are the inner strings.
11:48 - Up, up, right, left, down.
11:56 - Here we go.
11:56 - [UKULELE PLAYING]
12:02 - Yay.
12:03 - I got the food.
12:06 - So that perhaps wasn't
the best solution.
12:08 - Maybe something
I could have done
12:10 - was work with those confidence
scores in a more intentional
12:13 - way to make sure that some
classifications that I got back
12:17 - of the audio that
weren't as confident
12:19 - didn't disrupt the correct
direction of the snake
12:22 - that I had gotten
in the first place.
12:24 - I'm trying this again one
more time off of my own speech
12:27 - because I want to see
if I can really get
12:29 - finer control over that snake.
12:31 - So right now, I've collected
some data of me saying up,
12:33 - down, and a meow sound.
12:34 - And what I've been
doing is I've been
12:36 - trying to time the word-- the
time my saying of up and down
12:40 - and meow with
1-second intervals.
12:42 - Let me show you what
that looks like.
12:44 - Up, up, down, down, meow,
meow, meow, meow, meow.
12:53 - Now I'm going to go
and add whistling.
12:55 - [WHISTLING]
12:58 - Time to train this model.
13:00 - Up, down, meow.
13:05 - [WHISTLING]
13:07 - So we can see how this is much
more accurate than my attempts
13:11 - with a ukulele chord and notes.
13:14 - Export, Upload, Copy,
Paste, change the labels--
13:21 - up, down, right will be meow,
and left will be whistle.
13:28 - Meow.
13:30 - [WHISTLING]
13:32 - Down.
13:34 - Meow.
13:34 - [WHISTLING]
13:37 - Down.
13:39 - Meow.
13:43 - Oh, yes.
13:46 - I got the food, and then I died.
13:47 - But I don't care.
13:48 - It worked.
13:50 - But I think you'll
remember, from this example,
13:52 - it works quite well.
13:53 - And hopefully, this opens up a
lot of possibilities for you.
13:57 - If you made something,
I've got a link
13:58 - in this video subscription to
a page at the codingtrain.com
14:01 - where you could submit a URL
to a product you've made.
14:03 - And I would love
to check it out,
14:04 - share it on a future Livestream.
14:06 - I can't wait to see what
kind of stuff people
14:08 - make from my strange projects.
14:11 - And I look forward to seeing you
in a future Coding Train video.
14:15 - Goodbye.
14:16 - [TRAIN WHISTLE]
14:18 - That was the first
time that something
14:20 - happened when I blew
the train whistle
14:21 - at the end of the video.
14:22 - That is great.
14:22 - [BELL RINGS]
14:24 - [MUSIC PLAYING]

Cleaned transcript:

[TRAIN WHISTLE] Hello, and welcome to another Teachable Machine video tutorial. This time, I'm going to look at sound classification. So I have to say goodbye to my friend the unicorn but hello, again, to the ukulele, hello to the bell, and the train whistle. I am going to look at how I can use Teachable Machine to train a model to recognize different sounds. In the previous video, I looked at image classification. So you can see here, this example is recognizing the guitar. And now I want to make this exact same example but have it show the guitar when I play the ukulele, have it show the train whistle when I play the train whistle. To do this, instead of starting with an image project, I'm going to start with an audio project. Now, you might remember, in my previous video where I made an image classifier, I talked about the process of transfer learning, how Teachable Machine starts with a base model, in this case, MobileNet, which knows how to classify images into 1,000 categories, removes all those categories, replaces them with those of our own unicorns, train whistles, rainbows, and a ukulele and then retrains the model with new images. The sound classifier is going to do exactly the same thing. This time, however, the base model is something called speech commands 18w. The speech commands model is pretrained to recognize 18 words in English that a person might say so the digits 0 through 9 that's 10 up, down, left, right up to 14 stop, go, yes, no that's 18. It also has a category for unknown word and background noise, so really there's 20 but 18w for those 18 words. So when I make an audio project, I'm going to use that model, remove all of those labels, and put in my own with my own training sounds instead of images this time. And my labels will be train whistle, bell, and ukulele. So let's get started and make an audio project. The very first thing you need to do when making an audio classifier in Teachable Machine is record some samples or audio examples of background noise. The model we'll need this during the training process to have something to compare the other classes to. So let's do that. I'm going to attempt to be very quiet, which is quite hard for me, and record background samples. Add Samples, Use Mic, and then I'm going to record a 20second sample and attempt to meditate during that time. [MUSIC PLAYING] All right. I've got the 20 seconds of background noise. Now the model as its input doesn't take 20 seconds of audio. Any given example presented to the model during the training process has to be 1 second of audio. But I just recorded 20 seconds, so Teachable Machine required one additional step to prepare that training data. And that is through this button, Extract Sample. So if I click the Extract Sample button, it's going to take that 20 seconds of audio and convert it into 20 samples. Let's just record 20 more seconds of background audio just so I have 40 samples because I want to have a little bit more than the minimum required. [MUSIC PLAYING] Now I can start adding some of my own audio. Let's begin with the train whistle. So I'm going to change class 2 to train. I'm going to click Use Mic. Now you might notice that my browser is not asking me for any permission to use the mic. Most likely, you're going to see that. I just have it set already to allow that. So Use Mic and record two seconds of samples. [TRAIN WHISTLE] Once again, I need to extract those, and I have two samples. Now I need at least eight minimum, so let me add a whole bunch. And I'll speed through this for you. [TRAIN WHISTLE] And I've got 16 audio samples of train whistle sounds. Now we move on and try the bell. Add a class, bell, and Add Samples. [BELL RINGING] Two samples. Here's the thing. I know that I'm going to need maybe I want to have 16 bell samples. You know what? I'm going to just record for 16 seconds. [BELL RINGING] In some cases, I might want to consider when I'm actually hitting the bell in relationship to how it's chopping it up into 1second increments. But let's just see if it works even without me being thoughtful about this. I'm just ringing this bell and recording 16 seconds of it and letting Teachable Machine do its extraction however it's going to do it. And there we go, 18 samples. I'm well above the minimum. I can move on to the ukulele. Another feature under the Settings is a delay. And I want to give myself two seconds from when I hit that Record button to when it starts recording, so it can give me a minute to get set up with the ukulele. Save Settings, record 16 seconds, 2, 1. [UKULELE PLAYING] Now I can extract those, and I've got background noise, train whistle, the bell, and the ukulele. And I'm ready to train the model. Before I start the training process, let me address something. What are these images? And wait a second. Is this actually an image classifier? Because this kind of looks like what we had before. Only the images aren't things from the camera. They are these pictures that somehow appear based on the sound. And what they are in a way, this is kind of true because what these are visualizations of the audio signal, specifically the spectrogram. What are the various amplitudes of the different frequencies of the sound? Is it a very high pitch sound? Is it a very low pitch sound? So that's the actual data. That spectrogram of 1 second of audio is what is being sent into the machine learning model itself. Let's train the model. (SINGING) Don't switch the tabs. Don't switch the tabs. [INAUDIBLE] [BELL RINGS] Bell. [TRAIN WHISTLE] Train whistle. It works. And now we can take this model and follow the same steps we did with the image classifier. Step 1, export the model. I want to upload it. I can copy that URL. Click. Switch over to my p5.js sketch. In my code example from the previous video, which was trained to recognize a train whistle or a rainbow image train, rainbow, train, rainbow and I can switch it to instead of having an image classifier, a sound classifier. I can change the model URL to my new model URL. I don't need the video anymore, so I can delete that. I'm going to change this to Classify Audio. Unlike with the image classifier, the audio classifier doesn't need you to specifically say which sound you want to link it to. It's going to default to the microphone. So I can remove this video here. Keep this as gotResults. There's no video to draw. The categories, instead of train, rainbow, unicorn, and ukulele here are, well, train, then I've got bell, no unicorn it's so sad and ukulele. And in the audio case as well, something that's different is, instead of having to explicitly say now go ahead and classify the video again, the audio engine is going to just continue listening. So I can get rid of this classifyVideo function. And I can run this sketch. A train already? Wait, wait, wait, wait, wait. So I don't want I want to not make the same mistake I made in the first video. Let's consider what it should display if it doesn't hear anything. I'll just use headphones, so let me put some headphones in. Then, I'm going to actually say if the label is train, put in the train emoji. Now I'm going to start the scratch, and I'm going to attempt to be very quiet while I do so. [BELL RINGS] [UKULELE PLAYING] Ukulele. [TRAIN WHISTLE] [BELL RINGS] [UKULELE PLAYING] That works. Oh, that's so exciting. Interestingly, I wonder if I'm talking, what it thinks it is. [BELL RINGS] Hello. This is me talking. I probably was saying things while I was recording those ukulele sounds, or it just matches the most closely because me talking is not background noise. So I could have put another category of just me talking or specific words, oh, so many possibilities. Would it even work to train the model on different chords of the ukulele? My suspicion is that's not going to work particularly well because the quality of the sound of those chords, particularly if they share some of the notes, some of the frequencies is going to be quite similar. And the base model was trained on human speech. But let's give it a try, and maybe we can control the snake game with different ukulele chords. So I actually just went and trained a model with this idea with four chords C, G7, F, and A. And you could see it mostly works, or it kind of works. [UKULELE PLAYING] Give me a C. But it's really not getting as clearly distinct high confidence scores for what I want. Maybe if I try individual notes, it'll work better. The first note on the ukulele is A, the A string. [UKULELE PLAYING] E. [UKULELE PLAYING] C. [UKULELE PLAYING] And finally G. [UKULELE PLAYING] The images of the spectrogram look kind of distinct and different to me, so I view that as a good sign. Let's try training the model. And let's see how it performs. A. Yeah. We got a big bump there in the A confidence score. That's good. Let's try G. Big bump there in the G confidence score. Let's try C, E. So you can see this isn't perfect. These sounds are maybe not as distinct to this model that's based on how the pretrained model what kinds of audio it was trained on, but I'm getting something there. Let's see how well I can control that snake game with just these four notes. If you remember from before left, right, up, down still working. Go this way, left, left. Get that food. Let's try changing it to use the audio classifier. I forgot to export the model. Export, Upload, Copy. [MUSIC PLAYING] Now you need to decide which notes go with which movement. A is left. E is right. C is down. G is up. I made all the same exact changes just to convert this from classifying a video to classifying audio from the mic. Let's see what happens. For whatever reason, I typed audioClassifier when it's actually soundClassifier. And you can actually look at the ml5 website to find all the documentation for the soundClassifier. [UKULELE PLAYING] I was so close. I think I have a way of making this work better for my brain. Up is A. Down is g. So those are the outer strings. Then right and left are the inner strings. Up, up, right, left, down. Here we go. [UKULELE PLAYING] Yay. I got the food. So that perhaps wasn't the best solution. Maybe something I could have done was work with those confidence scores in a more intentional way to make sure that some classifications that I got back of the audio that weren't as confident didn't disrupt the correct direction of the snake that I had gotten in the first place. I'm trying this again one more time off of my own speech because I want to see if I can really get finer control over that snake. So right now, I've collected some data of me saying up, down, and a meow sound. And what I've been doing is I've been trying to time the word the time my saying of up and down and meow with 1second intervals. Let me show you what that looks like. Up, up, down, down, meow, meow, meow, meow, meow. Now I'm going to go and add whistling. [WHISTLING] Time to train this model. Up, down, meow. [WHISTLING] So we can see how this is much more accurate than my attempts with a ukulele chord and notes. Export, Upload, Copy, Paste, change the labels up, down, right will be meow, and left will be whistle. Meow. [WHISTLING] Down. Meow. [WHISTLING] Down. Meow. Oh, yes. I got the food, and then I died. But I don't care. It worked. But I think you'll remember, from this example, it works quite well. And hopefully, this opens up a lot of possibilities for you. If you made something, I've got a link in this video subscription to a page at the codingtrain.com where you could submit a URL to a product you've made. And I would love to check it out, share it on a future Livestream. I can't wait to see what kind of stuff people make from my strange projects. And I look forward to seeing you in a future Coding Train video. Goodbye. [TRAIN WHISTLE] That was the first time that something happened when I blew the train whistle at the end of the video. That is great. [BELL RINGS] [MUSIC PLAYING]
