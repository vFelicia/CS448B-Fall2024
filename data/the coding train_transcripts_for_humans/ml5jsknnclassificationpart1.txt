With timestamps:

00:01 - Hello.
00:01 - I am back in the
beginner's guide
00:03 - to Machine Learning with ML5.
00:05 - And today, I am
continuing this series
00:08 - and I'm going to do a few
videos about something
00:10 - called KNN Classification.
00:13 - And the thing that I'm going
to use it and apply it to,
00:17 - is building your own
teachable machine.
00:19 - Now this is the
Teachable Machine Project
00:22 - from Google Creative
Lab demonstrating
00:25 - this idea of transfer learning.
00:27 - Taking a pre-trained model
and using it as a foundation
00:31 - to train a new model on top of.
00:33 - Now you might be
thinking, didn't you
00:35 - already make these videos?
00:36 - There was that whole thing
with the feature extractor
00:38 - and classification
and regression
00:40 - and saving the model.
00:41 - Yes, you are right.
00:43 - I did already make those videos.
00:44 - However, I'm going
to, basically,
00:46 - do the exact same thing again
but with a slightly different
00:50 - technique.
00:51 - And that technique is
called KNN Classification,
00:53 - which has some advantages.
00:55 - There are pros and cons to
both of these techniques
00:57 - and at the end we maybe even
sort of wrap up and look
00:59 - at why you might pick
one over the other.
01:01 - But one of the main advantages
of KNN Classification
01:05 - is it's training the
model is happening
01:10 - as you're adding new images,
as opposed to a separate step.
01:13 - And I'll get into
why that is in a bit.
01:14 - So this is the Teachable
Machine Project.
01:17 - You can go to
TeachableMachineWithGoogle.com.
01:22 - And what I'm going to do is--
01:23 - I've already trained this.
01:24 - And so if I'm just standing
here in the middle,
01:28 - I've trained it with
just me standing here
01:30 - with kind of an angry
look on my face as orange.
01:33 - My hand to the left as green.
01:36 - My hand to the right as purple.
01:38 - So what I want to do is emphasis
that what you can actually do
01:41 - is build a gesture
based controller
01:44 - for some type of game
using this technique.
01:47 - You could do this with
the other way as well,
01:49 - but I really looked at
it as just a new way
01:52 - to classify new images.
01:53 - But here, let's
think about if I'm
01:55 - training a model in
real-time to understand
01:58 - that a hand is
holding up on my left,
02:00 - and my hand is holding up on
my right, or I have no hands
02:02 - up at all.
02:03 - You could imagine
I could control
02:04 - a character, a pong paddle
to move left or right,
02:07 - and maybe up or down.
02:09 - So that's what I'm going to do.
02:10 - It's going to take a few
videos, a bunch of things
02:12 - to get through.
02:13 - But that's the idea.
02:14 - All right.
02:14 - So you can play around with
this website to get a sense of--
02:17 - there's a really nice interface.
02:19 - There's a ML5 version of this.
02:22 - This is built with
TensorFlow dot js.
02:23 - ML5, which is a library,
built on top of TensorFlow js
02:26 - We have a similar version.
02:27 - All this work was done
by Yining Shi for ML5.
02:30 - And Yining Shi has
been teaching a class
02:32 - called "Machine Learning
for the Web" at ITP, NYU.
02:38 - Thank you so much, to
Yining, for allowing
02:40 - me to be inspired by
and use your materials
02:43 - in these tutorials.
02:44 - Hopefully, Yining will come and
be a guest again in "The Coding
02:47 - Train" and show some additional
examples of things you
02:49 - can do with KNN Classification.
02:51 - I highly recommend you
look through the syllabus
02:54 - and examples.
02:55 - And there's also a
whole presentation
02:57 - about KNN Classification
that's Yining's slides that you
03:00 - can look at.
03:00 - But what I'm going
to actually follow
03:03 - is this article, written
by Nikhil Thorat,
03:07 - who is one of the creators
of TensorFlow dot js,
03:10 - one of the members of the
research team at Google Brain
03:15 - developing TensorFlow
dot js, you can see here.
03:17 - And this is a notebook about
how all of this stuff works.
03:21 - And I'm going to,
basically, go through this,
03:24 - but not use the code here.
03:26 - This code, which is kind of
the raw TensorFlow js code,
03:30 - I encourage you to look at it
and think about how it works.
03:33 - And this might be a
place where you actually
03:35 - want to get more
lower level into it.
03:38 - But I'm going to use the
new ML5 KNN Classifier
03:43 - feature that is an ML5 library.
03:45 - Now this feature,
you're going to want
03:49 - to make sure you're using at
least version 0.2.1 of ML5.
03:57 - So as of version 0.2.1, the
KNN Classifier is part of ML5.
04:03 - But I will mention, at
the time of this recording
04:05 - the documentation is not
yet up on the website.
04:08 - But, hopefully, by
the time this video
04:09 - is released as part
of that playlist
04:12 - you will find the documentation
on the website, ML5js.org.
04:16 - All right.
04:17 - So let's start going
through this article
04:18 - and let's start right here
at this moment of Background,
04:22 - MobileNet.
04:22 - Now this is MobileNet,
the MobileNet model
04:25 - is something that I've
used in just about almost
04:27 - every video up until this point.
04:29 - So if you've been
watching the playlist,
04:31 - you're somewhat familiar to it.
04:32 - MobileNet, this is our
first step, load MobileNet.
04:36 - And MobileNet is a Machine
Learning, Image Classification
04:40 - model that's been
trained to recognize
04:42 - 1,000 classes of images.
04:46 - And it was trained
based on a huge database
04:48 - of images, known as ImageNet.
04:51 - I've said this
before, but this is
04:53 - a database that exists for
researchers to use and try
04:56 - and development machine
learning algorithms with,
04:58 - but it's not necessarily a model
that works particularly well
05:03 - in generic applications
of image classification
05:06 - out in the world.
05:06 - Because it knows a lot
about birds and dogs,
05:08 - but not about a
lot of other stuff
05:10 - that appears in the world.
05:12 - However, it is a
good basis to load,
05:14 - the first step one is
to load this model.
05:18 - It's a good basis from which
to do this feature extraction
05:21 - process, this transfer learning
process, which we did before
05:24 - and we're going to do
now in a different way.
05:26 - So the next step is
to then create what's
05:32 - called a feature extractor.
05:39 - I'm going to show
you in the code.
05:40 - This is actually, basically,
what I've done before.
05:43 - So this is what
MobileNet does here.
05:44 - You can see it can
look at an image,
05:46 - it can tell you
what the probability
05:48 - of a particular class, like
Egyptian cat, tabby cat, tiger
05:50 - cat, remote control apparently,
there is a almost 2%
05:55 - chance that it's
a remote control.
05:57 - Cat, you are not a
remote control to me.
05:59 - All right.
06:00 - So now what I have
already, which is basically
06:02 - from my previous
examples, is a sketch
06:05 - that loads the image
from the webcam
06:07 - and displays it in the window.
06:10 - And I also, like I said, have
a reference to the ML5 library
06:15 - and I also have this
feature extractor loaded.
06:18 - So I'm using P5, I'm
connecting to the webcam using
06:22 - a P5 library to
connect to the webcam
06:24 - and draw the webcam's image.
06:26 - And then I'm using ML5 to
create a feature extractor
06:30 - with the MobileNet model.
06:32 - So now we can go back
to Nikhil's article
06:36 - and scroll down.
06:37 - And be like, OK.
06:38 - So this is actually the end
result, the cat softmax.
06:42 - So what does this mean?
06:44 - So the MobileNet model
is a neural network that
06:51 - expects an input of an image.
06:58 - That image comes in and it's
processed over a variety
07:02 - of different layers.
07:05 - So the image is
transformed and processed
07:08 - and twisted and
turned, and all of that
07:10 - is the neural network process.
07:12 - Which you'd have
to refer to some
07:13 - of my other videos and
other reference materials
07:15 - to understand more how this
works underneath the hood.
07:19 - But by the end what it
gets is a big vector,
07:24 - a big list of numbers,
that are all probabilities.
07:28 - They all add up to 100%.
07:31 - And if you look at that here,
that's what we're seeing here.
07:34 - And we're seeing all
the way over here
07:36 - these classes have a
very high probability.
07:39 - So somewhere in here, we're
getting this number, like,
07:42 - 0.9, maybe we're getting 0.7,
maybe we're getting 0.05,
07:47 - and all sorts of other numbers.
07:49 - They add up to 100.
07:51 - And 0.9, this particular
number, is the number
07:55 - that corresponds to
the probability of it
07:57 - being an Egyptian cat.
07:59 - So this is what MobileNet
would do on its own.
08:03 - But this stage is called
the softmax stage.
08:07 - Softmax is a fancy term for
normalizing the output of all
08:11 - of this process to a set
of probability values
08:14 - that all add up to 100%.
08:16 - But there are stages before.
08:20 - One stage before is
called the logits.
08:27 - And by the way, before I
recorded this video I spent,
08:29 - like, a half an hour trying to
figure out how I pronounce it.
08:31 - Whether it's "law-gits"
or "low-gits."
08:33 - And a lot of places
said "low-gits,"
08:34 - and some other places
said "law-gits."
08:36 - Imma go with "law-gits."
08:37 - Like logic or logical.
08:39 - But you can see,
this is actually
08:41 - what the logits look like.
08:43 - It is the unnormalized,
the non-normalized output
08:47 - of the neural network.
08:49 - It is a layer--
08:54 - I shouldn't be using my hand.
08:55 - I have an eraser.
08:57 - It is the layer right before.
09:02 - This is called the logits.
09:04 - And it's just like
a lot of numbers.
09:06 - And softmax is applied to
get this last output, which
09:11 - is good for classification.
09:12 - But this layer, all
of these numbers,
09:15 - this is referred
to as the features.
09:17 - I know I explained this
in a previous video,
09:19 - but this is useful to
give this another try.
09:23 - Again, we haven't really
gotten to the next step,
09:26 - where something pretty
different is going to happen.
09:28 - This is still the
same as before.
09:30 - And so we can look at it.
09:31 - This is what those
cat logits look like.
09:37 - And you can notice, there's
peaks still around here.
09:40 - But there's lots of
other stuff going on.
09:42 - And you can see,
Nikhil writes, this is
09:44 - a "'semantic fingerprint'
of the image."
09:49 - Right?
09:50 - This is, basically, the
essence of the image.
09:53 - The boiled down, numerical
essence of the image as
09:57 - perceived by the
MobileNet model.
10:00 - So because an image itself--
10:04 - I forget what the dimensions
MobileNet requires
10:06 - is, maybe 224 by 224 pixels.
10:10 - This is a lot of numbers.
10:11 - So you could say, this is the
literal numeric representation
10:15 - of the image, is all of the RGB
values of every single pixel.
10:19 - But that's too many numbers to
work with and compare and try
10:22 - to make sense of the image.
10:23 - So this pre-trained
model has already
10:26 - learned how to boil it
down to just 1,000 numbers.
10:30 - And if we have 1,000
numbers, we can
10:34 - start to compare this image
to, let's say, another image.
10:41 - So this is the
feature extraction,
10:44 - getting these features,
that essence, the semantic--
10:47 - what did Nikhil write?
10:48 - "'Semantic fingerprint'
of the image."
10:51 - If I have two
"semantic fingerprints"
10:54 - of two images that are arrays
of numbers, there's lots of math
10:58 - that I could start
to apply to them.
10:59 - I can say, well, how
similar are they?
11:01 - Well, how similar are they
compared to these other images?
11:04 - Or what if I were to sort of
move interpolate between one
11:07 - image to another?
11:08 - And you've seen some of these,
like, artistic visualizations
11:13 - of the output of neural
networks as moving through,
11:15 - you might hear walking
through the latent space.
11:18 - So this idea of interpolated
between the features
11:21 - of images to generate something,
all of this is interrelated.
11:24 - But here this now, finally,
is the moment where I can say,
11:29 - this is how it relates
to KNN Classification.
11:33 - Because KNN stands for
K Nearest Neighbors.
11:37 - So if we get the
logits for an image,
11:42 - I could say, well, what other
images have I seen previously
11:45 - that this semantic fingerprint
is very similar to?
11:49 - Then this image is within
that category of those images.
11:54 - That's K Nearest Neighbor.
11:56 - So I'm going to wrap
this up and we're
11:57 - going to start implementing
the code in the next video.
12:00 - But let's take one more step.
12:02 - Let's actually look
and see with ML5
12:04 - how easy it is to
get those logits.
12:08 - So I'm gonna come
back over here.
12:14 - So this is my code, right?
12:15 - So in my code I have the video
and I have a feature extractor.
12:21 - The feature extractor, I'm
just calling it "features,"
12:23 - is preloaded with
the MobileNet model.
12:26 - So now, what I'm going
to do is, I'm just
12:29 - going to have "mouse pressed,"
which is not what I'm
12:31 - going to want to do ultimately.
12:33 - And I'm going to say,
"features dot infer video."
12:37 - So the infer function
is the naming--
12:39 - I'm little unsure
about this naming
12:42 - but maybe it'll change
in the API at some point.
12:45 - But this is the idea
of inferring the logits
12:49 - from this particular image.
12:51 - So this could be a
static image, but since I
12:52 - have the video it's whatever
the snapshot of the video
12:55 - is right now.
12:56 - And this could go
into the logits.
12:59 - This is, basically, giving
me that particular logits.
13:02 - And I'm going to do something
and is going to look weird.
13:05 - I'm gonna say, "console
log logits," like,
13:07 - shouldn't that show me
just an array of numbers?
13:10 - Not exactly, but I'm
going to show it to you.
13:13 - Let's see what happens.
13:14 - So let me refresh this.
13:16 - OK, the model is loaded.
13:18 - Now I'm going to click.
13:19 - And you say, what?
13:20 - What?
13:20 - What?
13:21 - Well this looks, kind
of, dtype float32.
13:24 - OK, floating point numbers,
there's 1,000 of them.
13:26 - What's all this nonsense?
13:29 - Look at all this,
this looks crazy.
13:30 - Well, guess what?
13:31 - We just waited, by accident
or kind of on purpose
13:34 - since I did it on purpose,
into TF dot js territory.
13:39 - Now to continue
writing this code,
13:42 - we don't ever have to look
at the TensorFlow dot js
13:45 - documentation.
13:46 - But this is a moment to realize
ML5 isn't some magical thing
13:50 - that exists on its own.
13:51 - It is operating on top
of TensorFlow dot js.
13:55 - Meaning it's managing something
called the tensors for you.
13:58 - What's a tensor?
13:59 - A tensor is, basically, a fancy
word for an array of numbers,
14:03 - or a matrix of numbers.
14:04 - And so, the logits
are a tensor, it's
14:07 - a one dimensional array
of all those numbers.
14:09 - So they are actually a tensor.
14:11 - I can actually look at
those values in the console
14:15 - by saying, "logits dot print."
14:20 - So let me do that now.
14:22 - And if I click here now, you're
going to see, there it is.
14:26 - Now it's not
printing it all out,
14:28 - but you can see, oh, that
looks like an array of numbers.
14:30 - And another way I could
do it is, I could say,
14:33 - "console dot log
logits data Sync."
14:37 - So Data Sync is a function--
these are functions
14:40 - that are part of
TensorFlow dot js
14:41 - and you can go back and look
at my TensorFlow dot js videos
14:45 - as background, where I go
through them more specifically.
14:47 - This is a function that gives
me that data as a regular array.
14:51 - So last step here
would be to do this.
14:55 - Click and there you go.
14:58 - These are the logits.
15:00 - This is the digital
fingerprint of that image.
15:04 - So we are now ready.
15:06 - We now, hopefully, from
watching this video,
15:09 - you understand
maybe a little bit
15:10 - about what KNN
Classification might be.
15:12 - But that's what
I'm gonna get into.
15:13 - That you're loading MobileNet,
the pre-trained model.
15:16 - You're creating a
feature extractor,
15:18 - which knows how to pull out the
logits of the MobileNet model
15:26 - with a given input image.
15:28 - And then we're passing
in an image from a video.
15:31 - And instead of getting
the classification,
15:33 - getting just that
digital fingerprint.
15:35 - And that semantic
digital fingerprint, this
15:38 - is exactly what we're going
to do in the next video,
15:41 - to train a KNN
Classification model.
15:43 - All right.
15:44 - So if you want to
keep watching, then
15:46 - keep going in this playlist.
15:47 - If the video doesn't
exist yet, it's
15:48 - because I'm still working on
it but it'll be there soon.
15:50 - Bye!
15:51 - [MUSIC PLAYING]

Cleaned transcript:

Hello. I am back in the beginner's guide to Machine Learning with ML5. And today, I am continuing this series and I'm going to do a few videos about something called KNN Classification. And the thing that I'm going to use it and apply it to, is building your own teachable machine. Now this is the Teachable Machine Project from Google Creative Lab demonstrating this idea of transfer learning. Taking a pretrained model and using it as a foundation to train a new model on top of. Now you might be thinking, didn't you already make these videos? There was that whole thing with the feature extractor and classification and regression and saving the model. Yes, you are right. I did already make those videos. However, I'm going to, basically, do the exact same thing again but with a slightly different technique. And that technique is called KNN Classification, which has some advantages. There are pros and cons to both of these techniques and at the end we maybe even sort of wrap up and look at why you might pick one over the other. But one of the main advantages of KNN Classification is it's training the model is happening as you're adding new images, as opposed to a separate step. And I'll get into why that is in a bit. So this is the Teachable Machine Project. You can go to TeachableMachineWithGoogle.com. And what I'm going to do is I've already trained this. And so if I'm just standing here in the middle, I've trained it with just me standing here with kind of an angry look on my face as orange. My hand to the left as green. My hand to the right as purple. So what I want to do is emphasis that what you can actually do is build a gesture based controller for some type of game using this technique. You could do this with the other way as well, but I really looked at it as just a new way to classify new images. But here, let's think about if I'm training a model in realtime to understand that a hand is holding up on my left, and my hand is holding up on my right, or I have no hands up at all. You could imagine I could control a character, a pong paddle to move left or right, and maybe up or down. So that's what I'm going to do. It's going to take a few videos, a bunch of things to get through. But that's the idea. All right. So you can play around with this website to get a sense of there's a really nice interface. There's a ML5 version of this. This is built with TensorFlow dot js. ML5, which is a library, built on top of TensorFlow js We have a similar version. All this work was done by Yining Shi for ML5. And Yining Shi has been teaching a class called "Machine Learning for the Web" at ITP, NYU. Thank you so much, to Yining, for allowing me to be inspired by and use your materials in these tutorials. Hopefully, Yining will come and be a guest again in "The Coding Train" and show some additional examples of things you can do with KNN Classification. I highly recommend you look through the syllabus and examples. And there's also a whole presentation about KNN Classification that's Yining's slides that you can look at. But what I'm going to actually follow is this article, written by Nikhil Thorat, who is one of the creators of TensorFlow dot js, one of the members of the research team at Google Brain developing TensorFlow dot js, you can see here. And this is a notebook about how all of this stuff works. And I'm going to, basically, go through this, but not use the code here. This code, which is kind of the raw TensorFlow js code, I encourage you to look at it and think about how it works. And this might be a place where you actually want to get more lower level into it. But I'm going to use the new ML5 KNN Classifier feature that is an ML5 library. Now this feature, you're going to want to make sure you're using at least version 0.2.1 of ML5. So as of version 0.2.1, the KNN Classifier is part of ML5. But I will mention, at the time of this recording the documentation is not yet up on the website. But, hopefully, by the time this video is released as part of that playlist you will find the documentation on the website, ML5js.org. All right. So let's start going through this article and let's start right here at this moment of Background, MobileNet. Now this is MobileNet, the MobileNet model is something that I've used in just about almost every video up until this point. So if you've been watching the playlist, you're somewhat familiar to it. MobileNet, this is our first step, load MobileNet. And MobileNet is a Machine Learning, Image Classification model that's been trained to recognize 1,000 classes of images. And it was trained based on a huge database of images, known as ImageNet. I've said this before, but this is a database that exists for researchers to use and try and development machine learning algorithms with, but it's not necessarily a model that works particularly well in generic applications of image classification out in the world. Because it knows a lot about birds and dogs, but not about a lot of other stuff that appears in the world. However, it is a good basis to load, the first step one is to load this model. It's a good basis from which to do this feature extraction process, this transfer learning process, which we did before and we're going to do now in a different way. So the next step is to then create what's called a feature extractor. I'm going to show you in the code. This is actually, basically, what I've done before. So this is what MobileNet does here. You can see it can look at an image, it can tell you what the probability of a particular class, like Egyptian cat, tabby cat, tiger cat, remote control apparently, there is a almost 2% chance that it's a remote control. Cat, you are not a remote control to me. All right. So now what I have already, which is basically from my previous examples, is a sketch that loads the image from the webcam and displays it in the window. And I also, like I said, have a reference to the ML5 library and I also have this feature extractor loaded. So I'm using P5, I'm connecting to the webcam using a P5 library to connect to the webcam and draw the webcam's image. And then I'm using ML5 to create a feature extractor with the MobileNet model. So now we can go back to Nikhil's article and scroll down. And be like, OK. So this is actually the end result, the cat softmax. So what does this mean? So the MobileNet model is a neural network that expects an input of an image. That image comes in and it's processed over a variety of different layers. So the image is transformed and processed and twisted and turned, and all of that is the neural network process. Which you'd have to refer to some of my other videos and other reference materials to understand more how this works underneath the hood. But by the end what it gets is a big vector, a big list of numbers, that are all probabilities. They all add up to 100%. And if you look at that here, that's what we're seeing here. And we're seeing all the way over here these classes have a very high probability. So somewhere in here, we're getting this number, like, 0.9, maybe we're getting 0.7, maybe we're getting 0.05, and all sorts of other numbers. They add up to 100. And 0.9, this particular number, is the number that corresponds to the probability of it being an Egyptian cat. So this is what MobileNet would do on its own. But this stage is called the softmax stage. Softmax is a fancy term for normalizing the output of all of this process to a set of probability values that all add up to 100%. But there are stages before. One stage before is called the logits. And by the way, before I recorded this video I spent, like, a half an hour trying to figure out how I pronounce it. Whether it's "lawgits" or "lowgits." And a lot of places said "lowgits," and some other places said "lawgits." Imma go with "lawgits." Like logic or logical. But you can see, this is actually what the logits look like. It is the unnormalized, the nonnormalized output of the neural network. It is a layer I shouldn't be using my hand. I have an eraser. It is the layer right before. This is called the logits. And it's just like a lot of numbers. And softmax is applied to get this last output, which is good for classification. But this layer, all of these numbers, this is referred to as the features. I know I explained this in a previous video, but this is useful to give this another try. Again, we haven't really gotten to the next step, where something pretty different is going to happen. This is still the same as before. And so we can look at it. This is what those cat logits look like. And you can notice, there's peaks still around here. But there's lots of other stuff going on. And you can see, Nikhil writes, this is a "'semantic fingerprint' of the image." Right? This is, basically, the essence of the image. The boiled down, numerical essence of the image as perceived by the MobileNet model. So because an image itself I forget what the dimensions MobileNet requires is, maybe 224 by 224 pixels. This is a lot of numbers. So you could say, this is the literal numeric representation of the image, is all of the RGB values of every single pixel. But that's too many numbers to work with and compare and try to make sense of the image. So this pretrained model has already learned how to boil it down to just 1,000 numbers. And if we have 1,000 numbers, we can start to compare this image to, let's say, another image. So this is the feature extraction, getting these features, that essence, the semantic what did Nikhil write? "'Semantic fingerprint' of the image." If I have two "semantic fingerprints" of two images that are arrays of numbers, there's lots of math that I could start to apply to them. I can say, well, how similar are they? Well, how similar are they compared to these other images? Or what if I were to sort of move interpolate between one image to another? And you've seen some of these, like, artistic visualizations of the output of neural networks as moving through, you might hear walking through the latent space. So this idea of interpolated between the features of images to generate something, all of this is interrelated. But here this now, finally, is the moment where I can say, this is how it relates to KNN Classification. Because KNN stands for K Nearest Neighbors. So if we get the logits for an image, I could say, well, what other images have I seen previously that this semantic fingerprint is very similar to? Then this image is within that category of those images. That's K Nearest Neighbor. So I'm going to wrap this up and we're going to start implementing the code in the next video. But let's take one more step. Let's actually look and see with ML5 how easy it is to get those logits. So I'm gonna come back over here. So this is my code, right? So in my code I have the video and I have a feature extractor. The feature extractor, I'm just calling it "features," is preloaded with the MobileNet model. So now, what I'm going to do is, I'm just going to have "mouse pressed," which is not what I'm going to want to do ultimately. And I'm going to say, "features dot infer video." So the infer function is the naming I'm little unsure about this naming but maybe it'll change in the API at some point. But this is the idea of inferring the logits from this particular image. So this could be a static image, but since I have the video it's whatever the snapshot of the video is right now. And this could go into the logits. This is, basically, giving me that particular logits. And I'm going to do something and is going to look weird. I'm gonna say, "console log logits," like, shouldn't that show me just an array of numbers? Not exactly, but I'm going to show it to you. Let's see what happens. So let me refresh this. OK, the model is loaded. Now I'm going to click. And you say, what? What? What? Well this looks, kind of, dtype float32. OK, floating point numbers, there's 1,000 of them. What's all this nonsense? Look at all this, this looks crazy. Well, guess what? We just waited, by accident or kind of on purpose since I did it on purpose, into TF dot js territory. Now to continue writing this code, we don't ever have to look at the TensorFlow dot js documentation. But this is a moment to realize ML5 isn't some magical thing that exists on its own. It is operating on top of TensorFlow dot js. Meaning it's managing something called the tensors for you. What's a tensor? A tensor is, basically, a fancy word for an array of numbers, or a matrix of numbers. And so, the logits are a tensor, it's a one dimensional array of all those numbers. So they are actually a tensor. I can actually look at those values in the console by saying, "logits dot print." So let me do that now. And if I click here now, you're going to see, there it is. Now it's not printing it all out, but you can see, oh, that looks like an array of numbers. And another way I could do it is, I could say, "console dot log logits data Sync." So Data Sync is a function these are functions that are part of TensorFlow dot js and you can go back and look at my TensorFlow dot js videos as background, where I go through them more specifically. This is a function that gives me that data as a regular array. So last step here would be to do this. Click and there you go. These are the logits. This is the digital fingerprint of that image. So we are now ready. We now, hopefully, from watching this video, you understand maybe a little bit about what KNN Classification might be. But that's what I'm gonna get into. That you're loading MobileNet, the pretrained model. You're creating a feature extractor, which knows how to pull out the logits of the MobileNet model with a given input image. And then we're passing in an image from a video. And instead of getting the classification, getting just that digital fingerprint. And that semantic digital fingerprint, this is exactly what we're going to do in the next video, to train a KNN Classification model. All right. So if you want to keep watching, then keep going in this playlist. If the video doesn't exist yet, it's because I'm still working on it but it'll be there soon. Bye! [MUSIC PLAYING]
