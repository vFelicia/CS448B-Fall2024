With timestamps:

00:00 - hello welcome to another guest video on
00:04 - the coding train today I have a very
00:06 - exciting guest for you
00:08 - Cabrillo from SEF science I'm probably
00:11 - saying that wrong gibreel told me like
00:13 - 10 different times how to say it I still
00:15 - couldn't get it right anyway gibreel is
00:16 - awesome I'm a big fan of his YouTube
00:18 - channel he actually came to visit NYU
00:20 - for a whole week and did a workshop and
00:22 - a talk and made a project and it's been
00:24 - sort of been an inspiring presence to
00:27 - have here for all this time and any what
00:29 - you're about to watch is an edited
00:31 - version of a live stream that the two of
00:32 - us did he is going to create or talk
00:34 - about a project that he recently made in
00:36 - JavaScript with his own from scratch
00:39 - neural network code where he makes a
00:41 - color predictor and in fact if you're
00:44 - interested in more about the color
00:45 - predictor you can click on
00:47 - well you can't click on that but I know
00:48 - it's something will come up over here
00:50 - that will maybe suggest that video over
00:52 - there the two of us made together for
00:54 - his channel so enjoy gibreel stay tuned
00:57 - later this week I will do my own coding
01:02 - challenge to try to make my own color
01:03 - predictor so we'll see how that goes
01:04 - thanks gibreel and for being here and
01:06 - hope you enjoy this edited version of
01:09 - the live stream we did together thanks
01:10 - very much
01:11 - alright howdy everyone how is it going
01:13 - so yeah I'm going to give a little brief
01:16 - overview about Who I am for those of you
01:18 - that do not know which I'm sure is all
01:20 - of you so my name is Jibril I run a
01:23 - little YouTube channel called girls here
01:25 - on YouTube and recently I converted my
01:28 - channel to focus on computer science
01:30 - that happened in September and that was
01:32 - probably one of the best things I've
01:34 - ever done because I learned that you
01:36 - know I had a great passion for writing
01:38 - code and write and making
01:39 - products and projects that were based on
01:42 - computer science and so yeah I mean
01:46 - obviously if I had a passion for that it
01:47 - was easy to show that in video projects
01:51 - as well
01:52 - and fast forward so one of the the
01:55 - biggest projects that I've produced to
01:57 - date is the run forest project that got
01:59 - a lot of eyes really grateful for and
02:02 - that really harnessed the power of
02:04 - machine learning which is a really big
02:06 - buzzword these days but yeah that's
02:08 - pretty much the overview I spend about
02:10 - nine months learning how to write
02:12 - learning algorithms from scratch because
02:15 - it was something AI is really cool to me
02:18 - I think and so yeah the rainforest was
02:21 - released and today today what we're
02:24 - gonna do is we're going to examine this
02:27 - really simple JavaScript machine
02:29 - learning application kind of how it was
02:32 - done it's another machine learning
02:35 - application written wrote from scratch
02:38 - so we're gonna take a look at the code
02:39 - and all that good stuff so let's let's
02:42 - get into this so here we have this
02:45 - example it's what I call a color
02:48 - predictor neural network to demo and it
02:51 - ask you a simple question that is white
02:54 - or black look better over this color and
02:57 - so the color is within the circle and
03:00 - it's randomly generated
03:01 - okay so what's important for us to start
03:04 - before we can get into the application
03:06 - we have to understand the the main
03:08 - computational part of this application
03:11 - so we have a color and as you know
03:14 - colors are they're represented as a
03:16 - vector of three or sometimes four if you
03:19 - include the Alpha but we're not
03:21 - including the Alpha we're only going to
03:23 - use the RGB values so we have our inputs
03:27 - which is three is that own friend yes
03:31 - all right so we have red green and blue
03:35 - and these are values between 0 and 255
03:41 - for each input so we need to build a
03:46 - neural network that will be able to take
03:48 - these inputs and then do a computation
03:50 - on them and then pass into an outputs to
03:52 - predict if it looks better over black or
03:55 - white
03:56 - so let's first draw our outputs just
03:58 - make sure it's all in frame yes that's
04:01 - good and this is going to be it predicts
04:05 - black and this predicts white so now we
04:08 - need a hidden layer is what we call
04:10 - hidden layer with artificial neural
04:13 - networks in the middle that does the
04:14 - computation part and this is our guests
04:17 - and so we are just going to arbitrarily
04:20 - just duplicate the same size of our
04:23 - inputs for our hidden layer
04:25 - we're just gonna say three it's a good
04:27 - place to start if we're really serious
04:29 - about this we could expand it try five
04:31 - try seven and just log the results for
04:35 - all of them and see which one works the
04:36 - best but where it's going to say three
04:38 - for this example make it nice and simple
04:40 - and so we have our RGB and if we go back
04:45 - to our example what's happening here is
04:49 - there's a computation that happens
04:54 - within our network to three one two
05:00 - three one two three and then feel free
05:05 - to interrupt if you think that I'm a
05:06 - little off-base with anything uh-huh
05:11 - uh-huh uh-huh
05:14 - okay so this what did I just do it looks
05:17 - really confusing but it's actually
05:18 - really simple so we we need to somehow
05:22 - get our inputs computation and into our
05:26 - outputs and the way that we do that is
05:28 - we use what I'm using bubbles to
05:30 - represent what are called weights within
05:32 - our known network and so every single
05:35 - node within our hidden layer has the
05:38 - same amount of weights as there are
05:40 - inputs so what that means is there's one
05:45 - way for this input there's one way for
05:47 - this input and there's one way for this
05:49 - input and the same for the rest of them
05:51 - one weight for this input one way for
05:54 - this input one way for that input and
05:57 - repeat I didn't do that right boom boom
06:02 - and so what then happens is that we pass
06:05 - this through we do our input times the
06:08 - weight and then plus our bias and we
06:11 - could repeat the process this will give
06:13 - us a value let's say that we after we
06:14 - compute all these sum them up add a bias
06:17 - it will give us let's just say 0.5 and
06:19 - then we'll pass that to our outputs
06:22 - which is 3 uh-huh uh-huh boom
06:31 - [Music]
06:33 - passes to our output and then that will
06:35 - give us a value for each of these let's
06:37 - say this is 0.3 and then
06:39 - point seven and then it's just as simple
06:41 - as we'll just say that this is higher
06:42 - 0.7 so it's guessing white so that's a
06:46 - quick overview on what's going on here
06:49 - with we're gonna Daniel's going to post
06:51 - a a more in-depth tutorial on this or
06:53 - you already have or well so I have
06:56 - tutorials yes like this that people
07:01 - could go back so this is you know the
07:03 - same kind of structure that I've used in
07:05 - my neural network library correct yes
07:07 - and I'm I was thinking at some point
07:09 - maybe next week hopefully I might try to
07:11 - recreate your project as like a coding
07:13 - challenge okay so we can put a link to
07:17 - yours okay so we'll put a link to Daniel
07:19 - Shipman series and what she goes in
07:21 - depth with this so if you want to learn
07:22 - more about what's going on here but
07:24 - that's a quick overview on the math on
07:25 - the computation so our inputs it gets
07:28 - times about weight and biases then we
07:29 - get a value and we pass that to our
07:31 - output same computation and then gives
07:33 - us a prediction so oh yeah let's do it
07:39 - all right is does the input have to be
07:41 - from 0 to 255
07:42 - okay sure inputs have to be normalized
07:45 - what's the from 0 to 1 yes great
07:48 - question great question so again I just
07:50 - glossed over this but so normalizing
07:53 - inputs for colors is actually really
07:55 - simple and it is always best to
07:58 - normalize your your input data so
08:01 - because we know that the the domain for
08:04 - a color value is always going to be 1
08:07 - the 256 or in computer languages we
08:10 - shift that by 1 0 255 we can simply just
08:12 - divide whenever this value is over 255
08:15 - and that will remap this between 0 and 1
08:17 - and so essentially when you're writing
08:20 - your program you would just pass the
08:22 - input through a function that would just
08:25 - divide about 255 so yes great question I
08:28 - have one more question so I'm kind of
08:30 - curious about this too but I sort of
08:32 - think it's probably ok it's with the
08:34 - pressure before you ask it yeah but is
08:36 - there a benefit to having two output
08:39 - nodes it rather than just have one since
08:44 - there's a
08:44 - that's like a ranger to negative one and
08:46 - yes so there's a lot of debate on this
08:49 - and I I and I agree with the side that
08:53 - it's easier when you have like
08:56 - classifiers versus like if you have just
08:59 - one output node that is mapped between
09:01 - negative 1 and 1 and right and then if
09:04 - it's if it's above 1 0 then it's going
09:07 - to be white if it's below 0 then it's
09:10 - going to be black
09:13 - yeah based on the research that I've
09:15 - read it's always best to go on a
09:17 - classifier yeah like maybe this would be
09:21 - fine in the case of there's only two
09:23 - labels or two classes right or the
09:26 - correct correct it's gonna be
09:28 - problematic and so that's all like
09:29 - demonstration and learning even though
09:31 - this might be a very like basic scenario
09:33 - it's useful to demonstrate the multiple
09:35 - outputs cuz you're gonna need to do that
09:37 - if you were to expand this for
09:38 - correcting and to the whole reason for
09:41 - that is because what what happens when
09:43 - you separate them is you get
09:44 - probabilities versus like you get a map
09:47 - of between 0 & 1 which again if it's one
09:49 - output you can get away with that but if
09:51 - you try and encode your outputs using
09:54 - this for like 30 different the neural
09:58 - network might not make good sense of
09:59 - that oh cool so let's let's continue on
10:02 - let's look at some of the code as to how
10:06 - we went about writing that part of our
10:09 - neural network suite so we set up our
10:11 - variables are jeebies our input data and
10:15 - then so one thing that's really
10:17 - important that I should go over just to
10:21 - just to make sense of what's going on
10:22 - here is so so it's really important in
10:27 - order for you to write your algorithm
10:29 - you need to you need to know how to
10:31 - compute this compute both of these so
10:34 - this is really just an array of values
10:38 - so we can call this array G of I right
10:43 - so this is G of 0 and this is G of 1 and
10:47 - G just stands for a guess I put 0 G of 1
10:50 - right and so we want to know what does G
10:54 - of I or what does G of 0
10:56 - what the G of one equal how can we get
10:58 - that equation well if we look at our
11:01 - diagram for our network it's actually
11:03 - quite simple so G of I which again is
11:06 - this array this output layer G of I
11:09 - equals the summation of hidden layer
11:16 - this is going to be hidden layer by I
11:18 - and this is going to be inputs of I
11:22 - that's how we define each of these
11:24 - vectors so G of I equals a summation of
11:28 - H L hidden layer and then we have to go
11:32 - into another loop because we can't use
11:36 - the same indicee of I and J because it
11:38 - yeah it won't return the right value so
11:42 - hidden layer of J which is just gonna be
11:45 - 0 1 2 times the weight of G of I right
11:56 - and then we simply just add our bias of
11:59 - G of I and so this is the equation that
12:04 - we can use to compute each of our output
12:07 - nodes and so just to clarify what's
12:10 - going on here this is summation symbol
12:12 - which simply means to add up all within
12:16 - the the array so hidden layer J times
12:20 - the weight this is a function which
12:22 - simply just grabs the weight of whatever
12:25 - output node you're on so if you pass G
12:29 - of 0 for example to do this weight
12:32 - function it will just grab whatever bias
12:34 - or I'm sorry whatever weight of G of I
12:37 - is there so that's actually G of I of J
12:42 - actually hereby up J huh so now we have
12:49 - this equation that tells us exactly what
12:51 - these values equal so now we don't know
12:55 - what HL j equals so we also have to
12:57 - define HL of J and we go about doing
13:00 - that by doing the same exact process
13:03 - after the super hio of I for indices
13:06 - same exact process summation of our
13:09 - inputs right inputs what I use InP
13:16 - inputs J times weight of HL of I so the
13:25 - same exact input we need H ll by M and
13:30 - then we simply just pass our bias and
13:33 - again this this right here is a function
13:34 - all it does is it grabs the bias for
13:38 - whatever node that we pass through it so
13:41 - bias of HL y mmm and there we have it we
13:45 - have our entire equation because we know
13:47 - exactly what inputs input of J equals
13:49 - it's going to be simply the random value
13:52 - of our color and so this is what we need
13:57 - to write in our software okay so same
14:02 - exact thing that you see on the board is
14:05 - what we write here in our code so first
14:08 - before we can get the what the guess no
14:11 - zico we have to first forget what the
14:13 - hidden layer nodes equals so simply put
14:15 - as we did um on the white board hidden
14:19 - layer 0 equals we'll get to what Ray Lu
14:21 - is in a second but hidden there 0 equals
14:24 - we did our input encoder which was a
14:27 - question that was asked earlier about
14:30 - normalizing our input data so this
14:33 - function simply just divides our input
14:35 - divided by Oh 255 and then we'll times
14:38 - that by the weight of our of our hidden
14:45 - layer so this this is an array function
14:48 - that I will go over really quickly that
14:53 - we instantiate to hold all of our
14:57 - weights so color predictor 0 0 0 I'll go
15:06 - over this I think it's important so so
15:10 - the function color predictor variable
15:15 - uh-huh so there's all these
15:19 - for it dimensions to it and I think it's
15:23 - interesting or it's important to go over
15:24 - what the dimensions mean so let's just
15:27 - get two and then let's just do I don't
15:30 - know one so what does this mean if you
15:32 - have color predictor eyes ero to one
15:35 - what does that mean well well so we want
15:42 - to store these arrays into our color
15:45 - predictor variable and we can go about
15:47 - doing that by defining the location of
15:50 - all of these so if the hidden layer is
15:52 - going to be 0 and then the guess is
15:55 - going to be 1 right and so all the notes
15:59 - are then going to have their own
16:01 - assignments so 0 1 and then 2 same here
16:06 - this is going to be 0 and 1 and then the
16:12 - weights are also gonna have their own
16:13 - assignment as well so 0 1 2 3 and the
16:17 - same day 0 1 2 3 and we repeat that for
16:21 - every single weight inside of the nodes
16:23 - right and so how this works is our color
16:26 - predictor is if we want to get grab
16:29 - reference to 0 that is going to be
16:31 - hidden layer and then 2 is going to be
16:34 - the last node and then 1 is going to be
16:37 - the the second way because start 0 1
16:40 - second way so this variable is grabbing
16:43 - a reference to this way that's exactly
16:45 - what color predictor is 0 to 1 is doing
16:48 - and so you see this eye
16:50 - that's an extra dimension that you might
16:52 - be confused about so let's talk about
16:53 - that real briefly so this so you see
16:56 - this extra eye and what does that mean
16:58 - so traditionally - traditionally with
17:01 - such an example you would use back
17:06 - propagation to Train this no network
17:07 - however time was a factor and as well as
17:11 - that we wanted to go over lesson of both
17:15 - neural networks and genetic algorithms
17:17 - so why not combine them together is what
17:20 - we did for this example so that eye is
17:22 - actually just grabbing a reference to
17:25 - what predictor we are currently using so
17:30 - genetic algorithms real quick you have
17:32 - to have a
17:33 - halation yet the assigned Fitness scores
17:35 - to every single what's sorting for the
17:41 - creature within the population and then
17:43 - you have to mutate them and breed them
17:45 - in XY and Z so we have a population of
17:48 - 800 predictors at four at start and then
17:52 - they all have randomly initialized
17:54 - weights and biases which again just to
17:56 - make sure it clear are all of these
17:58 - values wait-wait-wait by swim away bias
18:01 - these are all randomly initialized the
18:03 - function that we use for this program is
18:06 - randomized between zero and one and then
18:10 - they all based on their randomly
18:13 - initialized weights we'll make a guess
18:14 - on which one they think is correct and
18:17 - so most of them said that black is the
18:21 - correct color that looks best over this
18:25 - randomized color and then we simply just
18:32 - use a genetic algorithm to train this
18:36 - this predictor to converge on the best
18:38 - possible predictor and yet that's it
18:44 - that's a general overview on this the
18:48 - code is available on github and I left a
18:53 - lot of comments however that it will
18:58 - require a bit more in depth if you if
19:00 - you really want to like get a full grasp
19:03 - on this from learn and have fun knowing
19:05 - absolutely nothing if you already know
19:06 - some stuff about no numbers between
19:08 - learning I'm pretty sure this example is
19:10 - pretty straightforward from you but the
19:11 - benefit of doing of writing neural
19:14 - networks from scratch is that you really
19:16 - have a good grasp on what's going on
19:17 - behind the scenes versus using libraries
19:20 - and you're able to debug you know
19:24 - different problems that might arise when
19:26 - you are writing your no networks would
19:29 - be it from scratch or using libraries
19:32 - that is pretty much a we we do have
19:34 - plans to update it with the actual back
19:37 - propagation algorithm in there so that
19:40 - you can learn from that as well so let
19:44 - me
19:45 - a few more questions okay let's see what
19:49 - I can find here Oh for some first people
19:55 - had asked to is the code already at
19:57 - github are you're gonna on github are
19:59 - you gonna update oh yeah I I slept oh so
20:02 - stay tuned whenever whenever it's on
20:05 - github I will come back and edit the
20:07 - description for this livestream and put
20:09 - a link to but they can currently go to
20:11 - Steph's of the comic /color oh that the
20:16 - link that I use right yeah so you can go
20:18 - to let's let me zoom in here and show
20:20 - you so this one yes if you want to grab
20:26 - the code right now I don't know why
20:29 - whenever I paste links into the YouTube
20:32 - chat they don't seem to work for that
20:33 - interest so but anyway so I would paste
20:35 - this into the chat but that wouldn't
20:36 - even work so you can see it yeah up
20:39 - there that you can grab the code now but
20:42 - I will also include a link to github
20:46 - repository whatever that and it is my
20:48 - intention I think one of the reasons why
20:50 - I love this demo and people are kind of
20:52 - asking this a bit in the chat was like
20:54 - oh like do you really need a neural
20:55 - network for this right and I don't I
20:57 - think to me that I mean that's a
20:59 - perfectly valid and interesting question
21:00 - and probably the answer is no you don't
21:02 - need a neural network for this but when
21:04 - learning about neural networks when
21:06 - trying to build your own machine
21:07 - learning project if you can start with a
21:09 - well-defined small and scoped problem
21:12 - then you can really figure out and
21:14 - because you in some ways I do this
21:17 - similarly with that you're so my genetic
21:19 - algorithm problem projects I take a
21:22 - example where I know the answer right so
21:25 - I can see if the genetic algorithm
21:27 - worked because ultimately what I want to
21:29 - do is use a neural network or a genetic
21:31 - algorithm in some domain where maybe I
21:32 - don't know the answer I couldn't solve
21:34 - right so easily but to figure out how
21:36 - those things work I've got to come up
21:38 - with and so this is a really nice
21:39 - problem for that because it's simple
21:41 - small in scope and for people who want
21:43 - to do creative coding and graphics and
21:45 - design stuff it's got color and then and
21:48 - not to mention once you learn how to do
21:50 - this stuff from scratch this is easily
21:52 - be scalable for the most part you still
21:55 - have to worry about some other stuff
21:56 - like vanishing gradients and stuff like
21:57 - that but
21:58 - most part you can take this and scale
21:59 - this up and it'll work just about the
22:01 - same so there's also that benefit as
22:03 - well so here's a question from I am
22:05 - rashon on Twitter this is a big question
22:08 - ok let's do it I don't know and I think
22:11 - this is a question I've certainly
22:12 - touched on and but how is a I let me
22:16 - read the question how it actually is
22:18 - written how is AI different from a
22:20 - neural network or deep learning or
22:22 - machine learning they often seem to be
22:23 - used interchangeably and cause confusion
22:25 - so first of all I want to say like and I
22:29 - struggle with this question all the time
22:30 - because there's all these different
22:31 - terms so we so let me list those terms
22:33 - artificial intelligence deep learning
22:35 - machine learning and then I might put
22:37 - neural network in a like different
22:39 - category but that's another term as well
22:41 - I don't know if you have a kind of like
22:44 - way that you describe these the
22:46 - terminology of people when they ask
22:47 - those kind of yeah I'd like to start
22:48 - with English is difficult I like to
22:51 - start there but for the most part AI is
22:54 - it's like a grab all for everything a I
22:56 - like you can hard code AI you don't have
23:00 - to use machine learning so that that's
23:02 - kind of like they grab all for at all
23:03 - and then what was the other the code
23:05 - words would so oh boy all these things
23:07 - are coming in AI machine learning deep
23:10 - learning neural network yeah and so
23:12 - machine learning I think it's like the
23:14 - next level down so you don't need to use
23:17 - a neural network to do machine learning
23:19 - there are different ways you can go
23:20 - about teaching a machine how to learn
23:22 - one really good example is decision
23:25 - trees people have developed really
23:27 - complex decision trees and the machine
23:30 - just kind of explores the space and
23:31 - learns the best way to use this this is
23:33 - injury so that's another way of applying
23:35 - machine learning and then would you say
23:38 - neural network and defining yes yeah so
23:40 - no networks is essentially what we
23:42 - showed and even that's kind of to the
23:47 - bay because like recurrent node networks
23:49 - are no networks but they're not really
23:52 - you know neural networks you know if
23:56 - that makes sense so I don't know it's
23:58 - it's if you say no network people will
24:01 - know what you're talking with in most
24:02 - part all right I'm gonna try to give my
24:04 - take on this let's do it let's do it
24:07 - that work to me is a particular
24:11 - algorithm that involves connected nodes
24:15 - that and data flows from one node to the
24:19 - other they're coming different
24:19 - architectures and styles and so that
24:21 - neural network data structure algorithm
24:24 - can be applied in the field of
24:28 - artificial intelligence machine learning
24:29 - and deep learning but neural network is
24:30 - an example of a particular algorithm I
24:34 - would say you could sort of think of it
24:35 - also as a data structure but there's an
24:37 - algorithm there in terms of how the data
24:38 - flows through the structure so that's
24:42 - what that's what I think and then I
24:43 - think that artificial to me artificial
24:46 - intelligence I think that is a very
24:47 - broad umbrella terms are just the big
24:49 - field of like simulated intelligence
24:51 - rather is that is it real intelligence
24:53 - is it the illusion of intelligence is
24:54 - that the same things like kind of a deep
24:56 - philosophical question and then I
24:59 - machine learning to me is a subfield of
25:01 - artificial intelligence involving making
25:04 - sense of data
25:05 - mm-hmm so you have data and that's input
25:08 - to a system and you have some output
25:09 - which might be making sense of that data
25:11 - whether it's a prediction for something
25:13 - it's gonna happen in the future or a
25:14 - classification of something and then
25:16 - then I think of deep learning as a kind
25:20 - of read almost like a modern rebranding
25:23 - of machinery with neural networks it's
25:25 - like hey we have bigger datasets now and
25:27 - faster computers now all of a sudden the
25:29 - things that people researched many years
25:31 - ago called neural networks that nobody
25:33 - thought could really do anything or they
25:34 - thought good but couldn't now all the
25:36 - sudden we can do more of them with and
25:38 - so it's really just like it's not meant
25:41 - to be marketing but it's kind of like
25:42 - marketing this idea of big data under
25:45 - all networks and yeah III agree I think
25:47 - it's a lot of the marketing side of
25:50 - things because you'll reach so many
25:51 - different posts like what's their deep
25:53 - learning in machine learning yeah it has
25:55 - two hidden layers that's it
25:57 - yeah like literally I think it's more
25:59 - the marketing side my personal opinion
26:01 - but that's awesome cool well let me see
26:04 - if we have any other
26:05 - oh yeah neural network I like this
26:07 - definition neural network is a universal
26:10 - function approach
26:11 - semadar hi Emily this is actually like a
26:14 - really this is Robin I'm gonna hello
26:16 - let's try this oh this is crazy talk now
26:19 - now I'm coming over here I actually
26:20 - think this is really kind of a good way
26:21 - to think about it I was thinking about
26:23 - this the other day because what if we
26:26 - made like function color predictor and
26:31 - we just had like an if statement in
26:33 - there like and you give it a color so if
26:36 - the brightness blah blah blah of that
26:39 - color is greater than some value then
26:41 - you should put black on that color or
26:43 - white on that color otherwise so this is
26:45 - like a hard-coded function that takes
26:48 - inputs and returns an output right and
26:51 - so we could we could write a lot of if
26:53 - statements we could get really crazy
26:54 - complicated about this we could come up
26:56 - with a whole set of rules and a neural
26:58 - network in a way as a thing that you
26:59 - could put in here to kind of learn to
27:01 - return the value according to in a more
27:04 - mysterious way in a way like in a sense
27:07 - it can learn it could it acts so in a
27:09 - way like does do neural networks in
27:11 - machine learning replace coding right I
27:13 - don't think of them as we're plate maybe
27:15 - someday they will in some weird way but
27:17 - I think of it as like they don't replace
27:18 - coding but they can replace our act as a
27:21 - function in your code so that function
27:23 - that you might have hard-coded a lot of
27:24 - if statements can now have a machine
27:26 - learning system in it
27:27 - take some inputs and generate an output
27:28 - yeah I agree I mean when it's all said
27:31 - and done algorithms are input
27:34 - instructions output yeah and you're just
27:36 - replacing the engine Bert yeah totally
27:39 - thank you so much gibreel for being here
27:42 - for your for participating in the
27:44 - livestream for showing me your color
27:46 - predictor so much inspiration all week I
27:48 - don't know if you know this why you
27:50 - should know this but gibreel only
27:51 - started working with machine learning
27:52 - less than a year ago completely
27:54 - self-taught wrote his own neural network
27:56 - from scratch and JavaScript he did it in
27:58 - unity he's got a whole video project he
28:00 - made on his channel in unity just
28:01 - amazing really inspiring stuff so make
28:04 - sure you subscribe to his channel link
28:06 - in the description below click that
28:08 - alarm bell icon subscribed him on
28:10 - Twitter check out his website all that
28:11 - stuff will be in this video's
28:12 - description
28:13 - stay tuned for later this week I am
28:15 - going to attempt to make my own version
28:19 - of the color predictor
28:21 - with my toy neural network JavaScript
28:23 - library so that will come in as a coding
28:25 - challenge so stay tuned for that and
28:26 - I'll see you in the future
28:27 - good bye
28:32 - [Music]

Cleaned transcript:

hello welcome to another guest video on the coding train today I have a very exciting guest for you Cabrillo from SEF science I'm probably saying that wrong gibreel told me like 10 different times how to say it I still couldn't get it right anyway gibreel is awesome I'm a big fan of his YouTube channel he actually came to visit NYU for a whole week and did a workshop and a talk and made a project and it's been sort of been an inspiring presence to have here for all this time and any what you're about to watch is an edited version of a live stream that the two of us did he is going to create or talk about a project that he recently made in JavaScript with his own from scratch neural network code where he makes a color predictor and in fact if you're interested in more about the color predictor you can click on well you can't click on that but I know it's something will come up over here that will maybe suggest that video over there the two of us made together for his channel so enjoy gibreel stay tuned later this week I will do my own coding challenge to try to make my own color predictor so we'll see how that goes thanks gibreel and for being here and hope you enjoy this edited version of the live stream we did together thanks very much alright howdy everyone how is it going so yeah I'm going to give a little brief overview about Who I am for those of you that do not know which I'm sure is all of you so my name is Jibril I run a little YouTube channel called girls here on YouTube and recently I converted my channel to focus on computer science that happened in September and that was probably one of the best things I've ever done because I learned that you know I had a great passion for writing code and write and making products and projects that were based on computer science and so yeah I mean obviously if I had a passion for that it was easy to show that in video projects as well and fast forward so one of the the biggest projects that I've produced to date is the run forest project that got a lot of eyes really grateful for and that really harnessed the power of machine learning which is a really big buzzword these days but yeah that's pretty much the overview I spend about nine months learning how to write learning algorithms from scratch because it was something AI is really cool to me I think and so yeah the rainforest was released and today today what we're gonna do is we're going to examine this really simple JavaScript machine learning application kind of how it was done it's another machine learning application written wrote from scratch so we're gonna take a look at the code and all that good stuff so let's let's get into this so here we have this example it's what I call a color predictor neural network to demo and it ask you a simple question that is white or black look better over this color and so the color is within the circle and it's randomly generated okay so what's important for us to start before we can get into the application we have to understand the the main computational part of this application so we have a color and as you know colors are they're represented as a vector of three or sometimes four if you include the Alpha but we're not including the Alpha we're only going to use the RGB values so we have our inputs which is three is that own friend yes all right so we have red green and blue and these are values between 0 and 255 for each input so we need to build a neural network that will be able to take these inputs and then do a computation on them and then pass into an outputs to predict if it looks better over black or white so let's first draw our outputs just make sure it's all in frame yes that's good and this is going to be it predicts black and this predicts white so now we need a hidden layer is what we call hidden layer with artificial neural networks in the middle that does the computation part and this is our guests and so we are just going to arbitrarily just duplicate the same size of our inputs for our hidden layer we're just gonna say three it's a good place to start if we're really serious about this we could expand it try five try seven and just log the results for all of them and see which one works the best but where it's going to say three for this example make it nice and simple and so we have our RGB and if we go back to our example what's happening here is there's a computation that happens within our network to three one two three one two three and then feel free to interrupt if you think that I'm a little offbase with anything uhhuh uhhuh uhhuh okay so this what did I just do it looks really confusing but it's actually really simple so we we need to somehow get our inputs computation and into our outputs and the way that we do that is we use what I'm using bubbles to represent what are called weights within our known network and so every single node within our hidden layer has the same amount of weights as there are inputs so what that means is there's one way for this input there's one way for this input and there's one way for this input and the same for the rest of them one weight for this input one way for this input one way for that input and repeat I didn't do that right boom boom and so what then happens is that we pass this through we do our input times the weight and then plus our bias and we could repeat the process this will give us a value let's say that we after we compute all these sum them up add a bias it will give us let's just say 0.5 and then we'll pass that to our outputs which is 3 uhhuh uhhuh boom passes to our output and then that will give us a value for each of these let's say this is 0.3 and then point seven and then it's just as simple as we'll just say that this is higher 0.7 so it's guessing white so that's a quick overview on what's going on here with we're gonna Daniel's going to post a a more indepth tutorial on this or you already have or well so I have tutorials yes like this that people could go back so this is you know the same kind of structure that I've used in my neural network library correct yes and I'm I was thinking at some point maybe next week hopefully I might try to recreate your project as like a coding challenge okay so we can put a link to yours okay so we'll put a link to Daniel Shipman series and what she goes in depth with this so if you want to learn more about what's going on here but that's a quick overview on the math on the computation so our inputs it gets times about weight and biases then we get a value and we pass that to our output same computation and then gives us a prediction so oh yeah let's do it all right is does the input have to be from 0 to 255 okay sure inputs have to be normalized what's the from 0 to 1 yes great question great question so again I just glossed over this but so normalizing inputs for colors is actually really simple and it is always best to normalize your your input data so because we know that the the domain for a color value is always going to be 1 the 256 or in computer languages we shift that by 1 0 255 we can simply just divide whenever this value is over 255 and that will remap this between 0 and 1 and so essentially when you're writing your program you would just pass the input through a function that would just divide about 255 so yes great question I have one more question so I'm kind of curious about this too but I sort of think it's probably ok it's with the pressure before you ask it yeah but is there a benefit to having two output nodes it rather than just have one since there's a that's like a ranger to negative one and yes so there's a lot of debate on this and I I and I agree with the side that it's easier when you have like classifiers versus like if you have just one output node that is mapped between negative 1 and 1 and right and then if it's if it's above 1 0 then it's going to be white if it's below 0 then it's going to be black yeah based on the research that I've read it's always best to go on a classifier yeah like maybe this would be fine in the case of there's only two labels or two classes right or the correct correct it's gonna be problematic and so that's all like demonstration and learning even though this might be a very like basic scenario it's useful to demonstrate the multiple outputs cuz you're gonna need to do that if you were to expand this for correcting and to the whole reason for that is because what what happens when you separate them is you get probabilities versus like you get a map of between 0 & 1 which again if it's one output you can get away with that but if you try and encode your outputs using this for like 30 different the neural network might not make good sense of that oh cool so let's let's continue on let's look at some of the code as to how we went about writing that part of our neural network suite so we set up our variables are jeebies our input data and then so one thing that's really important that I should go over just to just to make sense of what's going on here is so so it's really important in order for you to write your algorithm you need to you need to know how to compute this compute both of these so this is really just an array of values so we can call this array G of I right so this is G of 0 and this is G of 1 and G just stands for a guess I put 0 G of 1 right and so we want to know what does G of I or what does G of 0 what the G of one equal how can we get that equation well if we look at our diagram for our network it's actually quite simple so G of I which again is this array this output layer G of I equals the summation of hidden layer this is going to be hidden layer by I and this is going to be inputs of I that's how we define each of these vectors so G of I equals a summation of H L hidden layer and then we have to go into another loop because we can't use the same indicee of I and J because it yeah it won't return the right value so hidden layer of J which is just gonna be 0 1 2 times the weight of G of I right and then we simply just add our bias of G of I and so this is the equation that we can use to compute each of our output nodes and so just to clarify what's going on here this is summation symbol which simply means to add up all within the the array so hidden layer J times the weight this is a function which simply just grabs the weight of whatever output node you're on so if you pass G of 0 for example to do this weight function it will just grab whatever bias or I'm sorry whatever weight of G of I is there so that's actually G of I of J actually hereby up J huh so now we have this equation that tells us exactly what these values equal so now we don't know what HL j equals so we also have to define HL of J and we go about doing that by doing the same exact process after the super hio of I for indices same exact process summation of our inputs right inputs what I use InP inputs J times weight of HL of I so the same exact input we need H ll by M and then we simply just pass our bias and again this this right here is a function all it does is it grabs the bias for whatever node that we pass through it so bias of HL y mmm and there we have it we have our entire equation because we know exactly what inputs input of J equals it's going to be simply the random value of our color and so this is what we need to write in our software okay so same exact thing that you see on the board is what we write here in our code so first before we can get the what the guess no zico we have to first forget what the hidden layer nodes equals so simply put as we did um on the white board hidden layer 0 equals we'll get to what Ray Lu is in a second but hidden there 0 equals we did our input encoder which was a question that was asked earlier about normalizing our input data so this function simply just divides our input divided by Oh 255 and then we'll times that by the weight of our of our hidden layer so this this is an array function that I will go over really quickly that we instantiate to hold all of our weights so color predictor 0 0 0 I'll go over this I think it's important so so the function color predictor variable uhhuh so there's all these for it dimensions to it and I think it's interesting or it's important to go over what the dimensions mean so let's just get two and then let's just do I don't know one so what does this mean if you have color predictor eyes ero to one what does that mean well well so we want to store these arrays into our color predictor variable and we can go about doing that by defining the location of all of these so if the hidden layer is going to be 0 and then the guess is going to be 1 right and so all the notes are then going to have their own assignments so 0 1 and then 2 same here this is going to be 0 and 1 and then the weights are also gonna have their own assignment as well so 0 1 2 3 and the same day 0 1 2 3 and we repeat that for every single weight inside of the nodes right and so how this works is our color predictor is if we want to get grab reference to 0 that is going to be hidden layer and then 2 is going to be the last node and then 1 is going to be the the second way because start 0 1 second way so this variable is grabbing a reference to this way that's exactly what color predictor is 0 to 1 is doing and so you see this eye that's an extra dimension that you might be confused about so let's talk about that real briefly so this so you see this extra eye and what does that mean so traditionally traditionally with such an example you would use back propagation to Train this no network however time was a factor and as well as that we wanted to go over lesson of both neural networks and genetic algorithms so why not combine them together is what we did for this example so that eye is actually just grabbing a reference to what predictor we are currently using so genetic algorithms real quick you have to have a halation yet the assigned Fitness scores to every single what's sorting for the creature within the population and then you have to mutate them and breed them in XY and Z so we have a population of 800 predictors at four at start and then they all have randomly initialized weights and biases which again just to make sure it clear are all of these values waitwaitwait by swim away bias these are all randomly initialized the function that we use for this program is randomized between zero and one and then they all based on their randomly initialized weights we'll make a guess on which one they think is correct and so most of them said that black is the correct color that looks best over this randomized color and then we simply just use a genetic algorithm to train this this predictor to converge on the best possible predictor and yet that's it that's a general overview on this the code is available on github and I left a lot of comments however that it will require a bit more in depth if you if you really want to like get a full grasp on this from learn and have fun knowing absolutely nothing if you already know some stuff about no numbers between learning I'm pretty sure this example is pretty straightforward from you but the benefit of doing of writing neural networks from scratch is that you really have a good grasp on what's going on behind the scenes versus using libraries and you're able to debug you know different problems that might arise when you are writing your no networks would be it from scratch or using libraries that is pretty much a we we do have plans to update it with the actual back propagation algorithm in there so that you can learn from that as well so let me a few more questions okay let's see what I can find here Oh for some first people had asked to is the code already at github are you're gonna on github are you gonna update oh yeah I I slept oh so stay tuned whenever whenever it's on github I will come back and edit the description for this livestream and put a link to but they can currently go to Steph's of the comic /color oh that the link that I use right yeah so you can go to let's let me zoom in here and show you so this one yes if you want to grab the code right now I don't know why whenever I paste links into the YouTube chat they don't seem to work for that interest so but anyway so I would paste this into the chat but that wouldn't even work so you can see it yeah up there that you can grab the code now but I will also include a link to github repository whatever that and it is my intention I think one of the reasons why I love this demo and people are kind of asking this a bit in the chat was like oh like do you really need a neural network for this right and I don't I think to me that I mean that's a perfectly valid and interesting question and probably the answer is no you don't need a neural network for this but when learning about neural networks when trying to build your own machine learning project if you can start with a welldefined small and scoped problem then you can really figure out and because you in some ways I do this similarly with that you're so my genetic algorithm problem projects I take a example where I know the answer right so I can see if the genetic algorithm worked because ultimately what I want to do is use a neural network or a genetic algorithm in some domain where maybe I don't know the answer I couldn't solve right so easily but to figure out how those things work I've got to come up with and so this is a really nice problem for that because it's simple small in scope and for people who want to do creative coding and graphics and design stuff it's got color and then and not to mention once you learn how to do this stuff from scratch this is easily be scalable for the most part you still have to worry about some other stuff like vanishing gradients and stuff like that but most part you can take this and scale this up and it'll work just about the same so there's also that benefit as well so here's a question from I am rashon on Twitter this is a big question ok let's do it I don't know and I think this is a question I've certainly touched on and but how is a I let me read the question how it actually is written how is AI different from a neural network or deep learning or machine learning they often seem to be used interchangeably and cause confusion so first of all I want to say like and I struggle with this question all the time because there's all these different terms so we so let me list those terms artificial intelligence deep learning machine learning and then I might put neural network in a like different category but that's another term as well I don't know if you have a kind of like way that you describe these the terminology of people when they ask those kind of yeah I'd like to start with English is difficult I like to start there but for the most part AI is it's like a grab all for everything a I like you can hard code AI you don't have to use machine learning so that that's kind of like they grab all for at all and then what was the other the code words would so oh boy all these things are coming in AI machine learning deep learning neural network yeah and so machine learning I think it's like the next level down so you don't need to use a neural network to do machine learning there are different ways you can go about teaching a machine how to learn one really good example is decision trees people have developed really complex decision trees and the machine just kind of explores the space and learns the best way to use this this is injury so that's another way of applying machine learning and then would you say neural network and defining yes yeah so no networks is essentially what we showed and even that's kind of to the bay because like recurrent node networks are no networks but they're not really you know neural networks you know if that makes sense so I don't know it's it's if you say no network people will know what you're talking with in most part all right I'm gonna try to give my take on this let's do it let's do it that work to me is a particular algorithm that involves connected nodes that and data flows from one node to the other they're coming different architectures and styles and so that neural network data structure algorithm can be applied in the field of artificial intelligence machine learning and deep learning but neural network is an example of a particular algorithm I would say you could sort of think of it also as a data structure but there's an algorithm there in terms of how the data flows through the structure so that's what that's what I think and then I think that artificial to me artificial intelligence I think that is a very broad umbrella terms are just the big field of like simulated intelligence rather is that is it real intelligence is it the illusion of intelligence is that the same things like kind of a deep philosophical question and then I machine learning to me is a subfield of artificial intelligence involving making sense of data mmhmm so you have data and that's input to a system and you have some output which might be making sense of that data whether it's a prediction for something it's gonna happen in the future or a classification of something and then then I think of deep learning as a kind of read almost like a modern rebranding of machinery with neural networks it's like hey we have bigger datasets now and faster computers now all of a sudden the things that people researched many years ago called neural networks that nobody thought could really do anything or they thought good but couldn't now all the sudden we can do more of them with and so it's really just like it's not meant to be marketing but it's kind of like marketing this idea of big data under all networks and yeah III agree I think it's a lot of the marketing side of things because you'll reach so many different posts like what's their deep learning in machine learning yeah it has two hidden layers that's it yeah like literally I think it's more the marketing side my personal opinion but that's awesome cool well let me see if we have any other oh yeah neural network I like this definition neural network is a universal function approach semadar hi Emily this is actually like a really this is Robin I'm gonna hello let's try this oh this is crazy talk now now I'm coming over here I actually think this is really kind of a good way to think about it I was thinking about this the other day because what if we made like function color predictor and we just had like an if statement in there like and you give it a color so if the brightness blah blah blah of that color is greater than some value then you should put black on that color or white on that color otherwise so this is like a hardcoded function that takes inputs and returns an output right and so we could we could write a lot of if statements we could get really crazy complicated about this we could come up with a whole set of rules and a neural network in a way as a thing that you could put in here to kind of learn to return the value according to in a more mysterious way in a way like in a sense it can learn it could it acts so in a way like does do neural networks in machine learning replace coding right I don't think of them as we're plate maybe someday they will in some weird way but I think of it as like they don't replace coding but they can replace our act as a function in your code so that function that you might have hardcoded a lot of if statements can now have a machine learning system in it take some inputs and generate an output yeah I agree I mean when it's all said and done algorithms are input instructions output yeah and you're just replacing the engine Bert yeah totally thank you so much gibreel for being here for your for participating in the livestream for showing me your color predictor so much inspiration all week I don't know if you know this why you should know this but gibreel only started working with machine learning less than a year ago completely selftaught wrote his own neural network from scratch and JavaScript he did it in unity he's got a whole video project he made on his channel in unity just amazing really inspiring stuff so make sure you subscribe to his channel link in the description below click that alarm bell icon subscribed him on Twitter check out his website all that stuff will be in this video's description stay tuned for later this week I am going to attempt to make my own version of the color predictor with my toy neural network JavaScript library so that will come in as a coding challenge so stay tuned for that and I'll see you in the future good bye
