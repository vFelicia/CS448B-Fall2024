With timestamps:

00:00 - hello it's me coming to you again from
00:01 - the future you might recognize me some
00:04 - from my fails videos as a calculus
00:07 - partial derivative hey I made some
00:09 - videos with some calculus stuffs they
00:11 - can turn out very well you can find them
00:13 - if you want they're kind of unlisted now
00:14 - but I just I tried again and so this
00:17 - video if you're watching it this is a
00:18 - follow-up to my linear regression with
00:20 - gradient descent video that video stands
00:24 - alone it's a programming video where all
00:26 - I do is walk through with the code for
00:28 - how to build an example that
00:30 - demonstrates linear regression with
00:31 - gradient descent and this is a puzzle
00:33 - piece in my machine learning series that
00:35 - will hopefully act as a foundation the
00:37 - building blocks your understanding of
00:38 - hopefully some more creative or
00:40 - practical examples that will come later
00:42 - this video that if you're watching it's
00:44 - totally optional to watch as part of
00:45 - this series because you just applied the
00:47 - formula but what I try to do in this
00:49 - video is give some backgrounds and I
00:51 - kind of worked it all out here this is
00:53 - the end this is what's on the whiteboard
00:54 - I thought so now if I use multiple
00:56 - colored markers would somehow make a
00:57 - better video I don't think I really did
00:58 - that but so I kind of walk through and
01:01 - try to describe the map I should say
01:03 - that you know is this involves topics
01:06 - from calculus and there's a great video
01:09 - series by three blue one brown on
01:11 - YouTube that gives you great background
01:13 - and more depth in calculus so I'll put
01:15 - links to those videos in this video's
01:17 - description honestly if you're really
01:19 - interested in kind of soaking up as much
01:21 - as this as you can I would go watch
01:22 - those videos first and then come back
01:24 - here it'll give you that background for
01:27 - understanding the pieces that I've done
01:29 - here so I look forward to your feedback
01:31 - positive and negative constructive
01:34 - feedback into whether this was helpful
01:36 - if it made sense and if you then go on
01:39 - and keep watching there'll be some
01:41 - future videos well be getting back into
01:42 - the code there's no code in this video
01:43 - just a math stuff next max okay I enjoy
01:48 - so to recap I have a bunch of data
01:56 - points in 2d space I have a line in that
02:01 - 2d space the formula for that line is y
02:05 - equals MX plus B when I try to make a
02:10 - prediction
02:11 - right I get a piece of input of data
02:16 - input X and from there I try to make
02:20 - again in addition to the guests I have
02:22 - the known Y so this is the correct data
02:28 - that goes with X my machine learning
02:31 - system makes a guess the error is the
02:37 - difference between those two things
02:39 - the error is y the correct answer - the
02:47 - guess so this relates to the idea of a
02:51 - cost function a loss function so if we
02:57 - want to evaluate how is our machine
02:59 - learning algorithm performing we have
03:03 - this large data set maybe it has n
03:05 - elements so what we want to do is from 1
03:11 - to n for all n elements we want to
03:17 - minimize that error so the cost function
03:21 - cost equals the sum of Y sub I every
03:28 - node answer - the guest sub I squared so
03:36 - this is the formula this is a cost known
03:39 - as a cost function this is the total
03:42 - error for the particular in the
03:46 - particular model being the current M and
03:50 - B values that describe this particular
03:52 - line this is the error so perhaps we can
03:56 - agree we can agree that our goal is to
04:01 - minimize this cost also known as maybe a
04:05 - loss we want to minimize that loss we
04:09 - want to have the lowest error we want
04:11 - the M and B values for the lowest error
04:14 - so we want to minimize this function now
04:17 - what does it mean to minimize a function
04:20 - so this function is something equals
04:24 - something school
04:25 - which is not that different from like me
04:28 - saying just for a moment like y equals x
04:30 - squared so if I were to take a Cartesian
04:34 - coordinate system and graph y equals x
04:39 - squared it would look something like
04:41 - this I'm drawing in purple now because
04:44 - I've stepped away from this notation and
04:48 - syntax for this particular scenario and
04:50 - I'm just talking about a function in
04:52 - general y equals x x squared you're
04:55 - going to also write this like f of X
04:57 - equals x squared
04:58 - but I'm graphing y equals x squared so
05:02 - what does it mean to find to minimize
05:06 - this function right I said I want to
05:08 - minimize the loss I want the smallest
05:10 - error
05:11 - I want the whatever line has the
05:13 - smallest error well what it means to
05:16 - minimize a function is that actually
05:18 - find the x value that produces the
05:22 - lowest y this is like the easiest thing
05:26 - in the world that we could ever possibly
05:27 - do right now you don't need any calculus
05:30 - fancy math or anything too fun to
05:32 - minimize this function is the lowest
05:36 - point zero its I could I could see it
05:39 - it's quite obvious so this is the thing
05:44 - eventually we're going to in in the
05:48 - machine learning systems that I'm going
05:50 - to get further into neural network based
05:52 - systems with many dimensions of data you
05:54 - know there might be some much more hard
05:57 - to describe crazy function that we're
05:59 - trying to approximate that it's much
06:01 - harder I mean of course we could eyeball
06:03 - this as well but as much part of to sort
06:05 - of mathematically just compute exactly
06:07 - where the minimum is especially if you
06:09 - imagine this as instead of a single line
06:13 - but a bowl and then what happens we can
06:15 - get into three dimensions and four
06:16 - dimensions and five dimensions things
06:18 - get kind of wonky but there is if we
06:23 - know the formula for this function there
06:26 - is another way that you can find that
06:29 - minimum that minimum minima minimum I
06:32 - don't know what your this and that is
06:38 - when I keep talking about gradient
06:40 - descent so let's think about what
06:44 - gradient descent means let's say we're
06:47 - looking at this point here and I'm gonna
06:53 - I'm gonna I'm oh I'm gonna walk along
06:56 - this function and I'm like I'm right
06:58 - here I'm like hello I'm looking for the
07:00 - minimum is it over there over there
07:02 - could you help me please keep these
07:04 - provide me can I use my like GPS Google
07:07 - Maps thing to find the minimum how would
07:09 - I find the minimum well if I'm right
07:11 - here
07:12 - I've got two options I could go this way
07:15 - or I could go this way and if I knew
07:19 - which direction I could go I could also
07:21 - say like I should take a big step or I
07:23 - should take a little step right there
07:26 - are all sorts of options so I need to
07:27 - know which way to go and how big of a
07:31 - step to take and there's a way to figure
07:33 - out how to do that and it's known as the
07:37 - derivative so the derivative is a term
07:40 - that comes from calculus and I would
07:44 - refer you to three blue one brown's
07:47 - calculus series or you can get a bit
07:49 - more background on how what the meaning
07:52 - of derivative is and how it works and
07:53 - how you can sort of think about these
07:55 - concepts from calculus but for us right
07:58 - now what we can think of is it's just
08:00 - the slope of the graph at this
08:04 - particular point and a way to describe
08:06 - that is like a tangent line to that
08:09 - graph so if I'm able to compute this
08:13 - line then I could say well this
08:18 - direction if I go this direction it's
08:20 - going up and I'm going away from the
08:22 - minimum if I go this direction I'm going
08:25 - down and I'm going towards the minimum
08:27 - so I want to go down and you can see
08:30 - like over here
08:31 - the slope is less extreme if I'm right
08:34 - here so maybe I don't need to go very
08:36 - far anymore
08:37 - but if I'm further up that slope is
08:39 - going to point much more this way oh I
08:41 - should take a bigger step down so this
08:43 - idea of being able to compute this slope
08:47 - this derivative of this function tells
08:49 - me how to search
08:50 - and find the bottom okay so this is the
08:53 - landscape of the puzzle we're trying to
08:57 - solve and pieces of that puzzle but what
09:00 - is the full what's what's the actual
09:01 - part of the code that I'm trying to give
09:03 - you more background on the actual part
09:06 - of the code that I'm trying to give you
09:07 - more background on is right over here so
09:11 - this is the gradient descent algorithm
09:14 - that I programmed in the previous video
09:16 - where what I did is we looked at every
09:20 - data point we made a guess we got the
09:23 - error the difference between the known
09:25 - output and the guess and then we
09:27 - adjusted the M and B values M equals so
09:32 - the idea here is that we want to say
09:33 - every firm as we're training the I don't
09:37 - know which color I'm using right now as
09:38 - we're training the system I want to say
09:42 - M equals M plus plus Delta M some change
09:48 - in n B equals B plus Delta B so I want
09:55 - to know what is a way that I could
09:58 - change the value of M in y equals MX
10:01 - plus B in order to make the error less
10:05 - the next step that I want to do is find
10:09 - the minimum cost I want to minimize this
10:13 - function for a particular I want to find
10:14 - the M and V values for the with the
10:17 - lowest error so to do that we've
10:19 - established that gradient descent says
10:21 - if I could find the derivative of a
10:23 - function I know which way to move to
10:24 - minimize it so somehow I need to find
10:26 - the derivative of this function to know
10:28 - which way to move okay so in order to do
10:31 - that though I'm going to have to rewrite
10:32 - this function in a different way
10:33 - so a couple things one is I think I made
10:36 - a mistake earlier where this should
10:39 - actually be done it's sort of doesn't
10:41 - matter but this should be on guess - why
10:44 - we were squaring it so in a way the
10:47 - positive negative doesn't matter but I
10:48 - think this is important for later so
10:50 - this should be guess - why that's
10:52 - technically the error is a guess - why
10:56 - not why I'm going to guess okay so I'm
10:58 - going to call the error function J and J
11:01 - is a function of M and B so I get
11:03 - something that sorry
11:04 - the error function office I'm about to
11:06 - call something else the error function
11:07 - the loss function the cost function J
11:09 - then I'm actually what I'm going to do
11:12 - is I'm going to say I'm going to just
11:13 - simplify this guess - why and I'm going
11:16 - to call that error I'm also going to
11:20 - take out the summation the summation is
11:21 - kind of important is but if this has to
11:23 - do with that stochastic versus batch
11:25 - gradient descent that I talked about in
11:27 - the previous video where I could I want
11:29 - to get the error over everything I just
11:31 - want to look at each error one at a time
11:32 - so let's simplify things that say we're
11:34 - looking at a chair one at a time so I'm
11:36 - going to now say this equals error
11:44 - squared so I have essentially rewritten
11:48 - this function and simplified it the cost
11:50 - J is equal to this error the guessed
11:53 - minus y squared so what I want to do is
11:58 - I want to find the derivatives of j
12:02 - relative to n i want to know how do I
12:08 - minimize J how how does J change when n
12:13 - changes dfj relative to n okay so in you
12:20 - know again I recommend that you go and
12:22 - check out some of the three blue one
12:24 - Brown calculus videos which will help
12:26 - give you more background here but what
12:31 - I'm actually going to need to do here is
12:33 - you use a use two rules from calculus
12:36 - I'm looking for another pen color for no
12:38 - reason I need to use the power rule that
12:46 - is one rule and I need to use the chain
12:49 - rule let me establish what the power
12:53 - rule is really quickly if I have a
12:55 - function like f of X equals x to the N
12:58 - the power rule says that the derivative
13:00 - is n times X to the N minus 1 so that's
13:07 - the power rule so I'm going to now apply
13:12 - that here and I'm going to say I don't
13:15 - know why I'm in purple now but I
13:17 - two times error to the first power so
13:22 - the power rule says now two times error
13:26 - okay but I also need the chain rule I'm
13:30 - not done
13:30 - why do I need the chain rule well the
13:32 - chain rule is a rule I'm going to erase
13:34 - this over here use another marker
13:37 - because somehow if I think these
13:39 - multiple colored markers all this will
13:41 - make sense the chain rule states who
13:44 - okay let's say I have a function like
13:47 - why can you reach you see this orange y
13:51 - equals x squared and I have a function
13:56 - like X equals Z squared so Y depends on
14:05 - X X depends on Z well what the chain
14:10 - rule says is if I want to get the
14:13 - derivative of Y relative to Z what I can
14:18 - do is I can get the derivative of Y
14:22 - relative to X to X and then multiply
14:26 - that by the derivative of X relative to
14:31 - Z which is then x 2z i i can change
14:37 - riveters I could get the derivative of 1
14:40 - relative to something times the
14:42 - derivative of that something relative to
14:44 - something else and that's actually
14:46 - weirdly what's going on here it may not
14:49 - be immediately apparent to you J is a
14:55 - function of error and error is a
14:58 - function of N and B because I'm
15:00 - computing the error as the guessed MX
15:03 - plus B minus a known Y so here I could
15:06 - then say get this derivative 2 times
15:09 - error and multiply that by the
15:13 - derivative of that error function itself
15:20 - relative to M because I'm trying to get
15:24 - Delta n now I could also also do it
15:27 - relative to B when I want
15:29 - to be and this has to do with a partial
15:31 - derivative we'll see there's so many
15:33 - concepts baked into this that are a lot
15:37 - maybe that again I'm sitting here being
15:39 - like this was all just a bad idea okay
15:41 - but what is this it is actually quite
15:45 - simple to work out and I'm going to do
15:48 - that for you right now I'm going to get
15:49 - the black marker and what I'm going to
15:51 - do is now I want the derivative of error
15:53 - relative to end okay well what is this
15:55 - actual if I unpack this function guess
15:57 - is M X plus B minus y error equals this
16:11 - so when I say partial derivative means
16:14 - like the derivative relative to M what I
16:18 - mean is everything else is a constant X
16:20 - is a constant B is a constant Y is a
16:22 - constant
16:23 - I mean x and y are actually already
16:25 - constants because those are the things
16:26 - that X is the input data Y is the known
16:30 - output result so this really I should
16:34 - write this as like x times M plus B
16:38 - minus y so this the derivative of this
16:42 - right the power rule says 1 times x
16:47 - times m to the 0 power which means x and
16:53 - the derivative of a constant is 0
16:55 - because the constant doesn't change
16:57 - right derivative describing how
16:59 - something changes the derivative of this
17:01 - is there so guess what it's just X
17:04 - meaning this whole thing turns out to
17:06 - just be x equals 2 times the error times
17:12 - X and guess what this - we're going up
17:16 - the whole point is if you watch the
17:17 - previous video is we're going to take
17:19 - this and multiply it by something called
17:21 - a learning rate because we want it to we
17:24 - want to like we know the direction to go
17:26 - this is giving us the direction to go to
17:28 - minimize that error minimize that cost
17:31 - but do I want to take a big step or a
17:33 - little step well if I'm going to
17:35 - multiply it by a learning rate anyway
17:36 - it's sort of like this - as no point
17:38 - I could have a learning rate that's
17:40 - twice as big or half as big so
17:42 - ultimately this is all it is
17:44 - air times X all of this math and
17:48 - craziness with powerful wood chain rule
17:50 - and partial derivative bits it all boys
17:53 - come to just finally we get this error
17:55 - times X that's what should go here in
17:59 - Delta M guess what let's go back over to
18:02 - our code and we can see there it is
18:06 - error times X air times X there we go
18:11 - that's it that's why that says error
18:14 - times X no it looks that was a lot
18:17 - that's why successes I feel so happy
18:19 - that we kind of even though it was not
18:21 - the best explanation and there's lots of
18:22 - confusing this in pieces I feel very
18:24 - happy to have arrived there this was
18:26 - useful for me just making this video
18:27 - makes me feel like something happened
18:30 - today okay so um two things I want to
18:34 - mention couple things I want to mention
18:35 - here a way that I can make this make a
18:39 - little bit more sense here although just
18:41 - to clarify this chain rule thing a
18:42 - little bit better thank you to K week
18:44 - mine in the black channel is that I
18:49 - could just to see here I'm what I'm
18:50 - looking for is the derivative of the
18:53 - cost function
18:54 - you know relative to M what happens when
18:56 - I change the end value what does that do
18:59 - to the cost and the chain rule says that
19:01 - if I look at the derivative of that
19:03 - function relative to the error I can
19:09 - multiply that by the derivative of the
19:11 - error relative to M right so this is
19:16 - actually the chain rule so I can get
19:18 - this by doing the derivative of relative
19:21 - to error the derivative error relative
19:23 - to m and that's what's going on here two
19:25 - times error times this and that's where
19:28 - I'm getting all this stuff okay so this
19:30 - is one way of looking at this and you
19:31 - can see like oh yeah it's kind of like
19:32 - the numerator and denominator cancel
19:34 - each other out so that makes sense the
19:35 - other thing is if I did this whole thing
19:38 - again
19:39 - but did the derivative down here
19:40 - relative to B right B instead of M what
19:46 - do I get here well I get this is now a
19:49 - constant so
19:50 - this becomes zero this is a constant
19:52 - this becomes zero and what is this I
19:54 - take the power rule so I take 1 times B
19:57 - to the zero I just get one so this
20:00 - becomes a error times rather than times
20:05 - points look at this mess that I wrote
20:07 - here can i we please end this video with
20:09 - this at least written it was a very nice
20:11 - handwriting so when it's relative to M
20:16 - this was 2 times error times X but when
20:24 - it's relative to B that's when it's
20:26 - relative to n but when it's relative to
20:28 - B it's 2 times error times 1 and again
20:32 - we could get rid of the 2 so it's really
20:33 - just error times X or error times 1 and
20:37 - then if I come back over here again
20:40 - there you go
20:41 - error times X M changes by era times xB
20:44 - changes by just error oh so that
20:56 - hopefully gives you some more background
20:58 - as to why these formulas exist this way
21:02 - and which chat and as I go forward in
21:09 - session for what comes after this is now
21:11 - session 4 where I'm going to build a
21:13 - neural network model for learning you're
21:16 - going to see this formula over and over
21:18 - again change the weight instead of st.
21:21 - and then B I'm going to say the weight
21:22 - well the weight changes based on the
21:25 - input multiplied by the error and then
21:28 - there's going to be a lot of other
21:29 - pieces of but this formula is going to
21:31 - be everywhere so I hope this is another
21:35 - attempt again did you know there's a lot
21:38 - of things I've glossed over here in
21:40 - terms of a lot of the background in
21:43 - terms of you know what really is a
21:44 - derivative why does calculus exist why
21:47 - is the chain rule work the way it works
21:49 - why is the power rule work the way it
21:50 - works what why what what move that
21:52 - partial derivative Han did you say that
21:54 - partial derivative and so again take a
21:56 - look at this video's description and I'm
21:57 - going to point you towards resources and
21:59 - tutorials that kind of dive into each of
22:01 - those components
22:02 - a bit more deeply but hopefully this
22:03 - gives you some semblance of that overall
22:06 - picture okay thanks for watching and uh
22:08 - I don't know maybe maybe you want to get
22:12 - like or subscribe to the honestly
22:13 - totally totally understand totally
22:16 - totally understand don't you get the
22:18 - thumbs down I get it again ok I'll see
22:20 - you in a future video maybe ok goodbye
22:26 - [Music]

Cleaned transcript:

hello it's me coming to you again from the future you might recognize me some from my fails videos as a calculus partial derivative hey I made some videos with some calculus stuffs they can turn out very well you can find them if you want they're kind of unlisted now but I just I tried again and so this video if you're watching it this is a followup to my linear regression with gradient descent video that video stands alone it's a programming video where all I do is walk through with the code for how to build an example that demonstrates linear regression with gradient descent and this is a puzzle piece in my machine learning series that will hopefully act as a foundation the building blocks your understanding of hopefully some more creative or practical examples that will come later this video that if you're watching it's totally optional to watch as part of this series because you just applied the formula but what I try to do in this video is give some backgrounds and I kind of worked it all out here this is the end this is what's on the whiteboard I thought so now if I use multiple colored markers would somehow make a better video I don't think I really did that but so I kind of walk through and try to describe the map I should say that you know is this involves topics from calculus and there's a great video series by three blue one brown on YouTube that gives you great background and more depth in calculus so I'll put links to those videos in this video's description honestly if you're really interested in kind of soaking up as much as this as you can I would go watch those videos first and then come back here it'll give you that background for understanding the pieces that I've done here so I look forward to your feedback positive and negative constructive feedback into whether this was helpful if it made sense and if you then go on and keep watching there'll be some future videos well be getting back into the code there's no code in this video just a math stuff next max okay I enjoy so to recap I have a bunch of data points in 2d space I have a line in that 2d space the formula for that line is y equals MX plus B when I try to make a prediction right I get a piece of input of data input X and from there I try to make again in addition to the guests I have the known Y so this is the correct data that goes with X my machine learning system makes a guess the error is the difference between those two things the error is y the correct answer the guess so this relates to the idea of a cost function a loss function so if we want to evaluate how is our machine learning algorithm performing we have this large data set maybe it has n elements so what we want to do is from 1 to n for all n elements we want to minimize that error so the cost function cost equals the sum of Y sub I every node answer the guest sub I squared so this is the formula this is a cost known as a cost function this is the total error for the particular in the particular model being the current M and B values that describe this particular line this is the error so perhaps we can agree we can agree that our goal is to minimize this cost also known as maybe a loss we want to minimize that loss we want to have the lowest error we want the M and B values for the lowest error so we want to minimize this function now what does it mean to minimize a function so this function is something equals something school which is not that different from like me saying just for a moment like y equals x squared so if I were to take a Cartesian coordinate system and graph y equals x squared it would look something like this I'm drawing in purple now because I've stepped away from this notation and syntax for this particular scenario and I'm just talking about a function in general y equals x x squared you're going to also write this like f of X equals x squared but I'm graphing y equals x squared so what does it mean to find to minimize this function right I said I want to minimize the loss I want the smallest error I want the whatever line has the smallest error well what it means to minimize a function is that actually find the x value that produces the lowest y this is like the easiest thing in the world that we could ever possibly do right now you don't need any calculus fancy math or anything too fun to minimize this function is the lowest point zero its I could I could see it it's quite obvious so this is the thing eventually we're going to in in the machine learning systems that I'm going to get further into neural network based systems with many dimensions of data you know there might be some much more hard to describe crazy function that we're trying to approximate that it's much harder I mean of course we could eyeball this as well but as much part of to sort of mathematically just compute exactly where the minimum is especially if you imagine this as instead of a single line but a bowl and then what happens we can get into three dimensions and four dimensions and five dimensions things get kind of wonky but there is if we know the formula for this function there is another way that you can find that minimum that minimum minima minimum I don't know what your this and that is when I keep talking about gradient descent so let's think about what gradient descent means let's say we're looking at this point here and I'm gonna I'm gonna I'm oh I'm gonna walk along this function and I'm like I'm right here I'm like hello I'm looking for the minimum is it over there over there could you help me please keep these provide me can I use my like GPS Google Maps thing to find the minimum how would I find the minimum well if I'm right here I've got two options I could go this way or I could go this way and if I knew which direction I could go I could also say like I should take a big step or I should take a little step right there are all sorts of options so I need to know which way to go and how big of a step to take and there's a way to figure out how to do that and it's known as the derivative so the derivative is a term that comes from calculus and I would refer you to three blue one brown's calculus series or you can get a bit more background on how what the meaning of derivative is and how it works and how you can sort of think about these concepts from calculus but for us right now what we can think of is it's just the slope of the graph at this particular point and a way to describe that is like a tangent line to that graph so if I'm able to compute this line then I could say well this direction if I go this direction it's going up and I'm going away from the minimum if I go this direction I'm going down and I'm going towards the minimum so I want to go down and you can see like over here the slope is less extreme if I'm right here so maybe I don't need to go very far anymore but if I'm further up that slope is going to point much more this way oh I should take a bigger step down so this idea of being able to compute this slope this derivative of this function tells me how to search and find the bottom okay so this is the landscape of the puzzle we're trying to solve and pieces of that puzzle but what is the full what's what's the actual part of the code that I'm trying to give you more background on the actual part of the code that I'm trying to give you more background on is right over here so this is the gradient descent algorithm that I programmed in the previous video where what I did is we looked at every data point we made a guess we got the error the difference between the known output and the guess and then we adjusted the M and B values M equals so the idea here is that we want to say every firm as we're training the I don't know which color I'm using right now as we're training the system I want to say M equals M plus plus Delta M some change in n B equals B plus Delta B so I want to know what is a way that I could change the value of M in y equals MX plus B in order to make the error less the next step that I want to do is find the minimum cost I want to minimize this function for a particular I want to find the M and V values for the with the lowest error so to do that we've established that gradient descent says if I could find the derivative of a function I know which way to move to minimize it so somehow I need to find the derivative of this function to know which way to move okay so in order to do that though I'm going to have to rewrite this function in a different way so a couple things one is I think I made a mistake earlier where this should actually be done it's sort of doesn't matter but this should be on guess why we were squaring it so in a way the positive negative doesn't matter but I think this is important for later so this should be guess why that's technically the error is a guess why not why I'm going to guess okay so I'm going to call the error function J and J is a function of M and B so I get something that sorry the error function office I'm about to call something else the error function the loss function the cost function J then I'm actually what I'm going to do is I'm going to say I'm going to just simplify this guess why and I'm going to call that error I'm also going to take out the summation the summation is kind of important is but if this has to do with that stochastic versus batch gradient descent that I talked about in the previous video where I could I want to get the error over everything I just want to look at each error one at a time so let's simplify things that say we're looking at a chair one at a time so I'm going to now say this equals error squared so I have essentially rewritten this function and simplified it the cost J is equal to this error the guessed minus y squared so what I want to do is I want to find the derivatives of j relative to n i want to know how do I minimize J how how does J change when n changes dfj relative to n okay so in you know again I recommend that you go and check out some of the three blue one Brown calculus videos which will help give you more background here but what I'm actually going to need to do here is you use a use two rules from calculus I'm looking for another pen color for no reason I need to use the power rule that is one rule and I need to use the chain rule let me establish what the power rule is really quickly if I have a function like f of X equals x to the N the power rule says that the derivative is n times X to the N minus 1 so that's the power rule so I'm going to now apply that here and I'm going to say I don't know why I'm in purple now but I two times error to the first power so the power rule says now two times error okay but I also need the chain rule I'm not done why do I need the chain rule well the chain rule is a rule I'm going to erase this over here use another marker because somehow if I think these multiple colored markers all this will make sense the chain rule states who okay let's say I have a function like why can you reach you see this orange y equals x squared and I have a function like X equals Z squared so Y depends on X X depends on Z well what the chain rule says is if I want to get the derivative of Y relative to Z what I can do is I can get the derivative of Y relative to X to X and then multiply that by the derivative of X relative to Z which is then x 2z i i can change riveters I could get the derivative of 1 relative to something times the derivative of that something relative to something else and that's actually weirdly what's going on here it may not be immediately apparent to you J is a function of error and error is a function of N and B because I'm computing the error as the guessed MX plus B minus a known Y so here I could then say get this derivative 2 times error and multiply that by the derivative of that error function itself relative to M because I'm trying to get Delta n now I could also also do it relative to B when I want to be and this has to do with a partial derivative we'll see there's so many concepts baked into this that are a lot maybe that again I'm sitting here being like this was all just a bad idea okay but what is this it is actually quite simple to work out and I'm going to do that for you right now I'm going to get the black marker and what I'm going to do is now I want the derivative of error relative to end okay well what is this actual if I unpack this function guess is M X plus B minus y error equals this so when I say partial derivative means like the derivative relative to M what I mean is everything else is a constant X is a constant B is a constant Y is a constant I mean x and y are actually already constants because those are the things that X is the input data Y is the known output result so this really I should write this as like x times M plus B minus y so this the derivative of this right the power rule says 1 times x times m to the 0 power which means x and the derivative of a constant is 0 because the constant doesn't change right derivative describing how something changes the derivative of this is there so guess what it's just X meaning this whole thing turns out to just be x equals 2 times the error times X and guess what this we're going up the whole point is if you watch the previous video is we're going to take this and multiply it by something called a learning rate because we want it to we want to like we know the direction to go this is giving us the direction to go to minimize that error minimize that cost but do I want to take a big step or a little step well if I'm going to multiply it by a learning rate anyway it's sort of like this as no point I could have a learning rate that's twice as big or half as big so ultimately this is all it is air times X all of this math and craziness with powerful wood chain rule and partial derivative bits it all boys come to just finally we get this error times X that's what should go here in Delta M guess what let's go back over to our code and we can see there it is error times X air times X there we go that's it that's why that says error times X no it looks that was a lot that's why successes I feel so happy that we kind of even though it was not the best explanation and there's lots of confusing this in pieces I feel very happy to have arrived there this was useful for me just making this video makes me feel like something happened today okay so um two things I want to mention couple things I want to mention here a way that I can make this make a little bit more sense here although just to clarify this chain rule thing a little bit better thank you to K week mine in the black channel is that I could just to see here I'm what I'm looking for is the derivative of the cost function you know relative to M what happens when I change the end value what does that do to the cost and the chain rule says that if I look at the derivative of that function relative to the error I can multiply that by the derivative of the error relative to M right so this is actually the chain rule so I can get this by doing the derivative of relative to error the derivative error relative to m and that's what's going on here two times error times this and that's where I'm getting all this stuff okay so this is one way of looking at this and you can see like oh yeah it's kind of like the numerator and denominator cancel each other out so that makes sense the other thing is if I did this whole thing again but did the derivative down here relative to B right B instead of M what do I get here well I get this is now a constant so this becomes zero this is a constant this becomes zero and what is this I take the power rule so I take 1 times B to the zero I just get one so this becomes a error times rather than times points look at this mess that I wrote here can i we please end this video with this at least written it was a very nice handwriting so when it's relative to M this was 2 times error times X but when it's relative to B that's when it's relative to n but when it's relative to B it's 2 times error times 1 and again we could get rid of the 2 so it's really just error times X or error times 1 and then if I come back over here again there you go error times X M changes by era times xB changes by just error oh so that hopefully gives you some more background as to why these formulas exist this way and which chat and as I go forward in session for what comes after this is now session 4 where I'm going to build a neural network model for learning you're going to see this formula over and over again change the weight instead of st. and then B I'm going to say the weight well the weight changes based on the input multiplied by the error and then there's going to be a lot of other pieces of but this formula is going to be everywhere so I hope this is another attempt again did you know there's a lot of things I've glossed over here in terms of a lot of the background in terms of you know what really is a derivative why does calculus exist why is the chain rule work the way it works why is the power rule work the way it works what why what what move that partial derivative Han did you say that partial derivative and so again take a look at this video's description and I'm going to point you towards resources and tutorials that kind of dive into each of those components a bit more deeply but hopefully this gives you some semblance of that overall picture okay thanks for watching and uh I don't know maybe maybe you want to get like or subscribe to the honestly totally totally understand totally totally understand don't you get the thumbs down I get it again ok I'll see you in a future video maybe ok goodbye
