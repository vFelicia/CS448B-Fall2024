With timestamps:

00:00 - okay I'm here I am back again oh I'm so
00:03 - close to the end of this now I'm sure
00:06 - there's a lot of mistakes I mean so I
00:07 - made a really an error here which is I
00:10 - didn't even try to run my code and there
00:12 - was a big there's a big typo here
00:13 - already which is that this should be the
00:16 - is this good this might actually even
00:18 - make weights like I think I might not be
00:20 - like being careful about my right these
00:22 - these that's weights plural so that's
00:24 - totally wrong weights plural and this is
00:28 - not this is input to hidden and this is
00:31 - also input to hidden Delta's so that I
00:34 - should have said and I wonder if I made
00:37 - that mistake here like this stock
00:38 - weights that should be weights I got
00:40 - this right so let's try let's try
00:42 - running the code you know I have no idea
00:46 - we put you could put odds on whether
00:47 - there's any errors I would put very
00:49 - things I give myself like fifty to one
00:51 - that there's no errors so let's go and
00:54 - just because if you recall I have in the
00:58 - sketch like I'm setting myself up for a
01:00 - very simple scenario of two inputs two
01:05 - inputs two hidden nodes and two outputs
01:08 - lying 99 typos so I guess I should check
01:12 - there might even be a typo in line 99 oh
01:14 - that's a comment so I don't know what
01:16 - that is that alright alright let's just
01:18 - let's just hit refresh here identify our
01:21 - inputs has already been declared neural
01:23 - network digest line 59 so let's see what
01:27 - that is oh I forgot so I don't oh yeah
01:33 - oh this interesting so this should
01:38 - probably be called input array like I
01:41 - did with feed-forward and in that sense
01:43 - this should probably be target array
01:45 - because and then targets equals matrix
01:50 - from array target array so let's just do
01:52 - that so that's important because I'm
01:54 - letting the end-user pass in the inputs
01:56 - and targets as simple arrays and
01:58 - internally in the library I'm converting
02:00 - those two matrix objects targets is not
02:04 - defined a neural network line 71 oh and
02:07 - there so this has to be let target's
02:08 - because it's a new variable here
02:11 - there's no errors okay weird all right
02:15 - so let's move on now this is really
02:17 - tricky I probably could have been much
02:19 - more thoughtful about how I'm doing this
02:20 - I'm but here's the thing I now need to
02:24 - add the Delta BOTS the deltas for the
02:26 - biases now if you've been following
02:27 - along here's how I've here's how I've
02:30 - connected all the stuff I made a video a
02:31 - long while back oh yesteryears of days
02:34 - when it was just a one-dimensional y
02:36 - equals MX plus B I made a video about
02:39 - gradient descent where Delta M is the
02:41 - learning rate times error times X and
02:43 - Delta B is just the learning rate times
02:45 - error well this is the analogous
02:46 - situation with matrices learning rate
02:48 - times error this gradient so to speak is
02:50 - this whole thing over here and the X is
02:54 - this thing over here and the same thing
02:55 - here this is the Delta the Delta I mean
02:59 - sorry the gradient which is this times
03:01 - the inputs which is X so do the deltas
03:05 - for the bias YZ is actually not a matrix
03:06 - right the by C's is a one column vector
03:08 - basically I want me one column matrix
03:10 - which is a vector and so the delta bias
03:13 - these is actually just this part this
03:17 - part and this part and guess what I have
03:20 - already calculated those things so if
03:24 - you look at that here oops if I go back
03:25 - to my code right this is before where
03:30 - where is it right here where am i odd
03:33 - gradients right right here I am passing
03:36 - the outputs through the derivative I am
03:40 - multiplying element-wise with the errors
03:42 - and the learning rate this and then I
03:44 - have to do the transpose and get but
03:46 - this is it these gradients I could just
03:48 - say bias sees this dot bias and where am
03:53 - I output dot ad gradients so let me
03:59 - actually will let me actually put this
04:00 - oh where did I go let me put this here
04:05 - so this is adjust the weights by deltas
04:13 - and now adjust the bias by its deltas
04:19 - which is just the gradients
04:23 - okay and then I should be able to do
04:26 - exactly the same thing with down here
04:32 - oops this will come after and what I
04:35 - want to do is adjust the hidden bias
04:38 - with the hidden gradient now one thing
04:43 - that's probably inconsistent maybe I'm
04:45 - just gonna kind of leave it but if
04:47 - anybody as you this is a hoping to put
04:50 - all this code in a repo which is already
04:54 - there she's are called like toy neural
04:55 - network and so one thing I probably
04:57 - should be consistent is when I'm saying
04:59 - like weights versus weight or gradient
05:01 - versus gradients so that can be cleaned
05:03 - up later we just see if I have any
05:05 - errors no errors all right dare I do
05:10 - something next okay so here's the thing
05:13 - I mentioned before now how you collect
05:18 - and prepare your data is so important in
05:24 - terms of the ethics of what you're doing
05:26 - the scientific accuracy of what you're
05:29 - doing and I'm kind of glossing all over
05:32 - that just to make this toy neural
05:34 - network library but beyond just sort of
05:36 - like being thoughtful about collecting
05:38 - your data we've got to figure out like
05:39 - how do I even like do this and so there
05:42 - are a variety of techniques in terms of
05:45 - calculating the error over time and
05:48 - batches and then adjusting the weights
05:51 - versus but I think what I'm going to do
05:52 - it's called stochastic gradient descent
05:55 - let's Google that but let's just make
05:58 - sure I've got the right term stochastic
06:01 - gradient descent is known as incremental
06:04 - this is a iterative method for
06:07 - minimizing object that's written as a
06:08 - symbol so I think I'm correct in that
06:11 - what I'm doing is in other words what
06:13 - I'm gonna do is stochastic gradient
06:14 - descent which I think is what I did with
06:16 - my perceptron example and my linear
06:17 - regression example is that for every
06:19 - single record every single data point
06:22 - I'm gonna pass it in calculate the error
06:24 - back propagated and adjust one at a time
06:28 - instead of doing batches that you're but
06:31 - but be aware of this idea of batches
06:32 - because that's a core concept as you
06:34 - start to use other people's like real
06:35 - actual
06:36 - working deep learning libraries and
06:39 - examples and that sort of thing so I'd
06:41 - let us do this stochastic idea which but
06:44 - one of the reasons why I want to do that
06:45 - I don't know why switched over there one
06:47 - of the reasons why I want to do this
06:48 - stochastic idea is it's basically
06:50 - already what I've done so this training
06:52 - function simply takes a single set of
06:55 - inputs and a single set of outputs
06:56 - targets does all that it needs to do to
07:00 - it adjust everything and finishes off
07:03 - let's do that I don't let's let's try oh
07:07 - my goodness
07:07 - ah so let's let's prepare a dataset I'm
07:10 - gonna do this okay so I like I'm like
07:13 - terrified to erase this I have to let me
07:18 - take a moment and erase this okay so
07:23 - here's the architecture I'm going to use
07:24 - I'm gonna have an input input layer I'm
07:26 - just gonna say layer two inputs I'm
07:29 - gonna have a hidden layer with two to
07:34 - two nodes then I'm going to have an
07:37 - output with just one this is a nice
07:41 - architecture for trying to solve the XOR
07:44 - problem exclusive or I just want the
07:46 - simplest thing just to kind into some
07:48 - way of kind of debugging and validating
07:50 - that's something in my code is doing it
07:52 - correctly so what I'm going to do is so
07:56 - this it's going to look like this
08:01 - and so for my data set width is going to
08:06 - be this following 1 comma 0 gives me a 1
08:15 - 0 comma 1 gives me a 1 1 comma 1 gives
08:23 - me a 0 and 0 comma 0 gives me a 0 so
08:30 - this is the classic non linearly
08:32 - separable problem and I discussed with
08:34 - the perceptron a single perceptron can't
08:37 - do it this is now a multi-layered
08:39 - perceptron with graty's assented back
08:41 - propagation in that code so I should be
08:44 - able to continuously feed it this
08:46 - training data set now I said
08:49 - you need a training data set and a test
08:51 - data set but this is like such a
08:53 - simplistic problem there's only two four
08:55 - possibilities and we interesting to look
08:57 - at
08:57 - you know visualizing it and letting
08:58 - these be continuous floating point
08:59 - values but let's look let me just do
09:02 - whatever so I'm gonna put into my code
09:04 - the training data so I'm gonna say I'm
09:09 - gonna say let training data equal and I
09:17 - have it be an array and each element of
09:19 - the array is gonna be an object inputs 0
09:23 - 0 targets 1 so I'm kind of I could put
09:29 - this in a JSON file or a spreadsheet but
09:31 - I'm gonna just do it like this
09:32 - hard-coded in just to make the point
09:35 - right and so now I have own okay so 0 1
09:40 - 1 0 0 0 is 0 0 0 is 1 1 is 0 ok so oh
09:49 - dear oh this should be a : ah silly
09:52 - syntax error that I then copy/paste
09:54 - everywhere okay so this is now my
09:58 - training data it is an array with
10:02 - objects so now what I need to do is I am
10:05 - going to say 2 2 1 that's my neural
10:12 - network and I'm going to say let's see
10:17 - what do I need to do for data of
10:22 - training data that's a nice little loop
10:25 - through everything that's in here I'm
10:27 - gonna say neural network trained data
10:31 - inputs data targets and I probably I'm
10:39 - just gonna you know I could pick it
10:40 - randomly I'm gonna go through the data
10:41 - and I'm gonna suit it like I'm gonna do
10:45 - it like a hundred times
10:46 - in the same order which is probably a
10:49 - problem I should probably randomize the
10:50 - order let's just do it this way and see
10:53 - what's going on so let's do that and
10:56 - then I'm going to test things by saying
11:02 - guests
11:03 - equals neural network feed-forward zero
11:08 - zero and actually you know I'm just
11:13 - going to do I'm going to say neural
11:14 - network fee for at zero zero print so
11:19 - this should give me everything in the
11:21 - console that I want so this is I haven't
11:26 - been to a mic being careful enough this
11:29 - is the inputs with the target this is
11:30 - the inputs with the target does the
11:31 - inputs the target isn't as a target I'm
11:33 - just going to train it with that like
11:34 - 100 times in the exact same order which
11:36 - is probably a terrible idea with
11:37 - stochastic gradient descent and then I'm
11:39 - just going to call feed for and I think
11:40 - I still have in my matrix library this
11:42 - print function which just like console
11:44 - dot tables the stuff out all right
11:47 - I don't know what could possibly go
11:50 - wrong what could possibly go wrong
11:56 - feed-forward print is not a function all
11:59 - right
11:59 - neural network oh you know what it gives
12:05 - me a nice little array I forgot I don't
12:07 - need to do this I could just consult I
12:09 - forgot that the library itself gives me
12:11 - a nice little array so let's just
12:12 - console.log I don't need that print
12:16 - thing so let's try this that doesn't
12:28 - look very good right I should be getting
12:31 - one one zero zero like or close to it
12:34 - let's train it like so let's try
12:37 - training it like 10,000 times
12:40 - hey this is maybe interestingly sort of
12:44 - better one zero zero one one one zero
12:51 - zero so I'm definitely feeding in the
12:53 - all the proper inputs is my training
12:56 - data correct
12:58 - 0 1 gives 1 1 0 is 1 0 0 gives 0 1 1 so
13:03 - I think I really need to randomize the
13:04 - order right I've really got to randomize
13:06 - the order
13:07 - so let's randomize the order and in fact
13:10 - what I'm gonna do
13:11 - forget about even randomizing the order
13:12 - I'm just gonna always pick a random one
13:14 - so I'm just gonna say
13:17 - let data and p5 as a nice function if I
13:20 - just give it random training data it's
13:23 - gonna give me a random one oh the
13:25 - learning rate is something I actually
13:26 - should really be careful about in point
13:28 - one so that's probably something I need
13:30 - to be more thoughtful about so let me so
13:33 - let me do it fifty thousand times and
13:35 - let's see what happens in a random order
13:40 - so interestingly enough so I want to do
13:49 - this as a coding challenge I want to
13:52 - actually write an example that sort of
13:55 - like animates and visualizes it as it's
13:57 - learning I'm saying I have can't believe
14:01 - this worked oh my god I can't believe I
14:04 - arrived I mean I'm sure there's like
14:05 - problems and it's the but at least it
14:07 - worked for this simple problem and but
14:11 - so somebody said I had a typo in the
14:15 - train function well I'm sure I do
14:19 - don't I'm so happy right now
14:21 - though so I need to do coding challenge
14:23 - which is actually the XOR problem and
14:27 - also animated as I'm going so I think I
14:29 - would essentially do the same thing but
14:31 - draw sort of like a pixel space and
14:34 - actually iterate over it and I should
14:36 - see it you know the corners would be the
14:38 - the full boolean that I'll explain this
14:40 - when I do the coding challenge I can't
14:41 - even think anymore just got this to work
14:48 - [Music]

Cleaned transcript:

okay I'm here I am back again oh I'm so close to the end of this now I'm sure there's a lot of mistakes I mean so I made a really an error here which is I didn't even try to run my code and there was a big there's a big typo here already which is that this should be the is this good this might actually even make weights like I think I might not be like being careful about my right these these that's weights plural so that's totally wrong weights plural and this is not this is input to hidden and this is also input to hidden Delta's so that I should have said and I wonder if I made that mistake here like this stock weights that should be weights I got this right so let's try let's try running the code you know I have no idea we put you could put odds on whether there's any errors I would put very things I give myself like fifty to one that there's no errors so let's go and just because if you recall I have in the sketch like I'm setting myself up for a very simple scenario of two inputs two inputs two hidden nodes and two outputs lying 99 typos so I guess I should check there might even be a typo in line 99 oh that's a comment so I don't know what that is that alright alright let's just let's just hit refresh here identify our inputs has already been declared neural network digest line 59 so let's see what that is oh I forgot so I don't oh yeah oh this interesting so this should probably be called input array like I did with feedforward and in that sense this should probably be target array because and then targets equals matrix from array target array so let's just do that so that's important because I'm letting the enduser pass in the inputs and targets as simple arrays and internally in the library I'm converting those two matrix objects targets is not defined a neural network line 71 oh and there so this has to be let target's because it's a new variable here there's no errors okay weird all right so let's move on now this is really tricky I probably could have been much more thoughtful about how I'm doing this I'm but here's the thing I now need to add the Delta BOTS the deltas for the biases now if you've been following along here's how I've here's how I've connected all the stuff I made a video a long while back oh yesteryears of days when it was just a onedimensional y equals MX plus B I made a video about gradient descent where Delta M is the learning rate times error times X and Delta B is just the learning rate times error well this is the analogous situation with matrices learning rate times error this gradient so to speak is this whole thing over here and the X is this thing over here and the same thing here this is the Delta the Delta I mean sorry the gradient which is this times the inputs which is X so do the deltas for the bias YZ is actually not a matrix right the by C's is a one column vector basically I want me one column matrix which is a vector and so the delta bias these is actually just this part this part and this part and guess what I have already calculated those things so if you look at that here oops if I go back to my code right this is before where where is it right here where am i odd gradients right right here I am passing the outputs through the derivative I am multiplying elementwise with the errors and the learning rate this and then I have to do the transpose and get but this is it these gradients I could just say bias sees this dot bias and where am I output dot ad gradients so let me actually will let me actually put this oh where did I go let me put this here so this is adjust the weights by deltas and now adjust the bias by its deltas which is just the gradients okay and then I should be able to do exactly the same thing with down here oops this will come after and what I want to do is adjust the hidden bias with the hidden gradient now one thing that's probably inconsistent maybe I'm just gonna kind of leave it but if anybody as you this is a hoping to put all this code in a repo which is already there she's are called like toy neural network and so one thing I probably should be consistent is when I'm saying like weights versus weight or gradient versus gradients so that can be cleaned up later we just see if I have any errors no errors all right dare I do something next okay so here's the thing I mentioned before now how you collect and prepare your data is so important in terms of the ethics of what you're doing the scientific accuracy of what you're doing and I'm kind of glossing all over that just to make this toy neural network library but beyond just sort of like being thoughtful about collecting your data we've got to figure out like how do I even like do this and so there are a variety of techniques in terms of calculating the error over time and batches and then adjusting the weights versus but I think what I'm going to do it's called stochastic gradient descent let's Google that but let's just make sure I've got the right term stochastic gradient descent is known as incremental this is a iterative method for minimizing object that's written as a symbol so I think I'm correct in that what I'm doing is in other words what I'm gonna do is stochastic gradient descent which I think is what I did with my perceptron example and my linear regression example is that for every single record every single data point I'm gonna pass it in calculate the error back propagated and adjust one at a time instead of doing batches that you're but but be aware of this idea of batches because that's a core concept as you start to use other people's like real actual working deep learning libraries and examples and that sort of thing so I'd let us do this stochastic idea which but one of the reasons why I want to do that I don't know why switched over there one of the reasons why I want to do this stochastic idea is it's basically already what I've done so this training function simply takes a single set of inputs and a single set of outputs targets does all that it needs to do to it adjust everything and finishes off let's do that I don't let's let's try oh my goodness ah so let's let's prepare a dataset I'm gonna do this okay so I like I'm like terrified to erase this I have to let me take a moment and erase this okay so here's the architecture I'm going to use I'm gonna have an input input layer I'm just gonna say layer two inputs I'm gonna have a hidden layer with two to two nodes then I'm going to have an output with just one this is a nice architecture for trying to solve the XOR problem exclusive or I just want the simplest thing just to kind into some way of kind of debugging and validating that's something in my code is doing it correctly so what I'm going to do is so this it's going to look like this and so for my data set width is going to be this following 1 comma 0 gives me a 1 0 comma 1 gives me a 1 1 comma 1 gives me a 0 and 0 comma 0 gives me a 0 so this is the classic non linearly separable problem and I discussed with the perceptron a single perceptron can't do it this is now a multilayered perceptron with graty's assented back propagation in that code so I should be able to continuously feed it this training data set now I said you need a training data set and a test data set but this is like such a simplistic problem there's only two four possibilities and we interesting to look at you know visualizing it and letting these be continuous floating point values but let's look let me just do whatever so I'm gonna put into my code the training data so I'm gonna say I'm gonna say let training data equal and I have it be an array and each element of the array is gonna be an object inputs 0 0 targets 1 so I'm kind of I could put this in a JSON file or a spreadsheet but I'm gonna just do it like this hardcoded in just to make the point right and so now I have own okay so 0 1 1 0 0 0 is 0 0 0 is 1 1 is 0 ok so oh dear oh this should be a ah silly syntax error that I then copy/paste everywhere okay so this is now my training data it is an array with objects so now what I need to do is I am going to say 2 2 1 that's my neural network and I'm going to say let's see what do I need to do for data of training data that's a nice little loop through everything that's in here I'm gonna say neural network trained data inputs data targets and I probably I'm just gonna you know I could pick it randomly I'm gonna go through the data and I'm gonna suit it like I'm gonna do it like a hundred times in the same order which is probably a problem I should probably randomize the order let's just do it this way and see what's going on so let's do that and then I'm going to test things by saying guests equals neural network feedforward zero zero and actually you know I'm just going to do I'm going to say neural network fee for at zero zero print so this should give me everything in the console that I want so this is I haven't been to a mic being careful enough this is the inputs with the target this is the inputs with the target does the inputs the target isn't as a target I'm just going to train it with that like 100 times in the exact same order which is probably a terrible idea with stochastic gradient descent and then I'm just going to call feed for and I think I still have in my matrix library this print function which just like console dot tables the stuff out all right I don't know what could possibly go wrong what could possibly go wrong feedforward print is not a function all right neural network oh you know what it gives me a nice little array I forgot I don't need to do this I could just consult I forgot that the library itself gives me a nice little array so let's just console.log I don't need that print thing so let's try this that doesn't look very good right I should be getting one one zero zero like or close to it let's train it like so let's try training it like 10,000 times hey this is maybe interestingly sort of better one zero zero one one one zero zero so I'm definitely feeding in the all the proper inputs is my training data correct 0 1 gives 1 1 0 is 1 0 0 gives 0 1 1 so I think I really need to randomize the order right I've really got to randomize the order so let's randomize the order and in fact what I'm gonna do forget about even randomizing the order I'm just gonna always pick a random one so I'm just gonna say let data and p5 as a nice function if I just give it random training data it's gonna give me a random one oh the learning rate is something I actually should really be careful about in point one so that's probably something I need to be more thoughtful about so let me so let me do it fifty thousand times and let's see what happens in a random order so interestingly enough so I want to do this as a coding challenge I want to actually write an example that sort of like animates and visualizes it as it's learning I'm saying I have can't believe this worked oh my god I can't believe I arrived I mean I'm sure there's like problems and it's the but at least it worked for this simple problem and but so somebody said I had a typo in the train function well I'm sure I do don't I'm so happy right now though so I need to do coding challenge which is actually the XOR problem and also animated as I'm going so I think I would essentially do the same thing but draw sort of like a pixel space and actually iterate over it and I should see it you know the corners would be the the full boolean that I'll explain this when I do the coding challenge I can't even think anymore just got this to work
