With timestamps:

00:00 - okay so this is the last video in this
00:02 - section of videos about images and
00:04 - pixels and
00:05 - what i want to talk about in this video
00:07 - is computer vision
00:09 - creating using using an image as
00:11 - something other than a thing we draw on
00:13 - the screen or a thing we look up colors
00:15 - from to draw other things on the screen
00:16 - what does it mean to try to have our
00:19 - program our thing we're making and
00:21 - processing
00:21 - see the world in some capacity maybe to
00:24 - determine if there's a user present
00:26 - maybe to determine if the user is waving
00:28 - his or her hand maybe to determine what
00:30 - color clothes the user is wearing
00:32 - there are lots of things we might be
00:34 - able to figure out from a scene
00:36 - if we have a camera looking at that
00:37 - scene or an image that was taken
00:40 - from somewhere okay so this is a huge
00:42 - topic and
00:44 - somebody who actually knows what they're
00:45 - talking about could probably make a
00:47 - long series of videos going quite in
00:49 - depth about
00:51 - topics and examples in computer vision
00:54 - what i'm going to do in this short 10 to
00:56 - 20 minutes here is simply uh kind of
00:59 - look at some of the basics what are some
01:01 - of the key fundamental pieces that we
01:03 - need to know about or
01:04 - figure out in order to build our own
01:06 - computer vision application
01:07 - and then what are some pathways to doing
01:10 - more stuff with that so let's
01:12 - let's let's kind of like start with a
01:14 - basic kind of classic scenario
01:16 - let's say we have a scene there is a
01:19 - camera from our computer pointed into a
01:23 - room
01:23 - and in that room there is a very bright
01:26 - light
01:29 - you know as a this sort of weird-looking
01:30 - light bulb thing that i drew here and
01:32 - maybe there is
01:33 - a person like i can't draw at all
01:36 - uh holding on to that light bulb and
01:38 - they're moving it around
01:40 - could we have a camera over here looking
01:42 - at this scene
01:43 - track that light bulb how would we do
01:45 - that well
01:46 - uh conceptually we might say aha let's
01:49 - find the brightest pixel in this image
01:52 - how could we find the brightest pixel in
01:54 - this image well we might start with this
01:56 - pixel
01:56 - and see that this pixel is very dark
01:59 - it's
01:59 - uh has a color a brightness value of you
02:02 - know zero
02:02 - and then we look at the next one and see
02:04 - it has a brightness value of zero and
02:05 - the next one has a brightness value of
02:06 - three and the next one's a brightness
02:07 - value of four and then zero again and
02:09 - eventually you might get around and be
02:10 - like oh look there's a pixel over here
02:11 - with a brightness value of 255.
02:13 - that's really bright let's remember
02:16 - where this xy location is and as we get
02:19 - to the end of looking at all the pixels
02:21 - could which one had the the highest
02:24 - brightest value
02:25 - this is searching through the pixels to
02:28 - find to try to see where the brightest
02:30 - uh pixel is so let's look at a kind of
02:32 - variation on this for a second
02:34 - over here i have an example which i'm
02:36 - going to run for you
02:38 - that's hopefully going to pull up an
02:39 - image i have a camera right here looking
02:41 - at me
02:42 - um and you can see there i am above over
02:45 - here and i'm going to
02:46 - take this blue marker and i'm going to
02:48 - click on it and you can see oh i'm kind
02:49 - of drawing a dot
02:51 - following this blue marker i'm tracking
02:53 - the blue color
02:55 - in this image that's coming from a
02:56 - camera how is this done
02:59 - now you can see there's a lot of
03:01 - problems there it wasn't so perfectly
03:02 - accurate and this is not
03:04 - there's a lot of issues here that i that
03:06 - i want to take a minute to discuss too
03:08 - but let's just say for the sake of
03:09 - argument
03:10 - that is the be-all end-all of computer
03:12 - vision how do we write a program that
03:14 - does that
03:14 - that recognizes the color in an image
03:17 - and continuously follows that color as
03:19 - it moves over time
03:21 - very similar we talked about here the
03:22 - difference is
03:24 - uh maybe we're looking for the color the
03:26 - the pixel that's the most
03:27 - blue not the pixel that's the most
03:29 - bright so one
03:30 - key thing we need to figure out here is
03:33 - how do we determine
03:34 - if a color is similar to another color
03:37 - so let's think about these two colors
03:39 - here zero comma 100 comma 255
03:43 - r g b
03:46 - and now let's take another this is some
03:49 - color it's kind of
03:50 - bluish greenish now let's take another
03:52 - color 250
03:55 - you know 255. so how
03:58 - similar are these colors well one thing
04:00 - we could say is like uh
04:01 - the red values are 200 units apart
04:05 - the green values are 50 units apart and
04:07 - the blue values are 0 units apart so i
04:10 - could kind of give it a similarity score
04:12 - of 250
04:13 - where zero would be the most similar
04:16 - right if all of these values were equal
04:18 - and uh 255 times three
04:22 - would be the largest uh difference if
04:25 - all of these if
04:26 - you know values were only 0-255 so we
04:28 - can see here like taking the difference
04:30 - subtracting one color from another and
04:32 - you'll notice
04:33 - i'm using the absolute value meaning 200
04:36 - minus 0 is 200
04:37 - 50 minus 100 is actually negative 50 but
04:40 - i don't care
04:41 - whether this one's greater this one's
04:43 - greater i just want to know how far
04:45 - apart are they
04:45 - so this is kind of a way conceptually we
04:47 - could see how different are these colors
04:50 - it turns out that uh uh an accurate
04:53 - way that we can get this all into one
04:55 - line of code
04:56 - let's say i have these two colors as in
04:58 - variables r1 g1 b1
05:01 - and r2 g2 b2 i can say
05:06 - float the difference between two colors
05:10 - is the distance between those colors
05:13 - r1 g1 b1 and r2
05:17 - g2 b2 this line of code right here
05:22 - using processing's distance function
05:24 - will actually give me a
05:26 - rating a value a numeric value that
05:29 - indicates
05:30 - how similar these two colors are how
05:33 - does that work
05:34 - well distance probably makes sense to
05:36 - you if i have a three-dimensional
05:38 - space which is where i am right now my
05:41 - hands
05:42 - are a certain distance apart now they're
05:43 - getting closer and closer and closer
05:45 - well what are there's there's three
05:47 - dimensional space there's like an
05:48 - x-axis a y-axis and a z axis
05:52 - and things in this room that are closer
05:54 - to each other
05:55 - have a lower distance well
05:58 - we could think of these axes instead of
06:00 - being x y and z
06:02 - as being the red axis the green axis the
06:04 - blue axis what if we
06:05 - filled this three-dimensional space with
06:07 - every possible color colors that are
06:09 - nearer to each other
06:10 - would be more similar than colors that
06:12 - are further apart from each other and
06:14 - this
06:14 - use of euclidean distance the distance
06:16 - formula even though
06:18 - conceptually we think of it as something
06:20 - that has to do with physical space or
06:21 - even
06:22 - two-dimensional space in the case of
06:24 - two-dimensional distance
06:25 - we can use that with color as well now
06:28 - why am i spending all this time
06:29 - talking about this just this distance
06:31 - formula this is what we need to do
06:33 - if we're trying to find the color that
06:36 - is the most
06:37 - blue what if i look at this pixel and
06:41 - find its distance from blue
06:43 - then the next pixel and find its
06:44 - distance from blue then the next pixel
06:46 - and find its distance to blue one of
06:48 - those pixels
06:49 - will hold the world record for smallest
06:52 - distance to blue
06:53 - and if i keep track of that record does
06:56 - this pixel
06:56 - break the record no throw it away does
06:58 - this pixel break the record no throw it
07:00 - away
07:00 - does this pixel break the record yes it
07:02 - does keep its x
07:03 - y now keep going and if i ever find
07:06 - anything else that's greater than that
07:07 - then keep those x y when i after i
07:09 - finish looping through
07:10 - all the pixels i'm going to find the x y
07:13 - location
07:14 - of the pixel that is most blue so the
07:17 - two things that we need are one we all
07:19 - the things we already have is we know
07:20 - how to loop through all the pixels
07:22 - the things that we don't have
07:23 - necessarily from before is how to find
07:25 - the similarity between two pixels this
07:28 - distance function is a great way of
07:29 - doing it
07:30 - and also how to keep track of which
07:33 - pixel is the one i want to remember
07:36 - later after i finish looping through
07:37 - everything so let's go back
07:39 - and look at this example again and i
07:42 - need to run it
07:44 - so um there's a few things i guess i'll
07:46 - point out in the code
07:48 - but let's just make this example run
07:49 - again i'm going to hold up this blue
07:51 - marker and click on it you can see yeah
07:54 - you know it's tracking it it's
07:55 - continually finding this blue marker now
07:58 - a couple things that are going to fail
07:59 - here let's see if i want to track the
08:00 - green
08:01 - green you know i'm clicking on it and
08:03 - you know
08:05 - there's this whole background is green
08:06 - so tracking something green is going to
08:07 - do very good
08:08 - what if i click on my nose to try to
08:10 - track the color of my nose
08:11 - you know the color of my nose similar to
08:13 - my forehead is similar to my hands
08:15 - so there's a lot of flaws in this
08:17 - scenario of just looking for a single
08:20 - pixel
08:20 - you know how much this is jumping around
08:22 - we could add a little easing or
08:23 - interpolation that might help that
08:25 - there's a lot of flaws in this and
08:27 - honestly if we really want to track a
08:29 - color an object that has a specific
08:31 - color in a
08:32 - in a image it probably makes more sense
08:35 - to look for an
08:36 - area of pixels that are very similar
08:38 - we're just looking for a single pixel so
08:40 - um but this you know while this might
08:42 - not be the most useful
08:44 - uh application in a kind of real
08:46 - interactive scenario
08:47 - this demonstrates a lot of the basic
08:49 - principles of computer vision so let's
08:51 - go
08:51 - let's look at a couple things one is in
08:53 - the in the code here
08:55 - one thing i want to point out is this is
08:57 - exactly
08:58 - what i'm talking about um we have
09:01 - a current color from the video and i
09:04 - need to get its red green and blue
09:05 - values
09:06 - and then i have a color that i'm
09:07 - tracking that's the color i'm searching
09:08 - for
09:09 - so i want to know how what's the
09:11 - distance between the
09:12 - current pixel i'm looking at and the
09:14 - color that i'm tracking so this
09:15 - here's the distance function being used
09:18 - inside of this nested pixel loop
09:20 - the other thing that i'll point out is
09:22 - we're starting before we go through the
09:24 - loop
09:25 - we keep track of there's what's the
09:26 - world record the world record
09:29 - some big number that that's obviously
09:31 - the first pixel is going to beat
09:32 - and then we need to keep track of that x
09:34 - and y point so at any moment
09:37 - if we find a pixel with a distance
09:40 - that's less than the world record
09:42 - then the new world record is that pixel
09:44 - and that x and y we should save
09:46 - so this is that second piece the first
09:48 - piece of something new here is that we
09:49 - need to figure out how our two
09:51 - images how are two colors different or
09:53 - similar using the distance function
09:55 - and another piece is how do we while
09:57 - we're looping through how do we keep
09:59 - track of which is our
10:00 - favorite pixel essentially is this our
10:02 - new favorite it is save it
10:03 - and we'll save it again if we find so it
10:05 - doesn't actually matter how what order
10:07 - we look through the pixels we're going
10:08 - to hold on to that
10:10 - quote unquote best pixel all the way
10:12 - through that loop
10:13 - okay so this is giving you some of the
10:16 - basics again you can see there's a lot
10:18 - of flaws here
10:19 - if i were to give you an exercise i
10:21 - might add try to add some easing
10:23 - so that uh let's look at so even if i'm
10:25 - oh this is a black pen let's go back to
10:27 - the blue
10:28 - uh so even if it's you we could probably
10:30 - get this to be at least
10:32 - somewhat smooth the other thing i should
10:33 - point out here is there's no need to
10:35 - display the video
10:36 - look at this woo i'm magically
10:37 - controlling the circle above me
10:39 - um so so one thing where they recognize
10:42 - here is the camera is really acting as a
10:44 - sensor there's nothing about it that
10:45 - we're using for display purposes we're
10:48 - just reading the image
10:49 - and getting some um kind of analyzing
10:51 - for some piece of information
10:53 - so uh uh just recently at the
10:56 - winter show here at itp students built a
10:59 - spray
11:00 - a paint app an interactive paint
11:02 - application where you would hold a spray
11:03 - can up to the wall
11:04 - and inside that spray can was a very
11:06 - bright green led
11:08 - and there was a camera on the wall so
11:10 - the camera was able to track
11:11 - where that spray cam was simply by
11:13 - tracking a very bright led
11:15 - so there's a lot of cases where you
11:16 - might want to track something
11:18 - find it with simply with color
11:22 - i should point out that another um gonna
11:28 - just
11:28 - jump to this another
11:31 - uh scenario where you might like to look
11:33 - at the difference between pixels
11:35 - is for looking for motion so i'm gonna
11:37 - stand here and try to stand very very
11:39 - still
11:39 - and you can see when i stand very very
11:41 - still except for my mouth moving
11:43 - um there you're not seeing any black
11:46 - pixels
11:47 - but if i move around a lot you're seeing
11:49 - a lot of black pixels
11:50 - so one thing that you can do is
11:53 - you can find motion by saying i have one
11:56 - frame of video and i have the next frame
11:58 - of video
11:59 - let me compare each pixel a pixel that
12:01 - changes
12:02 - is likely where there's motion right
12:04 - because
12:05 - if my hand is never moving the pixel
12:08 - color here is not changing
12:09 - but as soon as i move my hand the pixel
12:11 - colors are changing
12:13 - now notice that even just with subtle
12:15 - movements we're getting
12:16 - it's in a way we're only we're finding
12:17 - the edges of my hand because if you
12:19 - think about it
12:20 - the pixel even as i move my hand the
12:22 - pixel colors in the center of my hand
12:23 - aren't really changing
12:25 - it's only changing along the edge where
12:27 - this white part
12:28 - where this with this part of my skin
12:30 - meets the green part of this green
12:31 - screen that you can't see because you
12:32 - see a computer screen
12:33 - so this is another scenario and all
12:35 - these examples are in the
12:37 - learning processing github repository
12:39 - connect this one is
12:41 - example 16.13. so
12:44 - um this is a little bit about a computer
12:47 - vision here
12:48 - i guess i'll just show you one more here
12:49 - there's another example which
12:51 - we can look at just how many pixels have
12:54 - changed per frame
12:55 - so if very few pixels have changed per
12:57 - frame you see the small dot in the
12:59 - center
12:59 - and as i move around a lot a lot of
13:02 - pixels are changing the dot is getting
13:03 - bigger
13:04 - so this is a scenario where we can use
13:07 - and maybe an exercise might be could you
13:09 - find where the motion is
13:11 - so you could create an application where
13:13 - as i'm waving my hand
13:15 - uh i may something's able to follow the
13:17 - thing that is moving
13:18 - so there's a lot of possibilities here
13:21 - in how you use
13:22 - what the computer can see can you find
13:24 - the edges we looked at
13:27 - edge detection and image processing
13:30 - finding a specific color finding the
13:31 - brightest pixel finding the darkest
13:33 - pixel finding pixels that are moving
13:35 - these are the types of things you can do
13:37 - writing your own code in processing
13:40 - however this is not a new idea
13:44 - i didn't invent any of this in fact i
13:46 - know very little about this compared
13:48 - to a lot of people in this world and in
13:50 - fact if you want to work on a computer
13:52 - vision application it's quite
13:53 - likely that am i recording yes and how
13:56 - long uh 13 minutes we're fine so it's
13:58 - quite likely that what you want to end
13:59 - up doing is using a library uh
14:02 - that has a lot of computer vision
14:03 - functionality built into it so one
14:05 - library that i would recommend you
14:07 - look at is uh
14:10 - i'm going to have to just sort of google
14:11 - search this here really quickly
14:13 - github is opencv for processing so this
14:17 - is a library by greg borenstein
14:19 - um opencv is an open source computer
14:22 - vision library originally developed by
14:24 - intel
14:24 - now maintained by open source
14:27 - collectives
14:28 - this is me and you can see here that
14:30 - there's a lot of functionality
14:32 - it can find your face it can do all
14:34 - sorts of image processing it can look
14:36 - for blobs and contours
14:38 - so a lot of the stuff that we might
14:40 - spend hours or weeks or days or months
14:42 - trying to program from scratch
14:44 - are features that are built into a
14:46 - library and you can even see as i get
14:48 - down here there's lots of
14:49 - kind of really interesting features like
14:52 - um like i don't know recognizing a
14:56 - card for example okay so or yeah
14:59 - recognizing uh different markers in an
15:00 - image so anyway okay
15:02 - so well let me show you one one thing
15:04 - that this is kind of classically used
15:05 - for
15:06 - uh is for uh face detection
15:09 - so let me run this one we close these
15:13 - out
15:14 - and back to processing and run this so
15:17 - one of the things that uh opencv
15:19 - will do for you is it will find faces in
15:22 - an image
15:23 - so you can see here as soon as i turn to
15:25 - the side by the way it does not it's not
15:27 - recognizing my profile
15:28 - only if i'm looking dead on so
15:31 - what opencv will do for you is it will
15:33 - give you a rectangle now this is quite
15:35 - different than
15:36 - face recognition right it's this is just
15:38 - face detection oh there's a face
15:40 - there it is that's how big it is but you
15:42 - can imagine some applications you might
15:43 - do here is some
15:44 - how many people are there are somebody
15:46 - looking straight versus looking to the
15:47 - side
15:48 - uh there was a project just here in the
15:50 - itp winter show where
15:51 - two people are standing in scene and
15:54 - their faces are swapped
15:56 - so there's a lot of creative
15:57 - possibilities here you know if you if
15:59 - you use google hangout you can see that
16:01 - you know you can add uh there's all
16:02 - these features you can add a hat or a
16:03 - mustache on somebody you could do this
16:05 - but with the opencv library as well so
16:08 - this is something that i might encourage
16:09 - you to take a look
16:10 - look into as a as a possibility and one
16:13 - thing that opencv has also is blob
16:15 - detection so finding
16:16 - areas of brightness or areas of darkness
16:18 - which is very useful
16:19 - in a tracking sense as well so the last
16:22 - piece that i want to just kind of demo
16:24 - for you here and i'm going to have to uh
16:26 - plug it in is that there is a thing
16:30 - called the microsoft kinect i'm afraid
16:32 - to pick this up
16:33 - can you see this this is the microsoft
16:36 - kinect
16:37 - this thing has a camera in it and a
16:40 - regular
16:41 - rgb camera it has an infrared uh camera
16:44 - and has an infrared projector
16:46 - so what what this sensor is going to do
16:49 - this camera this
16:50 - depth sensor and let me see if i can
16:54 - open this example up
16:59 - and
17:03 - oops hello this is what this is what
17:05 - happened oh there we go okay
17:07 - so you can see that this is showing us a
17:09 - couple different things
17:10 - one is here's another image of me in
17:12 - this room you can see that i have a
17:13 - little tv over here where i can watch
17:15 - myself this is all very strange
17:16 - um and there's just a camera coming out
17:18 - of this connect but then there's also
17:20 - this image over here
17:21 - this image is a depth map now it's very
17:23 - the way that the screen captures work
17:24 - it's very hard to see what's going on
17:26 - here
17:26 - but you can see that my hand is a little
17:28 - bit brighter
17:29 - and now it's quite a bit darker and now
17:32 - and you can see as i put it in here we
17:33 - can't see it anymore
17:34 - so what the the microsoft connect and
17:36 - this is the old connect the 144 this is
17:39 - the
17:39 - original model from a few years ago 1414
17:41 - is the model number
17:42 - um what it does is it's able to
17:46 - not just say like here's the pixel and
17:48 - here's its red green and blue value
17:50 - what it says is here's a pixel and
17:51 - here's how far it is
17:53 - from the connect from the camera from
17:54 - the sensor in millimeters
17:56 - so you can know what's close what's far
17:58 - away and another
18:12 - it's a little hard to see probably where
18:14 - we are based on this like orientation
18:16 - i'm like looking at 12 different places
18:18 - but you can see that this is essentially
18:20 - the data visualized in three dimensions
18:22 - so some really basic
18:23 - 3d scanning of terms of a scene
18:27 - what i'm not using here is there's a
18:28 - library called open ni
18:30 - uh simple benight which is actually now
18:33 - open and i was purchased by apple okay
18:35 - here's the thing
18:37 - i i i don't want to like ramble for like
18:40 - the next 20 minutes about all the
18:41 - different versions of connects and which
18:42 - ones work with which operating system
18:44 - and which library
18:45 - but what you have right now at this very
18:46 - moment there's the original kinect that
18:48 - came out a few years ago that you could
18:50 - use there's a library for
18:51 - pc and processing that you can use
18:54 - there's two libraries for the mac
18:57 - and processing one which is open connect
18:59 - which is a library that i built
19:01 - on top of some open source uh drivers
19:03 - for the kinect and there's also called
19:05 - something called simple open ni
19:06 - which uses open and i which has since
19:08 - been purchased by apple and has been
19:10 - shut down
19:10 - so while all of this works if you're
19:13 - interested in the connect you probably
19:14 - want to go and look for connect version
19:16 - 2.
19:16 - that's the newer connect it's a bit
19:18 - higher resolution a lot of the
19:20 - skeleton tracking which i haven't really
19:21 - mentioned yet is a bit more accurate i
19:24 - think
19:24 - um however uh there aren't open source
19:28 - drivers as easily available for the new
19:30 - connect
19:31 - and so there is an official microsoft
19:33 - developers kit
19:34 - and you can use that with processing
19:37 - however it's only
19:38 - for windows and at the moment there are
19:40 - a lot of people here
19:41 - at itp and probably other places in the
19:43 - world working on ways to
19:45 - pass the information from the kinect and
19:47 - a pc over the network what you have to
19:49 - realize is
19:49 - the connect itself is just sending this
19:52 - raw data
19:53 - what is the distance for each pixel from
19:54 - the connect what you can do with that
19:56 - information is incredibly powerful and
19:58 - if we looked at any demo of the
19:59 - version 2 connect or the old connect you
20:01 - would see it can recognize
20:03 - the form of my body very quickly and and
20:05 - track where my arms are my hand is my
20:07 - head is my knees are and
20:08 - all sorts of stuff you can do with that
20:10 - we don't have time
20:11 - or i don't really uh to get into all of
20:13 - that here and perhaps someday there'll
20:15 - be
20:16 - more videos or more examples that i can
20:18 - do and help prepare in that direction
20:20 - however
20:22 - let's just i just want to jump back for
20:23 - a second and just show you what you can
20:25 - at least do
20:26 - uh something that the kinect makes
20:28 - possible um
20:29 - with just even just the raw depth
20:32 - information
20:32 - so uh one thing i want to do is do this
20:35 - okay
20:36 - so one thing you can see here is uh this
20:38 - is where i'm standing as i walk
20:40 - closer to the connect i start to turn
20:42 - red as i walk further away i
20:44 - i stop being red as i put my hand here
20:46 - my hand is red
20:47 - so with depth one thing that you could
20:49 - do rather easily is say
20:51 - where are the where is the thing that's
20:53 - closest to the camera and in the sense
20:54 - of a hand
20:55 - pointing straight out that's pretty easy
20:57 - to track now notice i put my other hand
20:59 - out
20:59 - now that dot is in between them take
21:01 - this one away
21:03 - switch so this isn't doing any
21:04 - sophisticated hand tracking i could like
21:06 - put my head here
21:07 - it's just looking for like a blob of
21:09 - stuff that's close to the camera
21:11 - i could take this marker and point it
21:13 - out
21:14 - like this uh and you can see it's sort
21:16 - of tracking that i think this marker is
21:18 - actually a little bit reflective
21:19 - so there's a lot more to this uh
21:23 - how it works and infrared light and the
21:25 - depth versus the skeleton
21:27 - and blah blah blah and i'm just kind of
21:28 - doing a terrible job here
21:30 - but what i would hopefully you got a
21:31 - sense in this video if you're still
21:33 - watching
21:34 - is that that for loop right of looking
21:37 - through all the pixels
21:38 - you need that you need that for for the
21:40 - image processing stuff you're doing you
21:41 - need that for some computers and stuff
21:43 - and in fact you need it for this
21:44 - because instead of looking for the color
21:47 - that's the most closest to whatever i'm
21:48 - looking for the pixel that's the closest
21:50 - to the um
21:52 - to the connect itself so that for loop
21:54 - is pretty crucial and there's a lot of
21:56 - things you can do with computer vision
21:58 - however there are also you could do from
22:01 - scratch on your own but there are also a
22:02 - tremendous set of libraries and
22:03 - resources and other devices
22:05 - like a depth sensor that you might
22:07 - consider as well
22:08 - very last thing is i'll i'll try to
22:10 - include a link also there are uh
22:12 - third-party applications open source
22:13 - ones uh like compute
22:15 - community computing community core
22:18 - vision
22:18 - open tsps these are applications that
22:21 - you can run
22:22 - behind the scenes on your computer and
22:24 - have them pass
22:25 - messages about what they're tracking to
22:26 - processing and that that's also a way
22:28 - that you might think about in developing
22:30 - uh computer vision an application that
22:32 - involves computer vision okay
22:34 - so someday i will um kind of revisit
22:37 - some of this stuff in a hopefully more
22:39 - organized or useful way but for now i'm
22:41 - just gonna
22:41 - say goodbye and i have no idea this is
22:44 - probably way too long
22:45 - okay uh but i did manage to get the
22:47 - kinect working in this video which is
22:49 - kind of interesting okay
22:50 - see you later

Cleaned transcript:

okay so this is the last video in this section of videos about images and pixels and what i want to talk about in this video is computer vision creating using using an image as something other than a thing we draw on the screen or a thing we look up colors from to draw other things on the screen what does it mean to try to have our program our thing we're making and processing see the world in some capacity maybe to determine if there's a user present maybe to determine if the user is waving his or her hand maybe to determine what color clothes the user is wearing there are lots of things we might be able to figure out from a scene if we have a camera looking at that scene or an image that was taken from somewhere okay so this is a huge topic and somebody who actually knows what they're talking about could probably make a long series of videos going quite in depth about topics and examples in computer vision what i'm going to do in this short 10 to 20 minutes here is simply uh kind of look at some of the basics what are some of the key fundamental pieces that we need to know about or figure out in order to build our own computer vision application and then what are some pathways to doing more stuff with that so let's let's let's kind of like start with a basic kind of classic scenario let's say we have a scene there is a camera from our computer pointed into a room and in that room there is a very bright light you know as a this sort of weirdlooking light bulb thing that i drew here and maybe there is a person like i can't draw at all uh holding on to that light bulb and they're moving it around could we have a camera over here looking at this scene track that light bulb how would we do that well uh conceptually we might say aha let's find the brightest pixel in this image how could we find the brightest pixel in this image well we might start with this pixel and see that this pixel is very dark it's uh has a color a brightness value of you know zero and then we look at the next one and see it has a brightness value of zero and the next one has a brightness value of three and the next one's a brightness value of four and then zero again and eventually you might get around and be like oh look there's a pixel over here with a brightness value of 255. that's really bright let's remember where this xy location is and as we get to the end of looking at all the pixels could which one had the the highest brightest value this is searching through the pixels to find to try to see where the brightest uh pixel is so let's look at a kind of variation on this for a second over here i have an example which i'm going to run for you that's hopefully going to pull up an image i have a camera right here looking at me um and you can see there i am above over here and i'm going to take this blue marker and i'm going to click on it and you can see oh i'm kind of drawing a dot following this blue marker i'm tracking the blue color in this image that's coming from a camera how is this done now you can see there's a lot of problems there it wasn't so perfectly accurate and this is not there's a lot of issues here that i that i want to take a minute to discuss too but let's just say for the sake of argument that is the beall endall of computer vision how do we write a program that does that that recognizes the color in an image and continuously follows that color as it moves over time very similar we talked about here the difference is uh maybe we're looking for the color the the pixel that's the most blue not the pixel that's the most bright so one key thing we need to figure out here is how do we determine if a color is similar to another color so let's think about these two colors here zero comma 100 comma 255 r g b and now let's take another this is some color it's kind of bluish greenish now let's take another color 250 you know 255. so how similar are these colors well one thing we could say is like uh the red values are 200 units apart the green values are 50 units apart and the blue values are 0 units apart so i could kind of give it a similarity score of 250 where zero would be the most similar right if all of these values were equal and uh 255 times three would be the largest uh difference if all of these if you know values were only 0255 so we can see here like taking the difference subtracting one color from another and you'll notice i'm using the absolute value meaning 200 minus 0 is 200 50 minus 100 is actually negative 50 but i don't care whether this one's greater this one's greater i just want to know how far apart are they so this is kind of a way conceptually we could see how different are these colors it turns out that uh uh an accurate way that we can get this all into one line of code let's say i have these two colors as in variables r1 g1 b1 and r2 g2 b2 i can say float the difference between two colors is the distance between those colors r1 g1 b1 and r2 g2 b2 this line of code right here using processing's distance function will actually give me a rating a value a numeric value that indicates how similar these two colors are how does that work well distance probably makes sense to you if i have a threedimensional space which is where i am right now my hands are a certain distance apart now they're getting closer and closer and closer well what are there's there's three dimensional space there's like an xaxis a yaxis and a z axis and things in this room that are closer to each other have a lower distance well we could think of these axes instead of being x y and z as being the red axis the green axis the blue axis what if we filled this threedimensional space with every possible color colors that are nearer to each other would be more similar than colors that are further apart from each other and this use of euclidean distance the distance formula even though conceptually we think of it as something that has to do with physical space or even twodimensional space in the case of twodimensional distance we can use that with color as well now why am i spending all this time talking about this just this distance formula this is what we need to do if we're trying to find the color that is the most blue what if i look at this pixel and find its distance from blue then the next pixel and find its distance from blue then the next pixel and find its distance to blue one of those pixels will hold the world record for smallest distance to blue and if i keep track of that record does this pixel break the record no throw it away does this pixel break the record no throw it away does this pixel break the record yes it does keep its x y now keep going and if i ever find anything else that's greater than that then keep those x y when i after i finish looping through all the pixels i'm going to find the x y location of the pixel that is most blue so the two things that we need are one we all the things we already have is we know how to loop through all the pixels the things that we don't have necessarily from before is how to find the similarity between two pixels this distance function is a great way of doing it and also how to keep track of which pixel is the one i want to remember later after i finish looping through everything so let's go back and look at this example again and i need to run it so um there's a few things i guess i'll point out in the code but let's just make this example run again i'm going to hold up this blue marker and click on it you can see yeah you know it's tracking it it's continually finding this blue marker now a couple things that are going to fail here let's see if i want to track the green green you know i'm clicking on it and you know there's this whole background is green so tracking something green is going to do very good what if i click on my nose to try to track the color of my nose you know the color of my nose similar to my forehead is similar to my hands so there's a lot of flaws in this scenario of just looking for a single pixel you know how much this is jumping around we could add a little easing or interpolation that might help that there's a lot of flaws in this and honestly if we really want to track a color an object that has a specific color in a in a image it probably makes more sense to look for an area of pixels that are very similar we're just looking for a single pixel so um but this you know while this might not be the most useful uh application in a kind of real interactive scenario this demonstrates a lot of the basic principles of computer vision so let's go let's look at a couple things one is in the in the code here one thing i want to point out is this is exactly what i'm talking about um we have a current color from the video and i need to get its red green and blue values and then i have a color that i'm tracking that's the color i'm searching for so i want to know how what's the distance between the current pixel i'm looking at and the color that i'm tracking so this here's the distance function being used inside of this nested pixel loop the other thing that i'll point out is we're starting before we go through the loop we keep track of there's what's the world record the world record some big number that that's obviously the first pixel is going to beat and then we need to keep track of that x and y point so at any moment if we find a pixel with a distance that's less than the world record then the new world record is that pixel and that x and y we should save so this is that second piece the first piece of something new here is that we need to figure out how our two images how are two colors different or similar using the distance function and another piece is how do we while we're looping through how do we keep track of which is our favorite pixel essentially is this our new favorite it is save it and we'll save it again if we find so it doesn't actually matter how what order we look through the pixels we're going to hold on to that quote unquote best pixel all the way through that loop okay so this is giving you some of the basics again you can see there's a lot of flaws here if i were to give you an exercise i might add try to add some easing so that uh let's look at so even if i'm oh this is a black pen let's go back to the blue uh so even if it's you we could probably get this to be at least somewhat smooth the other thing i should point out here is there's no need to display the video look at this woo i'm magically controlling the circle above me um so so one thing where they recognize here is the camera is really acting as a sensor there's nothing about it that we're using for display purposes we're just reading the image and getting some um kind of analyzing for some piece of information so uh uh just recently at the winter show here at itp students built a spray a paint app an interactive paint application where you would hold a spray can up to the wall and inside that spray can was a very bright green led and there was a camera on the wall so the camera was able to track where that spray cam was simply by tracking a very bright led so there's a lot of cases where you might want to track something find it with simply with color i should point out that another um gonna just jump to this another uh scenario where you might like to look at the difference between pixels is for looking for motion so i'm gonna stand here and try to stand very very still and you can see when i stand very very still except for my mouth moving um there you're not seeing any black pixels but if i move around a lot you're seeing a lot of black pixels so one thing that you can do is you can find motion by saying i have one frame of video and i have the next frame of video let me compare each pixel a pixel that changes is likely where there's motion right because if my hand is never moving the pixel color here is not changing but as soon as i move my hand the pixel colors are changing now notice that even just with subtle movements we're getting it's in a way we're only we're finding the edges of my hand because if you think about it the pixel even as i move my hand the pixel colors in the center of my hand aren't really changing it's only changing along the edge where this white part where this with this part of my skin meets the green part of this green screen that you can't see because you see a computer screen so this is another scenario and all these examples are in the learning processing github repository connect this one is example 16.13. so um this is a little bit about a computer vision here i guess i'll just show you one more here there's another example which we can look at just how many pixels have changed per frame so if very few pixels have changed per frame you see the small dot in the center and as i move around a lot a lot of pixels are changing the dot is getting bigger so this is a scenario where we can use and maybe an exercise might be could you find where the motion is so you could create an application where as i'm waving my hand uh i may something's able to follow the thing that is moving so there's a lot of possibilities here in how you use what the computer can see can you find the edges we looked at edge detection and image processing finding a specific color finding the brightest pixel finding the darkest pixel finding pixels that are moving these are the types of things you can do writing your own code in processing however this is not a new idea i didn't invent any of this in fact i know very little about this compared to a lot of people in this world and in fact if you want to work on a computer vision application it's quite likely that am i recording yes and how long uh 13 minutes we're fine so it's quite likely that what you want to end up doing is using a library uh that has a lot of computer vision functionality built into it so one library that i would recommend you look at is uh i'm going to have to just sort of google search this here really quickly github is opencv for processing so this is a library by greg borenstein um opencv is an open source computer vision library originally developed by intel now maintained by open source collectives this is me and you can see here that there's a lot of functionality it can find your face it can do all sorts of image processing it can look for blobs and contours so a lot of the stuff that we might spend hours or weeks or days or months trying to program from scratch are features that are built into a library and you can even see as i get down here there's lots of kind of really interesting features like um like i don't know recognizing a card for example okay so or yeah recognizing uh different markers in an image so anyway okay so well let me show you one one thing that this is kind of classically used for uh is for uh face detection so let me run this one we close these out and back to processing and run this so one of the things that uh opencv will do for you is it will find faces in an image so you can see here as soon as i turn to the side by the way it does not it's not recognizing my profile only if i'm looking dead on so what opencv will do for you is it will give you a rectangle now this is quite different than face recognition right it's this is just face detection oh there's a face there it is that's how big it is but you can imagine some applications you might do here is some how many people are there are somebody looking straight versus looking to the side uh there was a project just here in the itp winter show where two people are standing in scene and their faces are swapped so there's a lot of creative possibilities here you know if you if you use google hangout you can see that you know you can add uh there's all these features you can add a hat or a mustache on somebody you could do this but with the opencv library as well so this is something that i might encourage you to take a look look into as a as a possibility and one thing that opencv has also is blob detection so finding areas of brightness or areas of darkness which is very useful in a tracking sense as well so the last piece that i want to just kind of demo for you here and i'm going to have to uh plug it in is that there is a thing called the microsoft kinect i'm afraid to pick this up can you see this this is the microsoft kinect this thing has a camera in it and a regular rgb camera it has an infrared uh camera and has an infrared projector so what what this sensor is going to do this camera this depth sensor and let me see if i can open this example up and oops hello this is what this is what happened oh there we go okay so you can see that this is showing us a couple different things one is here's another image of me in this room you can see that i have a little tv over here where i can watch myself this is all very strange um and there's just a camera coming out of this connect but then there's also this image over here this image is a depth map now it's very the way that the screen captures work it's very hard to see what's going on here but you can see that my hand is a little bit brighter and now it's quite a bit darker and now and you can see as i put it in here we can't see it anymore so what the the microsoft connect and this is the old connect the 144 this is the original model from a few years ago 1414 is the model number um what it does is it's able to not just say like here's the pixel and here's its red green and blue value what it says is here's a pixel and here's how far it is from the connect from the camera from the sensor in millimeters so you can know what's close what's far away and another it's a little hard to see probably where we are based on this like orientation i'm like looking at 12 different places but you can see that this is essentially the data visualized in three dimensions so some really basic 3d scanning of terms of a scene what i'm not using here is there's a library called open ni uh simple benight which is actually now open and i was purchased by apple okay here's the thing i i i don't want to like ramble for like the next 20 minutes about all the different versions of connects and which ones work with which operating system and which library but what you have right now at this very moment there's the original kinect that came out a few years ago that you could use there's a library for pc and processing that you can use there's two libraries for the mac and processing one which is open connect which is a library that i built on top of some open source uh drivers for the kinect and there's also called something called simple open ni which uses open and i which has since been purchased by apple and has been shut down so while all of this works if you're interested in the connect you probably want to go and look for connect version 2. that's the newer connect it's a bit higher resolution a lot of the skeleton tracking which i haven't really mentioned yet is a bit more accurate i think um however uh there aren't open source drivers as easily available for the new connect and so there is an official microsoft developers kit and you can use that with processing however it's only for windows and at the moment there are a lot of people here at itp and probably other places in the world working on ways to pass the information from the kinect and a pc over the network what you have to realize is the connect itself is just sending this raw data what is the distance for each pixel from the connect what you can do with that information is incredibly powerful and if we looked at any demo of the version 2 connect or the old connect you would see it can recognize the form of my body very quickly and and track where my arms are my hand is my head is my knees are and all sorts of stuff you can do with that we don't have time or i don't really uh to get into all of that here and perhaps someday there'll be more videos or more examples that i can do and help prepare in that direction however let's just i just want to jump back for a second and just show you what you can at least do uh something that the kinect makes possible um with just even just the raw depth information so uh one thing i want to do is do this okay so one thing you can see here is uh this is where i'm standing as i walk closer to the connect i start to turn red as i walk further away i i stop being red as i put my hand here my hand is red so with depth one thing that you could do rather easily is say where are the where is the thing that's closest to the camera and in the sense of a hand pointing straight out that's pretty easy to track now notice i put my other hand out now that dot is in between them take this one away switch so this isn't doing any sophisticated hand tracking i could like put my head here it's just looking for like a blob of stuff that's close to the camera i could take this marker and point it out like this uh and you can see it's sort of tracking that i think this marker is actually a little bit reflective so there's a lot more to this uh how it works and infrared light and the depth versus the skeleton and blah blah blah and i'm just kind of doing a terrible job here but what i would hopefully you got a sense in this video if you're still watching is that that for loop right of looking through all the pixels you need that you need that for for the image processing stuff you're doing you need that for some computers and stuff and in fact you need it for this because instead of looking for the color that's the most closest to whatever i'm looking for the pixel that's the closest to the um to the connect itself so that for loop is pretty crucial and there's a lot of things you can do with computer vision however there are also you could do from scratch on your own but there are also a tremendous set of libraries and resources and other devices like a depth sensor that you might consider as well very last thing is i'll i'll try to include a link also there are uh thirdparty applications open source ones uh like compute community computing community core vision open tsps these are applications that you can run behind the scenes on your computer and have them pass messages about what they're tracking to processing and that that's also a way that you might think about in developing uh computer vision an application that involves computer vision okay so someday i will um kind of revisit some of this stuff in a hopefully more organized or useful way but for now i'm just gonna say goodbye and i have no idea this is probably way too long okay uh but i did manage to get the kinect working in this video which is kind of interesting okay see you later
