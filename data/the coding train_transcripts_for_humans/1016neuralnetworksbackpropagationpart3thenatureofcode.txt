With timestamps:

00:00 - hello all right this is a good moment
00:02 - for me I think I'm excited to see if I
00:04 - can get through this video because if I
00:07 - can implement this last piece of the
00:09 - Train function in the neural network
00:10 - library then I'll have a working version
00:13 - of some kind of neural network library
00:15 - like thing that I can start to finally
00:17 - apply to some projects and it's my goal
00:19 - actually that in some of the future
00:20 - videos I make that make use of the
00:22 - library and eventually other more
00:24 - sophisticated machine learning deep
00:26 - learning libraries like deep learning is
00:28 - a new library which i think is now gonna
00:31 - be called ml5 which i'll talk about in
00:33 - another video that I'll build really
00:35 - well I'm sad about it's to make stuff
00:36 - and show you how to make stuff but I'm
00:38 - kind of working through this because
00:39 - it's just something I have to do so I've
00:41 - started I have to finish I'm just trying
00:44 - to like vamp till you've stopped move on
00:46 - and watch something else because I'm not
00:48 - sure you should continue so but let's
00:49 - let me try to it's actually been a while
00:51 - since I recorded the previous video
00:52 - unfortunately might be watching them one
00:53 - after another let me try to like reset
00:55 - kind of where I think that we are so we
00:58 - have a something like a two layer
01:01 - network it has you know an input
01:07 - quote/unquote airboat layer it has these
01:10 - so these are the inputs this is the
01:13 - hidden and this is the output it is
01:18 - fully connected meaning every input is
01:22 - connected to every hidden and every
01:26 - output is connected to every every
01:30 - hidden is connected every input every
01:32 - output is connected every hidden and so
01:34 - the outputs come out here the inputs
01:36 - come out here all of these connections
01:38 - have a weight so and we can consider
01:42 - them in a weight matrix and I guess I
01:44 - should put this stuff back in here wait
01:47 - if this is like input one two three this
01:50 - is hidden one two
01:52 - this is weight one two one this is
01:55 - weight two to one this so these all end
01:59 - up in a nice weight matrix these end up
02:01 - in weight matrix the outputs come out oh
02:04 - boy in array like y1 y2 a vector the
02:09 - inputs come in in a vector
02:11 - x2 x3 okay don't get back into this
02:16 - I didn't even draw like off the top
02:19 - wonderful so now the idea is we want to
02:23 - do training and the kind of training
02:25 - that I'm doing right now is called
02:26 - supervised learning where I have some
02:31 - known output I've some known inputs with
02:33 - known outputs inputs with target outputs
02:36 - I send the inputs in I feed them forward
02:39 - I get some actual guests output back and
02:42 - I have some sort of error which we can
02:45 - think of as the error equals the guess
02:50 - I'm the guess - the target so I'm gonna
02:53 - say like y- target okay so this is the
02:58 - error now I should have mentioned before
03:00 - that there are a variety of ways to
03:03 - train a neural network and by training
03:07 - I mean adjusting all of these weights
03:09 - like their knobs to try to actually get
03:11 - the matched target output when you send
03:14 - in the training data so one thing I
03:15 - should mention I really gotta like even
03:17 - just take a minutes and make a video
03:18 - just about this but in most training
03:20 - situations you'll have training data
03:26 - test data and then of course there's the
03:29 - actual unknown data so we want to simply
03:34 - want to say like oh here's some labeled
03:36 - data I'm going to use that to train but
03:38 - I need to save some of that labeled data
03:40 - because what if I train my neural
03:42 - network only to work with the training
03:44 - data but it doesn't actually work with
03:46 - any other data well I can determine that
03:48 - by giving it some saved some data that I
03:50 - didn't train it with but I know the
03:51 - answer to see how it dumps with that and
03:53 - that's how we can evaluate it so I just
03:54 - want to mention that this is going to be
03:56 - the process now this idea of back
03:58 - propagation with gradient descent is one
04:01 - technique and it's a technique I'll put
04:02 - some links in this video's description
04:03 - you read about the history of it it's
04:05 - it's been around for a long time it's a
04:06 - big innovation in training neural
04:08 - networks but there is a lot of questions
04:11 - as to whether that's optimal best for
04:13 - the future etc there's different you
04:15 - know slept weeks of these algorithms and
04:18 - I actually next probably after I get
04:21 - through this plan to use a genetic
04:23 - algorithm to evolve the weights of
04:25 - network which opens up the door for a
04:27 - lot of kinds of projects that I excited
04:29 - to try to make so all that aside here we
04:33 - are what did we get so far we figured
04:35 - out in the previous video we calculated
04:37 - this error and then we calculated the
04:41 - hidden error which I'll just call each
04:43 - error right that's part of back
04:45 - propagation it's while we have this
04:46 - error how do we distribute this error
04:48 - all around so we can adjust all these
04:49 - weights the reason why we're doing this
04:51 - is because we actually did this already
04:52 - twice I've done this twice once with if
04:56 - you look at my videos about a single
04:57 - perceptron right which gets two inputs I
05:00 - forgot about the bias I always I always
05:02 - ever remember the bias I'm biased
05:05 - against the bias I'll come back to that
05:08 - but the single perceptron which just
05:10 - takes in two inputs and one output well
05:13 - it doesn't say it's a single neuron you
05:14 - can take in more than two inputs but a
05:16 - single neuron with multiple inputs and
05:17 - an output I actually did this we did
05:21 - this idea of training this with gradient
05:23 - descent and we also did it I did it we I
05:25 - did it in a video about linear
05:28 - regression where if I have a bunch of
05:33 - points in a two dimensional space I can
05:36 - find the line that fits these points the
05:39 - best and they did this what's
05:41 - interesting here is though this app so
05:44 - for a linear regression if you recall
05:45 - there's actually a mathematical formula
05:48 - to just compute the exact line of best
05:50 - fit called ordinary least squares I
05:52 - think I went through that in a previous
05:53 - video as well but the idea here and so
05:56 - the idea is we're trying to figure out y
05:58 - equals MX plus B the formula for this
06:02 - line well basically what this is doing
06:05 - is this is exactly the same process that
06:07 - we needed here only we have these
06:10 - weights you're going on you can almost
06:14 - think of this as like m and this is B
06:15 - maybe or something if there's just one
06:17 - input and a bias we had to fit we had to
06:21 - fit all the stuff coming through here
06:22 - into a line well this is actually what
06:24 - we need to do with all of these all of
06:27 - these places right we basically need to
06:29 - do the exact same algorithm that we did
06:31 - for this one line to compute this is the
06:34 - weight and this is the bias what we
06:37 - really have right is that why
06:38 - those MX plus B we have y equals M 1 X 1
06:43 - plus M 2 X 2 plus M 3 can you see that X
06:47 - 3 plus M 4 X 4 but added up plus B we
06:51 - have this we basically have exactly the
06:53 - same problem but in a multi-dimensional
06:55 - space so I just need to figure out how
06:58 - do I adjust each one of these M one and
07:00 - two and three and four and B all these
07:03 - weights inside of the matrix so the same
07:05 - training method I did for a linear
07:07 - regression gradient descent the same
07:08 - thing we did with the perceptron we now
07:10 - need to apply it here in this
07:11 - multi-dimensional space so what should I
07:15 - do next now first of all I forgot to
07:17 - thank a bunch of the resources hold on I
07:20 - some viewers rightly pointed out that
07:23 - when I did this previously and I guess
07:25 - the convention is target minus y the
07:27 - nice thing about this is when if you
07:29 - look at a cost function for a machine
07:31 - learning system you'll see that the cost
07:34 - function is the sum of all the errors
07:36 - squared so if you do target minus y or y
07:38 - minus target you square it doesn't
07:39 - really matter but it is an important
07:41 - distinction probably you have to get it
07:43 - right of course otherwise you might
07:45 - start trading in the wrong direction as
07:46 - you'll start to see as we do other stuff
07:48 - okay let me come over here and let me
07:51 - think so I've better come back to this
07:53 - video in a second but let me first say
07:56 - there are three things one is this is a
07:58 - new resource that just came out it's not
08:00 - a new resource but a new page on the ml
08:02 - for a ml for a github IO site this is a
08:06 - site put together by Jean Cogan has a
08:08 - ton of machine learning resources videos
08:11 - examples demos etc it's amazing and
08:14 - there is a nice article here about how
08:17 - neural networks are trained with a lot
08:19 - more detail than I'm gonna get into here
08:20 - but you can see the same sort of idea of
08:22 - talking about linear regression a loss
08:24 - function adding more dimensions this is
08:26 - the idea this is what we're doing
08:27 - of course I highly recommend you watch
08:31 - the three blue one Brown series about
08:34 - rating descent back propagation and back
08:36 - propagation calculus this will give you
08:38 - a massive extreme set background for
08:41 - what I'm gonna attempt to do it's just
08:42 - kind of like let me just tape this in
08:44 - code like kind of squint the press the
08:46 - button hope it works so this is great I
08:48 - highly recommend this and then I also
08:51 - have been using the make
08:52 - your own neural network book which I
08:53 - could hold up and wave around for you by
08:56 - Terry Gross she'd and there's a link to
08:58 - it on the coding train Amazon shop along
09:00 - with some other books that I've been
09:02 - using for videos and then so this is
09:04 - where I what I wanted to do now is try
09:06 - to connect back to here this is from my
09:12 - previous video entitled the mathematics
09:15 - of gradient descent where I go through a
09:17 - long algorithm to arrive at a very
09:20 - simple result which is in this case of y
09:27 - equals MX plus B what I'm looking to do
09:30 - is calculate the change in M and the
09:32 - change in B and so we can now write
09:34 - those formulas over here so in the case
09:38 - of y equals MX plus B we need to
09:42 - calculate Delta M how do we want em to
09:45 - change based on the error and it how we
09:49 - want em to change is the learning rate
09:50 - by the way learning right if you look in
09:52 - textbooks and stuff and sometimes
09:53 - written with this um the reek letter
09:55 - alpha I believe is that how far yeah
09:58 - learning rate times X times error and
10:06 - Delta B equals learning rate times error
10:12 - this is all done through some calculus
10:15 - in looking at the cost function looking
10:17 - at the derivative of the cost function
10:19 - the slope and how to walk around that
10:21 - cost function and find the bottom the
10:24 - minimum error how what values of m and B
10:27 - do we have to have the minimum error and
10:29 - that's what we want to do here what
10:31 - values of w w1 1 W 2 1 Dobie 300 400 I
10:35 - have to have the minimum error and that
10:37 - error each individual error we've got to
10:41 - like back it up and pass around and chop
10:43 - it up okay so how do we take this and
10:47 - move this to a multi-dimensional
10:51 - scenario I'm gonna rewrite these a
10:53 - little smaller further over to the left
10:54 - so I have more room to write out the
10:56 - formulas for the matrix version okay so
11:00 - these are the formulas for the change in
11:03 - slope and
11:05 - in by offset change and Hammack change
11:07 - and B for y equals MX plus B so I have
11:11 - the same situation except I have a
11:14 - slightly different one I have like the
11:16 - output equals Sigma of like that whole
11:20 - you know weight matrix what is it matrix
11:28 - product with the I guess it's uh with
11:32 - the inputs right I have this kind of
11:35 - before but it's basically the same thing
11:36 - plus the bias plus the bias which is a
11:39 - vector so I have like kind of like
11:40 - basically the same thing but instead of
11:42 - like single dimension these are all
11:44 - matrices Multi multi dimensional so what
11:47 - I'm going to attempt to do now is I'm
11:49 - going to just write out a notation for
11:52 - these formulas using matrices and try to
11:56 - like compare and contrast a little bit
11:58 - and I'm not sure I get it right because
12:01 - I kind of I'm using a bunk combination
12:03 - of trying to keep consistent with my
12:04 - notation from before and some
12:05 - conventions but let's see I've got it
12:07 - written down here let's look at this so
12:09 - let's first just say I just want to
12:11 - figure out Delta for these weights so in
12:14 - other words you can think of what I'm
12:16 - doing is instead just Delta n I want all
12:18 - of these weights so I want to say Delta
12:20 - all of the weights and the weights are
12:23 - from hidden to output the change in each
12:27 - weight IJ each row I column J I think
12:32 - that's what I've been using it's hard
12:34 - for me to remember what I used in the
12:35 - last video equals I'm just gonna look
12:37 - very similar first of all learning rate
12:39 - same I always have a learning rate
12:41 - learning rates gonna basically tune like
12:43 - how big of a step are we gonna take and
12:46 - I don't like the way I kind of want to
12:47 - rewrite this just because to have it
12:49 - match the way I'm gonna write the matrix
12:51 - version error times X I'm gonna write
12:57 - this times the vector E right that's
13:03 - that's the that's the vector of output 1
13:08 - minus target one output to our target
13:11 - one minus output one right that error
13:13 - vector that
13:14 - talked about and then X in this case is
13:17 - kind of like the input the input to this
13:22 - neuron to each of this output layer so
13:25 - I'm gonna call that a in this form gonna
13:27 - call that like I could call it x1 we
13:28 - call it H it's what's coming out of the
13:30 - hidden layer I'm gonna put that here
13:33 - when I put that at the end H so these
13:37 - are the components that exactly match up
13:39 - here at times H right we have same way
13:43 - of learning rate we have the error right
13:45 - and by the way if I were doing this for
13:47 - this layer if I would say Delta W Eights
13:50 - from I to J from input to hidden ih
13:53 - would equal do learning rate and then I
13:55 - would say times the hidden error right
13:59 - to remember that from the previous
14:00 - videos where I went through how to get
14:01 - the output error and the hidden error
14:03 - how we pass all that around and then
14:06 - this is times the input so this is the
14:10 - same exact formula but two different
14:11 - layers learning rate hearths or hidden
14:16 - errors the hidden output or the input
14:20 - output
14:20 - sort of way to say they hidden output
14:23 - the hidden output is the input to the
14:24 - output the input is the input to the
14:27 - hidden so you can sort of see how this
14:29 - is this formulas is looking at this part
14:31 - and this formula is looking at this part
14:33 - okay but I didn't put one other thing in
14:35 - here there's something funny that's
14:37 - gonna go in this spot right here so what
14:40 - goes here this is the question now this
14:42 - is where we're so happy to have this
14:44 - book make your up a neural network
14:45 - because we can just look up the formula
14:47 - in here but what's actually is gonna go
14:49 - here is the derivative of the output now
14:52 - let's think about this why why don't I
14:54 - come in with your videos you might have
14:55 - remembered back from the mathematics of
14:57 - gradient descent that we had to take the
15:00 - derivative but in this case of Y in this
15:04 - case it's MX plus B it's very simple
15:07 - derivatives a linear function here we
15:09 - have this sigmoid thing we need to
15:11 - calculate the gradient and the gradient
15:12 - is the errors time the error of the
15:16 - output times the derivative of the
15:18 - output what happened well has the output
15:21 - change has the output change relative to
15:24 - the errors so in this case right we need
15:27 - to add
15:27 - here the derivative of sinh way now
15:29 - here's the wonderful thing sigmoid is
15:31 - the function right the derivative of
15:33 - sigmoid s prime X is simply equal to the
15:38 - sigmoid of x times 1 minus can you see
15:43 - what I'm writing is sigmoid of X so in
15:47 - this case we need to calculate this
15:49 - gradient the derivative of sigmoid and
15:52 - right here now one thing I should really
15:55 - clarify here is so learning rate is a
15:58 - scalar number error is a vector learning
16:01 - rate is a scalar number I'm gonna put an
16:03 - asterisk here err a hidden error is a
16:05 - vector so I need to multiply and the
16:09 - output is a vector so these are actually
16:11 - this is element wise multiplication of
16:13 - the out now I've already what's coming
16:16 - out of here out of output has already
16:18 - had the sigmoid pass through it so I
16:19 - just need to say the output plus 1 minus
16:21 - the output the output plus 1 minus the
16:25 - output now what's weird about this is
16:29 - now the H is really this right we have
16:33 - an exact same formula I have the the
16:35 - learning rate the error gradient here
16:38 - you can sort of consider this gradient
16:39 - here is just the error the gradient here
16:41 - is the error times the derivative of the
16:44 - output and then if I'm multiplying I
16:46 - need to get a weight matrix so the input
16:48 - by the way is also a vector but I need
16:51 - to get a matrix so the interesting that
16:54 - happens if I use the matrix product here
16:56 - and transpose this vector right this is
17:00 - element wise multiplication this is just
17:03 - a scalar now here's the thing if I have
17:10 - the sorry if I have this error and type
17:15 - the gradient essentially at a called
17:17 - gradient as a single column vector for
17:25 - columns if I multiply that by a the
17:31 - hidden output which is also a single
17:33 - column vector but transposed
17:38 - if you take a single column matrix and
17:42 - multiply it by a single row matrix as
17:44 - long as it has this has the same number
17:47 - of columns as this has rows let's let's
17:49 - look at what happens right there's a
17:51 - wonderful website that I use often when
17:52 - I get stuck it's called matrix
17:55 - multiplication XYZ and here what I can
17:58 - do is I can make I'm gonna make this
17:59 - exact right here's an arbitrary single
18:02 - column vector and then I'm gonna make a
18:06 - single row vector so a 1 column matrix a
18:10 - 1 row matrix I'm gonna hit multiply so
18:12 - this is I'm just gonna go to the end
18:14 - here we can this is a nice little
18:16 - animation that goes through everything
18:17 - that I did in previous matrix
18:19 - multiplications and you can see we end
18:20 - up with a 4 by 4 matrix so this is
18:23 - exactly what we need to do to be able to
18:26 - get the deltas for all the weights ok so
18:33 - this also has to be I'm going here this
18:36 - has to be transposed and this should be
18:38 - element wise multiplication with the the
18:43 - I lost track here so I'm taking the
18:48 - output error and multiply by the
18:51 - derivative of the output here I'm taking
18:53 - the hidden error and multiplying it by
18:55 - the derivative of the hidden so that
18:57 - would be H plus h plus 1 so not plus Oh
19:02 - what I have plus here have had that
19:04 - wrong for so long times that's another
19:05 - element wise multiplication times 1 and
19:08 - this is a 1 not an I 1 minus H ok I
19:13 - think we've done it I'm gonna just check
19:15 - here my trusty guide but I highly
19:18 - recommend that you read instead of
19:20 - listing reads that are listening to me
19:21 - and I'm gonna look at this the change in
19:24 - weight matrix equals learning rate times
19:26 - the error times sigmoid of the output
19:30 - well I have sigmoid kind of built in
19:32 - here I'm assuming the output if the
19:34 - sigmoid has already been done
19:35 - times 1 minus Sigma or 2 the output I'm
19:37 - assuming Sigma so I've been done with
19:38 - the dot product of the matrix
19:40 - multiplication of the hidden the output
19:43 - transpose well this is right my notation
19:45 - is slightly different okay so I think
19:49 - we're just about right now of course I
19:51 - forgot
19:51 - the bias so I'm gonna have to basically
19:53 - do exactly the same thing with the bias
19:54 - bias is only simpler because I can just
19:57 - get rid of this so I'm assuming I could
19:58 - probably sort of do the same thing here
20:00 - so but I'll get to that in a bit
20:04 - let me at least try to implement this in
20:06 - the code and then I'll do that in the
20:08 - next video and then we'll come back to
20:10 - the bias
20:14 - [Music]

Cleaned transcript:

hello all right this is a good moment for me I think I'm excited to see if I can get through this video because if I can implement this last piece of the Train function in the neural network library then I'll have a working version of some kind of neural network library like thing that I can start to finally apply to some projects and it's my goal actually that in some of the future videos I make that make use of the library and eventually other more sophisticated machine learning deep learning libraries like deep learning is a new library which i think is now gonna be called ml5 which i'll talk about in another video that I'll build really well I'm sad about it's to make stuff and show you how to make stuff but I'm kind of working through this because it's just something I have to do so I've started I have to finish I'm just trying to like vamp till you've stopped move on and watch something else because I'm not sure you should continue so but let's let me try to it's actually been a while since I recorded the previous video unfortunately might be watching them one after another let me try to like reset kind of where I think that we are so we have a something like a two layer network it has you know an input quote/unquote airboat layer it has these so these are the inputs this is the hidden and this is the output it is fully connected meaning every input is connected to every hidden and every output is connected to every every hidden is connected every input every output is connected every hidden and so the outputs come out here the inputs come out here all of these connections have a weight so and we can consider them in a weight matrix and I guess I should put this stuff back in here wait if this is like input one two three this is hidden one two this is weight one two one this is weight two to one this so these all end up in a nice weight matrix these end up in weight matrix the outputs come out oh boy in array like y1 y2 a vector the inputs come in in a vector x2 x3 okay don't get back into this I didn't even draw like off the top wonderful so now the idea is we want to do training and the kind of training that I'm doing right now is called supervised learning where I have some known output I've some known inputs with known outputs inputs with target outputs I send the inputs in I feed them forward I get some actual guests output back and I have some sort of error which we can think of as the error equals the guess I'm the guess the target so I'm gonna say like y target okay so this is the error now I should have mentioned before that there are a variety of ways to train a neural network and by training I mean adjusting all of these weights like their knobs to try to actually get the matched target output when you send in the training data so one thing I should mention I really gotta like even just take a minutes and make a video just about this but in most training situations you'll have training data test data and then of course there's the actual unknown data so we want to simply want to say like oh here's some labeled data I'm going to use that to train but I need to save some of that labeled data because what if I train my neural network only to work with the training data but it doesn't actually work with any other data well I can determine that by giving it some saved some data that I didn't train it with but I know the answer to see how it dumps with that and that's how we can evaluate it so I just want to mention that this is going to be the process now this idea of back propagation with gradient descent is one technique and it's a technique I'll put some links in this video's description you read about the history of it it's it's been around for a long time it's a big innovation in training neural networks but there is a lot of questions as to whether that's optimal best for the future etc there's different you know slept weeks of these algorithms and I actually next probably after I get through this plan to use a genetic algorithm to evolve the weights of network which opens up the door for a lot of kinds of projects that I excited to try to make so all that aside here we are what did we get so far we figured out in the previous video we calculated this error and then we calculated the hidden error which I'll just call each error right that's part of back propagation it's while we have this error how do we distribute this error all around so we can adjust all these weights the reason why we're doing this is because we actually did this already twice I've done this twice once with if you look at my videos about a single perceptron right which gets two inputs I forgot about the bias I always I always ever remember the bias I'm biased against the bias I'll come back to that but the single perceptron which just takes in two inputs and one output well it doesn't say it's a single neuron you can take in more than two inputs but a single neuron with multiple inputs and an output I actually did this we did this idea of training this with gradient descent and we also did it I did it we I did it in a video about linear regression where if I have a bunch of points in a two dimensional space I can find the line that fits these points the best and they did this what's interesting here is though this app so for a linear regression if you recall there's actually a mathematical formula to just compute the exact line of best fit called ordinary least squares I think I went through that in a previous video as well but the idea here and so the idea is we're trying to figure out y equals MX plus B the formula for this line well basically what this is doing is this is exactly the same process that we needed here only we have these weights you're going on you can almost think of this as like m and this is B maybe or something if there's just one input and a bias we had to fit we had to fit all the stuff coming through here into a line well this is actually what we need to do with all of these all of these places right we basically need to do the exact same algorithm that we did for this one line to compute this is the weight and this is the bias what we really have right is that why those MX plus B we have y equals M 1 X 1 plus M 2 X 2 plus M 3 can you see that X 3 plus M 4 X 4 but added up plus B we have this we basically have exactly the same problem but in a multidimensional space so I just need to figure out how do I adjust each one of these M one and two and three and four and B all these weights inside of the matrix so the same training method I did for a linear regression gradient descent the same thing we did with the perceptron we now need to apply it here in this multidimensional space so what should I do next now first of all I forgot to thank a bunch of the resources hold on I some viewers rightly pointed out that when I did this previously and I guess the convention is target minus y the nice thing about this is when if you look at a cost function for a machine learning system you'll see that the cost function is the sum of all the errors squared so if you do target minus y or y minus target you square it doesn't really matter but it is an important distinction probably you have to get it right of course otherwise you might start trading in the wrong direction as you'll start to see as we do other stuff okay let me come over here and let me think so I've better come back to this video in a second but let me first say there are three things one is this is a new resource that just came out it's not a new resource but a new page on the ml for a ml for a github IO site this is a site put together by Jean Cogan has a ton of machine learning resources videos examples demos etc it's amazing and there is a nice article here about how neural networks are trained with a lot more detail than I'm gonna get into here but you can see the same sort of idea of talking about linear regression a loss function adding more dimensions this is the idea this is what we're doing of course I highly recommend you watch the three blue one Brown series about rating descent back propagation and back propagation calculus this will give you a massive extreme set background for what I'm gonna attempt to do it's just kind of like let me just tape this in code like kind of squint the press the button hope it works so this is great I highly recommend this and then I also have been using the make your own neural network book which I could hold up and wave around for you by Terry Gross she'd and there's a link to it on the coding train Amazon shop along with some other books that I've been using for videos and then so this is where I what I wanted to do now is try to connect back to here this is from my previous video entitled the mathematics of gradient descent where I go through a long algorithm to arrive at a very simple result which is in this case of y equals MX plus B what I'm looking to do is calculate the change in M and the change in B and so we can now write those formulas over here so in the case of y equals MX plus B we need to calculate Delta M how do we want em to change based on the error and it how we want em to change is the learning rate by the way learning right if you look in textbooks and stuff and sometimes written with this um the reek letter alpha I believe is that how far yeah learning rate times X times error and Delta B equals learning rate times error this is all done through some calculus in looking at the cost function looking at the derivative of the cost function the slope and how to walk around that cost function and find the bottom the minimum error how what values of m and B do we have to have the minimum error and that's what we want to do here what values of w w1 1 W 2 1 Dobie 300 400 I have to have the minimum error and that error each individual error we've got to like back it up and pass around and chop it up okay so how do we take this and move this to a multidimensional scenario I'm gonna rewrite these a little smaller further over to the left so I have more room to write out the formulas for the matrix version okay so these are the formulas for the change in slope and in by offset change and Hammack change and B for y equals MX plus B so I have the same situation except I have a slightly different one I have like the output equals Sigma of like that whole you know weight matrix what is it matrix product with the I guess it's uh with the inputs right I have this kind of before but it's basically the same thing plus the bias plus the bias which is a vector so I have like kind of like basically the same thing but instead of like single dimension these are all matrices Multi multi dimensional so what I'm going to attempt to do now is I'm going to just write out a notation for these formulas using matrices and try to like compare and contrast a little bit and I'm not sure I get it right because I kind of I'm using a bunk combination of trying to keep consistent with my notation from before and some conventions but let's see I've got it written down here let's look at this so let's first just say I just want to figure out Delta for these weights so in other words you can think of what I'm doing is instead just Delta n I want all of these weights so I want to say Delta all of the weights and the weights are from hidden to output the change in each weight IJ each row I column J I think that's what I've been using it's hard for me to remember what I used in the last video equals I'm just gonna look very similar first of all learning rate same I always have a learning rate learning rates gonna basically tune like how big of a step are we gonna take and I don't like the way I kind of want to rewrite this just because to have it match the way I'm gonna write the matrix version error times X I'm gonna write this times the vector E right that's that's the that's the vector of output 1 minus target one output to our target one minus output one right that error vector that talked about and then X in this case is kind of like the input the input to this neuron to each of this output layer so I'm gonna call that a in this form gonna call that like I could call it x1 we call it H it's what's coming out of the hidden layer I'm gonna put that here when I put that at the end H so these are the components that exactly match up here at times H right we have same way of learning rate we have the error right and by the way if I were doing this for this layer if I would say Delta W Eights from I to J from input to hidden ih would equal do learning rate and then I would say times the hidden error right to remember that from the previous videos where I went through how to get the output error and the hidden error how we pass all that around and then this is times the input so this is the same exact formula but two different layers learning rate hearths or hidden errors the hidden output or the input output sort of way to say they hidden output the hidden output is the input to the output the input is the input to the hidden so you can sort of see how this is this formulas is looking at this part and this formula is looking at this part okay but I didn't put one other thing in here there's something funny that's gonna go in this spot right here so what goes here this is the question now this is where we're so happy to have this book make your up a neural network because we can just look up the formula in here but what's actually is gonna go here is the derivative of the output now let's think about this why why don't I come in with your videos you might have remembered back from the mathematics of gradient descent that we had to take the derivative but in this case of Y in this case it's MX plus B it's very simple derivatives a linear function here we have this sigmoid thing we need to calculate the gradient and the gradient is the errors time the error of the output times the derivative of the output what happened well has the output change has the output change relative to the errors so in this case right we need to add here the derivative of sinh way now here's the wonderful thing sigmoid is the function right the derivative of sigmoid s prime X is simply equal to the sigmoid of x times 1 minus can you see what I'm writing is sigmoid of X so in this case we need to calculate this gradient the derivative of sigmoid and right here now one thing I should really clarify here is so learning rate is a scalar number error is a vector learning rate is a scalar number I'm gonna put an asterisk here err a hidden error is a vector so I need to multiply and the output is a vector so these are actually this is element wise multiplication of the out now I've already what's coming out of here out of output has already had the sigmoid pass through it so I just need to say the output plus 1 minus the output the output plus 1 minus the output now what's weird about this is now the H is really this right we have an exact same formula I have the the learning rate the error gradient here you can sort of consider this gradient here is just the error the gradient here is the error times the derivative of the output and then if I'm multiplying I need to get a weight matrix so the input by the way is also a vector but I need to get a matrix so the interesting that happens if I use the matrix product here and transpose this vector right this is element wise multiplication this is just a scalar now here's the thing if I have the sorry if I have this error and type the gradient essentially at a called gradient as a single column vector for columns if I multiply that by a the hidden output which is also a single column vector but transposed if you take a single column matrix and multiply it by a single row matrix as long as it has this has the same number of columns as this has rows let's let's look at what happens right there's a wonderful website that I use often when I get stuck it's called matrix multiplication XYZ and here what I can do is I can make I'm gonna make this exact right here's an arbitrary single column vector and then I'm gonna make a single row vector so a 1 column matrix a 1 row matrix I'm gonna hit multiply so this is I'm just gonna go to the end here we can this is a nice little animation that goes through everything that I did in previous matrix multiplications and you can see we end up with a 4 by 4 matrix so this is exactly what we need to do to be able to get the deltas for all the weights ok so this also has to be I'm going here this has to be transposed and this should be element wise multiplication with the the I lost track here so I'm taking the output error and multiply by the derivative of the output here I'm taking the hidden error and multiplying it by the derivative of the hidden so that would be H plus h plus 1 so not plus Oh what I have plus here have had that wrong for so long times that's another element wise multiplication times 1 and this is a 1 not an I 1 minus H ok I think we've done it I'm gonna just check here my trusty guide but I highly recommend that you read instead of listing reads that are listening to me and I'm gonna look at this the change in weight matrix equals learning rate times the error times sigmoid of the output well I have sigmoid kind of built in here I'm assuming the output if the sigmoid has already been done times 1 minus Sigma or 2 the output I'm assuming Sigma so I've been done with the dot product of the matrix multiplication of the hidden the output transpose well this is right my notation is slightly different okay so I think we're just about right now of course I forgot the bias so I'm gonna have to basically do exactly the same thing with the bias bias is only simpler because I can just get rid of this so I'm assuming I could probably sort of do the same thing here so but I'll get to that in a bit let me at least try to implement this in the code and then I'll do that in the next video and then we'll come back to the bias
