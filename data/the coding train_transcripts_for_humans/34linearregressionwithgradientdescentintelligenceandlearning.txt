With timestamps:

00:00 - hello okay so this is now another video
00:03 - in my series about linear regression now
00:06 - why are you watching these videos not my
00:08 - shirt but the topic here and the skills
00:12 - here I hope are laying a foundation for
00:15 - what I'm going to get to in future
00:17 - videos which is building a neural
00:19 - network based machine learning system so
00:20 - at the top of this video why am i making
00:23 - another video about linear regression so
00:24 - what I did in the previous two videos is
00:26 - I created a p5.js sketch which
00:30 - implements linear regression using the
00:32 - ordinary least-squares method so a
00:33 - statistical approach there are a whole
00:35 - bunch of data points into the space and
00:37 - I try to fit a line but that fits to
00:40 - that data as best as possible so that I
00:42 - could predict new data points in this
00:45 - space and you can see as I start to
00:47 - click around how the line is fit sort of
00:49 - changes now I also discussed a little
00:51 - bit of all does linear regression make
00:53 - sense based on your data and these are
00:55 - big important questions in working your
00:58 - data science and machine learning but
00:59 - right now we're just trying to focus on
01:00 - the techniques now one thing you'll
01:02 - notice is here is i refresh this page I
01:04 - make I click twice I get a line
01:06 - instantly because I'm actually
01:08 - calculating the perfect exact best fit
01:12 - line according to the least squares
01:14 - method but something day we will have a
01:19 - data set that's not two-dimensional
01:22 - someday we will have a data set that has
01:24 - hundreds that's a big data set that's a
01:28 - many many dimensional and in this case
01:30 - there isn't going to be an easy
01:33 - statistical approach that could just be
01:35 - done to fit to create a model that fits
01:39 - the data perfectly with a single
01:40 - calculation on it okay so this is this
01:45 - is the problem that machine learning
01:47 - neural network based deep learning based
01:50 - systems are here to solve to figure out
01:54 - a way to create a model to fit a given
01:56 - data set and one technique for doing
01:59 - that which is different than say
02:00 - ordinary least squares is to use a
02:03 - technique called gradient descent and
02:05 - what gradient ascent descent essentially
02:08 - does it says let me make a guess I'm
02:09 - just going to put the line here
02:11 - and then the see how is that line good
02:13 - or not so good that's good
02:15 - let me try shifting it a little closer
02:17 - to the data let me try shifting it a
02:18 - little closer to the data let me shift
02:19 - this let me shift it so making lots of
02:21 - little small nudges and tweaks into what
02:24 - that line is doing I think I have a
02:26 - better way of explaining that so I'm
02:27 - going to come over here the whiteboard
02:29 - I'm going to do a little magic here
02:30 - which is that I'm going to stand right
02:32 - over here I'm going to snap my fingers
02:36 - and the moment I snap my fingers the
02:38 - whiteboard behind me could be erased Wow
02:41 - oh that work dork that's kind of
02:45 - interesting
02:45 - okay so let's I'm going to make this
02:49 - same kind of diagram that I've made a
02:51 - few times now and I'm going to we're
02:53 - going to like really simplify it so we
02:56 - have this idea this two-dimensional
03:00 - space there is some piece of data we'll
03:05 - call it X for example the temperature
03:07 - that's the that it is outside today
03:10 - and we're trying to predict some outcome
03:12 - may be based on that temperature
03:14 - yesterday we talked as we'll call that Y
03:16 - we have at of the sales of ice cream I
03:20 - saw actually a dataset that was
03:21 - interesting about like the frequency at
03:23 - which crickets chirp according to the
03:25 - temperature outside
03:26 - that's the dataset you can find online
03:27 - somewhere of use of so we have this idea
03:30 - and maybe there's some existing data
03:32 - points based on ice cream store that we
03:36 - have studied and I can grasp that data
03:39 - so the idea here is that we have our
03:43 - machine learning recipe we are going to
03:48 - take and I know I'm out of the frame
03:50 - here you're going to take one of our
03:54 - inputs called X feed it into the machine
03:57 - learning recipe and the machine learning
03:59 - recipe is going to give us a prediction
04:03 - Y so we have known data and we could if
04:08 - we had new input data we could make a
04:10 - guess so yesterday my machine learning
04:14 - recipe was the ordinary least-squares
04:16 - method meaning I was able to do a
04:19 - statistical analysis of all this data
04:20 - and create the line of best fit
04:24 - and then if I had a new input you know X
04:27 - value of such-and-such I could look up
04:29 - its corresponding spot on the line and
04:32 - that would be the Y output this is a
04:33 - function the machine learning recipe is
04:36 - essentially solving for M and B in the
04:40 - equation of a line so that's what I did
04:42 - yesterday today's technique I want to
04:46 - demonstrate the technique known as
04:48 - gradient descent so the idea of gradient
04:55 - descent is okay so boy so much to say
04:59 - where should I start where should I end
05:01 - I really have no idea
05:02 - so one thing I'll mention is that the
05:05 - mass required for gradient descent
05:08 - typically involve calculus and they
05:11 - involve two concepts from calculus one
05:15 - called a partial derivative which if you
05:17 - don't know calculus or what a derivative
05:18 - is well how can you be expected know
05:20 - what a partial derivative is as well as
05:23 - a single the chain rule and I think what
05:27 - I'm going to do is I'm going to walk
05:29 - through this entire system and how it
05:32 - works and explain it without diving
05:35 - deeper into the mask um but I will make
05:43 - a follow-up video where I discuss some
05:45 - of those pieces in a bit more detail so
05:46 - so okay so but here's a way that you
05:49 - could think about gradient descent thats
05:51 - related to stuff that I have done in
05:53 - previous videos and in my book nature of
05:55 - code where I reference the work of Craig
05:58 - Reynolds steering behaviors so think
06:00 - about this for a second this is this is
06:02 - great let's say you have a
06:06 - two-dimensional space and you have a
06:10 - vehicle that is moving around an agent
06:13 - that is moving around this space and the
06:16 - vehicle has a particular velocity
06:20 - expressed as a vector or an arrow in
06:22 - this case now what if the goal of this
06:25 - vehicle is to reach this target well we
06:31 - could say that this vehicle has a
06:34 - desired velocity its desired velocity
06:38 - city is to move at maximum speed from
06:42 - its current location to towards the
06:44 - target so this is a vector which is its
06:47 - desired velocity and it's useful and you
06:50 - can think about the difference like the
06:52 - it's currently this vehicles current
06:54 - philosophy is like its guess I don't
06:56 - know where I should go I'm going to try
06:58 - going this way oh but really I should go
07:00 - this way well if I'm going this way but
07:02 - I really should go this way what if I
07:03 - just turn a little bit towards the
07:05 - target what if I were to just steer a
07:08 - little bit in that direction and this is
07:10 - what gradient descent does you can think
07:13 - of desired as the known output the
07:17 - correct output what if I feed in one of
07:21 - these data points right and I say look
07:27 - at this particular X Y hair let me feed
07:31 - it in let me try to get a guess
07:34 - just sometimes I think written is why a
07:36 - tick I think but I'm going to say what
07:39 - if I get Y i'ma say Y get the error is
07:45 - the difference between what I guessed it
07:49 - would be minus what it actually should
07:52 - be right if I start with an X Y pair
07:55 - this is the error and you'll notice if
07:57 - you look at Craig Reynolds steering
07:59 - behaviors and all of these animated
08:01 - systems that I said I implemented from
08:03 - that work you'll see there's a formula
08:05 - in it steering equals desired minus
08:11 - velocity so you know I put I put it get
08:15 - in you know I kind of do the reverse
08:19 - here because this is really the
08:20 - equivalent desire but the point is the
08:22 - difference between the way that I should
08:24 - go and the way that I am going that's
08:26 - the error the difference between what my
08:28 - machine learning recipe what my model
08:30 - currently thinks the output should be
08:32 - compared to the known output that is the
08:35 - error and steering X if I adjust my
08:38 - velocity if I steer towards the desired
08:41 - I'm going to get a better model I'm
08:43 - going to move towards the target if I
08:44 - use this error to tweak the parameters
08:48 - machine learning recipe I'm going to
08:50 - make my model I'm going to have better M
08:52 - and B values for the next time and I
08:54 - could do this over and over and over and
08:56 - over again and this is we've been
08:59 - talking about this supervised learning I
09:02 - can take the known data send it in get a
09:06 - guess look at the error tweak the knobs
09:08 - send the next data point and get a guess
09:11 - look at the error tweak the knob I can
09:13 - do this over and over and over again and
09:15 - I can just start with random values for
09:18 - M and B so I don't know what it B I'm
09:20 - going to just put a line here and then I
09:22 - can start moving the line around
09:24 - according to the error as I go through
09:25 - all the data so this is what we're
09:28 - trying to do okay so there's more to how
09:30 - the math behind this stuff works and how
09:33 - we look at the overall error and there's
09:35 - some stuff that involves the derivative
09:37 - and the slope of the graph of the error
09:40 - and I'm going to I think I'm going to
09:41 - come back some of that stuff in a second
09:43 - video where I go a bit further into some
09:45 - of the math here but what I'm actually
09:47 - going to do is just start showing you
09:48 - how to set up to do gradient descent in
09:51 - the code itself so let me come over here
09:53 - so this as you saw before this is the
09:56 - example from yesterday that's using the
09:58 - ordinary least-squares method so what
10:02 - I'm going to do now is I am going to
10:07 - this so I had this function linear
10:10 - regression and this linear regression
10:12 - function calculates the slope of the
10:15 - line and the y-intercept and B according
10:19 - to or nearly squares so what I'm going
10:22 - to do is I'm just going to completely
10:25 - get rid of this so now nothing happens
10:29 - there so I can click and the first guest
10:35 - of the line I just plugged in some
10:36 - values that typically speaking I think
10:38 - what's probably typically done is these
10:40 - values are initialized at 0 these are
10:43 - like weights so to speak and ultimately
10:45 - you can see these are analogous to the
10:47 - weights of connections in a neural
10:49 - network but this M and B values I could
10:51 - start with in randomly I could pick
10:53 - something and hard-coded I could get let
10:55 - them both be 0 I think imma stick with
10:57 - actually 1 comma 0 just to sort of see
10:58 - because then at least I can see that the
11:00 - line is there
11:00 - so now what I want to do is I want to
11:03 - look at with the existing data points I
11:06 - want to look at the error and I want to
11:09 - adjust M and B in the direction of the
11:12 - error so let's see how that goes so I'm
11:16 - going to call this now gradient descent
11:22 - and so in the draw function I think I
11:26 - want to call this now gradient descent
11:32 - I'm back a little digression there that
11:35 - has added out thanks for thanks for
11:38 - tuning in ok so where I am is that I'm
11:41 - changing the name of the function to
11:42 - gradient descent and what I want to do
11:44 - is I'm going to just look through all of
11:47 - the data so let's just first look
11:54 - through all the data and ok so for each
11:59 - data set I have the Y is data index iy
12:03 - so we can get the X and the y then I can
12:10 - actually calculate a guess so my guess
12:14 - is M times X plus B right this is my
12:18 - machine learning recipe I am taking the
12:22 - input data X I am multiplying it by n I
12:24 - am adding B and that is my guests so now
12:27 - my error equals my error equals y minus
12:35 - the guess and I think technically
12:37 - speaking I think I should be saying
12:38 - guess minus y now you may recall that in
12:43 - the ordinary least-squares method I
12:46 - would always square the error because I
12:48 - want to get rid of the sort of positive
12:50 - or negative aspect of it in this case
12:52 - and again I'm going to go a little
12:54 - further into this in the next video I
12:55 - actually want the positive or negative
12:57 - direction of the error because I want to
12:59 - know which way in essence to tune the N
13:03 - and B values to get a better result so
13:07 - the issue here is now and this is what's
13:10 - known as stochastic gradient descent
13:12 - so I want to make an enforceable I want
13:17 - to make a change to M and B so I need to
13:23 - calculate how should I change M and how
13:26 - should I change B so really what I'm
13:27 - saying is M equals M plus some amount of
13:30 - change B equals B plus some amount of
13:33 - change and we can in this case kind of
13:36 - say this is us one way to think about
13:38 - and understand it I have this error who
13:41 - is responsible who is to blame here is
13:43 - it um is it you be so who's in charge
13:45 - here what's the what's going on I got to
13:48 - figure this out so in essence we could
13:50 - say if I adjust those values according
13:53 - to the error maybe if I tried it again I
13:55 - would get a better result and in this
13:56 - case B can be adjusted directly by the
13:59 - error because it's just the y-intercept
14:01 - should I move it up or down and M which
14:04 - is the slope can be adjusted by the
14:06 - error but according to according to also
14:10 - the input value itself so this is how
14:12 - you can kind of intuitively understand
14:14 - it I want to adjust those values
14:16 - according to the error the slope also
14:18 - relates to what the input actually was
14:20 - the y-intercept just the error itself
14:22 - now so I'm missing a whole bunch of
14:23 - steps and a bit a few pieces of
14:25 - explanation here but let's just run this
14:27 - and see what happens so first I always
14:32 - have to click okay well first of all I
14:34 - got an error but uncaught reference
14:36 - error and is not defined in gradient
14:38 - descent whereas I have n Oh B equals B
14:41 - plus err yeah I don't know what n is so
14:44 - you can see like okay well I don't know
14:46 - where that line went it was there for a
14:48 - second and it just went far away so
14:50 - here's the thing if I come back to my
14:53 - analogy from the steering one of the
14:58 - things in the steering behavior examples
15:00 - from nature of code and Craig Reynolds
15:02 - example is that there was a variable
15:04 - called maximum force I hope you can see
15:09 - that maximum force because one thing you
15:11 - might think about it here is well how
15:12 - powerful I know what the error is
15:14 - between the way I'm going and where I
15:16 - want to go how powerful is my ability to
15:20 - turn
15:20 - well maybe I'm able to turn at like Inc
15:23 - with infinite power and that
15:25 - be good but not so good because if I try
15:27 - to like push myself I might end up going
15:28 - all the way down this way and I'm like
15:30 - oh my god I'm going in the wrong
15:31 - direction and then they have going all
15:32 - the way up in the other direction maybe
15:34 - I just won't we want to be able to make
15:36 - little adjustments because it's the
15:38 - wrong way I want to just make a slight
15:40 - adjustment I don't want to overshoot the
15:41 - target the target being I want to find
15:44 - the parameters I want to find the
15:46 - weights m and B values to minimize the
15:49 - error so so I don't want to overshoot
15:53 - what that minimum that that optimal
15:56 - value is and so that is where a variable
15:58 - sometimes called alpha but most commonly
16:00 - called learning rate comes in so I could
16:03 - have a variable called learning rate
16:05 - usually this is a small number something
16:07 - to really reduce the size of that error
16:09 - so in this case I would say well let me
16:12 - take this change in the value of the
16:15 - slope and multiply it by the learning
16:18 - rate and let me change take this for B
16:21 - and multiply it by the learning rate
16:23 - okay so now I'm going to try this again
16:26 - with a learning rate of point zero zero
16:29 - one hey that doesn't look right
16:35 - come back to me okay so let's think
16:38 - about what might be wrong over here
16:41 - I wrote guess minus y and that's really
16:47 - what I that's what I wrote here no I
16:48 - want Y minus guess I knew it was always
16:51 - the same
16:53 - so hopefully you're not watching this
16:57 - but in this case here right steering if
17:01 - I want to move towards the target the
17:03 - error is the desired the known result
17:07 - minus the velocity and so this should
17:10 - really be I want to move in that
17:12 - direction Y minus y guess let me change
17:17 - that to I changed it already wait how
17:19 - did I do that
17:20 - okay I must have done this before that I
17:22 - went to explain it let's try this looks
17:29 - pretty good right now here's the thing
17:32 - let me put nav back to zero
17:36 - hit refresh here and so let's see so we
17:40 - can see interestingly enough this isn't
17:42 - the correct correct line because the
17:45 - line should really go through those two
17:46 - points you know I think I've got an
17:48 - issue here with the learning rate so you
17:51 - can see how it was kind of like moving
17:53 - to the right spot but then it's still
17:54 - making very very small small changes
17:57 - only have two points not a lot of data
17:59 - not a lot of time for it to change I
18:01 - probably just need kind of a larger
18:02 - higher learning rate here just for this
18:04 - demonstration let's make it at 0.05 and
18:08 - we can see now it's kind of moving much
18:10 - more quickly and it's starting to turn
18:12 - I'll be it very slowly but you can see
18:15 - as it's slowly slowly turning
18:17 - approaching the correct the correct or
18:19 - the optimal spot for this line and as
18:21 - you can see if I were to click again and
18:23 - click again try to you know click a lot
18:27 - up here and a lot down here ultimately
18:29 - eventually I should start getting the
18:33 - line of best fit now so there are you
18:36 - know some strategies that in theory you
18:38 - should really need to adjust the
18:39 - learning rate over time but there is a
18:41 - technique and a lot of machine learning
18:43 - systems that you will say see that you
18:45 - can call annealing I think that's the
18:47 - right word where you kind of start with
18:49 - a high learning rate and then slowly
18:51 - over time reduce it so you can kind of
18:53 - get some big correction at the beginning
18:55 - and then find some some smaller
18:56 - Corrections ok so some folks in the chat
18:59 - we're asking about like ok well it's
19:01 - sort of performing weirdly if I put a
19:03 - lot of like points above and below but
19:05 - if I put you know points to the right
19:08 - and left it's kind of it fits the line
19:10 - very nicely you can see they're not so
19:13 - big now I'm doing above and below again
19:14 - so here's the thing collinear any
19:17 - meaning like a lot of vertical points is
19:19 - not really good this isn't real if data
19:21 - doesn't really make sense for linear
19:22 - regression I'll try to make prediction
19:24 - so we're not necessarily going to get a
19:26 - good line and part of what I'm doing
19:27 - again is not to demonstrate the optimal
19:30 - way to do linear regression but to
19:32 - demonstrate the technique coders grading
19:34 - a set descent of making small
19:36 - adjustments to weights to parameters to
19:39 - the slope and y-intercept based on an
19:41 - error based on the supervised learning
19:43 - process so this is a start to that you
19:47 - could stop here and I highly recommended
19:49 - you do
19:50 - is what I'm going to do in the next
19:51 - video I don't really know how it's going
19:52 - to go to be honest but I'm going to try
19:55 - to look a little bit more closely as to
19:58 - why this works out the way that it works
20:03 - how do I know how to change and and be
20:07 - how do I know exactly how to change M &
20:10 - B to minimize the error I said kind of
20:13 - well the error kind of gives us the
20:15 - direction in which to change this has to
20:17 - do with calculus it has to do with
20:19 - comparing how changing one variable
20:21 - affects another variable so if I change
20:25 - M how does that change the error and can
20:28 - I look at the slope of a graph perhaps
20:31 - to see how to move along that graph to
20:34 - minimize that error so this is what I'm
20:36 - going to cover a bit more in the next
20:38 - video I'm not going to really I may not
20:39 - actually also change this is I said I
20:41 - think I said this stochastic gradient
20:43 - descent meaning I'm adjusting the
20:45 - weights and adjusting the m and B values
20:47 - with every data point but I could also
20:50 - look at this with error in totality and
20:52 - then adjust the weights all at once at
20:55 - the end of one cycle through all of the
20:57 - data and that's known as batch gradient
20:59 - descent so I'm going to do what I'm
21:02 - going to do is explain a bit more about
21:03 - the math here and then I'm going to do
21:05 - it and change the code to batch gradient
21:07 - descent in the next video it might be
21:09 - many parts to be honest with you but I
21:12 - don't know it how it's going to go maybe
21:13 - this video is not going to exist the
21:14 - next one you can look see if it's there
21:15 - because I don't know if I should really
21:17 - make it okay see you soon thanks for
21:18 - watching this
21:24 - [Music]

Cleaned transcript:

hello okay so this is now another video in my series about linear regression now why are you watching these videos not my shirt but the topic here and the skills here I hope are laying a foundation for what I'm going to get to in future videos which is building a neural network based machine learning system so at the top of this video why am i making another video about linear regression so what I did in the previous two videos is I created a p5.js sketch which implements linear regression using the ordinary leastsquares method so a statistical approach there are a whole bunch of data points into the space and I try to fit a line but that fits to that data as best as possible so that I could predict new data points in this space and you can see as I start to click around how the line is fit sort of changes now I also discussed a little bit of all does linear regression make sense based on your data and these are big important questions in working your data science and machine learning but right now we're just trying to focus on the techniques now one thing you'll notice is here is i refresh this page I make I click twice I get a line instantly because I'm actually calculating the perfect exact best fit line according to the least squares method but something day we will have a data set that's not twodimensional someday we will have a data set that has hundreds that's a big data set that's a many many dimensional and in this case there isn't going to be an easy statistical approach that could just be done to fit to create a model that fits the data perfectly with a single calculation on it okay so this is this is the problem that machine learning neural network based deep learning based systems are here to solve to figure out a way to create a model to fit a given data set and one technique for doing that which is different than say ordinary least squares is to use a technique called gradient descent and what gradient ascent descent essentially does it says let me make a guess I'm just going to put the line here and then the see how is that line good or not so good that's good let me try shifting it a little closer to the data let me try shifting it a little closer to the data let me shift this let me shift it so making lots of little small nudges and tweaks into what that line is doing I think I have a better way of explaining that so I'm going to come over here the whiteboard I'm going to do a little magic here which is that I'm going to stand right over here I'm going to snap my fingers and the moment I snap my fingers the whiteboard behind me could be erased Wow oh that work dork that's kind of interesting okay so let's I'm going to make this same kind of diagram that I've made a few times now and I'm going to we're going to like really simplify it so we have this idea this twodimensional space there is some piece of data we'll call it X for example the temperature that's the that it is outside today and we're trying to predict some outcome may be based on that temperature yesterday we talked as we'll call that Y we have at of the sales of ice cream I saw actually a dataset that was interesting about like the frequency at which crickets chirp according to the temperature outside that's the dataset you can find online somewhere of use of so we have this idea and maybe there's some existing data points based on ice cream store that we have studied and I can grasp that data so the idea here is that we have our machine learning recipe we are going to take and I know I'm out of the frame here you're going to take one of our inputs called X feed it into the machine learning recipe and the machine learning recipe is going to give us a prediction Y so we have known data and we could if we had new input data we could make a guess so yesterday my machine learning recipe was the ordinary leastsquares method meaning I was able to do a statistical analysis of all this data and create the line of best fit and then if I had a new input you know X value of suchandsuch I could look up its corresponding spot on the line and that would be the Y output this is a function the machine learning recipe is essentially solving for M and B in the equation of a line so that's what I did yesterday today's technique I want to demonstrate the technique known as gradient descent so the idea of gradient descent is okay so boy so much to say where should I start where should I end I really have no idea so one thing I'll mention is that the mass required for gradient descent typically involve calculus and they involve two concepts from calculus one called a partial derivative which if you don't know calculus or what a derivative is well how can you be expected know what a partial derivative is as well as a single the chain rule and I think what I'm going to do is I'm going to walk through this entire system and how it works and explain it without diving deeper into the mask um but I will make a followup video where I discuss some of those pieces in a bit more detail so so okay so but here's a way that you could think about gradient descent thats related to stuff that I have done in previous videos and in my book nature of code where I reference the work of Craig Reynolds steering behaviors so think about this for a second this is this is great let's say you have a twodimensional space and you have a vehicle that is moving around an agent that is moving around this space and the vehicle has a particular velocity expressed as a vector or an arrow in this case now what if the goal of this vehicle is to reach this target well we could say that this vehicle has a desired velocity its desired velocity city is to move at maximum speed from its current location to towards the target so this is a vector which is its desired velocity and it's useful and you can think about the difference like the it's currently this vehicles current philosophy is like its guess I don't know where I should go I'm going to try going this way oh but really I should go this way well if I'm going this way but I really should go this way what if I just turn a little bit towards the target what if I were to just steer a little bit in that direction and this is what gradient descent does you can think of desired as the known output the correct output what if I feed in one of these data points right and I say look at this particular X Y hair let me feed it in let me try to get a guess just sometimes I think written is why a tick I think but I'm going to say what if I get Y i'ma say Y get the error is the difference between what I guessed it would be minus what it actually should be right if I start with an X Y pair this is the error and you'll notice if you look at Craig Reynolds steering behaviors and all of these animated systems that I said I implemented from that work you'll see there's a formula in it steering equals desired minus velocity so you know I put I put it get in you know I kind of do the reverse here because this is really the equivalent desire but the point is the difference between the way that I should go and the way that I am going that's the error the difference between what my machine learning recipe what my model currently thinks the output should be compared to the known output that is the error and steering X if I adjust my velocity if I steer towards the desired I'm going to get a better model I'm going to move towards the target if I use this error to tweak the parameters machine learning recipe I'm going to make my model I'm going to have better M and B values for the next time and I could do this over and over and over and over again and this is we've been talking about this supervised learning I can take the known data send it in get a guess look at the error tweak the knobs send the next data point and get a guess look at the error tweak the knob I can do this over and over and over again and I can just start with random values for M and B so I don't know what it B I'm going to just put a line here and then I can start moving the line around according to the error as I go through all the data so this is what we're trying to do okay so there's more to how the math behind this stuff works and how we look at the overall error and there's some stuff that involves the derivative and the slope of the graph of the error and I'm going to I think I'm going to come back some of that stuff in a second video where I go a bit further into some of the math here but what I'm actually going to do is just start showing you how to set up to do gradient descent in the code itself so let me come over here so this as you saw before this is the example from yesterday that's using the ordinary leastsquares method so what I'm going to do now is I am going to this so I had this function linear regression and this linear regression function calculates the slope of the line and the yintercept and B according to or nearly squares so what I'm going to do is I'm just going to completely get rid of this so now nothing happens there so I can click and the first guest of the line I just plugged in some values that typically speaking I think what's probably typically done is these values are initialized at 0 these are like weights so to speak and ultimately you can see these are analogous to the weights of connections in a neural network but this M and B values I could start with in randomly I could pick something and hardcoded I could get let them both be 0 I think imma stick with actually 1 comma 0 just to sort of see because then at least I can see that the line is there so now what I want to do is I want to look at with the existing data points I want to look at the error and I want to adjust M and B in the direction of the error so let's see how that goes so I'm going to call this now gradient descent and so in the draw function I think I want to call this now gradient descent I'm back a little digression there that has added out thanks for thanks for tuning in ok so where I am is that I'm changing the name of the function to gradient descent and what I want to do is I'm going to just look through all of the data so let's just first look through all the data and ok so for each data set I have the Y is data index iy so we can get the X and the y then I can actually calculate a guess so my guess is M times X plus B right this is my machine learning recipe I am taking the input data X I am multiplying it by n I am adding B and that is my guests so now my error equals my error equals y minus the guess and I think technically speaking I think I should be saying guess minus y now you may recall that in the ordinary leastsquares method I would always square the error because I want to get rid of the sort of positive or negative aspect of it in this case and again I'm going to go a little further into this in the next video I actually want the positive or negative direction of the error because I want to know which way in essence to tune the N and B values to get a better result so the issue here is now and this is what's known as stochastic gradient descent so I want to make an enforceable I want to make a change to M and B so I need to calculate how should I change M and how should I change B so really what I'm saying is M equals M plus some amount of change B equals B plus some amount of change and we can in this case kind of say this is us one way to think about and understand it I have this error who is responsible who is to blame here is it um is it you be so who's in charge here what's the what's going on I got to figure this out so in essence we could say if I adjust those values according to the error maybe if I tried it again I would get a better result and in this case B can be adjusted directly by the error because it's just the yintercept should I move it up or down and M which is the slope can be adjusted by the error but according to according to also the input value itself so this is how you can kind of intuitively understand it I want to adjust those values according to the error the slope also relates to what the input actually was the yintercept just the error itself now so I'm missing a whole bunch of steps and a bit a few pieces of explanation here but let's just run this and see what happens so first I always have to click okay well first of all I got an error but uncaught reference error and is not defined in gradient descent whereas I have n Oh B equals B plus err yeah I don't know what n is so you can see like okay well I don't know where that line went it was there for a second and it just went far away so here's the thing if I come back to my analogy from the steering one of the things in the steering behavior examples from nature of code and Craig Reynolds example is that there was a variable called maximum force I hope you can see that maximum force because one thing you might think about it here is well how powerful I know what the error is between the way I'm going and where I want to go how powerful is my ability to turn well maybe I'm able to turn at like Inc with infinite power and that be good but not so good because if I try to like push myself I might end up going all the way down this way and I'm like oh my god I'm going in the wrong direction and then they have going all the way up in the other direction maybe I just won't we want to be able to make little adjustments because it's the wrong way I want to just make a slight adjustment I don't want to overshoot the target the target being I want to find the parameters I want to find the weights m and B values to minimize the error so so I don't want to overshoot what that minimum that that optimal value is and so that is where a variable sometimes called alpha but most commonly called learning rate comes in so I could have a variable called learning rate usually this is a small number something to really reduce the size of that error so in this case I would say well let me take this change in the value of the slope and multiply it by the learning rate and let me change take this for B and multiply it by the learning rate okay so now I'm going to try this again with a learning rate of point zero zero one hey that doesn't look right come back to me okay so let's think about what might be wrong over here I wrote guess minus y and that's really what I that's what I wrote here no I want Y minus guess I knew it was always the same so hopefully you're not watching this but in this case here right steering if I want to move towards the target the error is the desired the known result minus the velocity and so this should really be I want to move in that direction Y minus y guess let me change that to I changed it already wait how did I do that okay I must have done this before that I went to explain it let's try this looks pretty good right now here's the thing let me put nav back to zero hit refresh here and so let's see so we can see interestingly enough this isn't the correct correct line because the line should really go through those two points you know I think I've got an issue here with the learning rate so you can see how it was kind of like moving to the right spot but then it's still making very very small small changes only have two points not a lot of data not a lot of time for it to change I probably just need kind of a larger higher learning rate here just for this demonstration let's make it at 0.05 and we can see now it's kind of moving much more quickly and it's starting to turn I'll be it very slowly but you can see as it's slowly slowly turning approaching the correct the correct or the optimal spot for this line and as you can see if I were to click again and click again try to you know click a lot up here and a lot down here ultimately eventually I should start getting the line of best fit now so there are you know some strategies that in theory you should really need to adjust the learning rate over time but there is a technique and a lot of machine learning systems that you will say see that you can call annealing I think that's the right word where you kind of start with a high learning rate and then slowly over time reduce it so you can kind of get some big correction at the beginning and then find some some smaller Corrections ok so some folks in the chat we're asking about like ok well it's sort of performing weirdly if I put a lot of like points above and below but if I put you know points to the right and left it's kind of it fits the line very nicely you can see they're not so big now I'm doing above and below again so here's the thing collinear any meaning like a lot of vertical points is not really good this isn't real if data doesn't really make sense for linear regression I'll try to make prediction so we're not necessarily going to get a good line and part of what I'm doing again is not to demonstrate the optimal way to do linear regression but to demonstrate the technique coders grading a set descent of making small adjustments to weights to parameters to the slope and yintercept based on an error based on the supervised learning process so this is a start to that you could stop here and I highly recommended you do is what I'm going to do in the next video I don't really know how it's going to go to be honest but I'm going to try to look a little bit more closely as to why this works out the way that it works how do I know how to change and and be how do I know exactly how to change M & B to minimize the error I said kind of well the error kind of gives us the direction in which to change this has to do with calculus it has to do with comparing how changing one variable affects another variable so if I change M how does that change the error and can I look at the slope of a graph perhaps to see how to move along that graph to minimize that error so this is what I'm going to cover a bit more in the next video I'm not going to really I may not actually also change this is I said I think I said this stochastic gradient descent meaning I'm adjusting the weights and adjusting the m and B values with every data point but I could also look at this with error in totality and then adjust the weights all at once at the end of one cycle through all of the data and that's known as batch gradient descent so I'm going to do what I'm going to do is explain a bit more about the math here and then I'm going to do it and change the code to batch gradient descent in the next video it might be many parts to be honest with you but I don't know it how it's going to go maybe this video is not going to exist the next one you can look see if it's there because I don't know if I should really make it okay see you soon thanks for watching this
