With timestamps:

00:01 - [Music]
00:03 - hello welcome Good Friday to you
00:06 - afternoon here it is in New York City uh
00:09 - my name is Dan schiffman this is the
00:11 - coding train a YouTube thing that
00:15 - happens every once in a while where I uh
00:18 - come and talk about programming topics
00:21 - and various other types of things and
00:23 - often uh embarrass myself in a variety
00:25 - of other ways all while live streaming
00:29 - from Tish School of the Arts here at new
00:31 - new new New York University uh here in
00:35 - New York City uh it's a very hot day
00:38 - outside it is a cool Breezy 72 degrees
00:41 - Fahrenheit here in this
00:43 - room uh I hope that the microphone is
00:47 - working fine you can hear me okay that
00:49 - this
00:51 - music that the levels with me and the
00:55 - music are quite reasonable because I had
00:58 - a few technical things and change some
01:00 - things around so welcome I um have a few
01:03 - quick announcements to make do I have
01:06 - any announcements I just said that
01:07 - without thinking it through I should
01:09 - plan I've got a plan for these things
01:11 - actually hold on hold on where where
01:13 - where is it ah it's over
01:17 - here oh there is a Discord server
01:20 - somebody made one uh Z I made some
01:23 - [Applause]
01:24 - notes I made some
01:26 - notes for today because topic that I
01:30 - want to cover
01:31 - today is linear
01:35 - algebra um as it relates to programming
01:38 - and matrices and code here's another
01:41 - book these are really just props these
01:42 - are actual linear algebra textbooks that
01:44 - I used at one point in my life but we're
01:46 - talking
01:48 - about okay let me do the math here
01:51 - divide by 12 carry the
01:53 - four uh I 25 years ago something like
01:57 - that so in any case uh but I'm going to
02:01 - return to that topic today now okay that
02:04 - stuff is going over there so what what
02:05 - oh a couple things I am taking a look at
02:10 - the YouTube chat over there on a screen
02:13 - and I see people saying has he already
02:15 - started hey you're doing linear algebra
02:17 - after
02:18 - calculus La how do you say lolz
02:23 - laws anyway LS lws
02:27 - anyway uh yes I am I don't I'm not
02:32 - unfortunately nothing on this channel
02:35 - has a
02:37 - particular is particularly thought out
02:40 - or planned with any like reasonable
02:42 - logic whatsoever I'm all about trying to
02:44 - figure stuff out pull from here pull
02:45 - from here piece this together piece this
02:47 - together make something creative see if
02:49 - we can get some code up and running
02:50 - compiling and running and doing
02:51 - something interesting so that's kind of
02:53 - my goal here um so I am seeing the
02:55 - YouTube chat over there I am looking
02:57 - also at a slack Channel which if you you
02:59 - would like to join the coding train
03:01 - Patron program making it sound like it's
03:03 - a thing you can go to
03:05 - patreon.com Trin that is a way of a
03:07 - crowdfunding the work that I'm doing
03:09 - here on YouTube uh and I have some
03:11 - benefits uh uh namely a slack Channel
03:13 - that I look at also during the uh during
03:15 - the live sessions okay so um bonjour to
03:21 - uh gaming with Izzy in the chat so it's
03:25 - summertime do I have any Summertime
03:28 - music
03:34 - is this Summertime music I don't
03:38 - know uh so I'm kind of on a little break
03:41 - from my usual schedule and it's actually
03:44 - been a bit more difficult to schedule
03:46 - these live sessions than I had imagined
03:48 - good news is I am here in town in New
03:51 - York City for a stretch of time from now
03:54 - until July 14th I will then be away for
03:57 - a couple weeks so just in case you're
03:58 - planning your summer around my ual which
04:00 - you really should not that's what's
04:02 - happening right now so I am hoping now
04:04 - to take this time from now until July
04:07 - 14th and really cover with some depth
04:11 - how to program a neural network from
04:14 - scratch in JavaScript and perhaps also
04:16 - in Java using
04:18 - processing most of my inspiration is
04:21 - coming
04:22 - from almost yeah I won't be doing this
04:25 - almost all of my inspiration is coming
04:26 - from this wonderful book that I
04:28 - purchased earlier this year called make
04:30 - your own neural network uh by Tariq
04:32 - Rashid uh I think believe who's based in
04:34 - London um this is a step by step with no
04:39 - prior knowledge needed of any maths or
04:42 - any programming actually um to build a
04:45 - neural network program in
04:46 - Python because reasonable people who
04:50 - live reasonable lives program their
04:53 - machine learning stuff in Python but I
04:55 - am not a reasonable person living a
04:57 - reasonable life and I will be doing this
05:00 - at least for now starting to do this in
05:02 - JavaScript and I have to say that the
05:04 - the ultimate goal here I know I'm going
05:07 - to uh end up just repeating myself again
05:09 - in a second is that I oh look it's
05:13 - already even open here I have actually
05:14 - already done this I have there's a
05:17 - GitHub repository uh neural network P5
05:20 - which is a my watch is beeping at me I'm
05:23 - going to press this
05:25 - um this is a JavaScript library uh with
05:28 - code based on the code from that book uh
05:31 - and it actually has a few uh demos
05:33 - associated with it already I'm going to
05:35 - click on this
05:36 - one this is kind of a classic uh classic
05:40 - machine learning scenario of learning to
05:42 - recognize uh digits and I have a little
05:44 - P5 example where I can uh draw the
05:47 - letter three there which it thinks is
05:49 - the letter letter three the number three
05:51 - the character three the number three
05:52 - thinks it's a five maybe after some more
05:54 - training eventually it will determine
05:57 - that it's a three we'll see um so I have
05:59 - this example I also have uh another
06:03 - example which is uh using the same nural
06:07 - network code oh it's
06:10 - broken I knew it I changed something I
06:13 - got to fix this so I'll will fix that so
06:15 - anyway I've done this already I did this
06:17 - for a course that I taught here at NYU
06:19 - at ITP earlier this spring and now what
06:22 - I would want to do is unpack this
06:25 - process unpack the process that I
06:27 - undertook to make this Library and do it
06:30 - over a series of many videos so on the
06:32 - one
06:34 - hand you should probably just go and
06:36 - enjoy your summer
06:38 - vacation uh go to the beach read a book
06:42 - and uh come back when I finished with
06:45 - this and then kind of get your hands
06:47 - into like the using the neural network
06:48 - library making some creative projects
06:50 - but as an exercise to help myself learn
06:53 - more background and more depth about
06:55 - this topic and for those of you who
06:56 - might be interested I'm going to build
06:58 - all this stuff over the course of a
07:01 - bunch of
07:03 - videos um which I will start today okay
07:07 - so that is my main
07:13 - Spiel now now it's about
07:16 - 2:30 uh schedule wise oh oh yeah yeah
07:19 - I'm going next week O'Reilly AI
07:24 - Conference next week I will have a
07:27 - limited schedule is this am I ah right
07:29 - here because I will be
07:32 - attending
07:34 - this O'Reilly artificial intelligence
07:37 - conference where hopefully I might learn
07:39 - something that I can take with me and
07:41 - then bring to this YouTube channel but
07:44 - uh so if you if anybody watching will be
07:46 - at this conference please tweet me at
07:48 - shiffman uh so maybe I can say hello uh
07:51 - give you a sticker coding train sticker
07:53 - or a processing sticker or P5 sticker I
07:55 - guess I can try to remember to bring
07:56 - those with me um and say hello to you at
07:58 - this conference I'm particularly excited
08:00 - I'm going to a session on deep
08:02 - reinforcement learning which is really
08:05 - the the kind of a machine learning
08:08 - algorithm that I'm most interested in in
08:10 - terms of how it applies to creative
08:13 - animations and interactive systems so
08:17 - okay
08:20 - uh trua in the chat is asking about the
08:23 - Chrome extension tutorial so
08:26 - unfortunately I did say I so okay let me
08:29 - let me answer this for a second oh look
08:31 - I'm actually whoa that's weird my web
08:34 - browser is on the web page that I was
08:36 - just going to go
08:37 - to but it wasn't before how did it get
08:40 - there anyway I must have clicked on it
08:42 - subconsciously um so this is a syllabus
08:46 - for a course which is um kind of
08:49 - misnamed because it's not a beginner
08:51 - course at all but programming from a toz
08:53 - it was a course from last fall uh and I
08:56 - made a a lot of video tutorials actually
08:58 - if I go to shift.net
09:01 - teing
09:02 - A2Z uh that's not the right
09:08 - page uh oh
09:13 - whoa where is my uh a toz page if I go
09:17 - to
09:20 - here oh just SL a toz just if I just go
09:23 - to shiftman Donnet toz you will find the
09:27 - um whole set of P each with videos and
09:31 - notes on a variety of topics and if I go
09:34 - down here to Chrome
09:36 - extensions here are the examples and a
09:38 - few notes but I didn't actually make any
09:41 - of the videos yet so this was a topic
09:43 - last fall I made videos for this course
09:45 - all fall along if that's an expression
09:47 - to say and I never got to the Chrome
09:49 - extension videos and I kept saying I was
09:50 - going to get to it and then it became
09:52 - kind of like a running joke so I almost
09:53 - feel like I can't ever do it because
09:55 - then we won't have the running joke
09:56 - anymore but I really do need to do it at
09:57 - some point this will happen for sure in
10:00 - the fall because I'll be teaching this
10:01 - course again and want to fill out um
10:04 - stuff and actually I'm going to add some
10:05 - machine learning stuff to this course I
10:07 - want to do word to VC I want to look at
10:09 - uh recurrent neural networks and text
10:11 - generation there's something else some
10:12 - chatbot stuff so that's coming as well
10:14 - in the
10:15 - fall
10:18 - okay so
10:21 - now
10:23 - um I'm looking at the chat to see if
10:26 - anyone's got any questions for me ah
10:28 - okay so because this conference next
10:30 - week um I will not be live again until
10:32 - next Friday I mean that's what I usually
10:34 - do anyway um so today's a little bit of
10:37 - like getting back into the swing of
10:39 - things because I was kind of been my
10:41 - last live stream was a week and a half
10:42 - ago and then I'll be back next Friday
10:44 - and then I've got two weeks in July that
10:47 - I'm here in town with a stretch of a
10:48 - pretty flexible schedule and I hope to
10:50 - get a whole bunch of live streams in
10:52 - then my goal is to have four per month
10:54 - and so uh because I'm going to be away
10:56 - the last two weeks of July that I might
10:57 - do two per week those two weeks that's
10:59 - my kind of goal and so I'm hoping that
11:02 - by July 14th I will have rebuilt
11:06 - everything about this particular neural
11:08 - network library uh in a video on live
11:13 - talking about it figuring it out and
11:14 - taking hopefully your helpful
11:16 - suggestions so let me I the problem is
11:18 - I'm going to repeat a lot of this stuff
11:19 - in a second because I got to do sort of
11:22 - introduce one of the things that I do
11:24 - for those of you who haven't watched
11:25 - live before is the live stream gets
11:27 - edited into a bunch of tutorials videos
11:29 - that then get put in a whole bunch of
11:30 - different playlists depending on what
11:31 - course they go with so a lot of this
11:33 - introductory stuff I might end up
11:34 - repeating again I apologize for that but
11:38 - um now I even forgot what I was gonna
11:41 - say um so let's see here so I think I
11:44 - just want to get
11:48 - started okay so
11:51 - um couple good suggestions from the
11:53 - slack chat patreon group uh mincut
11:55 - writes you should make a calendar with
11:57 - all the conferences you attend a good
11:59 - idea except to be honest with you I go
12:01 - to very very few conferences I would
12:03 - love to travel and go to more things but
12:06 - I just have a lot of work commitments
12:08 - and family commitments so generally I'm
12:10 - just kind of here in New York City but
12:12 - uh but that's a good idea we'll try to
12:13 - kind of keep that up to date or announce
12:15 - if I'm doing any other workshops or
12:16 - things outside conferences outside of
12:18 - the stuff that I usually do here um Cod
12:22 - codo verly asks will there be a separate
12:26 - math video for this topic so this is a
12:28 - good question
12:29 - and I've been struggling with this and
12:32 - this is where I have arrived at at the
12:34 - moment so I am going to
12:38 - do I'm going to build a neural network
12:40 - code and I'm going to use Matrix math to
12:45 - do the weighted sums of all the
12:48 - connections and I will get into the
12:49 - details of what that means so I am going
12:51 - to do some separate videos that cover
12:54 - the Matrix math required so there's I'm
12:57 - going to pick and choose little nugget
12:58 - get from linear algebra and cover those
13:01 - and also write the code to implement
13:03 - those
13:04 - things then at some point and i'
13:06 - probably not today I'm going to get to
13:08 - the part of the neural network thing
13:10 - where you're training it and adjusting
13:12 - the weights with a process called back
13:14 - propagation so we're going to talk about
13:15 - what that process is write all the code
13:17 - for it but I don't think that I'm going
13:20 - to derive the calculus maths that's
13:23 - required for the formulas that are used
13:24 - I'm just going to use those formulas and
13:26 - point people to other references or
13:28 - maybe come later do a follow-up video to
13:30 - unpack some of that math in a bit more
13:32 - detail so that's my plan and I kind of
13:34 - had a
13:36 - similar plan point of view with the uh
13:39 - linear regression stuff I mean it's
13:40 - actually the same math so um a lot of
13:43 - that will apply but where I try to just
13:45 - sort of like make a video and use the
13:46 - formulas and then come back and make a
13:47 - follow-up video for those who would be
13:49 - interested in that so that's my
13:52 - plan uh is this going to be posted later
13:57 - oh yeah and Carl's so let me make some
13:59 - notes Here I forgot so I also the issue
14:01 - is as much as I want
14:04 - to okay so hold on uh linear
14:09 - regression uh batch gradi I forgot some
14:13 - things that I forgot to do are uh and
14:16 - Carl so couple somebody asked in the
14:17 - chat will this be posted later yes
14:19 - everything is posted later there'll even
14:21 - be edited versions of this later if you
14:23 - don't want to watch all the long- winded
14:25 - stuff um batch gradient descent is
14:28 - something that I forgot to mention and
14:30 - cover as part of the linear regression
14:31 - tutorials and then there were some other
14:34 - additional things to the perceptron
14:35 - example that I wanted to do and I'm
14:37 - tempted to come back to the and
14:39 - everyone's asking for the f I did this
14:41 - to myself I wanted to do the fidget
14:44 - spinner uh code and I never did that and
14:48 - uh when I first mentioned it everybody
14:51 - had the what you know the reaction that
14:53 - one might expect which was like groan
14:56 - major groan I'm really could do a fidget
14:58 - spinner simulation video but then I got
15:01 - kind of excited about it and I talked
15:02 - about it and then some other people got
15:04 - excited about it and then I never did it
15:06 - so um and yes I'm definitely going to be
15:09 - doing neuroevolution is my happy place
15:12 - so one of the things we'll see as we
15:14 - build a neural network is that the whole
15:17 - point of one of the one of the uh one of
15:19 - the key pieces of working with neural
15:21 - networks is figuring out how to get the
15:24 - optimal weights of all these connections
15:26 - that are in that Network and so there
15:28 - are these methodologies for doing that
15:31 - uh there's the standard methodology
15:33 - which involves this gradient Ascent like
15:36 - back propagation algorithm tweaking
15:39 - according to an error to minimize the
15:41 - error changing weights to minimize the
15:43 - error but there is another method which
15:45 - involves an evolutionary approach to
15:47 - evolve the optimal weights and that's
15:49 - the method I love because it's just very
15:52 - more it's very intuitive and it's uh can
15:55 - understand it it relates to my genetic
15:56 - algorithm tutorials it doesn't involve
15:57 - all that sort of like calculus stuff so
16:00 - I'm I'm definitely planning to get to
16:01 - that uh yes I good so at least I'm
16:05 - getting some negative feedback about the
16:06 - fidget spinner which will keep me from
16:08 - doing that oh yeah and and and I have no
16:11 - I'm I I will sell out as soon as I
16:13 - possibly can right if I'm making the
16:16 - fidget spinner video will suddenly get
16:18 - me you know 100 million subscribers
16:20 - which it won't obviously uh and then
16:22 - I'll just do it but uh you know I have
16:24 - no
16:25 - qualms selling my soul yeah
16:28 - unfortunately if you're watching this
16:29 - live you cannot watch it at two 2x speed
16:31 - you have to watch it at 2x speed later
16:33 - but I could try to pretend that it's
16:34 - going at 2x speed
16:35 - because F to talk about that that's
16:37 - actually more like 4X
16:40 - speed uh what about polom regression oh
16:43 - my goodness o o there's too many things
16:46 - I didn't get to that
16:49 - either can I in one YouTube channel
16:52 - cover the entire known universe of
16:54 - knowledge no I cannot to be honest a lot
16:58 - I
16:59 - mean my the my goal with the channel is
17:02 - to create friendly and accessible
17:04 - tutorials for people who want to be
17:08 - creative and experiment with code in an
17:10 - informal way and so on the one hand you
17:14 - know I just don't it's not the goal of
17:17 - my channel to cover every sort of
17:20 - Statistics mathematics computer science
17:23 - algorithm from scratch in some sense
17:25 - that I'm going to show to you so you can
17:26 - memorize it and and redo it again again
17:28 - later I mean I'm not saying that doesn't
17:30 - have value and there aren't other
17:31 - channels that have that approach but my
17:34 - Approach is quite informal and loose and
17:36 - so Pol nomal aggression I don't know
17:38 - that I need that so much for where we're
17:40 - going because ultimately I think the
17:42 - creative examples will come from using
17:44 - neural networks to uh to um experiment
17:48 - with user interaction and other types of
17:52 - um interactive and animated
17:54 - possibilities
17:57 - okay
18:02 - all right I'm I'm reading this chat and
18:03 - it's just lovely to read all these nice
18:05 - messages um and people are having a
18:08 - discussion and I can't keep up with it
18:10 - at all uh I forgot the momentum so many
18:14 - things I forgot I can't do it all I
18:16 - can't
18:18 - trigonometry yes to mges I will be oh
18:23 - yeah I could do a fidget oh whoops I hit
18:25 - the Bell by accident I could do fidget
18:27 - spinner with
18:33 - querian if you're new here you know that
18:36 - if I ever say the word
18:42 - Quan I have to run away instantly it's
18:45 - terrifying they're terrifying all
18:49 - right I think what I'm going to
18:54 - do because I feel like I'm I want to get
18:57 - some momentum here and start the um
19:02 - neural network stuff I'm just walking
19:04 - over here to reset this camera um I
19:07 - think maybe I should hold
19:09 - off should I
19:13 - do so many requests I'm trying to decide
19:18 - I'll do the straw pole thing that's
19:19 - usually how it works
19:23 - best
19:26 - uh I kind of know the answer to this but
19:29 - let me just confirm here pick one finish
19:33 - off some details and things for
19:38 - perceptron uh actually uh just get
19:42 - started on a
19:45 - neural network so this is the
19:49 - multi-layered perceptron you guys
19:51 - probably can't see this but I
19:54 - will so here we go um create poll
20:07 - so here
20:08 - is the straw pole address W 6 Z
20:15 - D3
20:19 - [Music]
20:25 - e should I
20:29 - finish off some of these things that we
20:30 - didn't get to and like visualize and add
20:34 - some other stuff or let's just get
20:36 - started this is going to be a long
20:38 - process it won't finish won't be
20:39 - finished today but get started building
20:41 - this JavaScript neural network
20:43 - [Music]
20:51 - library this is why people can watch the
20:53 - edited verion or the archive because you
20:55 - could just skip over this part where I
20:58 - wait for the scrub Ball
21:00 - results
21:10 - and oh wait is this link not
21:15 - working oh oh no what
21:19 - happened what
21:24 - happened that was such a fail
21:29 - all right
21:30 - did let's try this again everybody is
21:33 - are people able to get the straw
21:36 - pole it's working
21:42 - [Music]
21:48 - okay what's funny is somebody in the
21:50 - chat said uh you know clicks link
21:53 - arrives at live
21:55 - stream sees guy dancing Le so this is
21:59 - good
22:00 - because if if somebody just joined this
22:03 - by accident and sort of sees the dancing
22:05 - isn't interested it's probably good to
22:07 - it's not going to like the rest of the
22:11 - [Music]
22:14 - video all
22:16 - [Music]
22:21 - right I think I know what this is going
22:23 - to
22:26 - be
22:29 - wow all right so what we're going to do
22:32 - today is just get started and we are
22:35 - going to build a neural network library
22:38 - in
22:42 - JavaScript and we should just all be
22:44 - aware that I'm not really an expert in
22:46 - this and I'm just doing this to learn it
22:48 - myself and give it a try and you're
22:50 - going to encourage me or not or whatever
22:52 - but that but I but I feel you out there
22:54 - I feel you watching how many people are
22:56 - watching 747 people that's terrifying
23:01 - it's absolutely terrifying okay all
23:04 - right so let's get
23:06 - started I really need some different
23:10 - music um okay
23:13 - so how to
23:16 - [Music]
23:19 - begin so I need a code
23:24 - [Music]
23:26 - editor
23:34 - [Music]
23:39 - okay
23:44 - [Music]
23:56 - um
24:00 - [Music]
24:19 - [Music]
24:26 - e
24:28 - [Music]
24:49 - [Music]
25:14 - okay I think I am ready now oh wait
25:17 - actually I want
25:20 - to
25:24 - oops there's something I want to do
25:26 - which is
25:30 - [Music]
25:35 - open this
25:36 - [Music]
25:41 - up
25:43 - okay
25:45 - so I should map this out a little
25:53 - bit um Okay
25:56 - so
25:58 - let's think about where where will these
26:00 - videos live this is a little
26:03 - tricky because there are two
26:06 - places right now I am here basically in
26:11 - uh week four of the intelligence and
26:15 - learning
26:16 - class but this these videos actually go
26:19 - along
26:21 - with chapter
26:23 - 10 of the nature of code book and there
26:27 - goes the
26:30 - camera so that music that I'm playing is
26:33 - by um Adam Blau who is a uh film and
26:36 - television composer based in Los Angeles
26:39 - uh he has a podcast that I'd love to
26:41 - plug uh called rarified air which is
26:43 - really terrific and I believe that is
26:45 - called Tory the
26:46 - dog said it to me as just some like
26:49 - extra uh extra spare music that I could
26:52 - use and Adam is the composer of the soon
26:54 - to be released coding Train theme song
26:58 - I know you can't wait for that um okay
27:01 - so I think this is not a coding
27:05 - challenge I will do some coding
27:06 - challenges with neural networks but I
27:08 - think this um and it would be
27:10 - interesting to try to do like a hey just
27:13 - program neural network in 20 minutes
27:15 - kind of thing but um I want to spend
27:17 - some time doing stuff step by step in a
27:19 - series of videos about building this
27:21 - neural network
27:22 - library so the thing that I want to
27:24 - start
27:25 - with is
27:28 - is
27:30 - yeah okay I have these slides from the
27:38 - perceptron boy I really um could have
27:41 - thought about this in a different way
27:42 - let me do an
27:44 - introduction
27:52 - video
27:55 - uh I'm really trying to
27:58 - I'm really trying to figure this out but
28:00 - I you know
28:02 - what I am not going to worry about it I
28:06 - am going to I am going to consider this
28:11 - to be a I got it I got it
28:13 - everybody we're good I figured it out
28:17 - this is a new playlist 10 new number 10
28:22 - in uh nature of
28:24 - code uh it is going to look if I go to
28:28 - my
28:29 - channel and go to uh
28:35 - here so basically you can see here what
28:38 - this is going to be is 10 neural
28:41 - networks 10 neural networks and what I'm
28:44 - going to do now is make an intro video
28:47 - that sets the stage for what's coming
28:49 - and the perceptron videos will actually
28:52 - follow that intro video and then the
28:54 - neural network I mean the multi-layered
28:57 - proc ctron Library building will follow
29:00 - that so I'm I'm making videos out of
29:03 - order so first I'm just going to do
29:04 - probably just like a couple minute
29:07 - introduction uh to the
29:13 - idea okay yeah I uh all right so I'm
29:17 - looking at the chat I'm seeing lots of
29:22 - stuff no you did not miss the train
29:25 - whistle there's the train whistle okay
29:28 - um all
29:30 - right here we
29:40 - go I need a moment to
29:42 - meditate shoulders hurts I need to
29:45 - stretch ah
29:48 - oh I think there's an issue where the
29:50 - way I have this set up the camera is
29:52 - actually right there but I'm kind of
29:53 - like standing over here so I have this
29:55 - like awkward neck craning thing to look
29:57 - at the the camera but we're just going
29:58 - to go all right so what time is it now
30:00 - oh my God 30 minutes in and I haven't
30:02 - really started doing anything yet but I
30:03 - was thinking this through and talking to
30:05 - you and you were watching apparently
30:07 - let's get let's get a move
30:10 - on hello welcome
30:13 - to a video that at this present time
30:17 - doesn't exist but when you are watching
30:19 - this video right there to the right of
30:22 - nine genetic algorithms will be the
30:24 - number 10 and we'll stay next to that
30:26 - neural networks so I am embarking on a
30:28 - journey uh to learn about neural
30:32 - networks what they are how you program
30:34 - them what are there what's kind of like
30:36 - math and stuff you need to know to make
30:38 - them work and then what kinds of
30:41 - creative and experimental outcomes can
30:43 - you have now it should be
30:46 - said that uh there are lots and lots of
30:49 - machine learning libraries out there uh
30:52 - there are lots of examples and resources
30:55 - for doing this uh I I want to hold on
30:59 - I'm still talking I don't know where
31:01 - where did I put that book ah it's over
31:03 - here I want to reference uh this book U
31:06 - make your own neural network by Tariq
31:08 - Rasheed which I used to develop a lot of
31:11 - the materials that I will be presenting
31:12 - to you and developing uh during this
31:15 - series of of videos and I should also
31:18 - say that um you know and this
31:22 - book this book has um all sorts as how
31:26 - to program your own network from scratch
31:28 - and without even knowing anything about
31:29 - programming in Python because as I might
31:31 - have said earlier today any reasonable
31:33 - person would start and make a video
31:36 - tutorial series about programming a
31:38 - neural network from scratch in Python
31:40 - but I don't really I'm not very
31:43 - reasonable or logical and I do make just
31:46 - constantly make mistakes with everything
31:47 - and here's a mistake that I'm going to
31:48 - make I'm going to do all this in
31:50 - JavaScript um and the reason for doing
31:52 - that is to have everything run in the
31:54 - browser on the web and also really for
31:57 - me to learn about how to do this stuff
31:58 - so I am going to build a set I'm going
32:03 - to build a a simple neural network
32:05 - library in JavaScript not to make
32:07 - something efficient not to make
32:09 - something robust but to learn about the
32:11 - mechanics of how all this stuff works
32:14 - because ultimately and you might want to
32:16 - just enjoy your summer or maybe you're
32:19 - watching this during the winter and get
32:21 - outside and do something else and not
32:23 - watch these videos and just skip ahead
32:25 - to like later cuz I'm going to do bunch
32:27 - of coding challenges and projects that
32:30 - involve that neural network library and
32:32 - also other neural network libraries
32:34 - namely something called tensorflow uh in
32:37 - future videos but these first videos of
32:40 - building the neural network library
32:41 - which I will do over a series uh or
32:44 - really just for me to to learn how to do
32:46 - this stuff and if you want to watch and
32:48 - sort of give me some good feedback and
32:49 - see if you can follow along and and and
32:51 - improve on what I'm doing and uh help me
32:53 - with it that would be great so okay what
32:57 - what hello am I just rambling here I am
33:01 - but why are we here so I'm going to go
33:04 - uh so the nature of code materials and
33:07 - this video sits in the nature of code
33:08 - playlist is all about looking at things
33:12 - in nature in our physical world and
33:15 - trying to unpack those things and
33:17 - understand the algorithms behind those
33:18 - things and see if we can convert those
33:21 - things those algorithms into code oh
33:23 - this is like going it's like Auto
33:25 - playing how do I stop that
33:27 - uh uh and uh turning those things into
33:30 - software to make animations and creative
33:35 - projects why not look at something
33:39 - really interesting in nature the brain
33:42 - so this is kind of a loose diagram of
33:44 - this idea of an actual biological neural
33:47 - network apparently I have one here
33:51 - struggling quite a bit these days uh
33:54 - where there are these entities called
33:56 - neurons and and they're connected to
33:57 - other neurons and there's a lot of you
34:00 - know mystery to this and a lot of recent
34:02 - research to your
34:04 - Neuroscience what I am focused on in
34:07 - this series of videos is what kinds of
34:11 - computational systems can be built
34:13 - inspired by the actual biological neural
34:16 - neural network biological brain and made
34:19 - into something called an artificial
34:22 - neural
34:22 - network and what kinds of applications
34:25 - and outcomes can we we can we create so
34:29 - what is the analog what is the
34:31 - neuron in by in our code how does it
34:35 - receive inputs how does it generate
34:38 - outputs so my brain does this it
34:40 - receives all these inputs you know from
34:43 - light in the room that travel through my
34:44 - retina and into the brain and the
34:46 - signals then produce outputs and allow
34:48 - me to catch something or read some words
34:51 - what how can that how can that process
34:53 - be simulated in software and what type
34:56 - types of outcomes can we generate and
34:58 - the very first thing that I'm going to
35:00 - do is look at the simplest possible
35:04 - neural network a net it's not even a
35:07 - network at all it has one neuron a
35:10 - processor neuron that receives two
35:12 - inputs and generates an output and
35:14 - that's called a perceptron so if you
35:16 - look at the next videos in this playlist
35:18 - I am going to build in processing uh
35:21 - perceptron example just to show the
35:24 - mechanics of how this works and to
35:26 - produce a sort of trivial example that
35:28 - doesn't necessarily have a very powerful
35:30 - outcome but gives us CU if we can build
35:33 - and understand how this single neuron
35:36 - receives inputs processes those and
35:38 - generates an output then we can start to
35:41 - connect those together to create more
35:44 - sophisticated systems that can begin to
35:46 - generate outputs based on more more
35:49 - complex outputs based on more complex
35:51 - inputs and this is kind of a fit sits
35:54 - right there in the world of machine
35:56 - learning
35:57 - this idea of I have some data that I
36:00 - want to make sense of that data is an
36:02 - input to a machine learning algorithm
36:04 - that algorithm is going to generate an
36:06 - an output so maybe the data is an image
36:09 - the machine learning algorithm is going
36:10 - to guess is it a cat or a dog or maybe
36:12 - that input is the specs of a house you
36:15 - know square footage uh number of
36:18 - bedrooms etc etc and this machine
36:21 - Learning System is going to generate an
36:23 - output a predicted price so there are
36:25 - lots of other machine learning
36:27 - algorithms besides just neural
36:28 - network-based ones and I do have another
36:31 - video series that covers some of those
36:33 - but ultimately I want to learn how a
36:34 - neural network works so I can place it
36:37 - right there and start to make sense of
36:39 - data generate outputs from it so if you
36:42 - want to continue along the way this
36:44 - video series will work first there'll be
36:46 - a perceptron which is uh this thing then
36:50 - I'm going to talk after the perceptron
36:51 - done I'm going to talk about what the
36:52 - limitations of the perceptrons are and
36:55 - why it is that if we could can create a
36:57 - multi-layered perceptron meaning many of
37:00 - these perceptrons all connected to each
37:02 - other what we can start to build uh and
37:04 - create afterwards so uh that's my
37:07 - rambling introduction that apparently
37:09 - you just watched
37:12 - because I mean maybe I'm that no one
37:14 - will ever watch this but but but
37:16 - probably somebody will and um I'll see
37:18 - you follow along I look forward to your
37:20 - feedback uh I hope this goes okay that's
37:23 - my that's a pretty good goal just okay
37:25 - is fine and I'll see you in these future
37:27 - videos as I keep going thanks for
37:33 - watching all right you know that's me
37:36 - that's this is my
37:41 - style uh okay so um looking at the chat
37:46 - so now
37:48 - fortunately so fortunately for all of
37:50 - you uh you don't have to now sit through
37:52 - me doing the
37:54 - perceptron because that is already
37:57 - oops
37:59 - H that oh my channel that is already um
38:04 - done and that is here well that's the
38:06 - followup but if I keep going this way we
38:09 - can see here it is the perceptron so
38:11 - there are two videos about the
38:12 - perceptron at some point I might come
38:14 - back and fit some little pieces in
38:18 - there
38:19 - um people are giving me great
38:21 - suggestions like
38:24 - yeah in the chat but
38:27 - now uh I code
38:31 - please everybody is very very focused on
38:34 - me getting to the point of things I I I
38:37 - I would tend to agree okay so the next
38:39 - thing that I want to do
38:41 - now
38:45 - is I want to talk
38:53 - about all right this is very
39:02 - hard this is difficult here all
39:06 - right so I want to talk about
39:12 - first hold
39:15 - on
39:20 - uh trying to
39:22 - see the thing that I need to
39:25 - reference
39:30 - I probably should have followed this
39:31 - along more closely I'm sorry I'm I'm
39:34 - looking at my notes
39:38 - Here Yeah
39:41 - okay
39:45 - um okay so there's
39:48 - this and there's
39:53 - also I have these notes
40:02 - okay
40:06 - okay okay
40:09 - so the next
40:12 - thing is to discuss why the perceptron
40:22 - okay all
40:24 - right let's see how this marker does so
40:27 - first of all
40:30 - um oh this is such a good marker that
40:33 - makes me so happy can you read that I'm
40:36 - going to go walk over to my monitor and
40:38 - see if I can even read that looks like
40:40 - the focus is kind of reasonable it's a
40:41 - little bit
40:43 - small um yasu or yasus to everybody from
40:47 - Greece in the chat
40:49 - um um I guess I just need to write
40:52 - bigger but uh that's good okay people
40:55 - are saying it's okay okay so where's my
41:09 - Eraser all
41:18 - right okay here we
41:21 - are uh I'm going to begin
41:24 - now a a little bit large would be fine
41:27 - okay font size plus three okay okay uh
41:31 - here we
41:38 - go hi again so maybe you just watched my
41:41 - previous videos about uh coding a
41:44 - perceptron and now I want to ask the
41:48 - question why not just stop here so okay
41:52 - so we had this like very simple scenario
41:55 - right where where we have a canvas and
41:59 - it has a whole bunch of points in that
42:01 - Canvas OR cartisian plane whatever we
42:03 - want to call it and we drew a line in
42:05 - between and we were trying to
42:07 - classify some points that are on one
42:09 - side of the line and some other points
42:11 - that are on another side of the line so
42:14 - that was a scenario where we had the
42:16 - single perceptron the sort of like
42:18 - processing unit we can call it the
42:20 - neuron or the processor and it received
42:23 - inputs it had like x0 and X1 were like
42:27 - the X and Y coordinates of the point it
42:30 - also had this thing called a
42:32 - bias and then it generated an
42:36 - output each one of these inputs was
42:39 - connected to the processor with a
42:42 - weight you know weight one weight two or
42:45 - whatever weight weight weight and the
42:47 - processor creates a weighted sum of all
42:50 - the inputs multiplied by the weights
42:53 - that weighted sum is passed through an
42:55 - activation fun
42:57 - function to generate the output so why
43:01 - isn't this good enough
43:04 - now let's first think about what what's
43:08 - so what's the limit here so the idea is
43:10 - that what if I want any number of inputs
43:14 - to generate any number of
43:17 - outputs that's the essence of what I
43:20 - want to do in a lot of different machine
43:22 - learning applications let's take a very
43:25 - classic
43:26 - classification uh algorithm which is to
43:29 - say okay well what if I have a
43:31 - handwritten digit like the number eight
43:34 - and I have all of the pixels of this
43:36 - digit and I want those to be the inputs
43:39 - to this perceptron and I want the output
43:41 - to tell me uh a set of
43:46 - probabilities as to which digit it is so
43:49 - the output should look something like
43:51 - you know there's a 0.1 chance it's a
43:54 - zero there's a point 2 chance it's a one
43:57 - there's a0 one chance it's a two 0 3 4 5
44:01 - 6 7 oh and is like a 9 chance it's an
44:05 - eight and a 0.05 chance it's a uh 10 and
44:09 - I don't think I got those to add up to
44:10 - one but you get the idea so the idea
44:13 - here is that we want to be able to have
44:15 - some type of processing unit that can
44:18 - take an arbitrary amount of inputs like
44:21 - maybe this is a 28 by 28 pixel image so
44:25 - there's 784 grayscale values and instead
44:29 - those are coming into the processor
44:31 - which is weighted and summ and all this
44:33 - stuff and we get an output that has some
44:34 - arbitrary amounts of probabilities to
44:37 - help us guess eight that this is an
44:42 - eight this model why couldn't I just
44:45 - have a whole bunch more inputs and then
44:47 - a whole bunch more outputs but still
44:48 - have one single processing unit and the
44:51 - reason why I can't is uh stems from an
44:56 - article I don't know sorry a book that
44:59 - was published in 1969 by Marvin Minsky
45:01 - and Seymour paper paper called
45:03 - perceptrons you know AI uh luminaries
45:06 - here I don't know if I click on this
45:08 - link where it goes
45:09 - to Amazon maybe oh MIT press so in this
45:15 - book Minsky and
45:18 - paper T edit time out for a second how
45:22 - do you pronounce Seymour paper's last
45:24 - name is it payal
45:26 - paper paper paper do I pronounce the
45:29 - T can somebody help me with this
45:32 - somebody said
45:42 - qu Tor the
45:45 - dog while I wait for somebody to help me
45:47 - with pronunciation paper with a t or no
45:52 - T paper with it's pronounced it's
45:55 - pronounced
45:58 - gift that was a good
46:00 - one oh come on I don't want daily hacks
46:03 - I'm not interested in your deal I want
46:05 - to know how to pronounce
46:10 - paper nobody will
46:13 - tell nobody will tell
46:15 - me I'm going to just mispronounce
46:19 - it I'm looking for at least the slack
46:21 - Channel I can rely on you patrons
46:24 - somebody must know
46:26 - [Music]
46:28 - look at this I really got the chat going
46:30 - crazy
46:31 - here PayPal yeah my PayPal is Daniel
46:34 - shiftman Donnet that's also of my
46:38 - email my Bitcoin address
46:41 - [Music]
46:43 - is
46:45 - uh nobody has any idea that's
46:48 - fine I will just suffer through the
46:52 - comments that I will get in the video at
46:53 - a later time check this video oh my God
46:57 - boy that the chat is going crazy I've
46:59 - never seen anything like it the t is
47:01 - pronounced thank you a AA you have come
47:04 - through for me thank you very
47:07 - much I are totally forgot what I was
47:09 - talking
47:13 - about
47:21 - okay in the book perceptron Marvin
47:24 - Minsky and see more
47:28 - paper point out that a simple perceptron
47:32 - the thing that I built in the previous
47:34 - two videos can only solve linearly
47:38 - separable problems so what does that
47:41 - mean anyway why should you care about
47:43 - that so let's think about this
47:46 - this over here is a linearly separable
47:50 - problem meaning I need to
47:53 - classify this stuff and if I were to
47:56 - visualize all that stuff I can draw a
47:59 - line in
48:02 - between this part of the DAT this stuff
48:05 - that's to this class and this stuff
48:07 - that's with this class the stuff itself
48:09 - is separable by a line in three
48:11 - dimensions I could put a plane and that
48:13 - would be linearly separable because I
48:16 - can kind of divide the space in half and
48:18 - and and and understand it that
48:21 - way the problem is most interesting
48:24 - problems are not linearly separable you
48:27 - know there might be some data
48:30 - which clusters all here in the center
48:33 - that is of one class but anything
48:35 - outside of it is of another class and I
48:38 - can't draw one line to separate that
48:40 - stuff and you might be even thinking but
48:42 - that's you know still so much you could
48:45 - do so much with linearly separable stuff
48:49 - well here I'm going to show you right
48:51 - now a particular problem I'm looking for
48:55 - an erase sir I'm walking around like a
48:57 - crazy person I'm going to show you a
48:59 - particular problem called
49:02 - xor so let's erase all
49:06 - this and making the case for why we need
49:08 - to go a step further and start to
49:18 - whoops I'm making the case for why we
49:20 - need to go a step further oh I just had
49:23 - an idea I'll come back to later I'm
49:26 - making the case for why we need to go to
49:28 - a go go a step further and make
49:32 - something called a multi-layered
49:33 - perceptron and I'm going to lay out that
49:35 - case for you right now so you might be
49:38 - familiar you might remember me from my
49:40 - videos on condition conditional
49:42 - statements and Boolean Expressions well
49:44 - in those videos I talked about
49:46 - operations like and and or which in
49:50 - computer programming syntax are often
49:52 - written you know double Amper sand or
49:54 - two pip
49:56 - the idea being that if I were to make a
49:59 - truth
50:01 - table
50:03 - true true false false so what I'm doing
50:08 - now is I'm showing you a truth table I
50:10 - have
50:12 - two
50:16 - elements I'm saying what if I say a and
50:19 - the
50:20 - B so if a is true whoa whoa whoa this
50:26 - makes no sense what I've drawn here
50:28 - because I am losing my brain cells
50:31 - slowly over time with every passing day
50:35 - it's very sad true false true false true
50:40 - and true yields true if I am hungry and
50:45 - I am thirsty I shall go and have
50:50 - lunch right true and true yields true
50:54 - true and false is false false and true
50:57 - is false false and false is false right
50:59 - if I have a Boolean Expression A and B I
51:02 - need both of those things to be true in
51:04 - order for me to get true interestingly
51:08 - enough this is a linearly separable
51:11 - problem I can draw a line right here and
51:14 - true is on one side and false is on the
51:16 - other side this means I could train a
51:22 - perceptron to receive two inputs
51:26 - true and false you know that are true or
51:30 - false oh I'm like way off the screen
51:32 - here that's not a screen that's a hold
51:39 - on how's
51:42 - that let's let let me go backwards for a
51:44 - second and redo this
51:50 - part this
51:52 - means this is I'm fing I'm going to get
51:54 - to the coding I swear
51:56 - I I don't know if I'm going to get to it
51:57 - today to be perfectly honest with you
51:58 - but I I've got all this stuff that I
52:00 - want to talk through I don't know if
52:01 - it's a good idea I'm giving it a
52:04 - try this means this is a this is a
52:07 - linearly separable problem which means I
52:09 - could create a perceptron that
52:11 - perceptron is going to have two inputs
52:13 - there are going to be Boolean values
52:15 - true or false true or false and I could
52:18 - train this perceptron to give me an
52:19 - output which if two truths come in I
52:22 - should get a true if one false at a true
52:24 - comes in I should get a false two falses
52:26 - come in I should get a false great or I
52:29 - could do the same thing what does or
52:31 - change into if I'm going to do
52:33 - or let me erase this dotted
52:36 - line and or now ah all of these become
52:39 - true because with an or operation A or B
52:43 - I only need one of these to be
52:45 - true in order to get true but if both
52:49 - are false I get false and guess what
52:51 - still a linearly separable problem and
52:55 - is linearly separable or is linearly
52:58 - separable we could have a perceptron
53:01 - learn to do both of those things
53:04 - now hold on a second there is another
53:08 - Boolean operator which you may you might
53:12 - not have heard of until this video which
53:13 - would be really kind of exciting for me
53:15 - it would make me very happy if somebody
53:16 - watching this has never heard of this
53:18 - before it is called xor can you see what
53:21 - I'm writing here X or the X stands for
53:25 - or
53:26 - exclusive
53:28 - exclusive it's exclusive or which
53:33 - means it's only true if one is true and
53:36 - one is false it's not true both are
53:39 - false this or that both of those things
53:41 - are false I'm still false but if both
53:44 - are true it's also false so this is
53:47 - exclusive or let me erase all
53:54 - this
53:55 - exclusive or means if one is one is true
54:00 - and one is false it's true if one is
54:02 - true is one is false is true if both are
54:04 - true it's false if both are false it's
54:07 - false this is exclusive or a very simple
54:11 - Boolean
54:12 - operation however I I triple dog dare
54:18 - with the cherry on top you to draw a
54:21 - single line through here to divide the
54:25 - falses and the truths I cannot I can
54:27 - draw if this is not a linearly separable
54:29 - problem this is the point of all this
54:30 - like
54:31 - rambling I could draw two lines one
54:35 - here and now I have all the trues in
54:38 - here and the false is outside of there
54:40 - this means a single perceptron the
54:43 - simplest cannot solve cannot solve the a
54:47 - simple operation like this so this is
54:49 - what Minsky and paper talked about in
54:52 - the book perceptrons well this is like
54:55 - an interesting idea
54:57 - conceptually it kind of seems very
54:59 - exciting but if it can't solve xor what
55:01 - are we supposed to do with
55:03 - this the answer to this is and you might
55:06 - have already thought of this yourself
55:08 - it's not too but believe I I I kind of
55:10 - missed a little piece of my diagram here
55:12 - right let's say
55:13 - this is a perceptron that knows how to
55:17 - solve
55:19 - and and this is a perceptron that knows
55:23 - how to solve or what if I took those
55:26 - same inputs and sent them into
55:29 - both and then I got the output here so
55:32 - this
55:34 - output would give me the result of and
55:38 - and this output would give me the result
55:40 - of or well what is xor really xor is
55:45 - actually or but not
55:49 - and right so if I can solve something
55:53 - and is linearly separable not and is
55:56 - also linearly separable so what I want
55:59 - then is for both of these outputs
56:01 - actually to go into another
56:04 - perceptron that would
56:06 - then be and so if this perceptron can
56:10 - solve not and and this perum can solve
56:13 - or and those output can come into here
56:15 - then this would be the result of both or
56:18 - is true and not and is true which is
56:21 - actually this these are the only two
56:23 - things where or is true but not and not
56:26 - but not and and so the idea here is that
56:28 - more complex problems that are not
56:31 - linearly separable can be solved by
56:34 - linking multiple perceptrons together
56:37 - and this is the idea of a
56:40 - multi-layered
56:44 - perceptron we have multiple layers and
56:47 - this is still a very simple diagram you
56:49 - could think of this almost as like if
56:50 - you were designing a circuit right if
56:53 - you decide whether electricity should
56:54 - flow and this were like a um these were
56:57 - switches you know how could you get a
56:59 - bunch of how could you have an LED turn
57:02 - on with exclusive or you would actually
57:05 - wire the circuit basically in exactly
57:08 - this way um so this is the idea here so
57:11 - what I am would like to do in the next
57:14 - so at some point I would like to make a
57:15 - video where I actually just kind of
57:17 - build take that previous percepton
57:19 - example and just take it a few steps
57:20 - farther to do exactly this but what I'm
57:23 - going to do actually in the next videos
57:25 - is diagram out this structure of a
57:29 - multi-layered perceptron how the inputs
57:31 - how the outputs work how the feed
57:34 - forward algorithm Works where the inputs
57:37 - come in get multiplied by weights get
57:38 - summed together and generate an output
57:41 - and build a simple JavaScript library
57:43 - that has all the pieces of that neural
57:45 - network system in it um okay so I hope
57:49 - that this video kind of gives you a nice
57:51 - followup from the perceptron and a sense
57:53 - of why this is important important and
57:56 - I'm not sure if I'm done yet I'm going
57:57 - to go check the live chat and see if
57:59 - there any questions or important things
58:01 - that I missed and then this video will
58:02 - be over time
58:04 - out
58:07 - okay all right what did I
58:13 - do okay I'm oh I'm I gotta turn this
58:17 - camera
58:22 - on okay so now is a brief
58:27 - moment where uh you can
58:31 - um point out things that I got horribly
58:34 - wrong that I should make sure I correct
58:35 - or ask some follow-up questions that
58:37 - might be
58:43 - important I see there's some chat going
58:46 - on here but I I think that has uh
58:49 - nothing to do with what I um so are we
58:52 - good are we good did that make sense did
58:56 - I get that about right I don't know if K
58:57 - weekman is watching I don't think so
59:00 - that's my uh sanity
59:03 - check um yeah I didn't mention hidden I
59:07 - probably should have uh mentioned that
59:12 - um um like this is technically the this
59:17 - is technically the hidden layer these
59:19 - are the inputs this diagram is terrible
59:21 - because this should be down here so
59:23 - maybe what I'm going to do is I'm going
59:25 - to connect you know that's what I'm
59:26 - actually going to do is maybe I'll do a
59:28 - quick redrawing of this so that it
59:30 - matches what people are used to
59:34 - um uh doing great oh thank you got to
59:38 - know doing
59:40 - great
59:41 - uh Place circuit scramble explains this
59:44 - thing perfectly that's
59:46 - interesting
59:48 - um Tom this is a coding train t-shirt
59:51 - that you can get at coding train.
59:53 - storenvy.com
59:56 - uh proof for xor is not and and or
59:59 - that's right right when I
60:02 - said it is both or and not
60:07 - and that's correct okay so I think I'm
60:11 - good I'm not seeing anyone telling me
60:13 - something that I've
60:15 - done
60:17 - um I'm not seeing
60:20 - anything uh uh that I've done that's
60:22 - sort of like horribly out of about my
60:25 - watch not the shirt this is a Fitbit I
60:28 - don't know it's the Fitbit whichever one
60:30 - looks like this um okay so let me do let
60:35 - me get a couple
60:36 - things um I would like to
60:39 - show so for those of you interested by
60:42 - the way this is a viewer um K
60:45 - weekman uh GitHub is K weekman who
60:48 - created a learning xor with a neural net
60:52 - example um and it visualizes it and kind
60:55 - of visualizes the connections and that
60:57 - sort of thing so I'm going to show this
61:00 - uh I'm going to read I'm going to fix
61:02 - the diagram here and mention hidden and
61:05 - then that's not a Wikipedia page by the
61:07 - way um I this is this page here is part
61:10 - of my um uh core syllabus it has a bunch
61:13 - of references
61:14 - here Daniel it be nice to know if the
61:17 - nonlinearity of the problem affects the
61:18 - number of neurons to use in the hidden
61:20 - layer so yeah so we'll get to this I
61:23 - mean yes yes and
61:26 - no it'd be really nice if I could just
61:28 - like open a door over here and like an
61:29 - actual machine learning expert come out
61:31 - and answer some of these harder
61:32 - questions but um uh let's let's let me
61:34 - get a little further and sort of come
61:35 - back to that I mean the complexity of
61:37 - the the number of hidden layers affects
61:39 - the number of parameters that you get to
61:41 - tweak which is uh the level of
61:44 - complexity that you can kind of apply to
61:46 - a problem so certainly
61:49 - um
61:53 - Okay so
61:56 - [Applause]
61:58 - um okay I'm back so there was one
62:00 - question which is important like oh what
62:02 - I heard some somebody in the chat asked
62:04 - what about the hidden layer and so this
62:06 - is jumping ahead a little bit because
62:07 - I'm going to get to this in more detail
62:08 - in the next video there's a the way that
62:10 - I drew this diagram is pretty awkward
62:12 - let me try to fix this up for a
62:15 - second
62:16 - imagine there were two inputs and I
62:19 - actually Drew those as if they were
62:21 - neurons and I know I'm out of the frame
62:23 - but I'm still here um and these inputs
62:26 - were connected to each of these
62:30 - perceptrons each was connected and each
62:32 - was weighted so this is actually what's
62:35 - now known as a thre layer Network there
62:39 - is the input
62:43 - layer this is the hidden layer and the
62:45 - reason why it's okay well actually let
62:46 - me go this is the output layer right
62:49 - that's obvious right this is the input
62:51 - those are the inputs the TRU and the
62:52 - falses this is the output lay layer that
62:55 - should give us a a result are we still
62:57 - true or are we
62:59 - false um and then the hidden layer are
63:02 - the neurons that sit in between the
63:05 - inputs and the outputs and they're
63:06 - called hidden because as a kind of user
63:09 - of the system we don't necessarily see
63:10 - them a user of the system is feeding in
63:12 - data and looking at the output the
63:14 - hidden layer in a sense is where the
63:16 - magic happens the hidden layer is what
63:18 - allows one to get around this sort of
63:21 - linearly separable question so the more
63:25 - hidden layers the more neurons the more
63:28 - amount of complexity in a way that the
63:30 - system the more weights the more
63:31 - parameters that need to be tweaked and
63:33 - we'll see that as we start to build the
63:34 - neural network library the way that I
63:36 - want that library to be set up I want to
63:38 - say I want to make a network with 10
63:40 - inputs three outputs one hidden layer
63:43 - with 15 like hidden neurons something
63:45 - like that but there could be multiple
63:46 - hidden layers and eventually as I get
63:49 - further and further down this road if I
63:50 - keep going we'll see that there are all
63:52 - sorts of other styles of how the network
63:55 - can be configured and set up and whether
63:57 - the output feeds back into the input
63:58 - that's something called a recurrent
63:59 - network convolutional network is if some
64:02 - this kind of like um set of image
64:04 - processing operations almost happens
64:06 - early on before as one of the layers so
64:08 - there's a lot of stuff in the grand
64:11 - scheme of things to get to but this is
64:13 - the fundamental building blocks uh so
64:15 - okay so I'm in the next video I'm going
64:18 - to start building the library and to be
64:19 - to be honest I think what I need to do
64:22 - no no no no yeah I'm
64:25 - next video I'm going to set up the basic
64:27 - skeleton of the neural network library
64:29 - and look at all the pieces that we need
64:31 - and then I'm G to have to keep going and
64:33 - look at some uh Matrix math that's going
64:35 - to be fun okay uh see you soon goodbye
64:38 - I'm g walk over
64:40 - here all
64:44 - right okay it is now
64:47 - 3:30 and I've been streaming for an
64:51 - hour um
65:00 - I'm checking out the chat which is nice
65:01 - to see okay
65:03 - so I think it's time to write some
65:05 - actual
65:11 - code
65:13 - um
65:16 - okay so what I want to
65:19 - do
65:21 - is oh I forgot to show this yeah I'll
65:25 - come back to it because maybe
65:28 - um I maybe maybe there'll be a time to
65:31 - come back to it um but I do want to
65:33 - build I want to do this kind of as a
65:35 - coding challenge but it'll make more
65:36 - sense to do it once I have the library I
65:37 - think actually in a
65:40 - way um okay so what I want to do
65:45 - now is go to
65:50 - here okay I can close this
65:54 - I can close this is there anything yeah
65:59 - okay
66:01 - um and here
66:04 - whoops uh let's get rid of
66:08 - this of
66:11 - this okay so I think now I'm ready to
66:14 - start writing the code
66:20 - and um and let me just look I'm going to
66:23 - look look at how I set it up here to do
66:26 - it the same
66:27 - way so I want to create this is how I'm
66:29 - going to set it up I want to create a
66:33 - Network that has a certain number of
66:35 - inputs so I'm going to have the library
66:38 - only create a thre layer Network and so
66:41 - it's made with a certain number of
66:42 - inputs a certain number of outputs and a
66:46 - certain number of hidden
66:48 - neurons okay and the viewers are
66:52 - dropping I'm not surprised you know this
66:56 - I I'm kind of you know I I feel like
66:59 - what I'm doing here is getting away from
67:02 - is moving a bit a skew or a scance a
67:05 - side adjacent to the sort of core
67:07 - Mission and and kind of stuff that I
67:09 - like to do on my channel but trying to
67:10 - take this twoe period in the summer to
67:12 - see if I can blast through this material
67:14 - and and hopefully get a sense of it and
67:16 - and begin to do some more interesting
67:18 - stuff applying it
67:19 - later
67:22 - um Alexander right maybe you should have
67:25 - an outline for every Live code session
67:27 - that would allow for a more smoother
67:30 - presentation duly noted duly noted
67:34 - there's nothing smooth about this
67:35 - whatsoever okay let me keep going I have
67:37 - a mental outline but I I'm I'm just so
67:41 - scatterbrained okay um so this is what
67:43 - we're going to
67:45 - do I'm not going to get very far with
67:48 - this but that's okay all
67:52 - right
67:59 - so let me see here
68:03 - okay um all right so here we
68:14 - go I'm going to move on
68:18 - now welcome back I'm going to actually
68:22 - write some code in this video
68:24 - um not that much so what I'm doing now
68:27 - welcome I made a few introductory videos
68:30 - covered some background uh about uh
68:33 - neural networks and why they exist and
68:35 - where I'm trying to go with this and in
68:37 - this video I'm going to actually begin
68:39 - to write the code for a simple
68:42 - JavaScript neural network library now
68:44 - I've actually already done this it
68:46 - exists here at this repository uh GitHub
68:49 - shiftman
68:50 - neural-network P5 I'm designing this
68:53 - library to be used with a set of p5js
68:55 - examples with a a library a JavaScript
68:58 - library called P5 although ultimately
69:00 - this Library stands alone on itself by
69:03 - itself you don't have to use it with
69:04 - just P5 so before I can write the code
69:08 - let me come over here to the Whiteboard
69:10 - and this is where I last left off
69:12 - talking about how the general structure
69:16 - of a neural network library Works uh a
69:19 - neural network system
69:20 - works and so what I need to do here
69:25 - is when in the code I
69:30 - create a neural network I want to create
69:34 - three
69:36 - things I want to create an input
69:42 - layer I want to create a hidden
69:47 - layer and I want to create an output
69:51 - layer so when I create a
69:54 - new the way I want to design this
69:57 - library is I want to say new neural
70:00 - network and I want to give it can you
70:02 - see this I think you can I want to give
70:03 - it three
70:05 - arguments the number of input neurons
70:09 - let's just use the word neurons the
70:11 - number of hidden neurons and the number
70:14 - of output neurons so I'm doing something
70:16 - which I typically don't do which is
70:18 - usually I like to have a specific
70:20 - problem that I'm trying to solve and
70:22 - like write the code for that problem and
70:24 - in here the problem that I want to solve
70:25 - is I want to make a generic kind of
70:28 - useful library that could be used in a
70:30 - bunch of different contexts so I don't
70:32 - know what those numbers are going to be
70:35 - uh I don't know what the data is I'm
70:36 - just kind of working on the skeleton the
70:38 - structure of the library before I start
70:39 - to apply it to things so let's just make
70:42 - up some numbers let's say there's going
70:44 - to be
70:47 - three input
70:49 - neurons
70:51 - four hidden neurons and two output
70:55 - neurons what this means now is in a feed
70:58 - forward neural network there are three
71:02 - inputs we could imagine again I'm using
71:04 - the kind of classic example of guessing
71:07 - the price of a house this could be
71:10 - number of bedrooms number of bathrooms
71:13 - square footage so those are like three
71:16 - parameters of a house these will connect
71:19 - to 1 2 3 four hidden neurons so this is
71:23 - the input
71:24 - layer this is the hidden
71:27 - layer and then I'm kind of running out
71:30 - of space here there will be two
71:32 - outputs and then this is the output
71:36 - layer so this is the configuration the
71:40 - idea of a uh and so what I'm building
71:43 - here is what's known as this is a
71:44 - multi-layered perceptron these are
71:47 - individual perceptron units essentially
71:50 - that are have multiple layers and it
71:53 - also is an another important term that I
71:56 - want to add here is I want to create a
71:58 - fully connected
72:01 - Network and now there are variations to
72:04 - this that we might see in future
72:05 - examples but the idea of a fully
72:07 - connected network is that every input is
72:10 - connected to every hidden every hidden
72:13 - is connected to every output but I so I
72:15 - can draw those connections and it's not
72:17 - so many that I you know if I were doing
72:20 - some kind of post- production I would
72:22 - speed this up but I'm going to just draw
72:25 - this web of all these connections so
72:27 - every input is connected to every hidden
72:30 - and every
72:32 - hidden is connected to every
72:36 - output whoa oops ah ah I messed this up
72:40 - but I'll get it eventually there we go
72:44 - right so you can see that every every
72:46 - node is connected to every node in the
72:49 - next layer so the idea is that those
72:51 - three inputs come in
72:54 - the data feeds forward and those two
72:56 - outputs come out so this is the
72:58 - structure now we have to get into a lot
73:00 - of details here well how do I keep track
73:02 - of all of these connections how do I
73:04 - actually do the loops to like do all the
73:06 - sums of everything and how do I read the
73:08 - outputs I'm going to get to all that but
73:10 - this is the overall structure so let's
73:12 - go back to the
73:14 - code and now let's actually try to like
73:17 - write a little bit of this Library very
73:19 - very
73:21 - little so where am I going here okay so
73:23 - this is my code there's nothing yet I'm
73:25 - going to create a new file and I'm going
73:27 - to call this nn. JS so this is now going
73:31 - to be my so here's the thing ultimately
73:35 - I want this to be like a proper
73:39 - JavaScript library but ultimately what
73:41 - is a JavaScript library but a file with
73:43 - some JavaScript in it so I might later
73:45 - as this gets more sophisticated optimize
73:48 - it and use some sort of like build
73:50 - process or break it up into multiple
73:52 - files but right now I just want to kind
73:53 - of like get the pieces going so I am
73:56 - going to I'm also going to use es5
73:59 - syntax this is the trajectory that I've
74:03 - been on soon in future videos I'm going
74:05 - to start adopting some es6 syntax But
74:08 - ultimately maybe this Library I'll do a
74:10 - follow-up and come back and kind of I'm
74:12 - going to do a lot of things maybe not in
74:13 - the most optimal or efficient way but
74:15 - hopefully in the most easy to understand
74:17 - and fall away so I want to create a
74:20 - Constructor
74:21 - function called ner
74:24 - Network okay and I should also mention
74:26 - again while we're here that I built this
74:29 - Library already and when I built it I
74:31 - based just about everything out of this
74:33 - book called make your own neural network
74:34 - by Tariq Rasheed and so while I'm doing
74:37 - this now kind of a bit more on the Fly
74:39 - I'm sure everything that's in my brain
74:40 - ultimately came from here and probably
74:42 - some other sources too okay
74:46 - so what do I want to do the core thing
74:48 - that I want to do is I want to create
74:51 - the N neural network with a certain
74:52 - number of input nodes number Hidden
74:54 - number of output so I'm going to add
74:57 - those as arguments here I'm going to say
75:01 - um number of input number of hidden
75:07 - number of output I'm going to create a
75:09 - neural network with three arguments and
75:11 - then I'm going to say uh input nodes I
75:16 - think I'm going to be long-winded about
75:17 - this equals number of
75:19 - input and so I'm going to create three
75:22 - uh hidden nodes is this argument and uh
75:29 - output nodes is this argument is that an
75:33 - O yes it is okay so this is we've
75:36 - actually written some code the idea
75:38 - being that what I want to do is say
75:41 - things
75:42 - like VAR
75:46 - brain and brain is a new
75:50 - neural network that has three inputs
75:54 - with three hidden and one output right
75:56 - this is the idea so I need to figure out
75:58 - what shape and shape using the word
76:00 - shape very specifically does the data
76:03 - come in that's how many input nodes I
76:08 - want what shape is the output that I
76:11 - want am I looking for a single output am
76:13 - I trying to look for a range of outputs
76:15 - that's how many outputs I want then how
76:17 - many hidden neurons do I want well
76:19 - that's kind of an open question well
76:21 - maybe I want as many as I could possibly
76:24 - fit in was the program running
76:26 - reasonably fast but it sort of depends
76:28 - on the complexity of the problem and
76:29 - we'll come back to that later and I
76:31 - should also note that I this is a simp
76:34 - oversimplification of how neural network
76:37 - architectures can be this is by
76:39 - definition a three layer Network and
76:41 - this library is only going to allow for
76:43 - a three layer Network an input a single
76:45 - hidden and a sing and an output but as
76:48 - something you might think about for the
76:49 - future how would you write the code to
76:50 - have multiple hidden layers because a
76:53 - lot of neural network-based Learning
76:55 - Systems need multiple hidden layers to
76:57 - be able to perform optimally but for now
77:00 - I'm going to keep things very
77:02 - simple okay I'm gonna pause for a second
77:05 - because I'm kind of
77:09 - thinking
77:11 - um I'm I'm seeing some interesting
77:15 - questions um
77:19 - and
77:22 - um
77:25 - so there's an interesting discussion
77:26 - going on is what's the relation number
77:28 - of input hidden and output so the the
77:31 - it's not that the hidden is arbitrary
77:33 - but it it can be kind of any number but
77:36 - you know if you're just going to have
77:37 - one it's not going to do going to work
77:39 - very well and so you know one thing that
77:42 - you might do is just kind of like well
77:44 - however many inputs let's just make the
77:46 - hidden layer the same number that could
77:47 - be like a good starting point I would
77:49 - say um Okay so the thing that we're
77:54 - going to need very quickly and I need to
77:56 - refill my water
77:59 - here is we're going to need the linear
78:01 - algebra
78:04 - stuff yeah
78:06 - um
78:10 - so I'm trying to I
78:13 - think I think I'll take this video a
78:15 - little bit
78:21 - further
78:26 - and then I have to stop and
78:30 - explain so here's the thing let me talk
78:32 - this through without this being part of
78:34 - the official
78:37 - tutorials ultimately what I'm going to
78:39 - do with each one of the
78:42 - actually why do I why do I even bother
78:44 - saying like let me talk let me just talk
78:46 - this through and this will be part of
78:47 - the
78:49 - video I don't know where I came over
78:51 - here I don't know where I was uh oh
78:54 - linear algebra yeah I don't even want to
78:56 - use that word but it's it's what we're
78:57 - going to do is actually quite it's not
78:59 - so hard to figure
79:03 - out okay what is the next
79:07 - step written we did write some code
79:10 - thankfully wrote some code now we got to
79:12 - stop again the next step is the feed
79:16 - forward process
79:21 - okay
79:23 - the
79:24 - feed
79:26 - forward
79:29 - process the way that the feed forward
79:32 - process
79:33 - works is that we
79:37 - receive these
79:39 - inputs oh there's so much to do so many
79:42 - pieces to this puzzle I'm excited to get
79:45 - through it all though so let's just say
79:47 - for example we're looking at this hidden
79:50 - neuron do you remember from the
79:51 - perceptron video videos maybe you didn't
79:53 - watch those so let's talk about it the
79:56 - idea is that we need to do something
79:59 - called a
80:00 - weighted sum so let's pretend this is
80:04 - the house prediction thing and this was
80:06 - the number of bedrooms three this is the
80:08 - number of bathrooms you know this is the
80:10 - number the square
80:13 - feet so each one of these connections
80:16 - right the data is going to flow in the
80:18 - data comes in here the number three
80:20 - comes in here and then look at this
80:23 - there's four outgoing
80:25 - connections each one of those
80:27 - connections has a weight to it now
80:30 - ultimately the whole point of doing this
80:33 - learning neural network based Learning
80:34 - System is we want to tweak those weights
80:38 - we want to train the brain train the
80:40 - neural network to have optimal weights
80:42 - to get good results results that make
80:45 - sense and that training process is
80:46 - something that I'm going to get to I
80:48 - don't how many videos down the road from
80:50 - now but not too far away these weight
80:52 - weights will have typically to start one
80:55 - way of thinking about them is they're
80:56 - going to just have random values between
80:59 - -1 and one and there's a wide variety of
81:02 - techniques and strategies for
81:03 - initializing random weights or not just
81:06 - random to a neural network but for right
81:08 - now good way for us to get started they
81:10 - all have random
81:12 - weights so even though I'm looking at
81:14 - each one of these flowing out slightly
81:17 - better way for me to look at this with
81:18 - you is actually just look at all the
81:20 - connections flowing in so this
81:22 - particular hidden neuron has three
81:25 - connections flowing in a three a and the
81:29 - input values are 3 2 and 1,000 each one
81:32 - of those has a weight so let's pretend
81:34 - this is like 0.5 U let's say this is
81:36 - like uh
81:39 - .5 and this particular weight is one so
81:43 - I'm making using very very simple
81:44 - numbers the idea is that each hidden
81:48 - neuron does something called a weighted
81:51 - sum so so it takes the input multiplied
81:54 - by the weight and adds that to the other
81:58 - input multiplied by the weight and adds
81:59 - that to the other input multiply by the
82:01 - weight so we could actually do this 3 *
82:04 - .5 is
82:05 - 1.5 plus 2 * .5 is -1 plus 1,00 * 1 is
82:13 - plus
82:14 - 1,000 so this value now is a
82:19 - [Music]
82:20 - 100.5 now we can see there's a huge flaw
82:24 - here which is that the fact that square
82:26 - footage is kind of a big number and
82:29 - number of bedrooms and number of
82:31 - bathrooms are small numbers means this
82:33 - kind of way of summing it is going to
82:35 - produce some like odd results this the
82:37 - square footage is going to be weighted
82:38 - so heavily just by the fact that it's
82:40 - bigger numbers so a lot of time in
82:43 - working with a machine learning or
82:44 - neural network based system we need to
82:45 - do some type of cleaning or normalizing
82:48 - of the data and we might do something
82:49 - where we you know we sample this down so
82:53 - that they you know we actually do the
82:54 - number of bedrooms between 0 and five as
82:56 - a value between 0 and one and number of
82:58 - bathrooms always as a value between 0
82:59 - one and square footage this would
83:01 - actually turn into 0.1 like because the
83:02 - range is between Z and 10,000 square
83:04 - feet or something so we would do some
83:06 - kind of normalization of these values
83:08 - but this is again further down the road
83:09 - when we start to apply the library in an
83:11 - actual
83:13 - project once this weighted sum is
83:16 - complete the result of that weighted sum
83:19 - gets sent out through the outgoing
83:21 - connections
83:23 - but it gets passed through an activation
83:26 - function so I'm going to come back to
83:29 - the activation function this is
83:30 - something we did with a perceptron and
83:32 - that's going to be a separate video
83:33 - where we look at different activation
83:34 - functions and how they work right now I
83:37 - want to focus on this weighted
83:40 - sum so I could keep going here I could
83:44 - create some type of array of I could
83:48 - create an object that's like each one of
83:50 - these nodes or neurons is an object
83:52 - object then I could iterate over I could
83:54 - have connection objects so there's a
83:56 - bunch of different approaches I could
83:57 - take but the classic and standard
84:01 - approach is actually to look at
84:05 - storing all of these weighted
84:07 - Connections in something called a
84:11 - matrix which is really just like a
84:13 - spreadsheet a grid of
84:15 - numbers looking at the inputs as an
84:19 - array and doing some type of math that
84:22 - basically takes take that array of
84:24 - inputs multiply it by that
84:27 - Matrix of weights and
84:30 - generate the outputs of this hidden
84:34 - layer so this is so give me a second
84:38 - here I'm going to erase I'm gon to I'm
84:40 - gonna I'm going to make the case for
84:41 - this with a simpler
84:43 - scenario okay so
84:48 - um so I'm going to erase
84:51 - this
85:00 - this can get edited
85:07 - out
85:14 - and uh let's see
85:20 - here you know a lot of times when I us
85:22 - to do these
85:23 - videos I would do the videos at the end
85:26 - of the day after I taught the material
85:27 - with in a classroom earlier in the day
85:30 - and I would sort of figure out in my
85:32 - head how to condense things
85:35 - down maybe through the magic of editing
85:37 - things will get condensed down but okay
85:38 - so um I'm gonna let's see let me
85:42 - just let's go back to this um simpler
85:51 - diagram
85:58 - okay
86:00 - okay okay I'm back I erased what I had
86:03 - and I drew I'm drawing a simpler diagram
86:05 - here now so let's look at this diagram
86:07 - which just has fewer connections it's
86:09 - going to be easier for us to unpack so
86:11 - we can think of these inputs as x0 and
86:14 - X1 let's not even worry about the output
86:17 - right now these are the inputs this is
86:20 - the hidden layer right hidden layer so
86:24 - let's think about this and and actually
86:26 - let me change these numbers to X1 and X2
86:30 - you know sometimes I like to count from
86:31 - zero sometimes I like to count from one
86:33 - I don't know why but I I I feel like in
86:34 - this case let's let's call it one and
86:36 - two so this is really like hidden one
86:40 - hidden two so each one of these
86:42 - connections right each one of these
86:44 - weights you could say here this is a
86:47 - weight that goes from one to one this is
86:51 - a weight that goes from one to two this
86:55 - is a weight that goes
86:58 - from two to one and this is a weight
87:03 - right here that goes from two to
87:05 - two so notice how there are two inputs
87:10 - two hidden neurons four weights in other
87:16 - words the weights and I'm going to draw
87:19 - I'm going to kind of use start to use
87:21 - Matrix not a little bit the weights can
87:24 - be expressed like this 1
87:27 - 1 1
87:30 - 2 2 1 2
87:34 - two row I want those to be row
87:38 - column hold on let me think about this
87:39 - for a
87:41 - second time out time out time
87:43 - out this second uh because maybe my
87:47 - numbering should actually be row row
87:51 - yeah row I did it right I did it right
87:53 - never
87:55 - mind
87:57 - okay okay so this is a way of expressing
88:00 - the weights and a way of expressing the
88:04 - inputs I could write it like this X1
88:08 - X2 okay so I'm making the case that I
88:14 - have two inputs and I have four weights
88:16 - and I could write it out like a matrix
88:19 - of numbers a 2X two Matrix
88:23 - and this is essentially a 2X one Matrix
88:27 - whenever I'm going to get more into
88:28 - matrices in the next video or am I in
88:32 - that video already I don't even remember
88:34 - I don't know where I am in my world but
88:37 - uh typically when we talk about a matrix
88:40 - a grid of numbers we reference it rows
88:44 - by columns 2 by two 2 by one okay so let
88:50 - me just show you something remember this
88:52 - we need a weighted sum here this
88:55 - weighted sum is
89:01 - X1
89:03 - times weight 1
89:06 - one plus
89:11 - X2 times weight
89:17 - 21 okay that's the weighted sum for this
89:21 - neuron or node the weighted sum for this
89:24 - neuron or node is
89:27 - X1 times the weight from 1 to
89:31 - 2 and X2 times the weight of 2 to
89:36 - two plus X2 times the weight of 2 to
89:43 - two it so
89:46 - happens I could take these two results I
89:49 - could call this like
89:52 - uh H1 and call this
89:55 - H2 and I could say let me actually say I
89:59 - could I could basically say this times
90:03 - this equals H1
90:08 - H2 so this is the actual math the way
90:11 - that we described it look at both inputs
90:13 - coming in multiplied by their weights
90:15 - and summed look at both inputs coming in
90:18 - multiply by their weights and summed
90:21 - these are the the it written out but it
90:24 - so just happens that this exact math
90:27 - writing it like this and producing this
90:30 - outcome is exactly the math that is part
90:34 - of a field of study called linear
90:37 - algebra linear algebra involves
90:40 - manipulating vectors and matrices a
90:43 - vector being a one-dimensional list of
90:46 - values a matrix being a two-dimensional
90:49 - list of values the inputs are always
90:51 - always onedimensional the outputs are
90:54 - always onedimensional the weights are
90:57 - always can always be expressed as twod
90:59 - dimensionals it's every input connected
91:01 - to every hidden you can think of it very
91:04 - much like pixels every row and every
91:07 - column so this is where I need to stop
91:11 - and what I want to do is do a few videos
91:14 - that cover this notation and math with a
91:17 - bit more detail writing a little
91:20 - JavaScript simple JavaScript Matrix
91:23 - library and ultimately once we done that
91:26 - we can come back here and see how if we
91:29 - have that Library written we can then
91:31 - use it to do the math between the inputs
91:34 - and the hidden and the hiddens to the
91:36 - output and ultimately later we're also
91:38 - going to go backwards through the
91:39 - network to tweak values and and train it
91:42 - and that's we're also going to use the
91:43 - same Matrix math so this is why we need
91:48 - or why we don't need because we could
91:49 - kind of do it without it but while it's
91:51 - useful
91:52 - to work with this idea of linear algebra
91:54 - and I should note once again that if we
91:57 - were doing this in something like python
91:59 - using a library like something called
92:01 - nump we would get all of this stuff for
92:03 - free and there are JavaScript Matrix
92:05 - libraries and might but I'm going to
92:07 - kind of unpack some of this and and
92:08 - write a lot of the code from scratch
92:09 - just to have a sense of how it's working
92:11 - because why not okay I'll see you in the
92:14 - next video where I look at this a bit
92:16 - more in a bit more detail thanks very
92:20 - much
92:22 - okay uh how we doing here what did I get
92:26 - wrong uh oh I'm
92:31 - wrong looks like I got something
92:34 - wrong
92:36 - shoot X1 * w11 X2 *
92:45 - w21 X1 * this plus this X2 * this plus
92:49 - this I don't see it being wrong
92:54 - let me go back and look at the
93:02 - chat I'm pretty sure I got it right oh
93:04 - whoops sorry I didn't change the camera
93:07 - I'm pretty sure I got it right the way
93:08 - that u i mean I'm going to cover this in
93:10 - the next video but the way that you do
93:12 - this is essentially uh these
93:16 - two
93:19 - um sorry I I lost my train of thought
93:23 - uh I basically take I do the dot product
93:26 - of like this vector and this Vector so
93:28 - that would be w11 * X1 plus W12 * X2 w11
93:34 - * X1 w21 * X2 and then I would do that
93:38 - for this and this
93:43 - W2 uh
93:46 - oh this is wrong
93:49 - maybe wait wa wait wait
93:52 - when I looked at it a second ago it was
93:54 - right
93:58 - oh maybe I've
94:01 - just did I write this in the wrong way
94:03 - and this should have been 21 up here
94:06 - ah
94:09 - shoot yeah this should be
94:13 - shoot I miswrote I wrote this wrong I
94:16 - knew this would
94:18 - happen uh and then this should be um
94:23 - one two I have to go back we have to go
94:27 - back we have to go
94:30 - back we have to go
94:33 - back hold let me check the
94:38 - chat uh uh no did I have it right the
94:42 - first
94:47 - time Matrix is correct okay hold on I I
94:52 - I I I did it to myself hold
94:58 - on was it
95:06 - correct oh
95:11 - why I'm look at the chat
95:14 - here formula is
95:17 - wrong the problem is you guys are behind
95:19 - me so
95:23 - you guys
95:25 - are uh yep the par below does not match
95:28 - the Matrix so here's the thing this is
95:32 - correct the way that I've let me go and
95:35 - um let me look at a
95:38 - a let me look at the way I mean I I
95:41 - I these definitely don't
95:44 - match these
95:47 - match and the way I thought of it
95:50 - writing it this match matches but I I'm
95:52 - pretty sure that
95:53 - this should actually be down here so let
95:56 - me look at how it's the notation is
95:59 - used in
96:01 - um in this
96:07 - book
96:09 - um yeah yeah yeah yeah yeah yeah I just
96:13 - shoot I messed up so I don't know if you
96:15 - guys can see
96:17 - this but
96:19 - um Tariq this book uses very similar
96:24 - notation and so I have to I'm trying to
96:26 - decide whether so I just I just want to
96:29 - um make this one two and this 2 one
96:32 - whether I want to I hate the fact that
96:34 - this is going to be wrong throughout the
96:36 - whole video and then I'm going to
96:37 - correct it at the
96:38 - end but I think if
96:41 - I go
96:43 - back I don't I don't want to like redo
96:45 - this whole
96:48 - section I wish I could watch this back
96:50 - right now
96:52 - actually I can sort of by
96:54 - stopping because like where where could
96:56 - where could I erase the board
97:05 - from yeah I'll just mention that it's
97:07 - wrong and I'll fix it in the next
97:09 - video it wasn't so long in the
97:13 - video lost is
97:16 - right um no nothing the this
97:20 - um
97:22 - I I just ended up notating it in this is
97:25 - one I I was actually asking myself this
97:27 - question I decided that I was right and
97:29 - then I realized I was wrong because I'm
97:31 - thinking of this row column row column
97:34 - really column row
97:38 - column
97:39 - row um this should all right
97:43 - so just trying to decide if I want to
97:45 - like re-record this tutorial with the
97:49 - correct value Val is there or just issue
97:52 - a correction at the end of a
97:54 - video I need to do another straw
97:57 - poll I I I just I wish I had a sense of
98:01 - where where I was where
98:06 - because I think I'll just issue a
98:10 - correction
98:12 - um oh yeah okay good
98:16 - idea so um Topher in the chat is just
98:21 - suggesting you know what I can do is
98:22 - when to make the edited version of this
98:25 - we can also I mean this is more work for
98:27 - MATA but um I know this is technically
98:29 - possible um we could maybe just put an
98:31 - actual annotation in the video like an
98:33 - overlay so I'm just going to issue a
98:35 - correction right now okay um so
98:39 - hopefully as you were watching the video
98:41 - you saw a little annotation um this is
98:43 - actually incorrect I mean everything
98:45 - about this math is correct this matches
98:47 - this right the weighted sum is X1 *
98:51 - weight one from 1 to 1 X2 * weight from
98:55 - 2 to 1 but actually the notation that I
98:58 - the way I wrote this Matrix as we go as
99:00 - I go into the next video where I
99:02 - actually look at how the Matrix math
99:03 - works this really should be written as
99:06 - one two and this should really be
99:08 - written as 21 the reason why that is is
99:11 - this should be X1 * w11 plus X2 * w21
99:18 - which is written right here so that
99:19 - Matrix mathod I'm going to go in more
99:21 - detail in the next video we take this
99:23 - row and multiply it by this column and
99:26 - this row and multiply it by this column
99:28 - and you can see that's what these two
99:30 - things are okay so thanks for bearing
99:33 - with me I there's a lot of little pieces
99:35 - but I am going to get back into the code
99:37 - so in the next video I'm not very
99:39 - confident about the order I'm doing all
99:40 - this in but it's just the way that I'm
99:42 - going to choose to build it and so in
99:44 - the ne again I'm saying this again the
99:46 - next video I'm going to look at the
99:47 - Matrix math again and then write a
99:50 - generic Library that does that math and
99:52 - then come back and put it back into the
99:54 - neural network itself okay so see you in
99:57 - the next video
100:01 - thanks um
100:05 - okay so
100:08 - um I think I got it is that correction
100:11 - good put some thumbs up the notation on
100:14 - The Matrix is always because there's so
100:16 - much going so much chatter going on I
100:18 - can't tell what people are referring
100:19 - to
100:24 - what time is it an hour and 40
100:26 - minutes
100:28 - um formula is wrong not the
100:35 - Matrix can we
100:37 - agree that what I have here right now is
100:43 - correct before I move on I want to make
100:45 - sure there's nothing else that's wrong
100:47 - here to me I feel I'm going to look at
100:50 - the my uh tariq's book
100:53 - again good job everything is good some
100:56 - people say no I
101:03 - forgot your notation is bad no it's
101:06 - correct all right let me look here boy
101:08 - I'm getting a lot of mixed signals here
101:11 - let me look and see
101:14 - how Tariq notates
101:19 - it um
101:25 - so I'm going to look I'm looking at this
101:27 - input one time weight 1 one plus input
101:33 - 2 time weight 2
101:36 - 1 input 1 * weight one 2 plus input 2 *
101:42 - weight 2 two okay and then the matrices
101:46 - are 1 1 2 1 1 2 22
101:51 - 1 2 so I now have just confirmed that my
101:55 - notation matches exactly this notation
101:58 - the one thing that I've done that's kind
102:00 - of incorrect is swapping the order of
102:03 - these I really should be saying weight
102:05 - one * X1 plus weight 1 1 * X1 weight 2 1
102:09 - * X2 I kind of wrote it out this way but
102:12 - that's a minor detail because it's the
102:15 - math is equivalent but maybe the
102:17 - standard would be to write this
102:19 - multiplied by this so I'm feeling based
102:22 - on the fact that I have now looked at
102:25 - this particular book and see that what I
102:28 - did matches I'm feeling more confident
102:32 - about
102:33 - it oh somebody is saying Matrix notation
102:36 - is different for Europe and
102:40 - America
102:44 - oh
102:46 - whoa oh no don't tell me I got to use
102:49 - the metric Matrix
102:53 - system this is so interesting is that
102:56 - really true let's Google
102:58 - this Matrix notation Europe versus
103:10 - America
103:12 - Two dimensions are read by 2 by3 that's
103:15 - how I do it two rows and three
103:18 - columns one one one two 1 1
103:21 - three
103:24 - ah so yeah so the awkward thing here
103:29 - is so this is the unfortunate awkward
103:33 - thing these are mapped to
103:36 - this but you wouldn't normally I think
103:39 - notate that's why I wrote it the other
103:40 - way the first time around you wouldn't
103:43 - normally notate it that way because you
103:47 - would sort of do in this notation you
103:49 - would say x by y or column by row but
103:52 - these aren't actual these aren't
103:53 - actually the column and row numbers
103:56 - they're the weight mapping so I I think
103:59 - I'm a little bit off in the weeds here
104:02 - um I'm going to leave this as is and
104:06 - when I do the generic linear algebra
104:09 - videos I'll try to use this the more St
104:12 - it won't be tied to the neural network
104:14 - stuff I can kind of think about it in a
104:16 - different way I'm going to go read the
104:17 - chat again one more
104:19 - time
104:24 - yeah the don't differ so far classic R
104:28 - someone is fooling with me all that also
104:30 - could be possible that people are
104:31 - trolling me
104:35 - because
104:39 - yeah it's the
104:42 - transpose uh all
104:45 - right okay I'm seeing some very
104:48 - interesting discussion but I am going to
104:50 - to
104:52 - um move on um and I guess I could add a
104:57 - little addendum
104:59 - there and maybe I'll just for the sake
105:03 - of
105:04 - argument uh I'm going to just record
105:06 - that saying that in one sentence to tack
105:09 - it
105:10 - on could be edited in I guess somehow if
105:14 - necessary let
105:16 - me um one more thing I should point out
105:19 - about now that I fixed this notation
105:21 - that's a bit awkward here is that there
105:23 - are different styles for notating Matrix
105:27 - you know and again I'm going to use the
105:29 - convention of rows by columns so this is
105:33 - a 2X two row column but you would notice
105:37 - here that typically these would then be
105:40 - row column row column so this would be
105:44 - one one one one2 that's why I Had It
105:47 - reversed the first place but because I'm
105:50 - taking it from these weights these
105:52 - aren't actually the row column numbers
105:53 - oh my head hurts already um but it's all
105:56 - going to work out it's all going to be
105:57 - fine uh just bear with me um um in this
106:01 - sort of notation
106:03 - snafu okay the important thing is that
106:07 - this actually matches the way I want to
106:08 - describe the weights and the and those
106:10 - weighted sums and I will we're going to
106:13 - double back and everything should
106:14 - hopefully as I get through more
106:16 - explanation stuff will start to make
106:17 - more sense okay um I don't think that
106:21 - should be edited in I don't think yes
106:23 - that should not be added let's just
106:26 - leave it as is people in the comments
106:27 - will complain but I'm I I had the
106:29 - whatever I'm GNA keep
106:32 - going yes I know about transposing
106:35 - matricies use please use square
106:38 - brackets
106:41 - um okay oh my God the chat is going
106:44 - crazy oh my God good
106:48 - job uh all right all right all right
106:50 - okay okay okay I'm I'm I'm I'm let me
106:52 - just say for the sake of argument I'm
106:55 - sorry I'm going to move on
106:59 - [Applause]
107:01 - [Laughter]
107:04 - okay this is now going to be
107:14 - erased all
107:16 - right
107:19 - now
107:22 - now we come to the
107:24 - point where I
107:26 - actually took some notes you might be
107:29 - shocked to hear this
107:36 - okay do I have the stamina and energy to
107:39 - keep
107:42 - going oh hold on hold on Jedi was
107:46 - writing something in the chat I can't
107:47 - you guys
107:49 - are
107:51 - write the hidden nodes are the rows and
107:54 - the input nodes are the
107:57 - columns
107:59 - exactly everyone knows how to troll me
108:02 - now I'm GNA keep
108:03 - going oh the fact that I wasn't using
108:05 - square brackets don't you worry I'll use
108:07 - square brackets now so I should
108:13 - use I should do square brackets
108:17 - okay hello here I
108:21 - am so I'm trying I'm moving along here
108:24 - through this journey of trying to
108:25 - program this neural network library
108:28 - again I might suggest skip ahead find
108:31 - some videos where I'm just using the
108:33 - library but I'm I'm doing this I'm
108:34 - exposing this process of a person
108:37 - struggling to make sense of the world
108:40 - but for this video I did actually make
108:42 - some notes um and I want to reference
108:45 - actually a there's a nice um a medium
108:47 - post about kind of what linear algebra
108:49 - you need to know for for deep learning
108:51 - that I will uh show you on my laptop in
108:53 - a second and and link to it in the video
108:56 - where I read that post this morning and
108:58 - helped me kind of gather my thoughts for
109:00 - this particular set of video so what
109:02 - I've done so far is I've established
109:04 - that we
109:05 - need this
109:08 - idea of linear
109:12 - algebra in order to perform some of the
109:17 - math in the neural network library that
109:19 - I'm building so what I want to do is
109:21 - take a
109:29 - break let me start
109:34 - over let me not start over that's fine
109:37 - it's
109:40 - fine so what I want to do is take a
109:43 - break from the neural network stuff
109:45 - itself and look at the linear algebra
109:47 - stuff in a vacuum and yes finally
109:50 - actually hopefully write some code
109:52 - because I want to talk through the math
109:54 - and implement the math in code in a
109:56 - generic way and then apply that to the
109:57 - neural network we're g to get through
110:00 - this
110:00 - everybody okay so what are the core so I
110:04 - have I I have some
110:07 - props wait time out so unnecessary but
110:10 - since I brought these up
110:14 - here I
110:17 - found my
110:19 - old linear algebra textbooks from 20
110:23 - some plus 25 some amount of years ago so
110:27 - I brought these as
110:28 - props I was reading them this morning
110:31 - but here's the thing this is not a
110:33 - course in linear algebra there's
110:34 - actually some great linear algebra
110:36 - videos on KH Academy um probably there
110:38 - are some other ones out there I will
110:39 - link to additional resources in the
110:41 - description of this video I want to do
110:42 - is cover the aspects of linear algebra
110:45 - that are necessary or relevant to the
110:48 - neural network stuff um um and kind of
110:51 - leave out the rest so I'm going to give
110:53 - that an attempt and see how it goes and
110:55 - write code along with it and you'll let
110:58 - me know how that goes okay so here's the
111:00 - thing there are two key Concepts in
111:03 - linear algebra there's the idea of a
111:05 - vector and there's the idea of a
111:07 - matrix now a vector is actually
111:10 - something that I've spent a lot of time
111:11 - in previous videos in this nature of
111:13 - code playlist talking about the idea of
111:16 - a two-dimensional
111:18 - Vector an entity magnitude and direction
111:21 - in a two-dimensional space we use this
111:24 - Vector for forces and velocity and all
111:26 - sorts of physics simulation all sorts of
111:28 - stuff but ultimately this
111:30 - Vector is just an X and A Y that two
111:35 - dimensional Vector from and of course
111:38 - could be a z if it were a
111:39 - three-dimensional Vector for all the
111:40 - computer graphics and animation physics
111:42 - simulation stuff I've done in previous
111:44 - videos we could think though about we
111:46 - can we can consider a vector as just an
111:48 - N dimensional list of
111:51 - values and I could make the notation
111:54 - like this and I could say x0 X1 X2 X3 X4
111:59 - X5 so this is a five-dimensional vector
112:02 - there you
112:04 - go so this is the idea of a vector now
112:07 - one thing I should note is that you will
112:09 - see a variety of different kinds of
112:12 - notation um you might see them am I
112:16 - still you might see things written like
112:18 - this XY you might see it written like
112:21 - this XY different textbooks different
112:25 - styles I'm going to use this square
112:28 - bracket notation for the algorithms and
112:32 - examples I'm going to demonstrate in
112:34 - this video and in future videos okay so
112:36 - that's the idea of a vector now if you
112:37 - also
112:39 - recall we can do math with vectors and
112:43 - there are a few different kinds of
112:45 - operations there's the idea of a scalar
112:47 - operation like let's say I have the
112:49 - vector two 2 three and I multiply that
112:53 - by the number
112:55 - two I could take this scalar value the
112:57 - single value and multiply it by each
112:59 - component of the
113:01 - vector and I would now have 4
113:05 - six there also are operations that are
113:08 - referred to as element
113:12 - wise this is the kind of operation that
113:14 - I did over and over again if I had a
113:16 - velocity vector and a position Vector so
113:19 - if I had a position Vector that was
113:22 - something like you know 2 three and then
113:24 - I had a velocity Vector that was you
113:26 - know
113:27 - -15 I could add elementwise add these
113:32 - together so the first element add to the
113:35 - this the the first elements get added
113:37 - together so 2 +1 is 1 the second two
113:42 - elements get added together 3 + 5 is 8
113:46 - so these are element wise operations now
113:50 - in addition to that there is also
113:53 - something reference referred to as
113:56 - Vector multiplication and there's like
113:58 - the dot product and the cross product
114:00 - there's like the Hadar how do you say
114:02 - that hodaru anyway there's so I don't so
114:06 - I'm kind of reminding you of some things
114:08 - and I I have a bunch of videos on the
114:10 - dot
114:12 - product the dot product I use in videos
114:15 - to look at the angle between two vectors
114:18 - there's a path finding example we really
114:20 - needed the dot product to figure out how
114:22 - to get a moving agent to follow a path
114:26 - and the way the dot product
114:30 - works is we
114:32 - take two vectors and get a single scalar
114:37 - value so you can see these scalar
114:39 - operations a vector by times a single
114:40 - number we get a vector these element
114:43 - wise operations a vector plus a vector
114:45 - we get a vector the dotproduct and the
114:47 - reason why I'm going through this is I'm
114:48 - going to use this again again once I get
114:50 - to Matrix Matrix is where the new stuff
114:53 - is the dot product if I
114:58 - have 2 three uh I just use these same
115:02 - values -1 5 the way that we do this is
115:06 - we take the first value we wait time out
115:10 - for a
115:11 - second I'm so used to looking at this in
115:14 - without actual
115:18 - numbers we take
115:24 - X
115:26 - like let me just look up the formula I'm
115:28 - like oh and I I'm going off this I I'm
115:32 - going off the window so let me correct
115:37 - this um let me try
115:43 - to
115:46 - oops so um just take a break for a
115:48 - second if I I had a b and c d the
115:54 - dotproduct would be a time D + B * C is
116:02 - that right losing my mind
116:05 - here is that right I it's like been such
116:08 - a long day and I'm doing so many
116:14 - things
116:18 - uh
116:21 - no I got it wrong hold
116:24 - on it's F wait I I I wrote this
116:35 - down A1 oh oh no no no no it's the X's
116:40 - plus the
116:41 - Y's a * C plus b * I I over complicated
116:45 - this in my head of course I'm such a
116:48 - sorry about that I know I I I got like
116:50 - confused for a second it's it's simpler
116:51 - than I think yeah yeah I don't know why
116:54 - I I was thinking about how I do yeah
116:57 - like here's all my excuses for I had
116:59 - that wrong but really I just kind of get
117:01 - confused a lot okay because this is none
117:04 - of this is fresh in my mind I'm like
117:06 - just pulling this out of a old body of
117:09 - knowledge I guess and uh it's usually
117:12 - easier just to look stuff up
117:17 - okay okay let's so the way that the dot
117:20 - product works is we actually take the if
117:23 - these were X and Y values we take the
117:25 - x's and multiply them together and the
117:27 - Y's and multiply them together and add
117:29 - them together it's kind of like that
117:31 - weighted sum thing that I was doing
117:33 - earlier in the sort of neural network in
117:34 - the perceptron stuff so I would take 2 *
117:37 - -1 which is -2 plus 3 * 5 which is 15
117:43 - and I would get uh 13 so that is the dot
117:48 - product so this is linear algebra now if
117:51 - I wanted to implement all this stuff in
117:54 - code I could actually come back over
117:56 - here oops hold on don't see that I have
117:59 - the dot product Wikipedia page up or
118:02 - anything um so let me actually let me go
118:04 - to where I want to go
118:08 - um I want to go to the
118:12 - uh github.com
118:21 - I'm going to do is go to Source
118:24 - math P5 do
118:28 - vector and you
118:36 - know
118:38 - um oh my God what
118:41 - a just want to find the dot product
118:48 - function
118:50 - oh this.x yeah yeah yeah sorry sorry
118:52 - okay ah sorry I forgot to switch the
118:54 - camera here I am
119:03 - um
119:05 - okay okay okay let me come
119:10 - back so I could take the next step and I
119:15 - could start to write code for all these
119:17 - operations for vectors but I'm not going
119:19 - to bother with that because ultimately
119:22 - what I need for the neural network
119:24 - library is the Matrix stuff but I
119:26 - starting with the vector stuff because
119:28 - it's all going to translate uh it's it's
119:30 - all going to it's going to be analogous
119:32 - but I should point out that this is all
119:34 - in if you're in p5js for example there's
119:37 - P5 vector. JS the source for the P5 is
119:40 - all on GitHub and you can actually find
119:43 - all of these operations here's the
119:44 - dotproduct function you know if I look
119:46 - for the uh add function here's you know
119:49 - adding two vectors together so you can
119:52 - start to actually go and unpack for
119:53 - these 2D and 3D vectors um how that math
119:56 - works in the source code
119:59 - but now what I want to do is redo this
120:04 - but not for vectors but for
120:09 - matricies so the idea
120:12 - here is what I want to now do is I want
120:16 - to understand well what if I'm storing
120:18 - numbers in a mat and why would I do that
120:20 - well there are so many reasons pixels
120:23 - live in a matrix data in a spreadsheet
120:26 - is in a matrix the weights of
120:28 - Connections in a neural networks can be
120:30 - in a neural network can be stored in a
120:31 - matrix so there are so many scenarios in
120:34 - programming where the numbers that we're
120:36 - working with are stored in a matrix and
120:38 - we could think of that like a
120:39 - two-dimensional array um that we want to
120:42 - perform these kind of mathematical
120:43 - operations very very often
120:46 - so what is a matrix a matrix instead of
120:49 - a a linear list of
120:51 - values is a two-dimensional grid of
120:54 - values and I could think of it like this
120:56 - A B C D E F and this would be a 2 by
121:05 - three Matrix typically we refer to
121:08 - matrix by the number of rows and the
121:10 - number of columns two rows three
121:15 - columns so in that sense we can redo
121:19 - all of these mathematical
121:23 - operations with a matrix
121:27 - so let's do these one at a time and then
121:30 - also write the code actually
121:34 - yeah anybody have a sense of like how
121:36 - long this particular video starting from
121:40 - the linear algebra stuff has been does
121:42 - anybody note the time where I started
121:45 - trying to decide like maybe do I want to
121:51 - keep going and do the code also in this
121:53 - video or maybe I should just
121:55 - describe describe actually describe
121:58 - these maybe I should come back and do
122:00 - this one in a separate video yeah yeah
122:03 - yeah let me see if anybody in the chat
122:05 - has told me about how long they think
122:06 - we've been going six
122:10 - hours it's no no not the whole live
122:13 - stream just since I
122:15 - started writing linear algebra up here
122:19 - so not all the other stuff I
122:23 - did 1 by two ah K weekman is here to
122:27 - correct everything shouldn't be a 1 by
122:30 - two a vector is a uh a vector is a one
122:34 - by is a 2 by
122:38 - one typically is usually how this camera
122:41 - went off um 15
122:44 - minutes around 30 minutes okay that's
122:46 - reasonable I'm going to keep going then
122:50 - um
122:53 - okay
122:57 - okay I lost my train of
123:01 - thought
123:02 - so so let's look at these kinds of
123:05 - mathematical operations now with a
123:08 - matrix so I could do a scalar and this
123:10 - should be an a I don't know scalar
123:12 - operation so let's say I have the
123:14 - Matrix
123:15 - 2 uh 3 -4
123:19 - uh
123:21 - n and if I were to multiply that by the
123:24 - number
123:26 - two an scalar operation would just
123:28 - double all of these values so this would
123:30 - give me then the Matrix 4
123:34 - 68 18 okay so let's actually let's pause
123:38 - for a second I'm not really going to
123:40 - pause and let's before we get to these
123:42 - other operations let's start to write
123:44 - some code okay so what I want to do is
123:48 - have a live library that allows me to
123:50 - create a matrix of values and then
123:52 - perform a scalar operation let's go
123:54 - write the code for that now I should
123:56 - point
123:57 - out that what I'm doing the nature of
123:59 - what I'm doing is kind of ridiculous
124:01 - because there is uh math. JS this is an
124:07 - extensive math library that includes an
124:09 - entire Matrix
124:11 - implementation there is also GPU
124:14 - dos which is a GPU accelerated
124:17 - JavaScript library for doing Matrix
124:19 - operations and you know uh talk about
124:22 - GPU stuff that a little while later but
124:25 - um there's also I think uh matrix. JS
124:27 - there's P5 as a matrix implementation
124:29 - but um I am going to write my own just
124:34 - to kind of understand how it works and
124:36 - then later as part of this Library I
124:38 - probably want to swap it out to have
124:39 - something more efficient that's going to
124:41 - actually you know opt do these Matrix
124:44 - operations optimally but so let's create
124:46 - a new file I'm going to call it matrix.
124:50 - JS and I'm going to write a Constructor
124:56 - function and I'm going to call that a
124:59 - matrix and the Constructor should get a
125:02 - n a certain amount of rows and
125:05 - columns and I should say this. rows
125:08 - equals
125:09 - rows it's been so long since I typed
125:11 - this dot it feels good this do calls
125:14 - equals columns okay so the idea being
125:18 - that I want to be able to say varm is a
125:21 - new Matrix 3x two something like that
125:24 - right that's the idea here I want to be
125:27 - able to just generate a
125:28 - matrix okay so for example I can do this
125:33 - just here in the console now oh let's
125:36 - actually go to
125:38 - index.html and add in the neural network
125:41 - library and the Matrix library
125:45 - now and I should be able to say varm
125:48 - equals a new Matrix 3 comma 2 and I can
125:52 - see there we go I have a matrix object
125:54 - with three rows and two
125:56 - columns okay
126:00 - now we got to come up with a way of at
126:03 - least initializing the values and this
126:05 - is this is 2x3 and I said 3x two but
126:08 - whatever so let's initialize all the
126:11 - values as zero so how do I do
126:16 - that well ultimately
126:19 - I need to have a variable and maybe I'll
126:22 - just actually call it Matrix I could
126:25 - call it values I don't know what to call
126:26 - it I'm going to call it
126:28 - Matrix equals an
126:31 - array now there are all sorts of
126:35 - sophisticated JavaScript ways you know
126:38 - I'm only ever going to put floating
126:39 - Point numbers in these I can have fixed
126:41 - size to allocate the memory in some
126:43 - optimal way but I'm just going to live
126:46 - in the
126:47 - breeze po this in the most kind easiest
126:50 - loosest friendliest way and then we can
126:52 - always come back and optimize to use
126:54 - some more efficient and optimal data
126:56 - structures later so what do I want to do
126:58 - I first want to have um a certain amount
127:04 - of uh
127:11 - columns time out for a
127:14 - second uh I want to see how did I do
127:17 - this I'm just curious in the library
127:18 - that I
127:20 - made
127:21 - here I just want to look at my
127:24 - implementation no I did Rose yeah
127:27 - because I did it Rose so
127:30 - interesting because we typically this is
127:33 - that back to that is rows first or
127:35 - columns first o o the bane of my
127:39 - existence I got to just go back to like
127:41 - generative art coding
127:45 - challenges I is row we are triggered
127:47 - yeah no kidding don't worry I'm now
127:49 - correcting
127:52 - that
127:56 - okay so what I first want to do again
128:00 - the traditional way to think
128:02 - about a matrix is rows by columns so I'm
128:06 - going to start with a loop through the
128:08 - number of
128:11 - rows and I'm going to say every
128:15 - single um row
128:21 - is also an
128:24 - array and then I am going to Loop
128:28 - through all of the
128:31 - columns and I have a j here an I here by
128:34 - accident and say then
128:38 - every single row column location is a
128:43 - value and let's just initialize them all
128:46 - at zero whoops
128:49 - so this is me now making a matrix of
128:53 - values everything with zero so let's go
128:56 - back to the browser oops let me mat you
129:00 - a little edit point there because I want
129:02 - it to be on this
129:05 - page let's go back to the
129:08 - browser and let's refresh the page and
129:12 - create that Matrix again and I should
129:15 - now see Matrix has three three rows and
129:19 - two columns and then it has an
129:22 - array each one of these rows has two
129:25 - values 0 Z 0 0 0
129:28 - 0 so this is now we can see the data is
129:32 - actually stored in there so I've got the
129:35 - beginnings of a matrix Library nothing
129:38 - about this is optimal or efficient but I
129:40 - have a library an object that stores the
129:43 - number of rows and the number of columns
129:44 - and creates a two-dimensional array fill
129:47 - the zeros
129:49 - okay so now what I'm going to do so we
129:53 - kind of now we have the ability of a
129:55 - library to create this Matrix the next
129:58 - thing that I want to do is add a
130:00 - function that performs a scalar
130:03 - operation so for example let's add a
130:06 - function that's called multiply which is
130:09 - the wording of this is a little bit
130:11 - tricky because ultimately Vector matrix
130:14 - multiplication can mean a lot of
130:16 - different things but just for right now
130:18 - I'm going to write a function matrix.
130:21 - prototype that's part of the Matrix
130:24 - object all Matrix objects I'm going to
130:25 - call it I guess I could call it scale
130:27 - let's just call it scale for right now
130:30 - um equals a function that's going to
130:32 - receive a single value n and what do I
130:37 - want to do I want to I'm going to do
130:39 - this a lot Loop through every single
130:44 - row Loop through every single column
130:49 - colum and say this.
130:54 - Matrix i j times
130:59 - equals that
131:01 - value let's call I'm going to call this
131:04 - multiply and then I'm going to add
131:06 - quickly add another one for another
131:07 - scalar operation called
131:11 - add and I'm going to uh say plus equals
131:16 - so again this is this idea I've written
131:18 - two functions these are scalar functions
131:21 - I just want to take a single value and
131:23 - multiply every value in the matrix by
131:25 - that value or I want to take a single
131:27 - value and add it to every single value
131:29 - in The Matrix that's what these two
131:30 - functions can do so let's now come back
131:33 - here once again oh I've got a syntax
131:36 - error I guess I have an extra Clos curly
131:40 - bracket so I'm going to create that 3x2
131:44 - Matrix again I'm going to say add five
131:48 - now let's look at it and I should see
131:51 - the values in it should be all fives
131:54 - right now again we're not really seeing
131:55 - the Nuance of this because there're not
131:57 - different values but it started as zeros
131:59 - and then I added fives to it and now I
132:01 - could say m. multiply
132:05 - -3 oops oh I called it
132:09 - multiply and if I look at M again now
132:12 - and I start to look at those values we
132:14 - can see all the values are -5
132:19 - so what do I have so far I have a simple
132:23 - Matrix implementation that allows me to
132:27 - initialize a grid of numbers by rows and
132:30 - columns and perform scalar operations I
132:33 - can multiply or I can um
132:38 - add so I'm going to pause here and in
132:41 - the next video I'm going to do
132:44 - elementwise
132:46 - operations and then we're going to start
132:48 - to look at other Vector multiplication
132:51 - which is really no longer the dot
132:52 - product but we I'll talk about sorry
132:54 - matrix multiplication so I'm going to
132:56 - kind of break these out into separate
132:57 - videos and I'm going to show you some
132:59 - interesting things about building a
133:00 - JavaScript library where I can actually
133:03 - determine what's coming in I can reuse
133:06 - the
133:07 - multiply uh and the add function um to
133:10 - determine am I adding a scaler or am I
133:12 - adding a whole other Matrix so I'm going
133:14 - to get to that in the next video okay
133:16 - thanks o I am running out of steam
133:24 - here oh
133:28 - boy this is how I feel right
133:33 - now where am I
133:36 - TimeWise oh two hours and 15 minutes
133:39 - toxic desire asked I'm sorry how long
133:43 - are we going to be here I wanted to at
133:45 - least get to the end of this
133:49 - uh but
133:52 - um can I
133:55 - yeah but it is it really worth the
133:58 - stress rebuilding existing libraries
133:59 - from scratch I don't
134:02 - know I feel like people are interested
134:04 - in learning this stuff and seeing the
134:06 - process of making it but um it's a good
134:11 - question I am more generally of the I I
134:14 - I suffer from the when I do video
134:15 - tutorials that I have kind of the
134:17 - theoretical infinite
134:19 - time and so I just like well I might as
134:21 - well fill in as many details as I can
134:23 - whereas if I were doing uh teaching a
134:25 - course and it just meets like a couple
134:27 - hours each week and I want to get to
134:28 - Applications I would kind of talk
134:30 - through the stuff in more generalities
134:32 - and show the library and that kind of
134:34 - thing so um but but uh fortunately a lot
134:39 - of people in the chat are enjoying this
134:41 - so much coffee shot
134:44 - um coffee shot for me yeah okay well is
134:47 - it's not just a matter of me running out
134:49 - of steam I uh have a endof year school
134:53 - picnic to attend I have no idea if it's
134:55 - does anybody know if it's raining
134:56 - outside in New York City right now
134:58 - because that picnic might not be I can
134:59 - actually look at
135:00 - my let's see uh I can try to look at the
135:03 - I mean I basically like a room with no
135:07 - windows uh I could try to look at the
135:10 - weather on my
135:12 - phone it doesn't say that it's raining
135:14 - right now it just has a 50% chance so I
135:16 - think that picnic is on
135:18 - yeah this is going to be a series I I
135:19 - wanted to I feel like I wanted to make
135:22 - it to a good end point today which was
135:25 - all the way
135:26 - through element wise and matrix
135:32 - multiplication let me at least let but I
135:34 - but I but I don't know if that's
135:37 - realistic um oh wait wait you can use
135:40 - table array to visualize in console this
135:43 - is a really good
135:45 - tip so are you saying if I do
135:48 - table m.
135:50 - Matrix how do I do
135:56 - that
136:00 - uh I guess I don't know so I if anybody
136:03 - has a tip for me there
136:11 - okay oh I forgot about the what is it
136:14 - called that hadam or hadam hadamard
136:18 - why is it called
136:23 - that sure product Oh French ma
136:27 - mathematician jacqu hadamard or German
136:29 - mathematician isai
136:31 - Shore
136:33 - okay these are really easy to
136:39 - do
136:42 - okay uh console. table thank
136:46 - you
136:55 - console. oh did
137:01 - I oh look at that oh my God my whole
137:05 - life has
137:07 - changed
137:10 - wow my whole life has just changed in
137:12 - this
137:13 - moment I did not know you could do that
137:18 - that is
137:22 - amazing I don't
137:25 - [Music]
137:33 - know I feel so
137:36 - happy that's like the greatest thing
137:37 - that happened to me in a long
137:40 - time I'm so tired it takes a lot of
137:43 - energy to do these you know I think also
137:45 - doing this stuff like Friday afternoon
137:46 - after a long busy week um hey let me try
137:50 - to get let me try to get a little bit
137:51 - further
137:55 - along I'm gonna actually open this video
137:58 - with
137:59 - that who who suggested that first I want
138:02 - to like thank the person I don't know
138:03 - who it was yeah Karma
138:07 - points
138:13 - okay
138:16 - okay let's keep
138:18 - going gelito was it gelito okay I think
138:22 - it was galito hopefully I got that
138:26 - right hi this you you don't know what
138:28 - you're about to watch this is ostensibly
138:31 - just like a random video in the middle
138:33 - of a very long series of building a
138:35 - neural network and I'm kind of now doing
138:38 - matrices and I already started it and
138:40 - this is really just a video about doing
138:42 - elementwise operations with a matrix and
138:44 - adding that to the Matrix little Matrix
138:46 - library that I'm building but guess what
138:48 - I'm opening this video with something
138:49 - really exciting that I just learned that
138:51 - I never knew thank you gelito from the
138:54 - YouTube chat for pointing this out but
138:57 - what let let me set the stage of where I
138:59 - am we're building this Matrix Library
139:03 - the idea is to be able to store numbers
139:05 - in a grid and perform different
139:07 - mathematical operations with them and
139:10 - we're going to
139:11 - ultimately use this library to do
139:15 - weighted sums in an neural network
139:18 - and right now I'm about to add an
139:19 - elementwise operation but I just did the
139:23 - basics of creating the Matrix and
139:30 - um the basics of creating the Matrix and
139:33 - multiplying or adding a value to it okay
139:36 - so now let's
139:39 - review I could say varm is a new Matrix
139:44 - and it's a 3X two and then I previously
139:48 - was looking at it like this and kind of
139:50 - going like this and trying to like look
139:51 - at the values in it but I learned that I
139:54 - can say
139:55 - console. table and then path in an array
140:00 - look at this
140:03 - console. path in an
140:10 - [Music]
140:15 - array my whole life is Chang in an
140:17 - instance realizing that now I can have
140:20 - this nice little tabular View and so I
140:22 - can just say uh multiply or I can do add
140:26 - five and then I can look at it again and
140:28 - I can see that there's fives in there
140:30 - and I could try to do other stuff and
140:32 - there's so many things I'm going to need
140:33 - to do to like check if it's working this
140:34 - is going to make it so much better I had
140:37 - to fake my reaction right now because I
140:39 - when I really first learned that I was
140:40 - genuinely oh that I'm still genuinely
140:42 - excited about it okay uh but for
140:44 - everybody watching live had to watch me
140:46 - get excited about it twice apolog for
140:47 - that okay so uh you watching this video
140:50 - right now let's add the next piece so
140:52 - what's interesting here is one of the
140:54 - things I wanted to do right is just say
140:56 - let me for example add this I multiplied
140:58 - but add the number two to each one of
141:01 - these values but what if I had another
141:04 - Matrix you know 3 1 4
141:09 - -3 and what if I wanted to add this
141:13 - Matrix to this Matrix element wise what
141:16 - element wise means is if I have two
141:19 - matrices A B C
141:23 - D and I have e fgh
141:27 - h i get a resulting
141:31 - Matrix that has a +
141:35 - e b +
141:38 - F C +
141:41 - g d +
141:44 - h i just take these two values and add
141:46 - them together these two values add them
141:48 - together these two values add them
141:49 - together these two values add them
141:50 - together now this will only work the way
141:53 - that I've described it to you if these
141:56 - Matrix matrices have the same dimensions
141:59 - the same number of rows and the same
142:01 - number of columns now there is there's
142:02 - something in Python the numpy library
142:05 - which is the core uh you know Matrix
142:07 - math library in Python has I forgot what
142:09 - it's called what's that thing called
142:11 - where it
142:13 - like time out for a second I'm going to
142:15 - edit this video right here and come back
142:16 - with the answer to
142:20 - what is that um what's that thing in
142:22 - Python what's that thing in Python where
142:25 - it um um it like can can actually do
142:29 - element wise with slightly different
142:30 - dimensions where was I reading this um
142:32 - it was actually on the um that blog post
142:35 - deep
142:36 - learning uh linear algebra I really
142:40 - should be thanking this blog post so
142:42 - this is u i got to remember to linear
142:44 - algebra cheat sheet for deep learning by
142:46 - Brendan Fortuner thank you so much this
142:48 - has really helped me um somewhere down
142:50 - towards the bottom
142:53 - here
142:57 - uh
142:59 - um am I losing my mind here
143:03 - broadcasting Elemental are relaxed via
143:05 - mechanism called
143:08 - broadcasting um yeah yeah yeah yeah okay
143:11 - thank you
143:15 - um okay where was I back
143:18 - here
143:21 - okay I'm back it's called broadcasting
143:24 - in numpy so but we're we're going to
143:26 - live in a simpler world where we for
143:28 - this we have to have the dimensions
143:30 - match exactly so what I want to do now
143:32 - is I want to keep those multiply and add
143:36 - functions I want them to be the same
143:38 - function but I want those functions to
143:40 - be able to receive a single number and
143:42 - add that single number to all of the
143:43 - values or receive another m Matrix and
143:48 - add all those Val add the values of of
143:51 - and add those values element wise so
143:53 - let's go back and add this now there's
143:54 - some things that I need to do for
143:56 - example I first why don't I at least uh
143:59 - write a function
144:04 - called uh
144:08 - randomize and what this function will do
144:10 - and you're going to see this and
144:11 - everything is
144:12 - just give each value a random
144:16 - value so I am going to um this I'm going
144:19 - to do something rather silly right now
144:20 - where I'm just going to say math. floor
144:23 - math.random time 10 so I'm not using the
144:27 - p5js random and floor functions writing
144:30 - this library because I want this library
144:32 - to be able to be used outside of the
144:34 - p5js library so I have to actually just
144:36 - use the native JavaScript random and
144:37 - floor functions so I should be able to
144:40 - now oops syntax error line 16 oh this
144:44 - should say equals
144:45 - function
144:47 - I should be able to say
144:50 - uh here's a new Matrix m.
144:54 - randomize and then let's look at its
144:58 - values and you can see there we go 1 1
145:00 - 18 38 1 14 those are random values so
145:03 - now if I were to say m. multiply by
145:08 - two and look at it again we can see
145:11 - there we go 216 616 28 great so now at
145:14 - least we can experiment and use
145:16 - different values
145:17 - now here's the
145:18 - thing look at this
145:21 - function matrix. prototype. add equals
145:24 - function n the argument coming in is n a
145:28 - single value but what if n isn't a
145:31 - single value what if it was actually a
145:34 - matrix so actually what I can do here is
145:37 - say if n is an instance of
145:43 - Matrix let's see is that right instance
145:46 - of of what does that mean I'm trying to
145:48 - determine what the type of n
145:51 - is so I can look at that here right m m
145:56 - instance of Matrix true M instance of
146:02 - what's another JavaScript object array
146:05 - false right so it's an if what I'm
146:09 - basically saying is here the add
146:11 - function receives an argument that
146:13 - argument might be a matrix it might be
146:15 - something else if it is a
146:17 - matrix what I want to do is
146:21 - add all the values elementwise
146:26 - otherwise now I should probably check
146:29 - like is it actually just a single number
146:31 - but I'm kind of going to assume here
146:33 - that there's only two possible ways any
146:35 - reasonable person would call this
146:36 - function either with a matrix or a
146:39 - single number so if it's a matrix add
146:41 - the values element wise every i j should
146:44 - get added to the corresponding i j
146:48 - otherwise um otherwise just add the
146:51 - single value to every single value so
146:53 - let's now see if this element wise
146:57 - works it gives myself some more space
147:00 - here so I'm going to make a
147:03 - matrix that is I'm going to call it
147:08 - M1 and I'm going to say M2 is also a 3x2
147:13 - matrix I'm going to randomize
147:17 - M1 I'm going to
147:19 - randomize M2 then I'm going to say
147:22 - console I'm going to look at them both
147:24 - table
147:25 - M1 oh whoops sorry let me clear
147:29 - this console table M1 do the actual
147:34 - array in there console. table
147:39 - M2 Matrix so we can see here these are
147:43 - my two matrices 6372 2 07 04
147:50 - 3173 let's double the values in M2 just
147:53 - to see that that works or let's add to
147:55 - the value sorry what I did this with ADD
147:57 - right so I'm going to say M2 do add one
148:00 - let's add one to every value in
148:03 - M2 and let
148:05 - me I'm going to make this font a little
148:07 - smaller hopefully you can still see it
148:10 - let's look at
148:11 - M1 and M2 so we can see yep every value
148:15 - in M2 increased by one now if I add M1
148:20 - to M2 I should get a matrix that has
148:24 - 78 right 6 + 1 3 + 5 7 + 4 2 + 2 let's
148:30 - say M1 do add
148:36 - M2 let's do that ah okay ah H cannot
148:41 - read property zero of undefined what did
148:44 - I get wrong matrix. JS 29
148:48 - what's wrong
148:50 - here n oh you know what I
148:54 - forgot probably a lot of you in the chat
148:57 - noticed this or if you watch this you
148:59 - notice this The Matrix object has inside
149:03 - of itself a variable that actually
149:04 - stores the values called Matrix and
149:06 - maybe I should call that something else
149:08 - I'm not so sure about this this has to
149:10 - be n dot matrix right if this is an
149:12 - instance of the Matrix object I want to
149:15 - add this matx matx values to the N
149:18 - Matrix values so unfortunately I'm going
149:21 - to have to redo all
149:26 - this I have one
149:29 - Matrix I have two
149:32 - matrices now I'm going to add one to the
149:36 - second one and then I'm going to
149:40 - add M2 to M1 okay didn't get an
149:44 - error now let's look
149:47 - [Music]
149:51 - oh wait I didn't randomize them they
149:53 - won't have the same
149:56 - values it's going to be
149:59 - zero at least okay give me a second here
150:02 - oh no M2 is going to have one in it so
150:04 - let's uh let's just let's randomize
150:08 - M1 let's randomize
150:12 - M2 let's add one to M2 takes a long time
150:16 - just to like get back to my test should
150:18 - prob just write this code into like a
150:20 - code example it would be much nicer that
150:22 - way uh I'll do that in the next video
150:25 - now let me look at all them uh
150:28 - console. m1m
150:30 - Matrix console. table M two. Matrix
150:35 - okay 1 + 5 is 6 6 + 5 is 11 so let's
150:41 - see what we
150:45 - get
150:47 - M M1 add
150:50 - M2 okay console. table
150:58 - M1 I no no
151:03 - no dot
151:07 - matrix I think that's right if I scroll
151:10 - back up oh I I cleared it I can't scroll
151:12 - back up someone will have to confirm the
151:14 - math but I think we have successfully
151:16 - now written a function into our library
151:18 - that can do either a scalar operation or
151:21 - an element operation element wise
151:23 - operation and it's the same function and
151:25 - if I go back to the library I could do
151:27 - this same thing with multiply however
151:29 - I'm going to leave that I'm going to
151:30 - just do that on my own time I'm going to
151:33 - leave that as an exercise for you so if
151:34 - you're following along and building this
151:36 - library with me now go and write the
151:40 - same code to make multiply work both
151:42 - scalar and element wise and we're the p
151:46 - to resin stall so to speak the thing
151:48 - that's the most important thing that I
151:50 - haven't gotten to is actual matrix
151:54 - multiplication that isn't uh element
151:57 - wise and this by the way this element
151:58 - wise matrix multiplication is referred
152:01 - to is commonly known as the hard Maru
152:08 - no oh no that's that's on Twitter hard
152:11 - on Twitter does amazing work but what's
152:13 - that product had hadamard product
152:17 - let's go to the Wikipedia page so that's
152:20 - what this is called hadamard also is the
152:23 - sure product uh um that's the element
152:26 - wise multiplication but matrix
152:28 - multiplication itself is actually going
152:30 - to work in a completely different way
152:32 - and is be going to become the
152:33 - fundamental piece of how we look at
152:37 - inputs and weights between layers in a
152:40 - neural network and multiply and add all
152:42 - those things
152:45 - together
152:48 - say that again it's going to be the
152:50 - fundamental piece of how we look at
152:53 - inputs and weights and how we multiply
152:56 - those things and add them all together
152:58 - in a neural network so so this is where
153:00 - we're building up to so in the next
153:01 - video I am going to look at uh matrix
153:05 - multiplication the sort of core piece
153:08 - and we're going to put that into our
153:09 - library there's some other things we got
153:10 - to look at transposing a matrix is
153:13 - something we'll need and a few other
153:14 - things too and then we'll be back into
153:16 - the Neal network uh and starting to put
153:18 - those pieces together there's a lot of a
153:20 - lot of elements to this a lot of videos
153:22 - but uh thanks for staying with me in
153:23 - this journey process thing and hopefully
153:25 - I'm doing okay see you
153:29 - soon oh okay uh hadamard thank you
153:34 - forgot to randomize okay um boy
153:37 - everybody is talking about gelito in the
153:40 - chat is that there's an actual person
153:43 - named gelito or did I make that up I'm
153:45 - in the wrong camera
153:47 - all right everybody I'm very sorry that
153:49 - this is as far as I got
153:51 - today
153:52 - um I don't even remember where I started
153:55 - I got to look at my phone I apologize
153:57 - for doing that in the middle of a live
153:58 - stream but I got to get to this picnic
154:01 - oh
154:05 - um um let's see okay hold on I'm send
154:09 - gotta send some text messages here hold
154:11 - on I'll play the this dot S as always I
154:14 - always forget the this dot this dot this
154:16 - this going to do this this to do
154:20 - this going to do this to do
154:25 - [Music]
154:34 - [Music]
154:39 - this this do this do this do I'm going
154:42 - to do this do this do going to this this
154:44 - do this do this do
154:47 - actual
154:55 - jugl I don't want
154:59 - [Music]
155:03 - break never
155:08 - forget okay I'm going to make
155:11 - a I'm going to make a prediction I
155:14 - remember distinctly when I've looked
155:16 - at my live dashboard a while ago at the
155:19 - beginning there were 747 people watching
155:22 - because I remember 7:47 like the like
155:25 - the airplane so I gone off the deep end
155:29 - here into Never Never Land of matrices
155:33 - and linear algebra and all this stuff
155:36 - I've left the core I think the core
155:38 - audience of this YouTube channel
155:40 - behind and uh I gonna guess that there
155:43 - are 300 people watching right now
155:47 - I will
155:49 - look
155:51 - 685 that's kind of amazing um okay uh
155:56 - let me go back let me check the slack
155:58 - Channel all right so everybody um I'm
156:02 - sorry that this is where I have to wrap
156:04 - up um I um this has been a 2our and 36
156:10 - minute live stream uh let me mention to
156:13 - you so what's what's coming
156:16 - um what's coming is next Friday I will
156:19 - be back to continue this so I'm just
156:22 - going to keep going I'm going to do the
156:23 - matrix multiplication I think things
156:25 - will pick up speed at some point then
156:27 - I'm going to put that matrix
156:30 - multiplication into the neural network
156:32 - code I got to do the training and the
156:34 - back propagation and all this stuff o ve
156:37 - but that's going to come next week uh
156:38 - next week I will be at if any the
156:41 - O'Reilly AI conference here in New York
156:44 - so if anybody happens to be watching is
156:45 - going to this conference uh send me a
156:47 - tweet at shiftman so I can say Hai and
156:49 - give say heli which is coding train for
156:53 - hello say hello and um give you a
156:58 - sticker um if you so that's that
157:03 - um I will take a short few minutes to
157:09 - answer a few questions it is so hot in
157:14 - here and and I'll be back next
157:19 - Friday the galito stuff is really kind
157:21 - of unbelievable I I wish I could follow
157:24 - the chat if someone could like summarize
157:26 - that for me like what what happened what
157:28 - went wrong uh and
157:32 - uh gelito if I Google gelito what do I
157:35 - get I really should not oh that is a
157:37 - thing okay I don't want to go any
157:38 - further I don't want to go any further
157:40 - with this what I'm doing okay um oh I'm
157:43 - not giving a talk at the conference
157:46 - I'm just I'm just trying to absorb and
157:48 - and go to some sessions and going to
157:49 - some tutorials and stuff
157:52 - um so um okay so let me see
158:00 - um oh boy there's no way I'm going to be
158:03 - a look at that right s multiply and S
158:05 - add e
158:07 - multiply that would
158:10 - be right okay so gelito is a real
158:13 - YouTube user I thought that maybe I was
158:15 - just like like told a fake name and
158:17 - there was some kind of joke so is so so
158:20 - I'm fine you guys have your own thing I
158:22 - have my thing which was actually
158:24 - thanking a real person whose YouTube
158:27 - name is jolito got
158:29 - it okay
158:31 - okay um let me see if um I'm going to
158:36 - put on this Goodbye
158:39 - song and I will take a few questions uh
158:43 - I'm looking at the slack chat um that's
158:46 - kind of uh if you are want to support
158:48 - what I'm doing patreon.com/crashcourse
158:54 - [Music]
159:15 - terribly mind you uh and then a many
159:19 - years later I ended up at a program
159:21 - called ITP which is where I teach right
159:23 - now and I learned about creative coding
159:26 - and lots of other interactive media
159:28 - stuff there at this
159:32 - [Music]
159:37 - program can I make a first person
159:41 - uh 3D style first person game um that's
159:45 - not something
159:47 - close on my radar right now that'll be
159:49 - interesting to kind of look at systems
159:51 - like that but um uh I do I would like to
159:53 - do more 3D stuff and particularly get
159:55 - more guests for 3D
159:57 - [Music]
160:03 - stuff where am I from I grew up in
160:05 - Baltimore
160:07 - Maryland do you plan on making searching
160:10 - sorting algorithm
160:12 - visualizations interesting uh yeah that
160:14 - would be a great topic boy that would be
160:16 - a really nice topic to do and I could
160:18 - actually probably do those in like quick
160:20 - short
160:20 - [Music]
160:33 - videos where's my coating train
160:36 - [Music]
160:41 - hat it's me your friendly
160:44 - neighborhood train
160:47 - [Music]
160:53 - conductor need the microphone to still
160:58 - be all
161:00 - aboard I really shouldn't have the math.
161:03 - JS in the background for this
161:06 - allo
161:08 - Cho I'm losing my mind can I balance
161:11 - this on my head or my chin oh that hurts
161:13 - my neck no I don't think so
161:17 - um
161:18 - [Music]
161:22 - yeah uh okay yes this is my engineer
161:26 - costume all right uh yes the renaming
161:29 - the functions I did see that
161:33 - um I think I
161:37 - think I don't think there's any
161:39 - questions left CU people are just
161:40 - talking about
161:41 - gelito so I think I'm going to be saying
161:44 - goodbye uh I see some people are typing
161:47 - in
161:48 - the um oh what
161:52 - live yeah having subtraction as its own
161:54 - function hold
161:56 - on so okay so k u there's been a
161:58 - discussion actually in the um in the
162:01 - slack channel in the slack live chat
162:04 - about what operations I have left to do
162:06 - so I need to do the um element wise for
162:10 - multiply to add that in I need to
162:13 - rethink the naming because what should I
162:16 - actually call the matrix multiplication
162:18 - function and you know versus element
162:21 - wise function so I think they need to
162:23 - have name I need to do the matrix
162:24 - multiplication I need to do the
162:26 - transpose um we can actually you can
162:28 - actually see all this stuff um is in the
162:32 - um you because I I did this a month ago
162:35 - or so
162:38 - already um so I can actually just look
162:40 - at that here and I'm kind of doing it
162:41 - from scratch again now so transpose copy
162:45 - add multiply what did I do I guess I
162:49 - called it a map is something we're going
162:50 - to need but I'll add that dot is what I
162:52 - called it is that technically a good
162:54 - thing to call it I'm not so sure so
162:56 - that's what I called it okay so that's
162:58 - yeah so that's pretty much what I have
162:59 - left to do and so one person in the chat
163:01 - asked about well I could have two
163:03 - different functions like s ad and E add
163:08 - like for scaler ad or element wise ad
163:10 - but I like the I mean so that would be a
163:13 - reasonable way but I I do kind of like
163:14 - the solution of reusing the same
163:17 - function and having the function kind of
163:18 - autod detect what's coming
163:21 - in um and so the other question was
163:24 - added was asked well should I have a
163:26 - subtract function and you could make the
163:27 - argument yes to have a subtract function
163:29 - but you could also just use add with a
163:31 - negative value and would have the same
163:33 - result but I I think it could have some
163:35 - utility especially for the element wise
163:37 - subtraction otherwise you have to like
163:39 - multip You' have to First Take a matrix
163:40 - and multiply by negative one and then
163:42 - add it that sort of
163:44 - thing
163:52 - all right thank you everybody for tuning
163:53 - in today uh thank you for bearing with
163:56 - me um I hope to get back to kind
163:59 - of just more creative examples and
164:02 - different generative algorithms and
164:04 - quick quick games and that kind of stuff
164:07 - uh more guests so but I want to try to
164:09 - see if I can get through this neural
164:12 - network stuff and um just make for
164:15 - people who are interested and um uh your
164:18 - feedback and your thoughts are highly
164:21 - appreciated and encouraged uh uh
164:25 - criticism and everything as well okay so
164:28 - um thank you guys I'm going to turn this
164:30 - button off I will be back next Friday
164:32 - and most likely a same sort of timing
164:34 - more a little bit later probably in the
164:37 - 300 p.m. eastern time which is I guess
164:39 - like 7 or 8:00 p.m. Greenwich meantime
164:42 - um okay so um see you all I don't have a
164:46 - great I don't do a great job of having a
164:47 - fixed schedule oh this is this computer
164:49 - is about to die it's not plugged in
164:53 - um this is actually those of you who
164:56 - want a little inside baseball here this
165:00 - computer has green green paper on it so
165:03 - that you don't see it uh but you can see
165:05 - I'm kind of like cut off by it if I walk
165:07 - over this way all right um so um see you
165:10 - all next Friday unless something magical
165:12 - happens I have time to do earlier but
165:14 - hopefully uh in between July 3rd and
165:17 - July 14th I'm going to have uh twice a
165:19 - week live stream so I'll update you
165:21 - about those schedule times and get
165:23 - through all this neural network learning
165:25 - stuff okay um thanks very much and I
165:28 - will see you next Friday
165:33 - goodbye

Cleaned transcript:

hello welcome Good Friday to you afternoon here it is in New York City uh my name is Dan schiffman this is the coding train a YouTube thing that happens every once in a while where I uh come and talk about programming topics and various other types of things and often uh embarrass myself in a variety of other ways all while live streaming from Tish School of the Arts here at new new new New York University uh here in New York City uh it's a very hot day outside it is a cool Breezy 72 degrees Fahrenheit here in this room uh I hope that the microphone is working fine you can hear me okay that this music that the levels with me and the music are quite reasonable because I had a few technical things and change some things around so welcome I um have a few quick announcements to make do I have any announcements I just said that without thinking it through I should plan I've got a plan for these things actually hold on hold on where where where is it ah it's over here oh there is a Discord server somebody made one uh Z I made some notes I made some notes for today because topic that I want to cover today is linear algebra um as it relates to programming and matrices and code here's another book these are really just props these are actual linear algebra textbooks that I used at one point in my life but we're talking about okay let me do the math here divide by 12 carry the four uh I 25 years ago something like that so in any case uh but I'm going to return to that topic today now okay that stuff is going over there so what what oh a couple things I am taking a look at the YouTube chat over there on a screen and I see people saying has he already started hey you're doing linear algebra after calculus La how do you say lolz laws anyway LS lws anyway uh yes I am I don't I'm not unfortunately nothing on this channel has a particular is particularly thought out or planned with any like reasonable logic whatsoever I'm all about trying to figure stuff out pull from here pull from here piece this together piece this together make something creative see if we can get some code up and running compiling and running and doing something interesting so that's kind of my goal here um so I am seeing the YouTube chat over there I am looking also at a slack Channel which if you you would like to join the coding train Patron program making it sound like it's a thing you can go to patreon.com Trin that is a way of a crowdfunding the work that I'm doing here on YouTube uh and I have some benefits uh uh namely a slack Channel that I look at also during the uh during the live sessions okay so um bonjour to uh gaming with Izzy in the chat so it's summertime do I have any Summertime music is this Summertime music I don't know uh so I'm kind of on a little break from my usual schedule and it's actually been a bit more difficult to schedule these live sessions than I had imagined good news is I am here in town in New York City for a stretch of time from now until July 14th I will then be away for a couple weeks so just in case you're planning your summer around my ual which you really should not that's what's happening right now so I am hoping now to take this time from now until July 14th and really cover with some depth how to program a neural network from scratch in JavaScript and perhaps also in Java using processing most of my inspiration is coming from almost yeah I won't be doing this almost all of my inspiration is coming from this wonderful book that I purchased earlier this year called make your own neural network uh by Tariq Rashid uh I think believe who's based in London um this is a step by step with no prior knowledge needed of any maths or any programming actually um to build a neural network program in Python because reasonable people who live reasonable lives program their machine learning stuff in Python but I am not a reasonable person living a reasonable life and I will be doing this at least for now starting to do this in JavaScript and I have to say that the the ultimate goal here I know I'm going to uh end up just repeating myself again in a second is that I oh look it's already even open here I have actually already done this I have there's a GitHub repository uh neural network P5 which is a my watch is beeping at me I'm going to press this um this is a JavaScript library uh with code based on the code from that book uh and it actually has a few uh demos associated with it already I'm going to click on this one this is kind of a classic uh classic machine learning scenario of learning to recognize uh digits and I have a little P5 example where I can uh draw the letter three there which it thinks is the letter letter three the number three the character three the number three thinks it's a five maybe after some more training eventually it will determine that it's a three we'll see um so I have this example I also have uh another example which is uh using the same nural network code oh it's broken I knew it I changed something I got to fix this so I'll will fix that so anyway I've done this already I did this for a course that I taught here at NYU at ITP earlier this spring and now what I would want to do is unpack this process unpack the process that I undertook to make this Library and do it over a series of many videos so on the one hand you should probably just go and enjoy your summer vacation uh go to the beach read a book and uh come back when I finished with this and then kind of get your hands into like the using the neural network library making some creative projects but as an exercise to help myself learn more background and more depth about this topic and for those of you who might be interested I'm going to build all this stuff over the course of a bunch of videos um which I will start today okay so that is my main Spiel now now it's about 230 uh schedule wise oh oh yeah yeah I'm going next week O'Reilly AI Conference next week I will have a limited schedule is this am I ah right here because I will be attending this O'Reilly artificial intelligence conference where hopefully I might learn something that I can take with me and then bring to this YouTube channel but uh so if you if anybody watching will be at this conference please tweet me at shiffman uh so maybe I can say hello uh give you a sticker coding train sticker or a processing sticker or P5 sticker I guess I can try to remember to bring those with me um and say hello to you at this conference I'm particularly excited I'm going to a session on deep reinforcement learning which is really the the kind of a machine learning algorithm that I'm most interested in in terms of how it applies to creative animations and interactive systems so okay uh trua in the chat is asking about the Chrome extension tutorial so unfortunately I did say I so okay let me let me answer this for a second oh look I'm actually whoa that's weird my web browser is on the web page that I was just going to go to but it wasn't before how did it get there anyway I must have clicked on it subconsciously um so this is a syllabus for a course which is um kind of misnamed because it's not a beginner course at all but programming from a toz it was a course from last fall uh and I made a a lot of video tutorials actually if I go to shift.net teing A2Z uh that's not the right page uh oh whoa where is my uh a toz page if I go to here oh just SL a toz just if I just go to shiftman Donnet toz you will find the um whole set of P each with videos and notes on a variety of topics and if I go down here to Chrome extensions here are the examples and a few notes but I didn't actually make any of the videos yet so this was a topic last fall I made videos for this course all fall along if that's an expression to say and I never got to the Chrome extension videos and I kept saying I was going to get to it and then it became kind of like a running joke so I almost feel like I can't ever do it because then we won't have the running joke anymore but I really do need to do it at some point this will happen for sure in the fall because I'll be teaching this course again and want to fill out um stuff and actually I'm going to add some machine learning stuff to this course I want to do word to VC I want to look at uh recurrent neural networks and text generation there's something else some chatbot stuff so that's coming as well in the fall okay so now um I'm looking at the chat to see if anyone's got any questions for me ah okay so because this conference next week um I will not be live again until next Friday I mean that's what I usually do anyway um so today's a little bit of like getting back into the swing of things because I was kind of been my last live stream was a week and a half ago and then I'll be back next Friday and then I've got two weeks in July that I'm here in town with a stretch of a pretty flexible schedule and I hope to get a whole bunch of live streams in then my goal is to have four per month and so uh because I'm going to be away the last two weeks of July that I might do two per week those two weeks that's my kind of goal and so I'm hoping that by July 14th I will have rebuilt everything about this particular neural network library uh in a video on live talking about it figuring it out and taking hopefully your helpful suggestions so let me I the problem is I'm going to repeat a lot of this stuff in a second because I got to do sort of introduce one of the things that I do for those of you who haven't watched live before is the live stream gets edited into a bunch of tutorials videos that then get put in a whole bunch of different playlists depending on what course they go with so a lot of this introductory stuff I might end up repeating again I apologize for that but um now I even forgot what I was gonna say um so let's see here so I think I just want to get started okay so um couple good suggestions from the slack chat patreon group uh mincut writes you should make a calendar with all the conferences you attend a good idea except to be honest with you I go to very very few conferences I would love to travel and go to more things but I just have a lot of work commitments and family commitments so generally I'm just kind of here in New York City but uh but that's a good idea we'll try to kind of keep that up to date or announce if I'm doing any other workshops or things outside conferences outside of the stuff that I usually do here um Cod codo verly asks will there be a separate math video for this topic so this is a good question and I've been struggling with this and this is where I have arrived at at the moment so I am going to do I'm going to build a neural network code and I'm going to use Matrix math to do the weighted sums of all the connections and I will get into the details of what that means so I am going to do some separate videos that cover the Matrix math required so there's I'm going to pick and choose little nugget get from linear algebra and cover those and also write the code to implement those things then at some point and i' probably not today I'm going to get to the part of the neural network thing where you're training it and adjusting the weights with a process called back propagation so we're going to talk about what that process is write all the code for it but I don't think that I'm going to derive the calculus maths that's required for the formulas that are used I'm just going to use those formulas and point people to other references or maybe come later do a followup video to unpack some of that math in a bit more detail so that's my plan and I kind of had a similar plan point of view with the uh linear regression stuff I mean it's actually the same math so um a lot of that will apply but where I try to just sort of like make a video and use the formulas and then come back and make a followup video for those who would be interested in that so that's my plan uh is this going to be posted later oh yeah and Carl's so let me make some notes Here I forgot so I also the issue is as much as I want to okay so hold on uh linear regression uh batch gradi I forgot some things that I forgot to do are uh and Carl so couple somebody asked in the chat will this be posted later yes everything is posted later there'll even be edited versions of this later if you don't want to watch all the long winded stuff um batch gradient descent is something that I forgot to mention and cover as part of the linear regression tutorials and then there were some other additional things to the perceptron example that I wanted to do and I'm tempted to come back to the and everyone's asking for the f I did this to myself I wanted to do the fidget spinner uh code and I never did that and uh when I first mentioned it everybody had the what you know the reaction that one might expect which was like groan major groan I'm really could do a fidget spinner simulation video but then I got kind of excited about it and I talked about it and then some other people got excited about it and then I never did it so um and yes I'm definitely going to be doing neuroevolution is my happy place so one of the things we'll see as we build a neural network is that the whole point of one of the one of the uh one of the key pieces of working with neural networks is figuring out how to get the optimal weights of all these connections that are in that Network and so there are these methodologies for doing that uh there's the standard methodology which involves this gradient Ascent like back propagation algorithm tweaking according to an error to minimize the error changing weights to minimize the error but there is another method which involves an evolutionary approach to evolve the optimal weights and that's the method I love because it's just very more it's very intuitive and it's uh can understand it it relates to my genetic algorithm tutorials it doesn't involve all that sort of like calculus stuff so I'm I'm definitely planning to get to that uh yes I good so at least I'm getting some negative feedback about the fidget spinner which will keep me from doing that oh yeah and and and I have no I'm I I will sell out as soon as I possibly can right if I'm making the fidget spinner video will suddenly get me you know 100 million subscribers which it won't obviously uh and then I'll just do it but uh you know I have no qualms selling my soul yeah unfortunately if you're watching this live you cannot watch it at two 2x speed you have to watch it at 2x speed later but I could try to pretend that it's going at 2x speed because F to talk about that that's actually more like 4X speed uh what about polom regression oh my goodness o o there's too many things I didn't get to that either can I in one YouTube channel cover the entire known universe of knowledge no I cannot to be honest a lot I mean my the my goal with the channel is to create friendly and accessible tutorials for people who want to be creative and experiment with code in an informal way and so on the one hand you know I just don't it's not the goal of my channel to cover every sort of Statistics mathematics computer science algorithm from scratch in some sense that I'm going to show to you so you can memorize it and and redo it again again later I mean I'm not saying that doesn't have value and there aren't other channels that have that approach but my Approach is quite informal and loose and so Pol nomal aggression I don't know that I need that so much for where we're going because ultimately I think the creative examples will come from using neural networks to uh to um experiment with user interaction and other types of um interactive and animated possibilities okay all right I'm I'm reading this chat and it's just lovely to read all these nice messages um and people are having a discussion and I can't keep up with it at all uh I forgot the momentum so many things I forgot I can't do it all I can't trigonometry yes to mges I will be oh yeah I could do a fidget oh whoops I hit the Bell by accident I could do fidget spinner with querian if you're new here you know that if I ever say the word Quan I have to run away instantly it's terrifying they're terrifying all right I think what I'm going to do because I feel like I'm I want to get some momentum here and start the um neural network stuff I'm just walking over here to reset this camera um I think maybe I should hold off should I do so many requests I'm trying to decide I'll do the straw pole thing that's usually how it works best uh I kind of know the answer to this but let me just confirm here pick one finish off some details and things for perceptron uh actually uh just get started on a neural network so this is the multilayered perceptron you guys probably can't see this but I will so here we go um create poll so here is the straw pole address W 6 Z D3 e should I finish off some of these things that we didn't get to and like visualize and add some other stuff or let's just get started this is going to be a long process it won't finish won't be finished today but get started building this JavaScript neural network library this is why people can watch the edited verion or the archive because you could just skip over this part where I wait for the scrub Ball results and oh wait is this link not working oh oh no what happened what happened that was such a fail all right did let's try this again everybody is are people able to get the straw pole it's working okay what's funny is somebody in the chat said uh you know clicks link arrives at live stream sees guy dancing Le so this is good because if if somebody just joined this by accident and sort of sees the dancing isn't interested it's probably good to it's not going to like the rest of the video all right I think I know what this is going to be wow all right so what we're going to do today is just get started and we are going to build a neural network library in JavaScript and we should just all be aware that I'm not really an expert in this and I'm just doing this to learn it myself and give it a try and you're going to encourage me or not or whatever but that but I but I feel you out there I feel you watching how many people are watching 747 people that's terrifying it's absolutely terrifying okay all right so let's get started I really need some different music um okay so how to begin so I need a code editor okay um e okay I think I am ready now oh wait actually I want to oops there's something I want to do which is open this up okay so I should map this out a little bit um Okay so let's think about where where will these videos live this is a little tricky because there are two places right now I am here basically in uh week four of the intelligence and learning class but this these videos actually go along with chapter 10 of the nature of code book and there goes the camera so that music that I'm playing is by um Adam Blau who is a uh film and television composer based in Los Angeles uh he has a podcast that I'd love to plug uh called rarified air which is really terrific and I believe that is called Tory the dog said it to me as just some like extra uh extra spare music that I could use and Adam is the composer of the soon to be released coding Train theme song I know you can't wait for that um okay so I think this is not a coding challenge I will do some coding challenges with neural networks but I think this um and it would be interesting to try to do like a hey just program neural network in 20 minutes kind of thing but um I want to spend some time doing stuff step by step in a series of videos about building this neural network library so the thing that I want to start with is is yeah okay I have these slides from the perceptron boy I really um could have thought about this in a different way let me do an introduction video uh I'm really trying to I'm really trying to figure this out but I you know what I am not going to worry about it I am going to I am going to consider this to be a I got it I got it everybody we're good I figured it out this is a new playlist 10 new number 10 in uh nature of code uh it is going to look if I go to my channel and go to uh here so basically you can see here what this is going to be is 10 neural networks 10 neural networks and what I'm going to do now is make an intro video that sets the stage for what's coming and the perceptron videos will actually follow that intro video and then the neural network I mean the multilayered proc ctron Library building will follow that so I'm I'm making videos out of order so first I'm just going to do probably just like a couple minute introduction uh to the idea okay yeah I uh all right so I'm looking at the chat I'm seeing lots of stuff no you did not miss the train whistle there's the train whistle okay um all right here we go I need a moment to meditate shoulders hurts I need to stretch ah oh I think there's an issue where the way I have this set up the camera is actually right there but I'm kind of like standing over here so I have this like awkward neck craning thing to look at the the camera but we're just going to go all right so what time is it now oh my God 30 minutes in and I haven't really started doing anything yet but I was thinking this through and talking to you and you were watching apparently let's get let's get a move on hello welcome to a video that at this present time doesn't exist but when you are watching this video right there to the right of nine genetic algorithms will be the number 10 and we'll stay next to that neural networks so I am embarking on a journey uh to learn about neural networks what they are how you program them what are there what's kind of like math and stuff you need to know to make them work and then what kinds of creative and experimental outcomes can you have now it should be said that uh there are lots and lots of machine learning libraries out there uh there are lots of examples and resources for doing this uh I I want to hold on I'm still talking I don't know where where did I put that book ah it's over here I want to reference uh this book U make your own neural network by Tariq Rasheed which I used to develop a lot of the materials that I will be presenting to you and developing uh during this series of of videos and I should also say that um you know and this book this book has um all sorts as how to program your own network from scratch and without even knowing anything about programming in Python because as I might have said earlier today any reasonable person would start and make a video tutorial series about programming a neural network from scratch in Python but I don't really I'm not very reasonable or logical and I do make just constantly make mistakes with everything and here's a mistake that I'm going to make I'm going to do all this in JavaScript um and the reason for doing that is to have everything run in the browser on the web and also really for me to learn about how to do this stuff so I am going to build a set I'm going to build a a simple neural network library in JavaScript not to make something efficient not to make something robust but to learn about the mechanics of how all this stuff works because ultimately and you might want to just enjoy your summer or maybe you're watching this during the winter and get outside and do something else and not watch these videos and just skip ahead to like later cuz I'm going to do bunch of coding challenges and projects that involve that neural network library and also other neural network libraries namely something called tensorflow uh in future videos but these first videos of building the neural network library which I will do over a series uh or really just for me to to learn how to do this stuff and if you want to watch and sort of give me some good feedback and see if you can follow along and and and improve on what I'm doing and uh help me with it that would be great so okay what what hello am I just rambling here I am but why are we here so I'm going to go uh so the nature of code materials and this video sits in the nature of code playlist is all about looking at things in nature in our physical world and trying to unpack those things and understand the algorithms behind those things and see if we can convert those things those algorithms into code oh this is like going it's like Auto playing how do I stop that uh uh and uh turning those things into software to make animations and creative projects why not look at something really interesting in nature the brain so this is kind of a loose diagram of this idea of an actual biological neural network apparently I have one here struggling quite a bit these days uh where there are these entities called neurons and and they're connected to other neurons and there's a lot of you know mystery to this and a lot of recent research to your Neuroscience what I am focused on in this series of videos is what kinds of computational systems can be built inspired by the actual biological neural neural network biological brain and made into something called an artificial neural network and what kinds of applications and outcomes can we we can we create so what is the analog what is the neuron in by in our code how does it receive inputs how does it generate outputs so my brain does this it receives all these inputs you know from light in the room that travel through my retina and into the brain and the signals then produce outputs and allow me to catch something or read some words what how can that how can that process be simulated in software and what type types of outcomes can we generate and the very first thing that I'm going to do is look at the simplest possible neural network a net it's not even a network at all it has one neuron a processor neuron that receives two inputs and generates an output and that's called a perceptron so if you look at the next videos in this playlist I am going to build in processing uh perceptron example just to show the mechanics of how this works and to produce a sort of trivial example that doesn't necessarily have a very powerful outcome but gives us CU if we can build and understand how this single neuron receives inputs processes those and generates an output then we can start to connect those together to create more sophisticated systems that can begin to generate outputs based on more more complex outputs based on more complex inputs and this is kind of a fit sits right there in the world of machine learning this idea of I have some data that I want to make sense of that data is an input to a machine learning algorithm that algorithm is going to generate an an output so maybe the data is an image the machine learning algorithm is going to guess is it a cat or a dog or maybe that input is the specs of a house you know square footage uh number of bedrooms etc etc and this machine Learning System is going to generate an output a predicted price so there are lots of other machine learning algorithms besides just neural networkbased ones and I do have another video series that covers some of those but ultimately I want to learn how a neural network works so I can place it right there and start to make sense of data generate outputs from it so if you want to continue along the way this video series will work first there'll be a perceptron which is uh this thing then I'm going to talk after the perceptron done I'm going to talk about what the limitations of the perceptrons are and why it is that if we could can create a multilayered perceptron meaning many of these perceptrons all connected to each other what we can start to build uh and create afterwards so uh that's my rambling introduction that apparently you just watched because I mean maybe I'm that no one will ever watch this but but but probably somebody will and um I'll see you follow along I look forward to your feedback uh I hope this goes okay that's my that's a pretty good goal just okay is fine and I'll see you in these future videos as I keep going thanks for watching all right you know that's me that's this is my style uh okay so um looking at the chat so now fortunately so fortunately for all of you uh you don't have to now sit through me doing the perceptron because that is already oops H that oh my channel that is already um done and that is here well that's the followup but if I keep going this way we can see here it is the perceptron so there are two videos about the perceptron at some point I might come back and fit some little pieces in there um people are giving me great suggestions like yeah in the chat but now uh I code please everybody is very very focused on me getting to the point of things I I I I would tend to agree okay so the next thing that I want to do now is I want to talk about all right this is very hard this is difficult here all right so I want to talk about first hold on uh trying to see the thing that I need to reference I probably should have followed this along more closely I'm sorry I'm I'm looking at my notes Here Yeah okay um okay so there's this and there's also I have these notes okay okay okay so the next thing is to discuss why the perceptron okay all right let's see how this marker does so first of all um oh this is such a good marker that makes me so happy can you read that I'm going to go walk over to my monitor and see if I can even read that looks like the focus is kind of reasonable it's a little bit small um yasu or yasus to everybody from Greece in the chat um um I guess I just need to write bigger but uh that's good okay people are saying it's okay okay so where's my Eraser all right okay here we are uh I'm going to begin now a a little bit large would be fine okay font size plus three okay okay uh here we go hi again so maybe you just watched my previous videos about uh coding a perceptron and now I want to ask the question why not just stop here so okay so we had this like very simple scenario right where where we have a canvas and it has a whole bunch of points in that Canvas OR cartisian plane whatever we want to call it and we drew a line in between and we were trying to classify some points that are on one side of the line and some other points that are on another side of the line so that was a scenario where we had the single perceptron the sort of like processing unit we can call it the neuron or the processor and it received inputs it had like x0 and X1 were like the X and Y coordinates of the point it also had this thing called a bias and then it generated an output each one of these inputs was connected to the processor with a weight you know weight one weight two or whatever weight weight weight and the processor creates a weighted sum of all the inputs multiplied by the weights that weighted sum is passed through an activation fun function to generate the output so why isn't this good enough now let's first think about what what's so what's the limit here so the idea is that what if I want any number of inputs to generate any number of outputs that's the essence of what I want to do in a lot of different machine learning applications let's take a very classic classification uh algorithm which is to say okay well what if I have a handwritten digit like the number eight and I have all of the pixels of this digit and I want those to be the inputs to this perceptron and I want the output to tell me uh a set of probabilities as to which digit it is so the output should look something like you know there's a 0.1 chance it's a zero there's a point 2 chance it's a one there's a0 one chance it's a two 0 3 4 5 6 7 oh and is like a 9 chance it's an eight and a 0.05 chance it's a uh 10 and I don't think I got those to add up to one but you get the idea so the idea here is that we want to be able to have some type of processing unit that can take an arbitrary amount of inputs like maybe this is a 28 by 28 pixel image so there's 784 grayscale values and instead those are coming into the processor which is weighted and summ and all this stuff and we get an output that has some arbitrary amounts of probabilities to help us guess eight that this is an eight this model why couldn't I just have a whole bunch more inputs and then a whole bunch more outputs but still have one single processing unit and the reason why I can't is uh stems from an article I don't know sorry a book that was published in 1969 by Marvin Minsky and Seymour paper paper called perceptrons you know AI uh luminaries here I don't know if I click on this link where it goes to Amazon maybe oh MIT press so in this book Minsky and paper T edit time out for a second how do you pronounce Seymour paper's last name is it payal paper paper paper do I pronounce the T can somebody help me with this somebody said qu Tor the dog while I wait for somebody to help me with pronunciation paper with a t or no T paper with it's pronounced it's pronounced gift that was a good one oh come on I don't want daily hacks I'm not interested in your deal I want to know how to pronounce paper nobody will tell nobody will tell me I'm going to just mispronounce it I'm looking for at least the slack Channel I can rely on you patrons somebody must know look at this I really got the chat going crazy here PayPal yeah my PayPal is Daniel shiftman Donnet that's also of my email my Bitcoin address is uh nobody has any idea that's fine I will just suffer through the comments that I will get in the video at a later time check this video oh my God boy that the chat is going crazy I've never seen anything like it the t is pronounced thank you a AA you have come through for me thank you very much I are totally forgot what I was talking about okay in the book perceptron Marvin Minsky and see more paper point out that a simple perceptron the thing that I built in the previous two videos can only solve linearly separable problems so what does that mean anyway why should you care about that so let's think about this this over here is a linearly separable problem meaning I need to classify this stuff and if I were to visualize all that stuff I can draw a line in between this part of the DAT this stuff that's to this class and this stuff that's with this class the stuff itself is separable by a line in three dimensions I could put a plane and that would be linearly separable because I can kind of divide the space in half and and and and understand it that way the problem is most interesting problems are not linearly separable you know there might be some data which clusters all here in the center that is of one class but anything outside of it is of another class and I can't draw one line to separate that stuff and you might be even thinking but that's you know still so much you could do so much with linearly separable stuff well here I'm going to show you right now a particular problem I'm looking for an erase sir I'm walking around like a crazy person I'm going to show you a particular problem called xor so let's erase all this and making the case for why we need to go a step further and start to whoops I'm making the case for why we need to go a step further oh I just had an idea I'll come back to later I'm making the case for why we need to go to a go go a step further and make something called a multilayered perceptron and I'm going to lay out that case for you right now so you might be familiar you might remember me from my videos on condition conditional statements and Boolean Expressions well in those videos I talked about operations like and and or which in computer programming syntax are often written you know double Amper sand or two pip the idea being that if I were to make a truth table true true false false so what I'm doing now is I'm showing you a truth table I have two elements I'm saying what if I say a and the B so if a is true whoa whoa whoa this makes no sense what I've drawn here because I am losing my brain cells slowly over time with every passing day it's very sad true false true false true and true yields true if I am hungry and I am thirsty I shall go and have lunch right true and true yields true true and false is false false and true is false false and false is false right if I have a Boolean Expression A and B I need both of those things to be true in order for me to get true interestingly enough this is a linearly separable problem I can draw a line right here and true is on one side and false is on the other side this means I could train a perceptron to receive two inputs true and false you know that are true or false oh I'm like way off the screen here that's not a screen that's a hold on how's that let's let let me go backwards for a second and redo this part this means this is I'm fing I'm going to get to the coding I swear I I don't know if I'm going to get to it today to be perfectly honest with you but I I've got all this stuff that I want to talk through I don't know if it's a good idea I'm giving it a try this means this is a this is a linearly separable problem which means I could create a perceptron that perceptron is going to have two inputs there are going to be Boolean values true or false true or false and I could train this perceptron to give me an output which if two truths come in I should get a true if one false at a true comes in I should get a false two falses come in I should get a false great or I could do the same thing what does or change into if I'm going to do or let me erase this dotted line and or now ah all of these become true because with an or operation A or B I only need one of these to be true in order to get true but if both are false I get false and guess what still a linearly separable problem and is linearly separable or is linearly separable we could have a perceptron learn to do both of those things now hold on a second there is another Boolean operator which you may you might not have heard of until this video which would be really kind of exciting for me it would make me very happy if somebody watching this has never heard of this before it is called xor can you see what I'm writing here X or the X stands for or exclusive exclusive it's exclusive or which means it's only true if one is true and one is false it's not true both are false this or that both of those things are false I'm still false but if both are true it's also false so this is exclusive or let me erase all this exclusive or means if one is one is true and one is false it's true if one is true is one is false is true if both are true it's false if both are false it's false this is exclusive or a very simple Boolean operation however I I triple dog dare with the cherry on top you to draw a single line through here to divide the falses and the truths I cannot I can draw if this is not a linearly separable problem this is the point of all this like rambling I could draw two lines one here and now I have all the trues in here and the false is outside of there this means a single perceptron the simplest cannot solve cannot solve the a simple operation like this so this is what Minsky and paper talked about in the book perceptrons well this is like an interesting idea conceptually it kind of seems very exciting but if it can't solve xor what are we supposed to do with this the answer to this is and you might have already thought of this yourself it's not too but believe I I I kind of missed a little piece of my diagram here right let's say this is a perceptron that knows how to solve and and this is a perceptron that knows how to solve or what if I took those same inputs and sent them into both and then I got the output here so this output would give me the result of and and this output would give me the result of or well what is xor really xor is actually or but not and right so if I can solve something and is linearly separable not and is also linearly separable so what I want then is for both of these outputs actually to go into another perceptron that would then be and so if this perceptron can solve not and and this perum can solve or and those output can come into here then this would be the result of both or is true and not and is true which is actually this these are the only two things where or is true but not and not but not and and so the idea here is that more complex problems that are not linearly separable can be solved by linking multiple perceptrons together and this is the idea of a multilayered perceptron we have multiple layers and this is still a very simple diagram you could think of this almost as like if you were designing a circuit right if you decide whether electricity should flow and this were like a um these were switches you know how could you get a bunch of how could you have an LED turn on with exclusive or you would actually wire the circuit basically in exactly this way um so this is the idea here so what I am would like to do in the next so at some point I would like to make a video where I actually just kind of build take that previous percepton example and just take it a few steps farther to do exactly this but what I'm going to do actually in the next videos is diagram out this structure of a multilayered perceptron how the inputs how the outputs work how the feed forward algorithm Works where the inputs come in get multiplied by weights get summed together and generate an output and build a simple JavaScript library that has all the pieces of that neural network system in it um okay so I hope that this video kind of gives you a nice followup from the perceptron and a sense of why this is important important and I'm not sure if I'm done yet I'm going to go check the live chat and see if there any questions or important things that I missed and then this video will be over time out okay all right what did I do okay I'm oh I'm I gotta turn this camera on okay so now is a brief moment where uh you can um point out things that I got horribly wrong that I should make sure I correct or ask some followup questions that might be important I see there's some chat going on here but I I think that has uh nothing to do with what I um so are we good are we good did that make sense did I get that about right I don't know if K weekman is watching I don't think so that's my uh sanity check um yeah I didn't mention hidden I probably should have uh mentioned that um um like this is technically the this is technically the hidden layer these are the inputs this diagram is terrible because this should be down here so maybe what I'm going to do is I'm going to connect you know that's what I'm actually going to do is maybe I'll do a quick redrawing of this so that it matches what people are used to um uh doing great oh thank you got to know doing great uh Place circuit scramble explains this thing perfectly that's interesting um Tom this is a coding train tshirt that you can get at coding train. storenvy.com uh proof for xor is not and and or that's right right when I said it is both or and not and that's correct okay so I think I'm good I'm not seeing anyone telling me something that I've done um I'm not seeing anything uh uh that I've done that's sort of like horribly out of about my watch not the shirt this is a Fitbit I don't know it's the Fitbit whichever one looks like this um okay so let me do let me get a couple things um I would like to show so for those of you interested by the way this is a viewer um K weekman uh GitHub is K weekman who created a learning xor with a neural net example um and it visualizes it and kind of visualizes the connections and that sort of thing so I'm going to show this uh I'm going to read I'm going to fix the diagram here and mention hidden and then that's not a Wikipedia page by the way um I this is this page here is part of my um uh core syllabus it has a bunch of references here Daniel it be nice to know if the nonlinearity of the problem affects the number of neurons to use in the hidden layer so yeah so we'll get to this I mean yes yes and no it'd be really nice if I could just like open a door over here and like an actual machine learning expert come out and answer some of these harder questions but um uh let's let's let me get a little further and sort of come back to that I mean the complexity of the the number of hidden layers affects the number of parameters that you get to tweak which is uh the level of complexity that you can kind of apply to a problem so certainly um Okay so um okay I'm back so there was one question which is important like oh what I heard some somebody in the chat asked what about the hidden layer and so this is jumping ahead a little bit because I'm going to get to this in more detail in the next video there's a the way that I drew this diagram is pretty awkward let me try to fix this up for a second imagine there were two inputs and I actually Drew those as if they were neurons and I know I'm out of the frame but I'm still here um and these inputs were connected to each of these perceptrons each was connected and each was weighted so this is actually what's now known as a thre layer Network there is the input layer this is the hidden layer and the reason why it's okay well actually let me go this is the output layer right that's obvious right this is the input those are the inputs the TRU and the falses this is the output lay layer that should give us a a result are we still true or are we false um and then the hidden layer are the neurons that sit in between the inputs and the outputs and they're called hidden because as a kind of user of the system we don't necessarily see them a user of the system is feeding in data and looking at the output the hidden layer in a sense is where the magic happens the hidden layer is what allows one to get around this sort of linearly separable question so the more hidden layers the more neurons the more amount of complexity in a way that the system the more weights the more parameters that need to be tweaked and we'll see that as we start to build the neural network library the way that I want that library to be set up I want to say I want to make a network with 10 inputs three outputs one hidden layer with 15 like hidden neurons something like that but there could be multiple hidden layers and eventually as I get further and further down this road if I keep going we'll see that there are all sorts of other styles of how the network can be configured and set up and whether the output feeds back into the input that's something called a recurrent network convolutional network is if some this kind of like um set of image processing operations almost happens early on before as one of the layers so there's a lot of stuff in the grand scheme of things to get to but this is the fundamental building blocks uh so okay so I'm in the next video I'm going to start building the library and to be to be honest I think what I need to do no no no no yeah I'm next video I'm going to set up the basic skeleton of the neural network library and look at all the pieces that we need and then I'm G to have to keep going and look at some uh Matrix math that's going to be fun okay uh see you soon goodbye I'm g walk over here all right okay it is now 330 and I've been streaming for an hour um I'm checking out the chat which is nice to see okay so I think it's time to write some actual code um okay so what I want to do is oh I forgot to show this yeah I'll come back to it because maybe um I maybe maybe there'll be a time to come back to it um but I do want to build I want to do this kind of as a coding challenge but it'll make more sense to do it once I have the library I think actually in a way um okay so what I want to do now is go to here okay I can close this I can close this is there anything yeah okay um and here whoops uh let's get rid of this of this okay so I think now I'm ready to start writing the code and um and let me just look I'm going to look look at how I set it up here to do it the same way so I want to create this is how I'm going to set it up I want to create a Network that has a certain number of inputs so I'm going to have the library only create a thre layer Network and so it's made with a certain number of inputs a certain number of outputs and a certain number of hidden neurons okay and the viewers are dropping I'm not surprised you know this I I'm kind of you know I I feel like what I'm doing here is getting away from is moving a bit a skew or a scance a side adjacent to the sort of core Mission and and kind of stuff that I like to do on my channel but trying to take this twoe period in the summer to see if I can blast through this material and and hopefully get a sense of it and and begin to do some more interesting stuff applying it later um Alexander right maybe you should have an outline for every Live code session that would allow for a more smoother presentation duly noted duly noted there's nothing smooth about this whatsoever okay let me keep going I have a mental outline but I I'm I'm just so scatterbrained okay um so this is what we're going to do I'm not going to get very far with this but that's okay all right so let me see here okay um all right so here we go I'm going to move on now welcome back I'm going to actually write some code in this video um not that much so what I'm doing now welcome I made a few introductory videos covered some background uh about uh neural networks and why they exist and where I'm trying to go with this and in this video I'm going to actually begin to write the code for a simple JavaScript neural network library now I've actually already done this it exists here at this repository uh GitHub shiftman neuralnetwork P5 I'm designing this library to be used with a set of p5js examples with a a library a JavaScript library called P5 although ultimately this Library stands alone on itself by itself you don't have to use it with just P5 so before I can write the code let me come over here to the Whiteboard and this is where I last left off talking about how the general structure of a neural network library Works uh a neural network system works and so what I need to do here is when in the code I create a neural network I want to create three things I want to create an input layer I want to create a hidden layer and I want to create an output layer so when I create a new the way I want to design this library is I want to say new neural network and I want to give it can you see this I think you can I want to give it three arguments the number of input neurons let's just use the word neurons the number of hidden neurons and the number of output neurons so I'm doing something which I typically don't do which is usually I like to have a specific problem that I'm trying to solve and like write the code for that problem and in here the problem that I want to solve is I want to make a generic kind of useful library that could be used in a bunch of different contexts so I don't know what those numbers are going to be uh I don't know what the data is I'm just kind of working on the skeleton the structure of the library before I start to apply it to things so let's just make up some numbers let's say there's going to be three input neurons four hidden neurons and two output neurons what this means now is in a feed forward neural network there are three inputs we could imagine again I'm using the kind of classic example of guessing the price of a house this could be number of bedrooms number of bathrooms square footage so those are like three parameters of a house these will connect to 1 2 3 four hidden neurons so this is the input layer this is the hidden layer and then I'm kind of running out of space here there will be two outputs and then this is the output layer so this is the configuration the idea of a uh and so what I'm building here is what's known as this is a multilayered perceptron these are individual perceptron units essentially that are have multiple layers and it also is an another important term that I want to add here is I want to create a fully connected Network and now there are variations to this that we might see in future examples but the idea of a fully connected network is that every input is connected to every hidden every hidden is connected to every output but I so I can draw those connections and it's not so many that I you know if I were doing some kind of post production I would speed this up but I'm going to just draw this web of all these connections so every input is connected to every hidden and every hidden is connected to every output whoa oops ah ah I messed this up but I'll get it eventually there we go right so you can see that every every node is connected to every node in the next layer so the idea is that those three inputs come in the data feeds forward and those two outputs come out so this is the structure now we have to get into a lot of details here well how do I keep track of all of these connections how do I actually do the loops to like do all the sums of everything and how do I read the outputs I'm going to get to all that but this is the overall structure so let's go back to the code and now let's actually try to like write a little bit of this Library very very little so where am I going here okay so this is my code there's nothing yet I'm going to create a new file and I'm going to call this nn. JS so this is now going to be my so here's the thing ultimately I want this to be like a proper JavaScript library but ultimately what is a JavaScript library but a file with some JavaScript in it so I might later as this gets more sophisticated optimize it and use some sort of like build process or break it up into multiple files but right now I just want to kind of like get the pieces going so I am going to I'm also going to use es5 syntax this is the trajectory that I've been on soon in future videos I'm going to start adopting some es6 syntax But ultimately maybe this Library I'll do a followup and come back and kind of I'm going to do a lot of things maybe not in the most optimal or efficient way but hopefully in the most easy to understand and fall away so I want to create a Constructor function called ner Network okay and I should also mention again while we're here that I built this Library already and when I built it I based just about everything out of this book called make your own neural network by Tariq Rasheed and so while I'm doing this now kind of a bit more on the Fly I'm sure everything that's in my brain ultimately came from here and probably some other sources too okay so what do I want to do the core thing that I want to do is I want to create the N neural network with a certain number of input nodes number Hidden number of output so I'm going to add those as arguments here I'm going to say um number of input number of hidden number of output I'm going to create a neural network with three arguments and then I'm going to say uh input nodes I think I'm going to be longwinded about this equals number of input and so I'm going to create three uh hidden nodes is this argument and uh output nodes is this argument is that an O yes it is okay so this is we've actually written some code the idea being that what I want to do is say things like VAR brain and brain is a new neural network that has three inputs with three hidden and one output right this is the idea so I need to figure out what shape and shape using the word shape very specifically does the data come in that's how many input nodes I want what shape is the output that I want am I looking for a single output am I trying to look for a range of outputs that's how many outputs I want then how many hidden neurons do I want well that's kind of an open question well maybe I want as many as I could possibly fit in was the program running reasonably fast but it sort of depends on the complexity of the problem and we'll come back to that later and I should also note that I this is a simp oversimplification of how neural network architectures can be this is by definition a three layer Network and this library is only going to allow for a three layer Network an input a single hidden and a sing and an output but as something you might think about for the future how would you write the code to have multiple hidden layers because a lot of neural networkbased Learning Systems need multiple hidden layers to be able to perform optimally but for now I'm going to keep things very simple okay I'm gonna pause for a second because I'm kind of thinking um I'm I'm seeing some interesting questions um and um so there's an interesting discussion going on is what's the relation number of input hidden and output so the the it's not that the hidden is arbitrary but it it can be kind of any number but you know if you're just going to have one it's not going to do going to work very well and so you know one thing that you might do is just kind of like well however many inputs let's just make the hidden layer the same number that could be like a good starting point I would say um Okay so the thing that we're going to need very quickly and I need to refill my water here is we're going to need the linear algebra stuff yeah um so I'm trying to I think I think I'll take this video a little bit further and then I have to stop and explain so here's the thing let me talk this through without this being part of the official tutorials ultimately what I'm going to do with each one of the actually why do I why do I even bother saying like let me talk let me just talk this through and this will be part of the video I don't know where I came over here I don't know where I was uh oh linear algebra yeah I don't even want to use that word but it's it's what we're going to do is actually quite it's not so hard to figure out okay what is the next step written we did write some code thankfully wrote some code now we got to stop again the next step is the feed forward process okay the feed forward process the way that the feed forward process works is that we receive these inputs oh there's so much to do so many pieces to this puzzle I'm excited to get through it all though so let's just say for example we're looking at this hidden neuron do you remember from the perceptron video videos maybe you didn't watch those so let's talk about it the idea is that we need to do something called a weighted sum so let's pretend this is the house prediction thing and this was the number of bedrooms three this is the number of bathrooms you know this is the number the square feet so each one of these connections right the data is going to flow in the data comes in here the number three comes in here and then look at this there's four outgoing connections each one of those connections has a weight to it now ultimately the whole point of doing this learning neural network based Learning System is we want to tweak those weights we want to train the brain train the neural network to have optimal weights to get good results results that make sense and that training process is something that I'm going to get to I don't how many videos down the road from now but not too far away these weight weights will have typically to start one way of thinking about them is they're going to just have random values between 1 and one and there's a wide variety of techniques and strategies for initializing random weights or not just random to a neural network but for right now good way for us to get started they all have random weights so even though I'm looking at each one of these flowing out slightly better way for me to look at this with you is actually just look at all the connections flowing in so this particular hidden neuron has three connections flowing in a three a and the input values are 3 2 and 1,000 each one of those has a weight so let's pretend this is like 0.5 U let's say this is like uh .5 and this particular weight is one so I'm making using very very simple numbers the idea is that each hidden neuron does something called a weighted sum so so it takes the input multiplied by the weight and adds that to the other input multiplied by the weight and adds that to the other input multiply by the weight so we could actually do this 3 * .5 is 1.5 plus 2 * .5 is 1 plus 1,00 * 1 is plus 1,000 so this value now is a 100.5 now we can see there's a huge flaw here which is that the fact that square footage is kind of a big number and number of bedrooms and number of bathrooms are small numbers means this kind of way of summing it is going to produce some like odd results this the square footage is going to be weighted so heavily just by the fact that it's bigger numbers so a lot of time in working with a machine learning or neural network based system we need to do some type of cleaning or normalizing of the data and we might do something where we you know we sample this down so that they you know we actually do the number of bedrooms between 0 and five as a value between 0 and one and number of bathrooms always as a value between 0 one and square footage this would actually turn into 0.1 like because the range is between Z and 10,000 square feet or something so we would do some kind of normalization of these values but this is again further down the road when we start to apply the library in an actual project once this weighted sum is complete the result of that weighted sum gets sent out through the outgoing connections but it gets passed through an activation function so I'm going to come back to the activation function this is something we did with a perceptron and that's going to be a separate video where we look at different activation functions and how they work right now I want to focus on this weighted sum so I could keep going here I could create some type of array of I could create an object that's like each one of these nodes or neurons is an object object then I could iterate over I could have connection objects so there's a bunch of different approaches I could take but the classic and standard approach is actually to look at storing all of these weighted Connections in something called a matrix which is really just like a spreadsheet a grid of numbers looking at the inputs as an array and doing some type of math that basically takes take that array of inputs multiply it by that Matrix of weights and generate the outputs of this hidden layer so this is so give me a second here I'm going to erase I'm gon to I'm gonna I'm going to make the case for this with a simpler scenario okay so um so I'm going to erase this this can get edited out and uh let's see here you know a lot of times when I us to do these videos I would do the videos at the end of the day after I taught the material with in a classroom earlier in the day and I would sort of figure out in my head how to condense things down maybe through the magic of editing things will get condensed down but okay so um I'm gonna let's see let me just let's go back to this um simpler diagram okay okay okay I'm back I erased what I had and I drew I'm drawing a simpler diagram here now so let's look at this diagram which just has fewer connections it's going to be easier for us to unpack so we can think of these inputs as x0 and X1 let's not even worry about the output right now these are the inputs this is the hidden layer right hidden layer so let's think about this and and actually let me change these numbers to X1 and X2 you know sometimes I like to count from zero sometimes I like to count from one I don't know why but I I I feel like in this case let's let's call it one and two so this is really like hidden one hidden two so each one of these connections right each one of these weights you could say here this is a weight that goes from one to one this is a weight that goes from one to two this is a weight that goes from two to one and this is a weight right here that goes from two to two so notice how there are two inputs two hidden neurons four weights in other words the weights and I'm going to draw I'm going to kind of use start to use Matrix not a little bit the weights can be expressed like this 1 1 1 2 2 1 2 two row I want those to be row column hold on let me think about this for a second time out time out time out this second uh because maybe my numbering should actually be row row yeah row I did it right I did it right never mind okay okay so this is a way of expressing the weights and a way of expressing the inputs I could write it like this X1 X2 okay so I'm making the case that I have two inputs and I have four weights and I could write it out like a matrix of numbers a 2X two Matrix and this is essentially a 2X one Matrix whenever I'm going to get more into matrices in the next video or am I in that video already I don't even remember I don't know where I am in my world but uh typically when we talk about a matrix a grid of numbers we reference it rows by columns 2 by two 2 by one okay so let me just show you something remember this we need a weighted sum here this weighted sum is X1 times weight 1 one plus X2 times weight 21 okay that's the weighted sum for this neuron or node the weighted sum for this neuron or node is X1 times the weight from 1 to 2 and X2 times the weight of 2 to two plus X2 times the weight of 2 to two it so happens I could take these two results I could call this like uh H1 and call this H2 and I could say let me actually say I could I could basically say this times this equals H1 H2 so this is the actual math the way that we described it look at both inputs coming in multiplied by their weights and summed look at both inputs coming in multiply by their weights and summed these are the the it written out but it so just happens that this exact math writing it like this and producing this outcome is exactly the math that is part of a field of study called linear algebra linear algebra involves manipulating vectors and matrices a vector being a onedimensional list of values a matrix being a twodimensional list of values the inputs are always always onedimensional the outputs are always onedimensional the weights are always can always be expressed as twod dimensionals it's every input connected to every hidden you can think of it very much like pixels every row and every column so this is where I need to stop and what I want to do is do a few videos that cover this notation and math with a bit more detail writing a little JavaScript simple JavaScript Matrix library and ultimately once we done that we can come back here and see how if we have that Library written we can then use it to do the math between the inputs and the hidden and the hiddens to the output and ultimately later we're also going to go backwards through the network to tweak values and and train it and that's we're also going to use the same Matrix math so this is why we need or why we don't need because we could kind of do it without it but while it's useful to work with this idea of linear algebra and I should note once again that if we were doing this in something like python using a library like something called nump we would get all of this stuff for free and there are JavaScript Matrix libraries and might but I'm going to kind of unpack some of this and and write a lot of the code from scratch just to have a sense of how it's working because why not okay I'll see you in the next video where I look at this a bit more in a bit more detail thanks very much okay uh how we doing here what did I get wrong uh oh I'm wrong looks like I got something wrong shoot X1 * w11 X2 * w21 X1 * this plus this X2 * this plus this I don't see it being wrong let me go back and look at the chat I'm pretty sure I got it right oh whoops sorry I didn't change the camera I'm pretty sure I got it right the way that u i mean I'm going to cover this in the next video but the way that you do this is essentially uh these two um sorry I I lost my train of thought uh I basically take I do the dot product of like this vector and this Vector so that would be w11 * X1 plus W12 * X2 w11 * X1 w21 * X2 and then I would do that for this and this W2 uh oh this is wrong maybe wait wa wait wait when I looked at it a second ago it was right oh maybe I've just did I write this in the wrong way and this should have been 21 up here ah shoot yeah this should be shoot I miswrote I wrote this wrong I knew this would happen uh and then this should be um one two I have to go back we have to go back we have to go back we have to go back hold let me check the chat uh uh no did I have it right the first time Matrix is correct okay hold on I I I I I did it to myself hold on was it correct oh why I'm look at the chat here formula is wrong the problem is you guys are behind me so you guys are uh yep the par below does not match the Matrix so here's the thing this is correct the way that I've let me go and um let me look at a a let me look at the way I mean I I I these definitely don't match these match and the way I thought of it writing it this match matches but I I'm pretty sure that this should actually be down here so let me look at how it's the notation is used in um in this book um yeah yeah yeah yeah yeah yeah I just shoot I messed up so I don't know if you guys can see this but um Tariq this book uses very similar notation and so I have to I'm trying to decide whether so I just I just want to um make this one two and this 2 one whether I want to I hate the fact that this is going to be wrong throughout the whole video and then I'm going to correct it at the end but I think if I go back I don't I don't want to like redo this whole section I wish I could watch this back right now actually I can sort of by stopping because like where where could where could I erase the board from yeah I'll just mention that it's wrong and I'll fix it in the next video it wasn't so long in the video lost is right um no nothing the this um I I just ended up notating it in this is one I I was actually asking myself this question I decided that I was right and then I realized I was wrong because I'm thinking of this row column row column really column row column row um this should all right so just trying to decide if I want to like rerecord this tutorial with the correct value Val is there or just issue a correction at the end of a video I need to do another straw poll I I I just I wish I had a sense of where where I was where because I think I'll just issue a correction um oh yeah okay good idea so um Topher in the chat is just suggesting you know what I can do is when to make the edited version of this we can also I mean this is more work for MATA but um I know this is technically possible um we could maybe just put an actual annotation in the video like an overlay so I'm just going to issue a correction right now okay um so hopefully as you were watching the video you saw a little annotation um this is actually incorrect I mean everything about this math is correct this matches this right the weighted sum is X1 * weight one from 1 to 1 X2 * weight from 2 to 1 but actually the notation that I the way I wrote this Matrix as we go as I go into the next video where I actually look at how the Matrix math works this really should be written as one two and this should really be written as 21 the reason why that is is this should be X1 * w11 plus X2 * w21 which is written right here so that Matrix mathod I'm going to go in more detail in the next video we take this row and multiply it by this column and this row and multiply it by this column and you can see that's what these two things are okay so thanks for bearing with me I there's a lot of little pieces but I am going to get back into the code so in the next video I'm not very confident about the order I'm doing all this in but it's just the way that I'm going to choose to build it and so in the ne again I'm saying this again the next video I'm going to look at the Matrix math again and then write a generic Library that does that math and then come back and put it back into the neural network itself okay so see you in the next video thanks um okay so um I think I got it is that correction good put some thumbs up the notation on The Matrix is always because there's so much going so much chatter going on I can't tell what people are referring to what time is it an hour and 40 minutes um formula is wrong not the Matrix can we agree that what I have here right now is correct before I move on I want to make sure there's nothing else that's wrong here to me I feel I'm going to look at the my uh tariq's book again good job everything is good some people say no I forgot your notation is bad no it's correct all right let me look here boy I'm getting a lot of mixed signals here let me look and see how Tariq notates it um so I'm going to look I'm looking at this input one time weight 1 one plus input 2 time weight 2 1 input 1 * weight one 2 plus input 2 * weight 2 two okay and then the matrices are 1 1 2 1 1 2 22 1 2 so I now have just confirmed that my notation matches exactly this notation the one thing that I've done that's kind of incorrect is swapping the order of these I really should be saying weight one * X1 plus weight 1 1 * X1 weight 2 1 * X2 I kind of wrote it out this way but that's a minor detail because it's the math is equivalent but maybe the standard would be to write this multiplied by this so I'm feeling based on the fact that I have now looked at this particular book and see that what I did matches I'm feeling more confident about it oh somebody is saying Matrix notation is different for Europe and America oh whoa oh no don't tell me I got to use the metric Matrix system this is so interesting is that really true let's Google this Matrix notation Europe versus America Two dimensions are read by 2 by3 that's how I do it two rows and three columns one one one two 1 1 three ah so yeah so the awkward thing here is so this is the unfortunate awkward thing these are mapped to this but you wouldn't normally I think notate that's why I wrote it the other way the first time around you wouldn't normally notate it that way because you would sort of do in this notation you would say x by y or column by row but these aren't actual these aren't actually the column and row numbers they're the weight mapping so I I think I'm a little bit off in the weeds here um I'm going to leave this as is and when I do the generic linear algebra videos I'll try to use this the more St it won't be tied to the neural network stuff I can kind of think about it in a different way I'm going to go read the chat again one more time yeah the don't differ so far classic R someone is fooling with me all that also could be possible that people are trolling me because yeah it's the transpose uh all right okay I'm seeing some very interesting discussion but I am going to to um move on um and I guess I could add a little addendum there and maybe I'll just for the sake of argument uh I'm going to just record that saying that in one sentence to tack it on could be edited in I guess somehow if necessary let me um one more thing I should point out about now that I fixed this notation that's a bit awkward here is that there are different styles for notating Matrix you know and again I'm going to use the convention of rows by columns so this is a 2X two row column but you would notice here that typically these would then be row column row column so this would be one one one one2 that's why I Had It reversed the first place but because I'm taking it from these weights these aren't actually the row column numbers oh my head hurts already um but it's all going to work out it's all going to be fine uh just bear with me um um in this sort of notation snafu okay the important thing is that this actually matches the way I want to describe the weights and the and those weighted sums and I will we're going to double back and everything should hopefully as I get through more explanation stuff will start to make more sense okay um I don't think that should be edited in I don't think yes that should not be added let's just leave it as is people in the comments will complain but I'm I I had the whatever I'm GNA keep going yes I know about transposing matricies use please use square brackets um okay oh my God the chat is going crazy oh my God good job uh all right all right all right okay okay okay I'm I'm I'm I'm let me just say for the sake of argument I'm sorry I'm going to move on [Laughter] okay this is now going to be erased all right now now we come to the point where I actually took some notes you might be shocked to hear this okay do I have the stamina and energy to keep going oh hold on hold on Jedi was writing something in the chat I can't you guys are write the hidden nodes are the rows and the input nodes are the columns exactly everyone knows how to troll me now I'm GNA keep going oh the fact that I wasn't using square brackets don't you worry I'll use square brackets now so I should use I should do square brackets okay hello here I am so I'm trying I'm moving along here through this journey of trying to program this neural network library again I might suggest skip ahead find some videos where I'm just using the library but I'm I'm doing this I'm exposing this process of a person struggling to make sense of the world but for this video I did actually make some notes um and I want to reference actually a there's a nice um a medium post about kind of what linear algebra you need to know for for deep learning that I will uh show you on my laptop in a second and and link to it in the video where I read that post this morning and helped me kind of gather my thoughts for this particular set of video so what I've done so far is I've established that we need this idea of linear algebra in order to perform some of the math in the neural network library that I'm building so what I want to do is take a break let me start over let me not start over that's fine it's fine so what I want to do is take a break from the neural network stuff itself and look at the linear algebra stuff in a vacuum and yes finally actually hopefully write some code because I want to talk through the math and implement the math in code in a generic way and then apply that to the neural network we're g to get through this everybody okay so what are the core so I have I I have some props wait time out so unnecessary but since I brought these up here I found my old linear algebra textbooks from 20 some plus 25 some amount of years ago so I brought these as props I was reading them this morning but here's the thing this is not a course in linear algebra there's actually some great linear algebra videos on KH Academy um probably there are some other ones out there I will link to additional resources in the description of this video I want to do is cover the aspects of linear algebra that are necessary or relevant to the neural network stuff um um and kind of leave out the rest so I'm going to give that an attempt and see how it goes and write code along with it and you'll let me know how that goes okay so here's the thing there are two key Concepts in linear algebra there's the idea of a vector and there's the idea of a matrix now a vector is actually something that I've spent a lot of time in previous videos in this nature of code playlist talking about the idea of a twodimensional Vector an entity magnitude and direction in a twodimensional space we use this Vector for forces and velocity and all sorts of physics simulation all sorts of stuff but ultimately this Vector is just an X and A Y that two dimensional Vector from and of course could be a z if it were a threedimensional Vector for all the computer graphics and animation physics simulation stuff I've done in previous videos we could think though about we can we can consider a vector as just an N dimensional list of values and I could make the notation like this and I could say x0 X1 X2 X3 X4 X5 so this is a fivedimensional vector there you go so this is the idea of a vector now one thing I should note is that you will see a variety of different kinds of notation um you might see them am I still you might see things written like this XY you might see it written like this XY different textbooks different styles I'm going to use this square bracket notation for the algorithms and examples I'm going to demonstrate in this video and in future videos okay so that's the idea of a vector now if you also recall we can do math with vectors and there are a few different kinds of operations there's the idea of a scalar operation like let's say I have the vector two 2 three and I multiply that by the number two I could take this scalar value the single value and multiply it by each component of the vector and I would now have 4 six there also are operations that are referred to as element wise this is the kind of operation that I did over and over again if I had a velocity vector and a position Vector so if I had a position Vector that was something like you know 2 three and then I had a velocity Vector that was you know 15 I could add elementwise add these together so the first element add to the this the the first elements get added together so 2 +1 is 1 the second two elements get added together 3 + 5 is 8 so these are element wise operations now in addition to that there is also something reference referred to as Vector multiplication and there's like the dot product and the cross product there's like the Hadar how do you say that hodaru anyway there's so I don't so I'm kind of reminding you of some things and I I have a bunch of videos on the dot product the dot product I use in videos to look at the angle between two vectors there's a path finding example we really needed the dot product to figure out how to get a moving agent to follow a path and the way the dot product works is we take two vectors and get a single scalar value so you can see these scalar operations a vector by times a single number we get a vector these element wise operations a vector plus a vector we get a vector the dotproduct and the reason why I'm going through this is I'm going to use this again again once I get to Matrix Matrix is where the new stuff is the dot product if I have 2 three uh I just use these same values 1 5 the way that we do this is we take the first value we wait time out for a second I'm so used to looking at this in without actual numbers we take X like let me just look up the formula I'm like oh and I I'm going off this I I'm going off the window so let me correct this um let me try to oops so um just take a break for a second if I I had a b and c d the dotproduct would be a time D + B * C is that right losing my mind here is that right I it's like been such a long day and I'm doing so many things uh no I got it wrong hold on it's F wait I I I wrote this down A1 oh oh no no no no it's the X's plus the Y's a * C plus b * I I over complicated this in my head of course I'm such a sorry about that I know I I I got like confused for a second it's it's simpler than I think yeah yeah I don't know why I I was thinking about how I do yeah like here's all my excuses for I had that wrong but really I just kind of get confused a lot okay because this is none of this is fresh in my mind I'm like just pulling this out of a old body of knowledge I guess and uh it's usually easier just to look stuff up okay okay let's so the way that the dot product works is we actually take the if these were X and Y values we take the x's and multiply them together and the Y's and multiply them together and add them together it's kind of like that weighted sum thing that I was doing earlier in the sort of neural network in the perceptron stuff so I would take 2 * 1 which is 2 plus 3 * 5 which is 15 and I would get uh 13 so that is the dot product so this is linear algebra now if I wanted to implement all this stuff in code I could actually come back over here oops hold on don't see that I have the dot product Wikipedia page up or anything um so let me actually let me go to where I want to go um I want to go to the uh github.com I'm going to do is go to Source math P5 do vector and you know um oh my God what a just want to find the dot product function oh this.x yeah yeah yeah sorry sorry okay ah sorry I forgot to switch the camera here I am um okay okay okay let me come back so I could take the next step and I could start to write code for all these operations for vectors but I'm not going to bother with that because ultimately what I need for the neural network library is the Matrix stuff but I starting with the vector stuff because it's all going to translate uh it's it's all going to it's going to be analogous but I should point out that this is all in if you're in p5js for example there's P5 vector. JS the source for the P5 is all on GitHub and you can actually find all of these operations here's the dotproduct function you know if I look for the uh add function here's you know adding two vectors together so you can start to actually go and unpack for these 2D and 3D vectors um how that math works in the source code but now what I want to do is redo this but not for vectors but for matricies so the idea here is what I want to now do is I want to understand well what if I'm storing numbers in a mat and why would I do that well there are so many reasons pixels live in a matrix data in a spreadsheet is in a matrix the weights of Connections in a neural networks can be in a neural network can be stored in a matrix so there are so many scenarios in programming where the numbers that we're working with are stored in a matrix and we could think of that like a twodimensional array um that we want to perform these kind of mathematical operations very very often so what is a matrix a matrix instead of a a linear list of values is a twodimensional grid of values and I could think of it like this A B C D E F and this would be a 2 by three Matrix typically we refer to matrix by the number of rows and the number of columns two rows three columns so in that sense we can redo all of these mathematical operations with a matrix so let's do these one at a time and then also write the code actually yeah anybody have a sense of like how long this particular video starting from the linear algebra stuff has been does anybody note the time where I started trying to decide like maybe do I want to keep going and do the code also in this video or maybe I should just describe describe actually describe these maybe I should come back and do this one in a separate video yeah yeah yeah let me see if anybody in the chat has told me about how long they think we've been going six hours it's no no not the whole live stream just since I started writing linear algebra up here so not all the other stuff I did 1 by two ah K weekman is here to correct everything shouldn't be a 1 by two a vector is a uh a vector is a one by is a 2 by one typically is usually how this camera went off um 15 minutes around 30 minutes okay that's reasonable I'm going to keep going then um okay okay I lost my train of thought so so let's look at these kinds of mathematical operations now with a matrix so I could do a scalar and this should be an a I don't know scalar operation so let's say I have the Matrix 2 uh 3 4 uh n and if I were to multiply that by the number two an scalar operation would just double all of these values so this would give me then the Matrix 4 68 18 okay so let's actually let's pause for a second I'm not really going to pause and let's before we get to these other operations let's start to write some code okay so what I want to do is have a live library that allows me to create a matrix of values and then perform a scalar operation let's go write the code for that now I should point out that what I'm doing the nature of what I'm doing is kind of ridiculous because there is uh math. JS this is an extensive math library that includes an entire Matrix implementation there is also GPU dos which is a GPU accelerated JavaScript library for doing Matrix operations and you know uh talk about GPU stuff that a little while later but um there's also I think uh matrix. JS there's P5 as a matrix implementation but um I am going to write my own just to kind of understand how it works and then later as part of this Library I probably want to swap it out to have something more efficient that's going to actually you know opt do these Matrix operations optimally but so let's create a new file I'm going to call it matrix. JS and I'm going to write a Constructor function and I'm going to call that a matrix and the Constructor should get a n a certain amount of rows and columns and I should say this. rows equals rows it's been so long since I typed this dot it feels good this do calls equals columns okay so the idea being that I want to be able to say varm is a new Matrix 3x two something like that right that's the idea here I want to be able to just generate a matrix okay so for example I can do this just here in the console now oh let's actually go to index.html and add in the neural network library and the Matrix library now and I should be able to say varm equals a new Matrix 3 comma 2 and I can see there we go I have a matrix object with three rows and two columns okay now we got to come up with a way of at least initializing the values and this is this is 2x3 and I said 3x two but whatever so let's initialize all the values as zero so how do I do that well ultimately I need to have a variable and maybe I'll just actually call it Matrix I could call it values I don't know what to call it I'm going to call it Matrix equals an array now there are all sorts of sophisticated JavaScript ways you know I'm only ever going to put floating Point numbers in these I can have fixed size to allocate the memory in some optimal way but I'm just going to live in the breeze po this in the most kind easiest loosest friendliest way and then we can always come back and optimize to use some more efficient and optimal data structures later so what do I want to do I first want to have um a certain amount of uh columns time out for a second uh I want to see how did I do this I'm just curious in the library that I made here I just want to look at my implementation no I did Rose yeah because I did it Rose so interesting because we typically this is that back to that is rows first or columns first o o the bane of my existence I got to just go back to like generative art coding challenges I is row we are triggered yeah no kidding don't worry I'm now correcting that okay so what I first want to do again the traditional way to think about a matrix is rows by columns so I'm going to start with a loop through the number of rows and I'm going to say every single um row is also an array and then I am going to Loop through all of the columns and I have a j here an I here by accident and say then every single row column location is a value and let's just initialize them all at zero whoops so this is me now making a matrix of values everything with zero so let's go back to the browser oops let me mat you a little edit point there because I want it to be on this page let's go back to the browser and let's refresh the page and create that Matrix again and I should now see Matrix has three three rows and two columns and then it has an array each one of these rows has two values 0 Z 0 0 0 0 so this is now we can see the data is actually stored in there so I've got the beginnings of a matrix Library nothing about this is optimal or efficient but I have a library an object that stores the number of rows and the number of columns and creates a twodimensional array fill the zeros okay so now what I'm going to do so we kind of now we have the ability of a library to create this Matrix the next thing that I want to do is add a function that performs a scalar operation so for example let's add a function that's called multiply which is the wording of this is a little bit tricky because ultimately Vector matrix multiplication can mean a lot of different things but just for right now I'm going to write a function matrix. prototype that's part of the Matrix object all Matrix objects I'm going to call it I guess I could call it scale let's just call it scale for right now um equals a function that's going to receive a single value n and what do I want to do I want to I'm going to do this a lot Loop through every single row Loop through every single column colum and say this. Matrix i j times equals that value let's call I'm going to call this multiply and then I'm going to add quickly add another one for another scalar operation called add and I'm going to uh say plus equals so again this is this idea I've written two functions these are scalar functions I just want to take a single value and multiply every value in the matrix by that value or I want to take a single value and add it to every single value in The Matrix that's what these two functions can do so let's now come back here once again oh I've got a syntax error I guess I have an extra Clos curly bracket so I'm going to create that 3x2 Matrix again I'm going to say add five now let's look at it and I should see the values in it should be all fives right now again we're not really seeing the Nuance of this because there're not different values but it started as zeros and then I added fives to it and now I could say m. multiply 3 oops oh I called it multiply and if I look at M again now and I start to look at those values we can see all the values are 5 so what do I have so far I have a simple Matrix implementation that allows me to initialize a grid of numbers by rows and columns and perform scalar operations I can multiply or I can um add so I'm going to pause here and in the next video I'm going to do elementwise operations and then we're going to start to look at other Vector multiplication which is really no longer the dot product but we I'll talk about sorry matrix multiplication so I'm going to kind of break these out into separate videos and I'm going to show you some interesting things about building a JavaScript library where I can actually determine what's coming in I can reuse the multiply uh and the add function um to determine am I adding a scaler or am I adding a whole other Matrix so I'm going to get to that in the next video okay thanks o I am running out of steam here oh boy this is how I feel right now where am I TimeWise oh two hours and 15 minutes toxic desire asked I'm sorry how long are we going to be here I wanted to at least get to the end of this uh but um can I yeah but it is it really worth the stress rebuilding existing libraries from scratch I don't know I feel like people are interested in learning this stuff and seeing the process of making it but um it's a good question I am more generally of the I I I suffer from the when I do video tutorials that I have kind of the theoretical infinite time and so I just like well I might as well fill in as many details as I can whereas if I were doing uh teaching a course and it just meets like a couple hours each week and I want to get to Applications I would kind of talk through the stuff in more generalities and show the library and that kind of thing so um but but uh fortunately a lot of people in the chat are enjoying this so much coffee shot um coffee shot for me yeah okay well is it's not just a matter of me running out of steam I uh have a endof year school picnic to attend I have no idea if it's does anybody know if it's raining outside in New York City right now because that picnic might not be I can actually look at my let's see uh I can try to look at the I mean I basically like a room with no windows uh I could try to look at the weather on my phone it doesn't say that it's raining right now it just has a 50% chance so I think that picnic is on yeah this is going to be a series I I wanted to I feel like I wanted to make it to a good end point today which was all the way through element wise and matrix multiplication let me at least let but I but I but I don't know if that's realistic um oh wait wait you can use table array to visualize in console this is a really good tip so are you saying if I do table m. Matrix how do I do that uh I guess I don't know so I if anybody has a tip for me there okay oh I forgot about the what is it called that hadam or hadam hadamard why is it called that sure product Oh French ma mathematician jacqu hadamard or German mathematician isai Shore okay these are really easy to do okay uh console. table thank you console. oh did I oh look at that oh my God my whole life has changed wow my whole life has just changed in this moment I did not know you could do that that is amazing I don't know I feel so happy that's like the greatest thing that happened to me in a long time I'm so tired it takes a lot of energy to do these you know I think also doing this stuff like Friday afternoon after a long busy week um hey let me try to get let me try to get a little bit further along I'm gonna actually open this video with that who who suggested that first I want to like thank the person I don't know who it was yeah Karma points okay okay let's keep going gelito was it gelito okay I think it was galito hopefully I got that right hi this you you don't know what you're about to watch this is ostensibly just like a random video in the middle of a very long series of building a neural network and I'm kind of now doing matrices and I already started it and this is really just a video about doing elementwise operations with a matrix and adding that to the Matrix little Matrix library that I'm building but guess what I'm opening this video with something really exciting that I just learned that I never knew thank you gelito from the YouTube chat for pointing this out but what let let me set the stage of where I am we're building this Matrix Library the idea is to be able to store numbers in a grid and perform different mathematical operations with them and we're going to ultimately use this library to do weighted sums in an neural network and right now I'm about to add an elementwise operation but I just did the basics of creating the Matrix and um the basics of creating the Matrix and multiplying or adding a value to it okay so now let's review I could say varm is a new Matrix and it's a 3X two and then I previously was looking at it like this and kind of going like this and trying to like look at the values in it but I learned that I can say console. table and then path in an array look at this console. path in an array my whole life is Chang in an instance realizing that now I can have this nice little tabular View and so I can just say uh multiply or I can do add five and then I can look at it again and I can see that there's fives in there and I could try to do other stuff and there's so many things I'm going to need to do to like check if it's working this is going to make it so much better I had to fake my reaction right now because I when I really first learned that I was genuinely oh that I'm still genuinely excited about it okay uh but for everybody watching live had to watch me get excited about it twice apolog for that okay so uh you watching this video right now let's add the next piece so what's interesting here is one of the things I wanted to do right is just say let me for example add this I multiplied but add the number two to each one of these values but what if I had another Matrix you know 3 1 4 3 and what if I wanted to add this Matrix to this Matrix element wise what element wise means is if I have two matrices A B C D and I have e fgh h i get a resulting Matrix that has a + e b + F C + g d + h i just take these two values and add them together these two values add them together these two values add them together these two values add them together now this will only work the way that I've described it to you if these Matrix matrices have the same dimensions the same number of rows and the same number of columns now there is there's something in Python the numpy library which is the core uh you know Matrix math library in Python has I forgot what it's called what's that thing called where it like time out for a second I'm going to edit this video right here and come back with the answer to what is that um what's that thing in Python what's that thing in Python where it um um it like can can actually do element wise with slightly different dimensions where was I reading this um it was actually on the um that blog post deep learning uh linear algebra I really should be thanking this blog post so this is u i got to remember to linear algebra cheat sheet for deep learning by Brendan Fortuner thank you so much this has really helped me um somewhere down towards the bottom here uh um am I losing my mind here broadcasting Elemental are relaxed via mechanism called broadcasting um yeah yeah yeah yeah okay thank you um okay where was I back here okay I'm back it's called broadcasting in numpy so but we're we're going to live in a simpler world where we for this we have to have the dimensions match exactly so what I want to do now is I want to keep those multiply and add functions I want them to be the same function but I want those functions to be able to receive a single number and add that single number to all of the values or receive another m Matrix and add all those Val add the values of of and add those values element wise so let's go back and add this now there's some things that I need to do for example I first why don't I at least uh write a function called uh randomize and what this function will do and you're going to see this and everything is just give each value a random value so I am going to um this I'm going to do something rather silly right now where I'm just going to say math. floor math.random time 10 so I'm not using the p5js random and floor functions writing this library because I want this library to be able to be used outside of the p5js library so I have to actually just use the native JavaScript random and floor functions so I should be able to now oops syntax error line 16 oh this should say equals function I should be able to say uh here's a new Matrix m. randomize and then let's look at its values and you can see there we go 1 1 18 38 1 14 those are random values so now if I were to say m. multiply by two and look at it again we can see there we go 216 616 28 great so now at least we can experiment and use different values now here's the thing look at this function matrix. prototype. add equals function n the argument coming in is n a single value but what if n isn't a single value what if it was actually a matrix so actually what I can do here is say if n is an instance of Matrix let's see is that right instance of of what does that mean I'm trying to determine what the type of n is so I can look at that here right m m instance of Matrix true M instance of what's another JavaScript object array false right so it's an if what I'm basically saying is here the add function receives an argument that argument might be a matrix it might be something else if it is a matrix what I want to do is add all the values elementwise otherwise now I should probably check like is it actually just a single number but I'm kind of going to assume here that there's only two possible ways any reasonable person would call this function either with a matrix or a single number so if it's a matrix add the values element wise every i j should get added to the corresponding i j otherwise um otherwise just add the single value to every single value so let's now see if this element wise works it gives myself some more space here so I'm going to make a matrix that is I'm going to call it M1 and I'm going to say M2 is also a 3x2 matrix I'm going to randomize M1 I'm going to randomize M2 then I'm going to say console I'm going to look at them both table M1 oh whoops sorry let me clear this console table M1 do the actual array in there console. table M2 Matrix so we can see here these are my two matrices 6372 2 07 04 3173 let's double the values in M2 just to see that that works or let's add to the value sorry what I did this with ADD right so I'm going to say M2 do add one let's add one to every value in M2 and let me I'm going to make this font a little smaller hopefully you can still see it let's look at M1 and M2 so we can see yep every value in M2 increased by one now if I add M1 to M2 I should get a matrix that has 78 right 6 + 1 3 + 5 7 + 4 2 + 2 let's say M1 do add M2 let's do that ah okay ah H cannot read property zero of undefined what did I get wrong matrix. JS 29 what's wrong here n oh you know what I forgot probably a lot of you in the chat noticed this or if you watch this you notice this The Matrix object has inside of itself a variable that actually stores the values called Matrix and maybe I should call that something else I'm not so sure about this this has to be n dot matrix right if this is an instance of the Matrix object I want to add this matx matx values to the N Matrix values so unfortunately I'm going to have to redo all this I have one Matrix I have two matrices now I'm going to add one to the second one and then I'm going to add M2 to M1 okay didn't get an error now let's look oh wait I didn't randomize them they won't have the same values it's going to be zero at least okay give me a second here oh no M2 is going to have one in it so let's uh let's just let's randomize M1 let's randomize M2 let's add one to M2 takes a long time just to like get back to my test should prob just write this code into like a code example it would be much nicer that way uh I'll do that in the next video now let me look at all them uh console. m1m Matrix console. table M two. Matrix okay 1 + 5 is 6 6 + 5 is 11 so let's see what we get M M1 add M2 okay console. table M1 I no no no dot matrix I think that's right if I scroll back up oh I I cleared it I can't scroll back up someone will have to confirm the math but I think we have successfully now written a function into our library that can do either a scalar operation or an element operation element wise operation and it's the same function and if I go back to the library I could do this same thing with multiply however I'm going to leave that I'm going to just do that on my own time I'm going to leave that as an exercise for you so if you're following along and building this library with me now go and write the same code to make multiply work both scalar and element wise and we're the p to resin stall so to speak the thing that's the most important thing that I haven't gotten to is actual matrix multiplication that isn't uh element wise and this by the way this element wise matrix multiplication is referred to is commonly known as the hard Maru no oh no that's that's on Twitter hard on Twitter does amazing work but what's that product had hadamard product let's go to the Wikipedia page so that's what this is called hadamard also is the sure product uh um that's the element wise multiplication but matrix multiplication itself is actually going to work in a completely different way and is be going to become the fundamental piece of how we look at inputs and weights between layers in a neural network and multiply and add all those things together say that again it's going to be the fundamental piece of how we look at inputs and weights and how we multiply those things and add them all together in a neural network so so this is where we're building up to so in the next video I am going to look at uh matrix multiplication the sort of core piece and we're going to put that into our library there's some other things we got to look at transposing a matrix is something we'll need and a few other things too and then we'll be back into the Neal network uh and starting to put those pieces together there's a lot of a lot of elements to this a lot of videos but uh thanks for staying with me in this journey process thing and hopefully I'm doing okay see you soon oh okay uh hadamard thank you forgot to randomize okay um boy everybody is talking about gelito in the chat is that there's an actual person named gelito or did I make that up I'm in the wrong camera all right everybody I'm very sorry that this is as far as I got today um I don't even remember where I started I got to look at my phone I apologize for doing that in the middle of a live stream but I got to get to this picnic oh um um let's see okay hold on I'm send gotta send some text messages here hold on I'll play the this dot S as always I always forget the this dot this dot this this going to do this this to do this going to do this to do this this do this do this do I'm going to do this do this do going to this this do this do this do actual jugl I don't want break never forget okay I'm going to make a I'm going to make a prediction I remember distinctly when I've looked at my live dashboard a while ago at the beginning there were 747 people watching because I remember 747 like the like the airplane so I gone off the deep end here into Never Never Land of matrices and linear algebra and all this stuff I've left the core I think the core audience of this YouTube channel behind and uh I gonna guess that there are 300 people watching right now I will look 685 that's kind of amazing um okay uh let me go back let me check the slack Channel all right so everybody um I'm sorry that this is where I have to wrap up um I um this has been a 2our and 36 minute live stream uh let me mention to you so what's what's coming um what's coming is next Friday I will be back to continue this so I'm just going to keep going I'm going to do the matrix multiplication I think things will pick up speed at some point then I'm going to put that matrix multiplication into the neural network code I got to do the training and the back propagation and all this stuff o ve but that's going to come next week uh next week I will be at if any the O'Reilly AI conference here in New York so if anybody happens to be watching is going to this conference uh send me a tweet at shiftman so I can say Hai and give say heli which is coding train for hello say hello and um give you a sticker um if you so that's that um I will take a short few minutes to answer a few questions it is so hot in here and and I'll be back next Friday the galito stuff is really kind of unbelievable I I wish I could follow the chat if someone could like summarize that for me like what what happened what went wrong uh and uh gelito if I Google gelito what do I get I really should not oh that is a thing okay I don't want to go any further I don't want to go any further with this what I'm doing okay um oh I'm not giving a talk at the conference I'm just I'm just trying to absorb and and go to some sessions and going to some tutorials and stuff um so um okay so let me see um oh boy there's no way I'm going to be a look at that right s multiply and S add e multiply that would be right okay so gelito is a real YouTube user I thought that maybe I was just like like told a fake name and there was some kind of joke so is so so I'm fine you guys have your own thing I have my thing which was actually thanking a real person whose YouTube name is jolito got it okay okay um let me see if um I'm going to put on this Goodbye song and I will take a few questions uh I'm looking at the slack chat um that's kind of uh if you are want to support what I'm doing patreon.com/crashcourse terribly mind you uh and then a many years later I ended up at a program called ITP which is where I teach right now and I learned about creative coding and lots of other interactive media stuff there at this program can I make a first person uh 3D style first person game um that's not something close on my radar right now that'll be interesting to kind of look at systems like that but um uh I do I would like to do more 3D stuff and particularly get more guests for 3D stuff where am I from I grew up in Baltimore Maryland do you plan on making searching sorting algorithm visualizations interesting uh yeah that would be a great topic boy that would be a really nice topic to do and I could actually probably do those in like quick short videos where's my coating train hat it's me your friendly neighborhood train conductor need the microphone to still be all aboard I really shouldn't have the math. JS in the background for this allo Cho I'm losing my mind can I balance this on my head or my chin oh that hurts my neck no I don't think so um yeah uh okay yes this is my engineer costume all right uh yes the renaming the functions I did see that um I think I think I don't think there's any questions left CU people are just talking about gelito so I think I'm going to be saying goodbye uh I see some people are typing in the um oh what live yeah having subtraction as its own function hold on so okay so k u there's been a discussion actually in the um in the slack channel in the slack live chat about what operations I have left to do so I need to do the um element wise for multiply to add that in I need to rethink the naming because what should I actually call the matrix multiplication function and you know versus element wise function so I think they need to have name I need to do the matrix multiplication I need to do the transpose um we can actually you can actually see all this stuff um is in the um you because I I did this a month ago or so already um so I can actually just look at that here and I'm kind of doing it from scratch again now so transpose copy add multiply what did I do I guess I called it a map is something we're going to need but I'll add that dot is what I called it is that technically a good thing to call it I'm not so sure so that's what I called it okay so that's yeah so that's pretty much what I have left to do and so one person in the chat asked about well I could have two different functions like s ad and E add like for scaler ad or element wise ad but I like the I mean so that would be a reasonable way but I I do kind of like the solution of reusing the same function and having the function kind of autod detect what's coming in um and so the other question was added was asked well should I have a subtract function and you could make the argument yes to have a subtract function but you could also just use add with a negative value and would have the same result but I I think it could have some utility especially for the element wise subtraction otherwise you have to like multip You' have to First Take a matrix and multiply by negative one and then add it that sort of thing all right thank you everybody for tuning in today uh thank you for bearing with me um I hope to get back to kind of just more creative examples and different generative algorithms and quick quick games and that kind of stuff uh more guests so but I want to try to see if I can get through this neural network stuff and um just make for people who are interested and um uh your feedback and your thoughts are highly appreciated and encouraged uh uh criticism and everything as well okay so um thank you guys I'm going to turn this button off I will be back next Friday and most likely a same sort of timing more a little bit later probably in the 300 p.m. eastern time which is I guess like 7 or 800 p.m. Greenwich meantime um okay so um see you all I don't have a great I don't do a great job of having a fixed schedule oh this is this computer is about to die it's not plugged in um this is actually those of you who want a little inside baseball here this computer has green green paper on it so that you don't see it uh but you can see I'm kind of like cut off by it if I walk over this way all right um so um see you all next Friday unless something magical happens I have time to do earlier but hopefully uh in between July 3rd and July 14th I'm going to have uh twice a week live stream so I'll update you about those schedule times and get through all this neural network learning stuff okay um thanks very much and I will see you next Friday goodbye
