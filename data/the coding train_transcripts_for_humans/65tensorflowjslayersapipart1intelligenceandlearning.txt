With timestamps:

00:00 - hello welcome to another tensorflow das
00:04 - tutorial now I'm very excited about this
00:09 - one I'm generally excited about a lot of
00:11 - things but in this tutorial everything
00:13 - that I've done so far has just used
00:16 - tensors operations to kind of create
00:20 - lists and matrices of numbers and
00:22 - multiply them and add them and optimize
00:24 - loss functions that kind of stuff now
00:28 - and I could keep going and there's a lot
00:30 - that I could do with just that alone but
00:32 - tensorflow digest I talked about this in
00:34 - the first video it has the sort of core
00:36 - API which has the tensors in it the
00:40 - operations and it's what I use to do a
00:43 - linear regression and a polynomial
00:45 - regression demonstration it also has
00:48 - something called the layers API and the
00:53 - layers API might look familiar to you if
00:56 - you've ever used something called Kerris
00:57 - because these are really Kerris layers
00:59 - caris is a lot machine learning library
01:02 - that is a higher level that allows you
01:04 - to create these machine learning models
01:06 - and underneath the hood a lower level
01:09 - code like tensorflow
01:10 - will be running so when Ted's floater
01:13 - chess was created it was created with
01:15 - both those for lower level stuff and the
01:17 - slightly higher level stuff and I also
01:20 - am working on with a lot of
01:21 - collaborators here at NYU ITP an even
01:24 - higher level library that's built on top
01:28 - of the layers API called ml5 I'll be
01:30 - getting to that eventually in this video
01:32 - I just want to talk about what the
01:34 - layers API is and its core features and
01:37 - so the way that I'm going to do that is
01:39 - by looking at a sort of basic diagram of
01:42 - a neural network and how you would put
01:45 - together that neural network with the
01:47 - layers API and the kind of neural
01:50 - network that I'm going to diagram is the
01:52 - same exact one that I did in my very
01:53 - long tutorial series about built writing
01:56 - a neural network all from scratch so the
01:58 - point this is you don't have to write it
02:00 - all from scratch you can just architect
02:02 - it with the layers API but if you want
02:05 - if you want to like get everything you
02:06 - possibly could ever get you could go
02:08 - back and look at some
02:08 - those videos if you want so what is so
02:12 - I'm gonna look at a simple well it's not
02:14 - simple but a basic feed-forward
02:16 - multi-layered perceptron it's gonna have
02:18 - just two layers so it often looks like
02:22 - three layers because there's also the
02:23 - inputs there's the inputs the hidden and
02:25 - the outputs that's three things but
02:28 - technically there's only two layers and
02:29 - you'll see why so let's consider that we
02:33 - have this neural network it's going to
02:35 - have inputs let's say that it has I
02:38 - don't know two inputs then it has a
02:45 - hidden layer how many nodes are in the
02:50 - hidden layer I don't know let's say four
02:56 - then it has an output layer how many
03:01 - outputs are there I don't know maybe
03:03 - we're doing some kind of classification
03:04 - task and there's three possibilities
03:06 - it's either a cat a dog or turtle so
03:09 - there will be three outputs you know I
03:12 - started diagramming the next the thing
03:14 - the scenario that I'm gonna do in the
03:16 - next video maybe as a coding challenge
03:18 - is I'm going to solve again XOR problem
03:20 - I really would like to get to some
03:22 - real-world applicable problems but still
03:25 - here in the weeds of just like I want to
03:26 - see how things work and use kind of
03:28 - trivial and known problems just to see
03:32 - if I can get the solution that I know
03:33 - I'm supposed to get okay so if this is
03:36 - my diagram if you if you've never seen
03:39 - the neural network before again you
03:41 - could go back and look at some other
03:42 - videos but the ideas are some data X I'm
03:46 - going to call this like x0 and x1 and
03:48 - that data each each of those inputs gets
03:52 - sent to each hidden node there are
04:00 - weights here the inputs get multiplied
04:03 - by the weights added together and then
04:04 - pass through an activation function and
04:07 - then get sent out to each of the outputs
04:11 - and so that happens at each one of these
04:15 - to draw this
04:17 - I'll be sure I missed the connection by
04:21 - the way something called dropout so I've
04:22 - done that correct I've dropout correct
04:24 - me if I forgot to draw something but
04:25 - I'll get to that in another video so
04:26 - then then we have the outputs and those
04:29 - are again the weighted sum of all of the
04:32 - outputs of the hidden layer the hidden
04:34 - nodes comes into the output pass through
04:36 - an activation function and we see their
04:38 - result so the idea here is maybe that I
04:41 - have a image with just two pixels in it
04:43 - and two two pixels come in they get
04:47 - multiplied by all these weights
04:50 - activated of the activation function
04:52 - sent to the outputs and then I get some
04:53 - values that tell me the probability of
04:55 - those two pixels were a cat dog or a
04:56 - turtle that would be an image
04:57 - classification task so how do I use how
05:01 - do I use the layers API to create a
05:03 - neural network with this exact
05:06 - architecture so the first thing that I
05:08 - need to do is create something called TF
05:12 - sequential the TF sequential let's go
05:16 - look at that in the API Docs and I'm
05:19 - right here all right
05:20 - TF win is a secretes a sequential model
05:23 - is any model where the outputs of one
05:26 - layer are the inputs to the next layer
05:30 - that's what it says right there guess
05:31 - what and I know I'm sorry that I've kind
05:33 - of gone a little bit too high in my
05:35 - writing but the outputs of one layer are
05:38 - the inputs to the next layer the inputs
05:41 - go into the hidden the outputs of the
05:42 - hidden go down so that's exactly what I
05:44 - want I should say there is also
05:46 - something in the layers API called TF
05:51 - model which I'll click on here and the
05:55 - key difference is that TF model is more
05:57 - generic so there's kind of more
05:59 - possibilities there but if I really just
06:01 - want a simple basic feed-forward neural
06:03 - network or the data flows in one
06:05 - direction between layers TF sequential
06:07 - will work so if I'm writing some code I
06:10 - would say Const model equals T f dot
06:14 - sequential so there we go I'm done well
06:21 - not exactly
06:22 - so that's just creating a sort of empty
06:24 - so all I've done is created this kind of
06:27 - empty architecture so what I need to do
06:30 - now and by the way this is what makes
06:32 - working with something like the layers
06:34 - API really powerful if you watch my
06:36 - tutorials where I did the whole neural
06:37 - network from scratch it was so much
06:39 - easier just to have one layer and I
06:41 - never kind of got to like multiple
06:42 - lakers cuz i have to rewrite the code
06:44 - and think about the layers and how
06:45 - they're connected and have a loop and
06:46 - all this layer object well guess what
06:48 - the layers api has this for you so i can
06:51 - actually just say TF now add layer so I
06:56 - can I can actually I can create a layer
06:57 - so the kind of layer that I want to
06:59 - create is known as dense so what is a
07:03 - dense layer a dense layer is the
07:05 - terminology for a fully connected layer
07:08 - meaning that every every node in that
07:13 - layer is fully connected to every node
07:16 - from the previous layer and that's
07:17 - exactly what I have here these are dense
07:19 - layers all of the connections are there
07:22 - so and you'll see you'll see there's
07:24 - like other kinds of layers there's a
07:25 - convolutional layer that i'll use one at
07:28 - some point when I talk about
07:28 - convolutional neural networks other
07:30 - things too but dense is where we're
07:31 - gonna get started so let's come back and
07:33 - look at now the API Docs for dense and I
07:37 - think I've opened that up here yes TF
07:41 - layer sorry it's TF dot layers dense so
07:44 - I need to say Const and I'm gonna call
07:47 - this hidden equals TF layers dense Const
07:53 - output equals TF layers dense so I'm
07:57 - missing a lot of but this is the idea so
08:00 - like I will have a model which is it's a
08:02 - sequential and then I have a hidden
08:03 - layer and output layer we'll talk about
08:04 - the inputs in a second and then I would
08:07 - just say model dot add layer hidden
08:09 - model dot add layer output and I'm like
08:16 - think I'd like to say outputs I think is
08:19 - it's the output layer whatever nuts to
08:21 - say output so this is the idea this is
08:23 - how in theory simple it is to build your
08:26 - own model using the layers API now
08:29 - what's missing here like I could weirdly
08:31 - enough let's just like run this code and
08:33 - see if we get any errors so it cannot
08:37 - read property name of undefined so okay
08:40 - so so who knows what that error is
08:43 - thing that I'm really missing here is I
08:44 - need to be more specific like I need to
08:47 - say when I make a layer what is the
08:50 - shape of that layer in other words how
08:53 - many nodes are there with the shape of
08:54 - the inputs how is it connected what
08:56 - activation function am i using those
08:58 - types of things so those are if we look
09:00 - at the API Docs the configuration of the
09:03 - layer so so I need to actually configure
09:05 - the model itself and configure each
09:08 - layer so let's go look at that and see
09:09 - if we can figure that out so let's go
09:11 - look again at TF sequential and you know
09:18 - actually I think we're going to be fine
09:19 - right now with there's an optional this
09:22 - question mark means optional there's an
09:23 - optional optional optional parameter
09:27 - config what I am going to skip that
09:30 - right now and yet it could be I could
09:35 - actually create it with a bunch of
09:37 - layers already and a name but I'm gonna
09:39 - skip that what's more important here is
09:41 - the sorry the the dense the config
09:47 - object for the dense layers which is
09:49 - required so that's why I'm getting error
09:51 - this is required so I need to specify
09:55 - some configuration options and this
09:59 - should be listed for me here units
10:01 - activation use bias kernel initializer
10:04 - bla bla bla bla bla bla bla bla and
10:06 - sorry for these markers here so let's
10:07 - start adding something what that means
10:09 - is I need to create an object I'm gonna
10:12 - call it config I could call it and here
10:15 - is where I'm gonna set up the
10:16 - configuration of this hidden layer so
10:20 - what I'm gonna do here is let's look at
10:22 - what some of these are so units so what
10:25 - I want is what's the unit so in this
10:26 - case I want to have the hidden layer has
10:29 - four units and so I'm gonna say units
10:34 - four maybe I want to specify the
10:37 - activation function
10:38 - so what activation function you use is a
10:41 - fascinating topic that you could go down
10:44 - many rabbit holes for different
10:46 - scenarios but and I'm just going to put
10:49 - sigmoid in there as kind of it for
10:51 - historical reasons and that's also the
10:54 - activation function you
10:55 - in my toy neural network JavaScript
10:57 - library but eventually as I start to
10:58 - build out examples I'm gonna be taking
11:00 - out that sigmoid and using other ones so
11:03 - I'm gonna say sigmoid and I'm gonna put
11:07 - config here I'll just say config 1 or
11:09 - config hidden again I could put the
11:12 - object itself directly in here there's
11:15 - lots of ways you could probably write
11:16 - this code in much shorter way I'm trying
11:18 - to write it as long away as possible
11:19 - to be most clear so let's do that and
11:23 - let's do config output and what did I
11:28 - say how many outputs do I have 3 and
11:32 - I'll also use sigmoid so I'm going to
11:34 - say 3 and I'm gonna say I'm gonna say
11:39 - config output ok so things are going
11:44 - pretty well now I'm gonna hit refresh
11:48 - here config is not defined sketch let's
11:51 - just line 7 oh right config hidden ooh
11:56 - Oh interesting
11:58 - amusingly I got this weird error message
12:01 - which makes no sense at all it actually
12:04 - makes sense but it's because I've got
12:06 - the p5 library involved here and p5
12:08 - actually has a function called model so
12:10 - let me just write now p5 is irrelevant
12:12 - for this discussion so I'm just gonna
12:13 - comment out the p5 library and then hit
12:17 - refresh here now add layer is not a
12:19 - function so I must have imagined that
12:21 - this is how you add a layer to the model
12:24 - let's actually go and look and where
12:26 - would I find that out once again if I go
12:28 - to so here what I want to look at is all
12:33 - right it's a little hard for me to find
12:34 - things Oh No ok I know what I want to
12:36 - look for us I'm gonna just search for it
12:37 - I want to look for the TF sequential
12:40 - class so TF not sequential the function
12:43 - creates an object that is a TF not
12:46 - sequential so if I look at this we now
12:48 - look that the function is just add
12:50 - there's no add layer function it's just
12:52 - add which is a lot nicer actually so
12:55 - this is meant to just be add and this is
12:57 - meant to just be add ok so now this is
13:01 - really bothering me that it's calling
13:03 - this polynomial regression so I'm going
13:06 - to have to chain
13:06 - this two layers API explanation there we
13:14 - go okay uncaught error the first layer
13:17 - in a sequential model must get an input
13:20 - shape or a batch input shape argument
13:23 - all right so what did I miss this is
13:24 - what I was talking about so the inputs
13:26 - are technically not a layer themselves
13:29 - the inputs are in a way part of this
13:32 - hidden layer
13:33 - they are the inputs to that hidden layer
13:35 - the inputs to the output output layer
13:37 - are the outputs of the hidden layer so
13:40 - one of the things one of those
13:41 - properties that I have to specify is the
13:44 - input shape now interestingly enough it
13:47 - says I need an input shape or an input
13:49 - batch shape and what's interesting about
13:51 - this is what is the difference so here I
13:53 - can clearly say that the input shape is
13:56 - two there are two inputs it's just a no
13:59 - actually it's one well no no it's - it's
14:05 - a regular number to it it's confusing
14:07 - this is what it is it's one-dimensional
14:10 - it's a one-dimensional array with two
14:12 - spots in it
14:13 - but someday there might come a time
14:16 - where I have a data set that is just
14:18 - each each each each record of that data
14:22 - set is two numbers but I want to send in
14:25 - a hundred of them at once that's known
14:26 - as a batch so the shape might be
14:28 - something like 2 comma 100 but this is
14:31 - not super relevant for right now this is
14:33 - what I need to specify so if we come
14:35 - back to the code
14:37 - I should here oops going back to sketch
14:40 - touch is what I need to specify here is
14:43 - input shape - that's it again I made
14:48 - this up like I'm just saying my model
14:51 - architecture has - so now I should be
14:55 - able to no errors so I created that
14:59 - model and I can even take a look at it
15:01 - here there it is now you know all this
15:03 - stuff input layers we can see all sorts
15:06 - of stuff here now I there's not much
15:08 - there actually cuz I've forgotten a
15:09 - really crucial step but let me keep
15:10 - going there's some more stuff to discuss
15:12 - so the input shape is - what's the thing
15:15 - how come I don't need to specify an
15:18 - input Shea
15:19 - here you would think that I might after
15:22 - all I had to say that there were two
15:25 - inputs to hit it do I don't have to say
15:28 - that there's four inputs to outputs well
15:31 - the reason why I don't is because
15:33 - tensorflow digest the layers API can
15:35 - infer the input shape of the outputs
15:39 - layer because it has to be the number of
15:41 - units in that hidden layer and by the
15:45 - way it's you know actually naming these
15:47 - things like hidden and output is almost
15:49 - less relevant now it's really kind of
15:51 - like layer 1 layer 2 that sort of thing
15:53 - so so I could put input shape here and I
15:59 - would put for now let's just see if I
16:02 - have any errors everything's fine now
16:04 - what if I put like a tear am I gonna get
16:08 - an error look I didn't get an error
16:10 - that's weird now why couldn't I get an
16:12 - error well maybe it should give me an
16:15 - air I don't think so though I should get
16:16 - an air though I'm missing a crucial step
16:18 - here I actually have just set up the
16:21 - idea of this sequential model I have the
16:24 - model object I have the hidden layer I
16:26 - have the output layer I've added them
16:28 - both in but I actually haven't like
16:30 - plugged all the pieces into each other
16:31 - yet and finished it off that has to have
16:34 - come as I separate it's not building up
16:36 - the model as you're creating it or
16:38 - configuring it it's at the layers API so
16:41 - you configure it and then call a
16:42 - function called compile so I know I'm
16:45 - making lists of and this by the way was
16:46 - in TF dot layers dot dense but another
16:50 - really important function here compile
16:53 - is part of a sequential object ad ad was
16:56 - the other one if I'm keeping track of
16:58 - the things that I'm looking at so I add
17:00 - I looked at now I also need compile so
17:02 - let's go take a look at that so if I go
17:06 - to the documentation here we can see
17:09 - there's ad evaluate there's a bunch of
17:11 - things I'm looking for compile we maybe
17:13 - compile is not part of TF sequential oh
17:18 - it is it is it's just listed as part of
17:22 - TF model because TF sequential is based
17:26 - off of TF model so this is what I'm
17:27 - looking for this is me needing to
17:30 - compile the model now I need to come
17:33 - pilot with an optimizer and a loss
17:36 - function aha
17:38 - so if you watched my linear regression
17:42 - with tension flow Jas videos you might
17:45 - remember that I had to create something
17:47 - called an optimizer and the optimizers
17:51 - job was to minimize a loss function so
18:00 - the same thing the idea you know I just
18:02 - had y equals MX plus B now I have a more
18:05 - complex architecture to learn about a
18:07 - data set so what I want to do with that
18:10 - architecture is the same though I want
18:12 - to feed it a lot of known data and have
18:15 - it optimize all of the weights of all
18:17 - these connections to fit and this can be
18:19 - a very good fit that data so I need to
18:21 - specify those things so how do I do that
18:24 - so first I'm gonna make I'm gonna call
18:30 - this just um like config and I'm going
18:35 - to say optimizer is and so some options
18:39 - here a string or a chi F train optimizer
18:43 - so I think what I want actually want to
18:45 - create my own optimizer and I'm gonna go
18:48 - do that I'm by saying a constant
18:51 - optimizer equals TF train SGD zero point
18:57 - one so if you remember this is a way
19:00 - this is I mean this you might not have
19:01 - seen before if you're watching this
19:02 - video for the first time but this is
19:04 - exactly the same code that I had in my
19:09 - linear regression example I'm creating
19:11 - an optimizer from TF train the optimizer
19:14 - uses to cast a gradient descent and the
19:16 - learning rate is point one so this will
19:20 - be the optimizer so I'm gonna say SGD
19:22 - optimizer like that then what else do I
19:27 - need a loss function so the loss
19:30 - function can be I could probably to find
19:32 - my own loss function or I can use a
19:34 - string so I'm not seeing here the
19:39 - options for the loss function strings
19:41 - but let's see a loss function like
19:44 - basically what I want to add here
19:46 - is like mean squared error or something
19:50 - that's my loss functional I don't think
19:52 - that's right but let's just see now if
19:56 - what I would do is say my a model dot
19:58 - compile config so now right what is what
20:04 - is Ruth let's review make create the TF
20:08 - sequential object configure some layers
20:11 - add them to that model then configure
20:14 - the the Optima basically make an
20:17 - optimizer and a loss function to find
20:18 - those and then compile the model with
20:20 - those so I'm sure there's gonna be lots
20:23 - of errors here and there's things that
20:24 - I've done incorrectly let's so unknown
20:26 - loss mean squared error so let's figure
20:28 - out how do I look that up so let's see
20:31 - here means squared look I found it in an
20:36 - example ah so losses let's look at this
20:40 - I'm just looking for the list of them
20:42 - but I'm gonna I'm gonna come back I'm
20:45 - gonna find that and then come back and
20:47 - show it to you but now that I see it's
20:49 - actually this is just lowercase M so
20:50 - somewhere the documentation I want to
20:52 - find what the options I could put here
20:53 - are let's run let's now hit refresh
20:56 - oh no errors interesting so this is
21:00 - weird I'm surprised it off this is a bug
21:02 - or not but I'm surprised that I didn't
21:05 - get an error for this input shape here
21:06 - for the output maybe once I started
21:09 - feeding it data again there maybe I
21:11 - never would way I would clarify
21:13 - something really important here because
21:14 - I I kind of put this in here as like a
21:16 - demonstration of some goofiness I was
21:18 - trying to like figure out but the input
21:20 - shape so a couple things let me first
21:23 - let me first mention something somebody
21:24 - in the chat just asked me oh isn't it 12
21:27 - because there's 4 here and there's 3
21:30 - here and 4 times 3 equals 12 well the
21:33 - number 12 that I know I just sort of
21:35 - wrote that up too high the number 12 is
21:36 - an important one there are 12
21:38 - connections meaning 12 Waits
21:40 - but this is now we're in the place of
21:42 - using Valerius api that is specified for
21:44 - us we just need to say there are 4 here
21:48 - there are 3 here and we need to say
21:51 - there are 2 coming into here and there
21:54 - for going into there so too is the input
21:57 - shape to here for is the input shape to
22:00 - here now I have this weird eight in my
22:02 - code because I was like kind of messing
22:04 - around like what happens if you put like
22:05 - the quote unquote incorrect input shape
22:08 - and so this really should if I want my
22:10 - network to be to match what I'm drawing
22:13 - over there this should be a four but I
22:15 - don't need it because it can be inferred
22:17 - by here and I think what I'm gonna do
22:19 - actually now to make this a little more
22:20 - readable is I think it's as much as I
22:22 - wanted to try to write this in a
22:23 - long-winded way I think I'm gonna take
22:25 - the configuration and just put it right
22:28 - here inside the creation of this layer
22:31 - and then I'm gonna take this object I
22:33 - mean I don't need to name these objects
22:35 - and do this I think this is actually
22:39 - easier to follow so now you can see I
22:44 - think this is easier to follow right
22:45 - there's and I you know so there's the
22:49 - model there's the hidden layer the
22:51 - output layer I need to specify the input
22:53 - shape always of the first layer and
22:55 - maybe what I want to do is actually say
22:57 - add that hidden layer then create the
22:59 - output it doesn't really matter what
23:00 - order because I'm gonna compile
23:01 - everything so I have the create that
23:03 - let's review did I already do this I'm I
23:05 - need to do it again I need to create the
23:08 - sequential object I need to make
23:10 - whatever layers I want configure them
23:12 - add them in and then I need to compile
23:14 - it and notice compiling it I need both
23:16 - an optimizer which I created one
23:19 - stochastic gradient descent I could use
23:21 - the add a blonde or any of the other
23:22 - ones again what these are and what the
23:24 - formulas are there's a lot we could go
23:26 - down many different paths and rabbit
23:28 - holes for a lot of depth here but I'm
23:29 - just kind of looking at the higher level
23:30 - point of view here and then I need to
23:32 - compile it with a loss function so me
23:34 - and squared error being one of them and
23:36 - I actually the all of the loss functions
23:39 - are listed out here this is where they
23:41 - are so for example so I can name it by
23:44 - the string or I can actually just
23:47 - reference the function name directly
23:49 - like this so this should also not give
23:52 - me any errors and you could see now that
23:55 - if I wanted to use like Oh cosine
23:58 - distance I heard that that's the loss
24:00 - function I should be using I could put
24:02 - that in here and I could also hit
24:04 - refresh again and now I'm using cosine
24:05 - distance so again what did different
24:07 - loss
24:07 - Czar why should use one versus the other
24:09 - hopefully I might get into these things
24:11 - as I start building examples and have to
24:14 - make those decisions but right now I'm
24:15 - just looking at how you put it together
24:16 - okay um so this is gonna be the layers
24:20 - tutorial part one I'm gonna do a second
24:22 - part to this because all I've done right
24:25 - now is I've created the model and I have
24:31 - compiled it the two things that I need
24:33 - to do with this is I need to send data
24:36 - through it I want to put data through it
24:38 - and look at the outputs what are the
24:40 - things I might want to do the two things
24:41 - I might want to do is use predict
24:45 - predict is a function where I give the
24:48 - model inputs and I get out of predict
24:51 - the outputs presumably I would only be
24:54 - doing predict after I've trained the
24:56 - model so I'd want to train the bottle
24:59 - with some training data it's finished
25:01 - then I can make predictions with new
25:03 - unknown data so how do I train the model
25:06 - I use a function called fit fit is a
25:09 - function that I can basically say I want
25:11 - to fit just like we had to in the linear
25:14 - regression example I had a lot of points
25:16 - and I had to find the line that fits
25:18 - those points I need to find all the
25:21 - weights of this machine learning neural
25:24 - network model that fit the data and
25:26 - guess what this is what tensorflow jess
25:28 - is going to do behind the scenes it's
25:30 - gonna do all of the stochastic gradient
25:32 - descent math it's going to use its own
25:35 - loss function everything underneath the
25:37 - hood so all the stuff I did in that
25:39 - build a neural network from scratch now
25:41 - is now done for us by tension flow yes
25:44 - and it has many more sophisticated
25:45 - options so that's what's coming in the
25:47 - next video looking at fit and predict
25:55 - [Music]

Cleaned transcript:

hello welcome to another tensorflow das tutorial now I'm very excited about this one I'm generally excited about a lot of things but in this tutorial everything that I've done so far has just used tensors operations to kind of create lists and matrices of numbers and multiply them and add them and optimize loss functions that kind of stuff now and I could keep going and there's a lot that I could do with just that alone but tensorflow digest I talked about this in the first video it has the sort of core API which has the tensors in it the operations and it's what I use to do a linear regression and a polynomial regression demonstration it also has something called the layers API and the layers API might look familiar to you if you've ever used something called Kerris because these are really Kerris layers caris is a lot machine learning library that is a higher level that allows you to create these machine learning models and underneath the hood a lower level code like tensorflow will be running so when Ted's floater chess was created it was created with both those for lower level stuff and the slightly higher level stuff and I also am working on with a lot of collaborators here at NYU ITP an even higher level library that's built on top of the layers API called ml5 I'll be getting to that eventually in this video I just want to talk about what the layers API is and its core features and so the way that I'm going to do that is by looking at a sort of basic diagram of a neural network and how you would put together that neural network with the layers API and the kind of neural network that I'm going to diagram is the same exact one that I did in my very long tutorial series about built writing a neural network all from scratch so the point this is you don't have to write it all from scratch you can just architect it with the layers API but if you want if you want to like get everything you possibly could ever get you could go back and look at some those videos if you want so what is so I'm gonna look at a simple well it's not simple but a basic feedforward multilayered perceptron it's gonna have just two layers so it often looks like three layers because there's also the inputs there's the inputs the hidden and the outputs that's three things but technically there's only two layers and you'll see why so let's consider that we have this neural network it's going to have inputs let's say that it has I don't know two inputs then it has a hidden layer how many nodes are in the hidden layer I don't know let's say four then it has an output layer how many outputs are there I don't know maybe we're doing some kind of classification task and there's three possibilities it's either a cat a dog or turtle so there will be three outputs you know I started diagramming the next the thing the scenario that I'm gonna do in the next video maybe as a coding challenge is I'm going to solve again XOR problem I really would like to get to some realworld applicable problems but still here in the weeds of just like I want to see how things work and use kind of trivial and known problems just to see if I can get the solution that I know I'm supposed to get okay so if this is my diagram if you if you've never seen the neural network before again you could go back and look at some other videos but the ideas are some data X I'm going to call this like x0 and x1 and that data each each of those inputs gets sent to each hidden node there are weights here the inputs get multiplied by the weights added together and then pass through an activation function and then get sent out to each of the outputs and so that happens at each one of these to draw this I'll be sure I missed the connection by the way something called dropout so I've done that correct I've dropout correct me if I forgot to draw something but I'll get to that in another video so then then we have the outputs and those are again the weighted sum of all of the outputs of the hidden layer the hidden nodes comes into the output pass through an activation function and we see their result so the idea here is maybe that I have a image with just two pixels in it and two two pixels come in they get multiplied by all these weights activated of the activation function sent to the outputs and then I get some values that tell me the probability of those two pixels were a cat dog or a turtle that would be an image classification task so how do I use how do I use the layers API to create a neural network with this exact architecture so the first thing that I need to do is create something called TF sequential the TF sequential let's go look at that in the API Docs and I'm right here all right TF win is a secretes a sequential model is any model where the outputs of one layer are the inputs to the next layer that's what it says right there guess what and I know I'm sorry that I've kind of gone a little bit too high in my writing but the outputs of one layer are the inputs to the next layer the inputs go into the hidden the outputs of the hidden go down so that's exactly what I want I should say there is also something in the layers API called TF model which I'll click on here and the key difference is that TF model is more generic so there's kind of more possibilities there but if I really just want a simple basic feedforward neural network or the data flows in one direction between layers TF sequential will work so if I'm writing some code I would say Const model equals T f dot sequential so there we go I'm done well not exactly so that's just creating a sort of empty so all I've done is created this kind of empty architecture so what I need to do now and by the way this is what makes working with something like the layers API really powerful if you watch my tutorials where I did the whole neural network from scratch it was so much easier just to have one layer and I never kind of got to like multiple lakers cuz i have to rewrite the code and think about the layers and how they're connected and have a loop and all this layer object well guess what the layers api has this for you so i can actually just say TF now add layer so I can I can actually I can create a layer so the kind of layer that I want to create is known as dense so what is a dense layer a dense layer is the terminology for a fully connected layer meaning that every every node in that layer is fully connected to every node from the previous layer and that's exactly what I have here these are dense layers all of the connections are there so and you'll see you'll see there's like other kinds of layers there's a convolutional layer that i'll use one at some point when I talk about convolutional neural networks other things too but dense is where we're gonna get started so let's come back and look at now the API Docs for dense and I think I've opened that up here yes TF layer sorry it's TF dot layers dense so I need to say Const and I'm gonna call this hidden equals TF layers dense Const output equals TF layers dense so I'm missing a lot of but this is the idea so like I will have a model which is it's a sequential and then I have a hidden layer and output layer we'll talk about the inputs in a second and then I would just say model dot add layer hidden model dot add layer output and I'm like think I'd like to say outputs I think is it's the output layer whatever nuts to say output so this is the idea this is how in theory simple it is to build your own model using the layers API now what's missing here like I could weirdly enough let's just like run this code and see if we get any errors so it cannot read property name of undefined so okay so so who knows what that error is thing that I'm really missing here is I need to be more specific like I need to say when I make a layer what is the shape of that layer in other words how many nodes are there with the shape of the inputs how is it connected what activation function am i using those types of things so those are if we look at the API Docs the configuration of the layer so so I need to actually configure the model itself and configure each layer so let's go look at that and see if we can figure that out so let's go look again at TF sequential and you know actually I think we're going to be fine right now with there's an optional this question mark means optional there's an optional optional optional parameter config what I am going to skip that right now and yet it could be I could actually create it with a bunch of layers already and a name but I'm gonna skip that what's more important here is the sorry the the dense the config object for the dense layers which is required so that's why I'm getting error this is required so I need to specify some configuration options and this should be listed for me here units activation use bias kernel initializer bla bla bla bla bla bla bla bla and sorry for these markers here so let's start adding something what that means is I need to create an object I'm gonna call it config I could call it and here is where I'm gonna set up the configuration of this hidden layer so what I'm gonna do here is let's look at what some of these are so units so what I want is what's the unit so in this case I want to have the hidden layer has four units and so I'm gonna say units four maybe I want to specify the activation function so what activation function you use is a fascinating topic that you could go down many rabbit holes for different scenarios but and I'm just going to put sigmoid in there as kind of it for historical reasons and that's also the activation function you in my toy neural network JavaScript library but eventually as I start to build out examples I'm gonna be taking out that sigmoid and using other ones so I'm gonna say sigmoid and I'm gonna put config here I'll just say config 1 or config hidden again I could put the object itself directly in here there's lots of ways you could probably write this code in much shorter way I'm trying to write it as long away as possible to be most clear so let's do that and let's do config output and what did I say how many outputs do I have 3 and I'll also use sigmoid so I'm going to say 3 and I'm gonna say I'm gonna say config output ok so things are going pretty well now I'm gonna hit refresh here config is not defined sketch let's just line 7 oh right config hidden ooh Oh interesting amusingly I got this weird error message which makes no sense at all it actually makes sense but it's because I've got the p5 library involved here and p5 actually has a function called model so let me just write now p5 is irrelevant for this discussion so I'm just gonna comment out the p5 library and then hit refresh here now add layer is not a function so I must have imagined that this is how you add a layer to the model let's actually go and look and where would I find that out once again if I go to so here what I want to look at is all right it's a little hard for me to find things Oh No ok I know what I want to look for us I'm gonna just search for it I want to look for the TF sequential class so TF not sequential the function creates an object that is a TF not sequential so if I look at this we now look that the function is just add there's no add layer function it's just add which is a lot nicer actually so this is meant to just be add and this is meant to just be add ok so now this is really bothering me that it's calling this polynomial regression so I'm going to have to chain this two layers API explanation there we go okay uncaught error the first layer in a sequential model must get an input shape or a batch input shape argument all right so what did I miss this is what I was talking about so the inputs are technically not a layer themselves the inputs are in a way part of this hidden layer they are the inputs to that hidden layer the inputs to the output output layer are the outputs of the hidden layer so one of the things one of those properties that I have to specify is the input shape now interestingly enough it says I need an input shape or an input batch shape and what's interesting about this is what is the difference so here I can clearly say that the input shape is two there are two inputs it's just a no actually it's one well no no it's it's a regular number to it it's confusing this is what it is it's onedimensional it's a onedimensional array with two spots in it but someday there might come a time where I have a data set that is just each each each each record of that data set is two numbers but I want to send in a hundred of them at once that's known as a batch so the shape might be something like 2 comma 100 but this is not super relevant for right now this is what I need to specify so if we come back to the code I should here oops going back to sketch touch is what I need to specify here is input shape that's it again I made this up like I'm just saying my model architecture has so now I should be able to no errors so I created that model and I can even take a look at it here there it is now you know all this stuff input layers we can see all sorts of stuff here now I there's not much there actually cuz I've forgotten a really crucial step but let me keep going there's some more stuff to discuss so the input shape is what's the thing how come I don't need to specify an input Shea here you would think that I might after all I had to say that there were two inputs to hit it do I don't have to say that there's four inputs to outputs well the reason why I don't is because tensorflow digest the layers API can infer the input shape of the outputs layer because it has to be the number of units in that hidden layer and by the way it's you know actually naming these things like hidden and output is almost less relevant now it's really kind of like layer 1 layer 2 that sort of thing so so I could put input shape here and I would put for now let's just see if I have any errors everything's fine now what if I put like a tear am I gonna get an error look I didn't get an error that's weird now why couldn't I get an error well maybe it should give me an air I don't think so though I should get an air though I'm missing a crucial step here I actually have just set up the idea of this sequential model I have the model object I have the hidden layer I have the output layer I've added them both in but I actually haven't like plugged all the pieces into each other yet and finished it off that has to have come as I separate it's not building up the model as you're creating it or configuring it it's at the layers API so you configure it and then call a function called compile so I know I'm making lists of and this by the way was in TF dot layers dot dense but another really important function here compile is part of a sequential object ad ad was the other one if I'm keeping track of the things that I'm looking at so I add I looked at now I also need compile so let's go take a look at that so if I go to the documentation here we can see there's ad evaluate there's a bunch of things I'm looking for compile we maybe compile is not part of TF sequential oh it is it is it's just listed as part of TF model because TF sequential is based off of TF model so this is what I'm looking for this is me needing to compile the model now I need to come pilot with an optimizer and a loss function aha so if you watched my linear regression with tension flow Jas videos you might remember that I had to create something called an optimizer and the optimizers job was to minimize a loss function so the same thing the idea you know I just had y equals MX plus B now I have a more complex architecture to learn about a data set so what I want to do with that architecture is the same though I want to feed it a lot of known data and have it optimize all of the weights of all these connections to fit and this can be a very good fit that data so I need to specify those things so how do I do that so first I'm gonna make I'm gonna call this just um like config and I'm going to say optimizer is and so some options here a string or a chi F train optimizer so I think what I want actually want to create my own optimizer and I'm gonna go do that I'm by saying a constant optimizer equals TF train SGD zero point one so if you remember this is a way this is I mean this you might not have seen before if you're watching this video for the first time but this is exactly the same code that I had in my linear regression example I'm creating an optimizer from TF train the optimizer uses to cast a gradient descent and the learning rate is point one so this will be the optimizer so I'm gonna say SGD optimizer like that then what else do I need a loss function so the loss function can be I could probably to find my own loss function or I can use a string so I'm not seeing here the options for the loss function strings but let's see a loss function like basically what I want to add here is like mean squared error or something that's my loss functional I don't think that's right but let's just see now if what I would do is say my a model dot compile config so now right what is what is Ruth let's review make create the TF sequential object configure some layers add them to that model then configure the the Optima basically make an optimizer and a loss function to find those and then compile the model with those so I'm sure there's gonna be lots of errors here and there's things that I've done incorrectly let's so unknown loss mean squared error so let's figure out how do I look that up so let's see here means squared look I found it in an example ah so losses let's look at this I'm just looking for the list of them but I'm gonna I'm gonna come back I'm gonna find that and then come back and show it to you but now that I see it's actually this is just lowercase M so somewhere the documentation I want to find what the options I could put here are let's run let's now hit refresh oh no errors interesting so this is weird I'm surprised it off this is a bug or not but I'm surprised that I didn't get an error for this input shape here for the output maybe once I started feeding it data again there maybe I never would way I would clarify something really important here because I I kind of put this in here as like a demonstration of some goofiness I was trying to like figure out but the input shape so a couple things let me first let me first mention something somebody in the chat just asked me oh isn't it 12 because there's 4 here and there's 3 here and 4 times 3 equals 12 well the number 12 that I know I just sort of wrote that up too high the number 12 is an important one there are 12 connections meaning 12 Waits but this is now we're in the place of using Valerius api that is specified for us we just need to say there are 4 here there are 3 here and we need to say there are 2 coming into here and there for going into there so too is the input shape to here for is the input shape to here now I have this weird eight in my code because I was like kind of messing around like what happens if you put like the quote unquote incorrect input shape and so this really should if I want my network to be to match what I'm drawing over there this should be a four but I don't need it because it can be inferred by here and I think what I'm gonna do actually now to make this a little more readable is I think it's as much as I wanted to try to write this in a longwinded way I think I'm gonna take the configuration and just put it right here inside the creation of this layer and then I'm gonna take this object I mean I don't need to name these objects and do this I think this is actually easier to follow so now you can see I think this is easier to follow right there's and I you know so there's the model there's the hidden layer the output layer I need to specify the input shape always of the first layer and maybe what I want to do is actually say add that hidden layer then create the output it doesn't really matter what order because I'm gonna compile everything so I have the create that let's review did I already do this I'm I need to do it again I need to create the sequential object I need to make whatever layers I want configure them add them in and then I need to compile it and notice compiling it I need both an optimizer which I created one stochastic gradient descent I could use the add a blonde or any of the other ones again what these are and what the formulas are there's a lot we could go down many different paths and rabbit holes for a lot of depth here but I'm just kind of looking at the higher level point of view here and then I need to compile it with a loss function so me and squared error being one of them and I actually the all of the loss functions are listed out here this is where they are so for example so I can name it by the string or I can actually just reference the function name directly like this so this should also not give me any errors and you could see now that if I wanted to use like Oh cosine distance I heard that that's the loss function I should be using I could put that in here and I could also hit refresh again and now I'm using cosine distance so again what did different loss Czar why should use one versus the other hopefully I might get into these things as I start building examples and have to make those decisions but right now I'm just looking at how you put it together okay um so this is gonna be the layers tutorial part one I'm gonna do a second part to this because all I've done right now is I've created the model and I have compiled it the two things that I need to do with this is I need to send data through it I want to put data through it and look at the outputs what are the things I might want to do the two things I might want to do is use predict predict is a function where I give the model inputs and I get out of predict the outputs presumably I would only be doing predict after I've trained the model so I'd want to train the bottle with some training data it's finished then I can make predictions with new unknown data so how do I train the model I use a function called fit fit is a function that I can basically say I want to fit just like we had to in the linear regression example I had a lot of points and I had to find the line that fits those points I need to find all the weights of this machine learning neural network model that fit the data and guess what this is what tensorflow jess is going to do behind the scenes it's gonna do all of the stochastic gradient descent math it's going to use its own loss function everything underneath the hood so all the stuff I did in that build a neural network from scratch now is now done for us by tension flow yes and it has many more sophisticated options so that's what's coming in the next video looking at fit and predict
