With timestamps:

00:00 - Hello.
00:00 - Welcome to a
continuation of my series
00:03 - on convolutional neural
networks at ml5js.
00:06 - The last time I recorded one
of these was February 24, 2020.
00:12 - It is now October 2020.
00:15 - I would like to keep this mask
on for the entire recording
00:17 - of this video, but I cannot,
because it fogs up my glasses
00:20 - and I can't see anything.
00:22 - And fortunately I'm in a
hermetically sealed room
00:25 - here by myself, where it is
safe for me to take off my mask.
00:29 - So I'm sorry that
it took me so long
00:31 - to get to continue this series.
00:32 - But I am very excited
to do it with you today.
00:34 - I spent the last half an
hour re-watching this video,
00:37 - and getting myself
centered to where I am
00:40 - and where I want to pick up
with now in this video tutorial.
00:43 - The first thing I want
to highlight for you
00:45 - is that I made a couple
errors in the previous video
00:48 - when I was discussing how
the resolution changes
00:52 - from layer to layer within a
convolutional neural network.
00:55 - I was off by 1 or 2 or maybe
by a power of 2 here and there.
01:00 - Thank you to returnexitsuccess,
who eight months ago pointed
01:04 - out that that 28 by 28
image actually becomes 26
01:08 - by 26, when the 3 by 3
filter is passed over it,
01:12 - leaving out all the edge pixels
in the new processed image.
01:16 - Additionally, Luis
points out here
01:18 - that the resolution, the
total number of pixels,
01:21 - is reduced by a
quarter, not by a half.
01:23 - Because if the width is
reduced by half and the height
01:26 - is reduced by half,
the pixels being
01:28 - the width times the height,
are reduced by one quarter.
01:31 - Thank you for saying
I'm awesome, too.
01:33 - I mean that's nice.
01:35 - That's nice.
01:36 - I appreciate it.
01:37 - When I watched the video
again this morning,
01:39 - I discovered that I promised
some things at the end.
01:42 - And so I'm here to
deliver on that promise.
01:44 - And the first thing
that I'm going to do
01:46 - is take the example from "Train
a Neural Network With Pixels,"
01:50 - which didn't use
convolutional layers,
01:52 - and update that example to
have convolutional layers,
01:55 - and just look at how
the code is different,
01:57 - and see if it performs
differently at all.
01:59 - As a reminder of what
this example does, is this
02:01 - takes a low resolution 10
by 10 image from the webcam,
02:05 - and I've already
trained the model
02:07 - to recognize it as
label a when I'm
02:10 - standing in front the camera,
and label b when I move away
02:13 - from the camera.
02:14 - So ultimately this kind of
simple binary classification
02:19 - with very clear
distinctive images
02:21 - works fine without the
convolutional layers.
02:24 - But let's try something a
little bit more sophisticated.
02:26 - Could we perhaps get it
to recognize whether I'm
02:28 - wearing my mask or no mask?
02:30 - And let's add convolutional
layers to this example
02:33 - and see how that works.
02:34 - I should also quickly
mention that there
02:36 - have been some updates
to the ml5 library
02:38 - since the last time I recorded.
02:40 - And you'll want to make sure you
have version, at least 0.6.0.
02:44 - That's the version I'm using for
this particular demonstration.
02:47 - The first step for adding
convolutional layers
02:50 - to your ml5 neural network
is to change the task.
02:53 - So the task that
I'm going to specify
02:56 - is image classification.
02:58 - I should probably point out
the convolutional neural
03:00 - networks are not limited
to working with images.
03:02 - They're super effective for
lots of other kinds of data.
03:04 - And I would certainly like to
get into that at some point,
03:06 - and look at some other examples.
03:08 - But a primary use case is
image classification. ml5
03:12 - knows how to work with images.
03:14 - So this is kind of
our starting point.
03:15 - So the terminology, the
friendly term if you will,
03:18 - to having convolutional layers
in your ml5 neural network
03:21 - is specifying the task
as image classification.
03:25 - Other thing I need to do
is be much more specific
03:27 - about the input data here.
03:29 - So with a regular
neural network in ml5,
03:31 - it was just about
the number of inputs.
03:33 - Were there three?
03:34 - Were there 104?
03:35 - Whatever number you might
pick based on your data.
03:37 - Here I'm going to be
sending it images.
03:40 - So I need to tell ml5 what are
the dimensions of the image,
03:43 - width and height, and how many
channels does the image have.
03:46 - Is it an RGB or RGB alpha image?
03:50 - Is it a gray-scale image?
03:51 - And working in p5 generally,
it's going to be--
03:56 - it's up to me to specify
the resolution of the image.
03:58 - In this case, it's 10 by 10.
04:00 - Lets up that resolution
to 64, 64 by 64.
04:04 - So I'm going to say in an
array, 64, commas, 64, comma, 4.
04:10 - Because the pixels of the
images that I'm going to pass in
04:13 - have red, green, blue
and alpha values.
04:15 - Ultimately here, the
alpha information
04:18 - is useless, because I don't have
any transparency in the images.
04:21 - So I might want to
filter that out.
04:23 - But I'm not going to worry
about that right now.
04:25 - I'm just going to let it be
64 by 64 with four channels.
04:28 - I'm noticing here that
it says outputs 3,
04:30 - and I don't remember
why I that's there.
04:34 - Probably when I was
doing this the last time,
04:36 - I knew in my head, oh, I'm going
to have three labels, three
04:39 - possibilities.
04:40 - But actually ml5
will quite nicely
04:43 - figure out how
many outputs there
04:44 - are, if it's a classification
problem, based on the data
04:48 - itself.
04:48 - So I'm just going
to take that out.
04:50 - And I've got inputs, image
classification, and debug set
04:53 - to true, because I want to
see the graph of the loss
04:55 - as it's going.
04:56 - Now if you recall, I
put almost no thought
04:58 - into the data
collection interface.
05:00 - There is no interface.
05:01 - When I press the key a, saying
these images are a, label a.
05:05 - When I press the key b,
these images are label b.
05:07 - So I'm going to keep
that model, but I
05:09 - do need to now adjust
this addExample function.
05:13 - The nice wonderful thing about
the fact that I've specified it
05:15 - as an image
classification problem,
05:17 - is ml5 knows how to work with
p5 images, or raw pixel data.
05:22 - Both are possible.
05:23 - But I'm just going to--
since I have a p5 image,
05:25 - I'm going to use the p5 image.
05:26 - So rather than have to
loop through all the pixels
05:29 - and normalize the
data myself, I could
05:31 - do something much more simple.
05:33 - So I'm actually
going to remove all
05:35 - of this processing
of the pixels.
05:36 - And the input itself, which is
really a single input image,
05:41 - I'll call it input image is,
I need to make it an object.
05:44 - And the property, I'll
just name it image.
05:46 - And the image that I want to
send in is the video itself.
05:50 - Then for the target,
the training target,
05:52 - I'm going to also just be
consistent and make this
05:55 - an object called label, and
that will actually be the label
05:59 - itself.
06:00 - One of the nice things, by the
way I can do a JavaScript here,
06:02 - is when I'm creating
these object literals,
06:04 - I want to have a object with
property image and value video.
06:08 - Here, the property
name happens to be
06:10 - the variable name of the value.
06:12 - So I can use an
enhanced object literal.
06:13 - It just makes the code a
little bit shorter and cleaner.
06:16 - And I can just say target
equals curly brackets
06:19 - with label inside.
06:20 - So now I just need to change
this inputs to input image.
06:24 - And I now have my
data that I'm using
06:27 - to train the model is the
video image itself, as well
06:30 - as the target label, both
wrapped into objects.
06:33 - I also need to do the same
thing in the classified video
06:36 - function, because
that's where I'm also
06:38 - sending an image into the
model for a prediction.
06:40 - So I can actually just remove
this entire bit of code,
06:43 - and replace it with that
same object literal,
06:48 - and pass that in.
06:50 - All right, I'm going
to try to run this.
06:51 - I'm really not so confident
it's going to work.
06:53 - And there's more
that I want to say
06:55 - about this, a couple of things
I want to add to this example.
06:57 - And then there's going
to be another video where
06:59 - I think it would be a really
excellent demonstration
07:01 - to show a use case where
I've collected essentially
07:05 - a database of images that I
want to use to train the model.
07:08 - But let's just run this
and see what happens.
07:11 - OK, well first of all,
I'm seeing the image
07:13 - being drawn by 64 by 64.
07:15 - Let me change the way
I'm drawing the image.
07:17 - I don't think I need to draw
every single pixel individually
07:20 - as a rectangle here
in this use case.
07:23 - So I'm going to take this out,
and just draw the video itself,
07:28 - and stretch it out over the
width and height of the canvas.
07:32 - It's important to realize
that I've actually still set
07:34 - the image to be 64 by 64.
07:36 - So that's what's
actually being passed
07:37 - into the machine
learning model here.
07:40 - But we're seeing a higher
resolution version of the image
07:42 - stretched out over the canvas.
07:45 - I'm going to try exactly
what I did before,
07:46 - which is every time I press
the key a, I get a new training
07:50 - image with label a.
07:54 - Now I'm going to step away,
and give it a bunch of training
07:56 - images with label b.
08:00 - And then when I press
t, it trains the model.
08:07 - That didn't seem
to work so well.
08:08 - So I'm glad that happened,
because this is inevitably
08:11 - going to happen to you.
08:12 - Something went wrong, because
the loss is not going down.
08:15 - In fact, the loss is above--
08:17 - between 4 and 5, which are
very high numbers for a loss.
08:21 - So I was just doing
some debugging
08:23 - to try to figure this out.
08:24 - And I downloaded the data
that ml5 neural network
08:27 - saves the data from
your training data set.
08:29 - And I looked at it
and I thought, oh, I
08:32 - forgot to normalize the data.
08:35 - So all of these numbers are all
pixel values between 0 and 255,
08:39 - which makes sense.
08:40 - That's the way pixels
are stored in p5.js.
08:43 - But I need them to be normalized
between a range of 0 and 1
08:46 - for the neural network to work.
08:48 - So right before I
train the model,
08:49 - I need to add one line of
code to normalize the data.
08:53 - So I'm hoping this fixes it,
but it remains to be seen.
08:59 - I could have reloaded the
data I saved previously,
09:01 - but I'm just doing it
again, just to do it again.
09:06 - I saved my model,
training prayer.
09:08 - And then I press t.
09:12 - Ah, that's a loss
function I like to see.
09:19 - a, b, a, b.
09:26 - So there's a live
chat going while I'm
09:27 - recording this right now.
09:28 - And the question comes up, can't
you add the normalizing layer?
09:32 - And so first of all,
the normalizing--
09:34 - normalizing the data is not a
layer of the neural network.
09:36 - It's like a pre-processing step.
09:39 - And that could be something
that ml5 just always does
09:41 - by default. But there
are some cases where
09:44 - you don't want to
normalize the data,
09:45 - or you want to normalize
the data in your own way.
09:47 - And so that's
explicitly something
09:49 - you do have to call with ml5.
09:50 - That's an interesting
question whether or not
09:52 - ml5, the library itself,
should change the way it works.
09:55 - But for now, I do
have to call it.
09:57 - So this worked.
09:57 - And ultimately it's
the same exact result
09:59 - of what I had in
the previous video.
10:01 - So why are we even here?
10:03 - So I would like to
make the case for you,
10:05 - why you might want to use the
convolutional neural network
10:08 - functionality in ml5 beyond
just the regular neural network
10:11 - stuff.
10:12 - So one is that it's
my suspicion here
10:14 - that if I gave it a much harder
problem, more complex images
10:19 - with less obvious distinctive
differences to classify,
10:22 - that the convolutional
neural networks are
10:23 - going to perform better.
10:25 - This was such an easy case of am
I standing in front the camera
10:28 - or not, it's only two classes.
10:29 - So we're not really
seeing a difference.
10:31 - So I might try it with my mask.
10:32 - That might be a
slightly harder problem.
10:34 - The other thing you
might be wondering
10:35 - is why are we even
doing this when
10:36 - we have this whole system
about transfer learning?
10:39 - I mean, after all,
this is basically
10:41 - exactly the examples from
the Teachable Machine videos.
10:44 - Well in truth, if
what I wanted to do
10:46 - was quickly whip up an
image classifier that's
10:49 - recognizing some
gestures and movements,
10:50 - am I in front of the camera
or not in front of camera,
10:52 - or a particular object; using
Transfer Learning and Teachable
10:55 - Machine will get me probably
better, more accurate
10:58 - results more quickly.
10:59 - But there are a couple of
reasons why you might not
11:01 - want to go that route.
11:02 - One is maybe you
don't want your model
11:04 - to be based on any
pre-existing model or data set.
11:08 - You don't want to use
MobileNet model which
11:11 - was trained on the
ImageNet database
11:13 - as part of what you're doing.
11:14 - Also maybe the images
you're using really
11:17 - have nothing to do with
the everyday objects
11:21 - that the MobileNet
model was trained on.
11:23 - So for example, if you're
trying to recognize
11:25 - drawings, or circuits, or some
kind of obscure specific design
11:29 - pattern that's not something you
see like scissors, and phones,
11:33 - and remote controls,
and coffee cups;
11:36 - then that Transfer
Learning approach
11:38 - isn't going to really help you.
11:39 - Because the data of the
pre-trained model that you're
11:43 - basing on does not
match your current data,
11:45 - and that's what I'm
going to show you
11:47 - in the next video, where I want
to look at doodles, and shapes,
11:50 - and other kinds of data that
just aren't photographic images
11:54 - from everyday life.
11:55 - But here, let's
just make the case
11:56 - that this is going to
work a little bit better.
11:58 - And let's see if I can get it to
work, do the mask and no mask.
12:01 - So let's add some more specific
labels here in the key press.
12:04 - So if I press the
key m, then I'm
12:10 - going to add an example
with the label mask.
12:18 - And actually the labels,
this is a little bit silly,
12:21 - because I could
just code it to like
12:23 - say a message when it
sees a certain label.
12:25 - But the labels could be
just any arbitrary string.
12:27 - So the label is going to
be "Nice mask!" for when
12:29 - I'm wearing it, and
"Keep others safe!
12:31 - Wear your mask," for
when I'm not wearing it.
12:33 - All right, let's see
if we get this to work.
12:36 - So let's first do the no mask.
12:41 - Now on with the mask, all right,
hopefully that's enough data.
12:48 - Let's train the model.
12:53 - Wow, that's some wacky loss.
12:54 - But it looks like
it figured it out.
12:58 - Hey, it likes my mask.
13:03 - Oh, whoops.
13:06 - I didn't really think
about the design here.
13:09 - OK.
13:10 - I fixed the layout.
13:13 - So now it will tell
me to put on my mask,
13:18 - if I am not wearing it.
13:20 - So here I am wearing my
mask sitting at my computer.
13:22 - And I take it off, and
it tells me to put in on.
13:24 - So this is great.
13:25 - Our convolutional
neural network,
13:26 - there's no transfer learning.
13:27 - There's no base model.
13:28 - This was all done in the web
browser, in p5.js, with ml5.
13:33 - Amazing.
13:34 - All right, before I
go from this video,
13:37 - I want to just return to
this model summary panel
13:40 - in the debug view, when
you're training the model.
13:42 - So this is ordinarily
the part that I maybe
13:44 - try to stay away from a little
bit, the model architecture
13:46 - or the lower level details.
13:47 - But I think it's
really important
13:49 - to make the connection between
these layers and the diagrams
13:53 - that I showed you in
the previous two videos
13:54 - about what is a
convolutional neural network.
13:57 - And we can see right here
the convolutional layers,
14:00 - the pooling layers, and then
the flattening, the flat layer,
14:03 - and the final output label.
14:04 - And you can see that there's
two outputs, because there
14:08 - is a probability
confidence score for mask
14:11 - and one for no mask.
14:12 - So that would be a higher number
if there are more categories.
14:15 - But this is the
default architecture
14:18 - that ml5 will make when you
say image classification.
14:21 - And it's actually possible for
you to configure this yourself,
14:25 - to say how many convolutional
layers you want,
14:27 - to say how big you
want the kernel--
14:30 - the filter kernel to be, how
do you want to do pooling,
14:34 - what is your stride;
all of those parameters
14:36 - that I mentioned in the previous
videos are configurable here.
14:39 - So if this isn't working
for you or you just
14:41 - want to play and experiment,
let me show you how to do that.
14:44 - In the ml5js reference,
you'll find a section
14:47 - under neural network for
defining custom layers.
14:51 - And this is where you can
actually configure individually
14:54 - the number of layers
and what those layers
14:56 - do in an ml5 neural network.
14:58 - So this is the
default set of layers
15:00 - for a classification,
the default set
15:01 - of layers for a regression.
15:03 - And I want to look down
here at this default image
15:06 - classification layers.
15:07 - All you have to do is create
an array called layers,
15:10 - fill that array
with objects that
15:12 - include the various
details for each layer.
15:16 - Let me copy this to the
clipboard, go back to my code,
15:20 - and right here in setup,
right before options,
15:23 - I'm going to paste that in.
15:26 - So now I have a variable that's
holding onto the custom layer
15:29 - configuration.
15:32 - And I'm going to go down
to my ml5 neural network,
15:35 - and I'm going to add a
property, layers, custom layers.
15:40 - So here, I have the input,
dimensions, the task
15:43 - that I'm doing--
15:43 - I want to debug it
while I'm training it--
15:45 - and now the custom
configuration for the layers.
15:48 - And I could start to
experiment with this.
15:51 - And you can see
here in this array,
15:52 - my first layer is a
convolutional layer.
15:54 - I want to have 8 filters,
a kernel size of 5,
15:57 - and a strides of 1.
15:59 - I haven't talked too much
about activation functions,
16:01 - and what's this
kernel initializer.
16:03 - So these are--
16:04 - I'll try to put some resources
in the video's description
16:06 - where you can read up more about
some of these other properties
16:09 - that you can experiment with.
16:10 - But again, this is the
size of max pooling.
16:13 - What would happen
if I did it 3 by 3,
16:16 - and change the number of
strides along the x and y-axis?
16:20 - And you could see one thing.
16:21 - The thing that I think that's
important for me to point out
16:23 - is there are two
convolutional layers.
16:25 - And as the resolution
is decreasing,
16:28 - the number of filters
is increasing.
16:30 - Not a blanket rule,
but that's one way
16:32 - to approach
architecting your model.
16:35 - Trial and error is your
friend here, experimentation,
16:39 - talking to somebody
else who knows
16:40 - about convolutional
neural networks
16:42 - or has tried it before to
get some advice about what
16:44 - might work well for you in
your particular scenario.
16:47 - I encourage you to
experiment with that.
16:49 - Leave your feedback and things
you've tried in the comments.
16:51 - And in the next
video, I am going
16:53 - to look at a convolutional
neural network that is
16:56 - trained off images of shapes--
16:58 - squares, circles,
and triangles; so
17:00 - that I could create
something where maybe I'm
17:02 - drawing on a piece of paper,
and the neural network guesses.
17:05 - Did I just draw a circle,
a square, or a triangle?
17:08 - And then later, I'm
also going to show you
17:10 - some pre-trained conventional
neural networks that
17:12 - are in ml5, like
DoodleNet which is trained
17:15 - on a whole lot of drawings
from the Google Quick Draw
17:18 - data set to recognize
various kinds of doodles.
17:20 - And these are
scenarios where using
17:22 - your own convolutional
neural network
17:24 - really makes sense, because
it's the kind of data
17:26 - that we're working with.
17:27 - Drawing and shapes
and abstract geometry,
17:30 - isn't something that
the original MobileNet
17:32 - model, image classification
model, was trained on.
17:34 - So transfer learning
doesn't necessarily apply.
17:37 - And also there are
some reasons why
17:38 - you might want to train your
model from scratch with only
17:42 - your own data, and you have a
real control and understanding
17:45 - of how that data was collected,
and how that model is being
17:48 - used, as opposed to a situation
where you're doing transfer
17:51 - learning, and basing your model
off of a pre-trained model
17:54 - that you might not
know as much about.
17:56 - OK, so I hope that next video
won't take a year to come out.
18:02 - But who knows what's coming
next in 2020 into 2021?
18:06 - I hope good things for you.
18:08 - And I will see you in
a future ml5 video.
18:13 - If you're watching this
in years into the future,
18:15 - well, it's a little
bit of history
18:16 - for you, a history lesson.
18:17 - And I don't know what I'll be
doing, or what you're doing,
18:19 - but I'm glad that you're here,
and that I'm here with you
18:22 - in this virtual mediated way.
18:24 - All right.
18:24 - See you soon.
18:25 - Goodbye
18:25 - [MUSIC PLAYING]

Cleaned transcript:

Hello. Welcome to a continuation of my series on convolutional neural networks at ml5js. The last time I recorded one of these was February 24, 2020. It is now October 2020. I would like to keep this mask on for the entire recording of this video, but I cannot, because it fogs up my glasses and I can't see anything. And fortunately I'm in a hermetically sealed room here by myself, where it is safe for me to take off my mask. So I'm sorry that it took me so long to get to continue this series. But I am very excited to do it with you today. I spent the last half an hour rewatching this video, and getting myself centered to where I am and where I want to pick up with now in this video tutorial. The first thing I want to highlight for you is that I made a couple errors in the previous video when I was discussing how the resolution changes from layer to layer within a convolutional neural network. I was off by 1 or 2 or maybe by a power of 2 here and there. Thank you to returnexitsuccess, who eight months ago pointed out that that 28 by 28 image actually becomes 26 by 26, when the 3 by 3 filter is passed over it, leaving out all the edge pixels in the new processed image. Additionally, Luis points out here that the resolution, the total number of pixels, is reduced by a quarter, not by a half. Because if the width is reduced by half and the height is reduced by half, the pixels being the width times the height, are reduced by one quarter. Thank you for saying I'm awesome, too. I mean that's nice. That's nice. I appreciate it. When I watched the video again this morning, I discovered that I promised some things at the end. And so I'm here to deliver on that promise. And the first thing that I'm going to do is take the example from "Train a Neural Network With Pixels," which didn't use convolutional layers, and update that example to have convolutional layers, and just look at how the code is different, and see if it performs differently at all. As a reminder of what this example does, is this takes a low resolution 10 by 10 image from the webcam, and I've already trained the model to recognize it as label a when I'm standing in front the camera, and label b when I move away from the camera. So ultimately this kind of simple binary classification with very clear distinctive images works fine without the convolutional layers. But let's try something a little bit more sophisticated. Could we perhaps get it to recognize whether I'm wearing my mask or no mask? And let's add convolutional layers to this example and see how that works. I should also quickly mention that there have been some updates to the ml5 library since the last time I recorded. And you'll want to make sure you have version, at least 0.6.0. That's the version I'm using for this particular demonstration. The first step for adding convolutional layers to your ml5 neural network is to change the task. So the task that I'm going to specify is image classification. I should probably point out the convolutional neural networks are not limited to working with images. They're super effective for lots of other kinds of data. And I would certainly like to get into that at some point, and look at some other examples. But a primary use case is image classification. ml5 knows how to work with images. So this is kind of our starting point. So the terminology, the friendly term if you will, to having convolutional layers in your ml5 neural network is specifying the task as image classification. Other thing I need to do is be much more specific about the input data here. So with a regular neural network in ml5, it was just about the number of inputs. Were there three? Were there 104? Whatever number you might pick based on your data. Here I'm going to be sending it images. So I need to tell ml5 what are the dimensions of the image, width and height, and how many channels does the image have. Is it an RGB or RGB alpha image? Is it a grayscale image? And working in p5 generally, it's going to be it's up to me to specify the resolution of the image. In this case, it's 10 by 10. Lets up that resolution to 64, 64 by 64. So I'm going to say in an array, 64, commas, 64, comma, 4. Because the pixels of the images that I'm going to pass in have red, green, blue and alpha values. Ultimately here, the alpha information is useless, because I don't have any transparency in the images. So I might want to filter that out. But I'm not going to worry about that right now. I'm just going to let it be 64 by 64 with four channels. I'm noticing here that it says outputs 3, and I don't remember why I that's there. Probably when I was doing this the last time, I knew in my head, oh, I'm going to have three labels, three possibilities. But actually ml5 will quite nicely figure out how many outputs there are, if it's a classification problem, based on the data itself. So I'm just going to take that out. And I've got inputs, image classification, and debug set to true, because I want to see the graph of the loss as it's going. Now if you recall, I put almost no thought into the data collection interface. There is no interface. When I press the key a, saying these images are a, label a. When I press the key b, these images are label b. So I'm going to keep that model, but I do need to now adjust this addExample function. The nice wonderful thing about the fact that I've specified it as an image classification problem, is ml5 knows how to work with p5 images, or raw pixel data. Both are possible. But I'm just going to since I have a p5 image, I'm going to use the p5 image. So rather than have to loop through all the pixels and normalize the data myself, I could do something much more simple. So I'm actually going to remove all of this processing of the pixels. And the input itself, which is really a single input image, I'll call it input image is, I need to make it an object. And the property, I'll just name it image. And the image that I want to send in is the video itself. Then for the target, the training target, I'm going to also just be consistent and make this an object called label, and that will actually be the label itself. One of the nice things, by the way I can do a JavaScript here, is when I'm creating these object literals, I want to have a object with property image and value video. Here, the property name happens to be the variable name of the value. So I can use an enhanced object literal. It just makes the code a little bit shorter and cleaner. And I can just say target equals curly brackets with label inside. So now I just need to change this inputs to input image. And I now have my data that I'm using to train the model is the video image itself, as well as the target label, both wrapped into objects. I also need to do the same thing in the classified video function, because that's where I'm also sending an image into the model for a prediction. So I can actually just remove this entire bit of code, and replace it with that same object literal, and pass that in. All right, I'm going to try to run this. I'm really not so confident it's going to work. And there's more that I want to say about this, a couple of things I want to add to this example. And then there's going to be another video where I think it would be a really excellent demonstration to show a use case where I've collected essentially a database of images that I want to use to train the model. But let's just run this and see what happens. OK, well first of all, I'm seeing the image being drawn by 64 by 64. Let me change the way I'm drawing the image. I don't think I need to draw every single pixel individually as a rectangle here in this use case. So I'm going to take this out, and just draw the video itself, and stretch it out over the width and height of the canvas. It's important to realize that I've actually still set the image to be 64 by 64. So that's what's actually being passed into the machine learning model here. But we're seeing a higher resolution version of the image stretched out over the canvas. I'm going to try exactly what I did before, which is every time I press the key a, I get a new training image with label a. Now I'm going to step away, and give it a bunch of training images with label b. And then when I press t, it trains the model. That didn't seem to work so well. So I'm glad that happened, because this is inevitably going to happen to you. Something went wrong, because the loss is not going down. In fact, the loss is above between 4 and 5, which are very high numbers for a loss. So I was just doing some debugging to try to figure this out. And I downloaded the data that ml5 neural network saves the data from your training data set. And I looked at it and I thought, oh, I forgot to normalize the data. So all of these numbers are all pixel values between 0 and 255, which makes sense. That's the way pixels are stored in p5.js. But I need them to be normalized between a range of 0 and 1 for the neural network to work. So right before I train the model, I need to add one line of code to normalize the data. So I'm hoping this fixes it, but it remains to be seen. I could have reloaded the data I saved previously, but I'm just doing it again, just to do it again. I saved my model, training prayer. And then I press t. Ah, that's a loss function I like to see. a, b, a, b. So there's a live chat going while I'm recording this right now. And the question comes up, can't you add the normalizing layer? And so first of all, the normalizing normalizing the data is not a layer of the neural network. It's like a preprocessing step. And that could be something that ml5 just always does by default. But there are some cases where you don't want to normalize the data, or you want to normalize the data in your own way. And so that's explicitly something you do have to call with ml5. That's an interesting question whether or not ml5, the library itself, should change the way it works. But for now, I do have to call it. So this worked. And ultimately it's the same exact result of what I had in the previous video. So why are we even here? So I would like to make the case for you, why you might want to use the convolutional neural network functionality in ml5 beyond just the regular neural network stuff. So one is that it's my suspicion here that if I gave it a much harder problem, more complex images with less obvious distinctive differences to classify, that the convolutional neural networks are going to perform better. This was such an easy case of am I standing in front the camera or not, it's only two classes. So we're not really seeing a difference. So I might try it with my mask. That might be a slightly harder problem. The other thing you might be wondering is why are we even doing this when we have this whole system about transfer learning? I mean, after all, this is basically exactly the examples from the Teachable Machine videos. Well in truth, if what I wanted to do was quickly whip up an image classifier that's recognizing some gestures and movements, am I in front of the camera or not in front of camera, or a particular object; using Transfer Learning and Teachable Machine will get me probably better, more accurate results more quickly. But there are a couple of reasons why you might not want to go that route. One is maybe you don't want your model to be based on any preexisting model or data set. You don't want to use MobileNet model which was trained on the ImageNet database as part of what you're doing. Also maybe the images you're using really have nothing to do with the everyday objects that the MobileNet model was trained on. So for example, if you're trying to recognize drawings, or circuits, or some kind of obscure specific design pattern that's not something you see like scissors, and phones, and remote controls, and coffee cups; then that Transfer Learning approach isn't going to really help you. Because the data of the pretrained model that you're basing on does not match your current data, and that's what I'm going to show you in the next video, where I want to look at doodles, and shapes, and other kinds of data that just aren't photographic images from everyday life. But here, let's just make the case that this is going to work a little bit better. And let's see if I can get it to work, do the mask and no mask. So let's add some more specific labels here in the key press. So if I press the key m, then I'm going to add an example with the label mask. And actually the labels, this is a little bit silly, because I could just code it to like say a message when it sees a certain label. But the labels could be just any arbitrary string. So the label is going to be "Nice mask!" for when I'm wearing it, and "Keep others safe! Wear your mask," for when I'm not wearing it. All right, let's see if we get this to work. So let's first do the no mask. Now on with the mask, all right, hopefully that's enough data. Let's train the model. Wow, that's some wacky loss. But it looks like it figured it out. Hey, it likes my mask. Oh, whoops. I didn't really think about the design here. OK. I fixed the layout. So now it will tell me to put on my mask, if I am not wearing it. So here I am wearing my mask sitting at my computer. And I take it off, and it tells me to put in on. So this is great. Our convolutional neural network, there's no transfer learning. There's no base model. This was all done in the web browser, in p5.js, with ml5. Amazing. All right, before I go from this video, I want to just return to this model summary panel in the debug view, when you're training the model. So this is ordinarily the part that I maybe try to stay away from a little bit, the model architecture or the lower level details. But I think it's really important to make the connection between these layers and the diagrams that I showed you in the previous two videos about what is a convolutional neural network. And we can see right here the convolutional layers, the pooling layers, and then the flattening, the flat layer, and the final output label. And you can see that there's two outputs, because there is a probability confidence score for mask and one for no mask. So that would be a higher number if there are more categories. But this is the default architecture that ml5 will make when you say image classification. And it's actually possible for you to configure this yourself, to say how many convolutional layers you want, to say how big you want the kernel the filter kernel to be, how do you want to do pooling, what is your stride; all of those parameters that I mentioned in the previous videos are configurable here. So if this isn't working for you or you just want to play and experiment, let me show you how to do that. In the ml5js reference, you'll find a section under neural network for defining custom layers. And this is where you can actually configure individually the number of layers and what those layers do in an ml5 neural network. So this is the default set of layers for a classification, the default set of layers for a regression. And I want to look down here at this default image classification layers. All you have to do is create an array called layers, fill that array with objects that include the various details for each layer. Let me copy this to the clipboard, go back to my code, and right here in setup, right before options, I'm going to paste that in. So now I have a variable that's holding onto the custom layer configuration. And I'm going to go down to my ml5 neural network, and I'm going to add a property, layers, custom layers. So here, I have the input, dimensions, the task that I'm doing I want to debug it while I'm training it and now the custom configuration for the layers. And I could start to experiment with this. And you can see here in this array, my first layer is a convolutional layer. I want to have 8 filters, a kernel size of 5, and a strides of 1. I haven't talked too much about activation functions, and what's this kernel initializer. So these are I'll try to put some resources in the video's description where you can read up more about some of these other properties that you can experiment with. But again, this is the size of max pooling. What would happen if I did it 3 by 3, and change the number of strides along the x and yaxis? And you could see one thing. The thing that I think that's important for me to point out is there are two convolutional layers. And as the resolution is decreasing, the number of filters is increasing. Not a blanket rule, but that's one way to approach architecting your model. Trial and error is your friend here, experimentation, talking to somebody else who knows about convolutional neural networks or has tried it before to get some advice about what might work well for you in your particular scenario. I encourage you to experiment with that. Leave your feedback and things you've tried in the comments. And in the next video, I am going to look at a convolutional neural network that is trained off images of shapes squares, circles, and triangles; so that I could create something where maybe I'm drawing on a piece of paper, and the neural network guesses. Did I just draw a circle, a square, or a triangle? And then later, I'm also going to show you some pretrained conventional neural networks that are in ml5, like DoodleNet which is trained on a whole lot of drawings from the Google Quick Draw data set to recognize various kinds of doodles. And these are scenarios where using your own convolutional neural network really makes sense, because it's the kind of data that we're working with. Drawing and shapes and abstract geometry, isn't something that the original MobileNet model, image classification model, was trained on. So transfer learning doesn't necessarily apply. And also there are some reasons why you might want to train your model from scratch with only your own data, and you have a real control and understanding of how that data was collected, and how that model is being used, as opposed to a situation where you're doing transfer learning, and basing your model off of a pretrained model that you might not know as much about. OK, so I hope that next video won't take a year to come out. But who knows what's coming next in 2020 into 2021? I hope good things for you. And I will see you in a future ml5 video. If you're watching this in years into the future, well, it's a little bit of history for you, a history lesson. And I don't know what I'll be doing, or what you're doing, but I'm glad that you're here, and that I'm here with you in this virtual mediated way. All right. See you soon. Goodbye [MUSIC PLAYING]
