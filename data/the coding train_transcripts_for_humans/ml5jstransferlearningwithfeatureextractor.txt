With timestamps:

00:00 - [BELL RINGS]
00:00 - Hello.
00:01 - Welcome to another ml5.js video.
00:03 - Now I am really very
excited about this one.
00:06 - I have made three videos
so far at the moment
00:08 - at the time of this
recording, sort of Intro
00:11 - to ml5 and Machine Learning,
a video about doing image
00:14 - classification, and
a video about doing
00:16 - image classification with
real-time images coming in
00:19 - from a webcam.
00:20 - And both of those image
classification projects
00:22 - use a pre-trained
model called MobileNet.
00:24 - So just to pick off where we
last left off, here it is.
00:29 - This is the image
classification MobileNet
00:32 - with MobileNet example.
00:34 - You can see that I am,
today, a-- ah, come on.
00:36 - I want to be a snorkel.
00:37 - I really feel like a snorkel.
00:39 - I am kind of like an oboe.
00:40 - But we could see here, I think,
ah, if I get this ukulele
00:44 - and put it in here,
it's going to see
00:46 - it's an acoustic
guitar pretty quickly.
00:48 - Let me put this down over here.
00:49 - So here's the thing.
00:50 - We have determined
that this model is not
00:54 - good at recognizing
certain things.
00:55 - Like it cannot recognize
a train whistle.
00:58 - It thinks it's a
syringe or an oboe.
01:00 - It cannot recognize my
purple water bottle,
01:06 - sponsor of The
Coding Train water.
01:08 - It thinks it's a power
drill or a microphone.
01:10 - [SINGING] Hello.
01:12 - OK.
01:13 - So what if I forgot what I
was doing or why I'm here?
01:16 - What if I want to have this
example recognize things
01:22 - that I have here in this room
that it doesn't recognize?
01:25 - Could I train it
with my own data?
01:28 - So this brings up
so many questions,
01:31 - and there's so many different
paths we could go down.
01:33 - But the path I want to
go down in this video
01:36 - today is something
called Transfer Learning.
01:44 - So with the concept
of machine learning,
01:48 - I could have a massive database
of images and train a model,
01:52 - and label all those images,
and train a model based
01:55 - on that data, and
show it new images,
01:57 - and it'll tell me what's
inside those new images
01:59 - based on what it learned.
02:00 - But I am a person with no
massive database of images.
02:04 - So something that I could do
is use somebody else's model
02:08 - that already was trained with
a massive database of images
02:11 - and just kind of
say, eh, I'm going
02:12 - to use these images
on top of it.
02:13 - It's not a perfect solution,
but it is a quite powerful one
02:16 - that allows you to do certain
kinds of things very quickly.
02:18 - In fact, I just came
over here apparently
02:20 - to write Transfer Learning.
02:21 - There is a project that
I want to show you--
02:23 - let me just hit Back here--
02:25 - called Teachable Machine.
02:27 - So Teachable Machine is a
project made by various--
02:30 - a collaboration of many
different researchers
02:32 - at Google, led by the
Google Creative Lab.
02:35 - And I'm going to run
through this in a second.
02:38 - And basically, what
I'm going to do,
02:40 - I'm going to introduce
the idea of--
02:42 - show you Teachable
Machine, introduce
02:43 - the idea of transfer
learning, talk
02:45 - about how it works in
ml5 with MobileNet,
02:47 - we're going to take
a break, and then
02:49 - I'm going to come back and
actually make the code example.
02:52 - I'm going to make a
teachable machine.
02:55 - So let's first just
run this and see.
02:57 - So I'm going to
skip the tutorial,
02:59 - and I'm going to
just open this up.
03:02 - I'm going to zoom in
a little bit here.
03:03 - So I should say that
Teachable Machine
03:05 - is using a slightly different
algorithm behind the scenes
03:08 - than what I'm ultimately
going to implement.
03:10 - And I'll kind of talk
about maybe the differences
03:12 - between those after I kind
of get done with all this.
03:15 - But conceptually, it's
exactly the same thing.
03:17 - So right now, I can see there
are three categories-- green
03:21 - purple, orange.
03:22 - So what I'm going to do
is I'm going to say--
03:25 - so now I'm going to attempt
to train the teachable machine
03:28 - with my own images.
03:28 - So I'm going to step out of the
frame just for the time being.
03:32 - I'm going to say, Train Green.
03:33 - I'm holding this
down, so I'm kind of--
03:35 - I'm giving it lots of
examples of a ukulele
03:37 - at different kind of angles.
03:38 - Of course, it has
my arm in it too.
03:40 - That's a part of
what it's learning.
03:41 - Then I'm going to stop.
03:42 - I'm going to put the
ukulele down awkwardly.
03:44 - I'm going to grab
this train whistle,
03:48 - and I'm going to
hit Train Purple.
03:51 - I'm going to give it lots of
examples of the train whistle,
03:54 - and then I'm going to let go.
03:56 - And then let's do one more.
04:00 - Oh, I really should have
made this one purple.
04:02 - I'm going to train it
orange, because it is purple.
04:05 - This is my water bottle.
04:07 - So I'm going to train
it with the water
04:09 - bottle a bunch of times.
04:10 - I'm sort of in the frame.
04:11 - I'm going to get out of the
frame, and I'm going to let go.
04:13 - All right, so I
finished training
04:14 - this teachable machine.
04:16 - If I come into the
picture, it actually
04:18 - thinks I'm the water bottle.
04:19 - I'm orange.
04:20 - But look at this.
04:21 - Let me now show it the ukulele.
04:23 - Confidence-- 99%.
04:26 - Let me now show it
the train whistle.
04:31 - Confidence purple-- 99%.
04:33 - And now let me show
it the water bottle.
04:38 - Confidence orange, so you can
see this works quite well.
04:41 - If I stand in there, it's
kind of confused now.
04:44 - You see, I was standing in
a lot of the training images
04:47 - with the water
bottle, so it really
04:49 - thinks my train whistle
is the water bottle.
04:50 - If I'm not in the picture, it
knows it's a train whistle.
04:53 - So this is very important.
04:54 - You have to remember
the machine learning
04:57 - system is not learning anything
about these particular objects.
05:02 - It's learning about the
exact sets of pixels
05:04 - you're showing it to.
05:06 - So if I'm standing in the
background with the ukulele
05:08 - every time, and then I'm not
in the background anymore,
05:11 - it's going to be confused.
05:12 - We see this in some
machine learning models
05:15 - that if you hold something up,
no matter what you hold up,
05:17 - it always just says Cell
Phone, because there's so
05:19 - many training images of
people holding a cell phone,
05:21 - it thinks, well,
whatever you're holding,
05:23 - it's got to be a cell phone.
05:24 - So this is the idea, so the
wonderful thing about this
05:29 - is it's really fun.
05:30 - There's a really nice interface.
05:32 - It's designed really well.
05:33 - I can have it show different
GIFs, play different sounds,
05:37 - but I'm limited to
these three categories--
05:40 - green, purple, orange.
05:42 - And I want to be
able to do something
05:43 - like this in my own
interactive experiments.
05:46 - OK, so let's talk about
how does it even work.
05:48 - How does this even work?
05:50 - Before we get to the code,
let's talk about how it works.
05:52 - All right, so let's talk about
how image classification works.
05:57 - We said, or I said,
I think, there
06:00 - is something called MobileNet.
06:03 - This is the pre-trained
model that I've talked about.
06:06 - It has 1,000 image
classes, and it
06:09 - was trained on a database
of like 15 million images
06:12 - from a database called ImageNet.
06:16 - When we use it, we send
our own image unit.
06:20 - We send an image.
06:22 - Maybe it's from a webcam.
06:23 - Maybe it's a PNG.
06:24 - We send it into MobileNet,
and then MobileNet gives us
06:27 - back a label and a probability.
06:33 - So maybe it says something like,
cat, 90%, bird, 5%, clock, 5%.
06:43 - This is what it gives us back.
06:46 - So how are we going
to retrain this model?
06:50 - Well, here's the thing.
06:51 - There's a lot of stuff
going on in here,
06:55 - and in order to retrain it,
we need to kind of like peel
06:58 - it open a little bit.
07:00 - And the thing that we're
going to use to peel it open
07:02 - is something that's built
into the ml5 library called
07:06 - a feature extractor.
07:11 - OK, well, internally inside
of the MobileNet model,
07:14 - the MobileNet model is
running a neural network.
07:17 - You might have heard
that term before,
07:19 - and at some point in
this video series,
07:20 - we might get more into that.
07:22 - But a neural network is
something that has layers.
07:26 - It has multiple layers.
07:27 - Maybe it has Layer
1, Layer 2, Layer 3,
07:34 - and that data is actually
being passed into Layer 1.
07:37 - It's being processed, and
then sent into Layer 2.
07:40 - It's being processed again.
07:42 - It's being sent to the Layer 3.
07:43 - It's processed again.
07:45 - Those processes-- there are
different kinds of processes.
07:48 - For example, most likely, since
we're sending it image data,
07:51 - it's using something
called-- you might have heard
07:53 - this term-- a convolution.
07:55 - A convolution is actually
an image process,
07:57 - which is the same thing that
happens like in Photoshop
08:00 - or any kind of image
processing utility, where
08:02 - you open up an image and say,
hey, let's make it brighter.
08:04 - So a neural network is
doing that to the image.
08:07 - It's doing all of
these processes
08:09 - over and over and over again
to try to reduce the image.
08:11 - This thing's got a
lot of pixels in it.
08:12 - Let's process it
down to something
08:14 - smaller and down to
something smaller,
08:15 - and let's do that many,
many, many, many times
08:17 - over multiple layers to
eventually get to something,
08:22 - which we can call features.
08:24 - So if I say that the
last layer, after it
08:27 - does all these processes, is
something called features,
08:30 - and then those features,
which exist here,
08:34 - they are then sent into--
08:37 - they are then converted--
sorry-- through another layer
08:40 - into labels and probabilities.
08:43 - So there's this
whole process that's
08:44 - happening inside of MobileNet.
08:46 - The image comes in.
08:47 - It's processed through
a convolutional layer,
08:49 - maybe through another
convolutional layer,
08:51 - then through some other kind
of layer, blah, blah, blah,
08:53 - ends with all these num
features, which are really just
08:55 - numbers, a whole lot of numbers.
08:58 - And then those numbers are
processed one more time
09:00 - to get probabilities.
09:02 - Well, what if what transfer
learning is is it's,
09:05 - hey, let's just
delete this part.
09:09 - Let's go into MobileNet
and stop right here.
09:11 - Feature extractor-- let's make a
version of the MobileNet model,
09:15 - where we stop right here
at the feature extractor.
09:17 - And then we take those
features and tr-- we
09:21 - put our own training images in.
09:23 - We say, take this training
image of the ukulele.
09:27 - Send it all the way through.
09:28 - Don't bother to get the label.
09:29 - Don't get acoustic guitar.
09:31 - Just stop here, and
say, hey, you know what?
09:34 - These features, these
features are 100% ukulele.
09:39 - So we're going to retrain
the model to map--
09:42 - basically map of the features
to our own labels instead
09:46 - of the labels that
are previously
09:49 - existed in MobileNet.
09:50 - All right, so what
are the features?
09:52 - So here's the thing.
09:54 - In theory, we could-- this--
09:57 - I mean I think in some crazy
sort of theoretical sense,
10:01 - we could eliminate
all of these layers
10:03 - and just teach a machine to
learn that this set of pixels
10:07 - are a cat, and this set
of pixels are a bird.
10:09 - And I'll look at this
whole other set of pixels.
10:11 - Which set of pixels
does it resemble?
10:13 - That's kind of what we're doing,
but what you have to remember
10:17 - is that images have
so many pixels.
10:20 - And so when you have to
compare images pixel by pixel
10:24 - by pixel by pixel,
there's so much data.
10:26 - And so actually, this idea of
features-- the idea of features
10:30 - is boiling the
essence of an image
10:33 - down to a smaller set
of manageable numbers.
10:37 - So in essence, this
image, which might have
10:40 - started as like a 512 by 512--
10:43 - you do the math--
10:44 - 512 squared number
of pixels maybe
10:46 - ends up as 100 features,
just 100 numbers.
10:51 - I don't actually remember
what it is in MobileNet.
10:53 - We should look it up.
10:54 - And then those
numbers are typically
10:56 - just numbers between 0 and 1.
10:59 - This is also often
referred to as a vector,
11:02 - meaning a list of numbers.
11:03 - It's the essence-- it's the
numeric essence of that image
11:07 - that you just passed in.
11:08 - And it's been-- it's learned
these features over lots
11:12 - and lots of time of being
trained with millions
11:14 - and millions of images.
11:16 - So what we need to do in ml5 is
ml5 is a higher level library.
11:21 - We don't have to do all
this manipulation ourselves.
11:23 - We basically just say, hey,
instead of making an image
11:26 - classifier with
MobileNet, we're going
11:28 - to make a feature
extractor with MobileNet,
11:31 - and then turn that feature
extractor into a classifier,
11:35 - and train it with
our own images.
11:38 - So this is what I'm going
to do in the next video.
11:40 - I'm actually going to write
the code to do exactly this,
11:44 - and I'm going to train it with
a few sets of images in here.
11:46 - And actually, what
you'll see is what's
11:47 - interesting is you
can actually get
11:48 - it to work with things like
different facial expressions
11:51 - or different gestures.
11:52 - There's a lot of wonderful
possibilities there.
11:55 - [MUSIC PLAYING]

Cleaned transcript:

[BELL RINGS] Hello. Welcome to another ml5.js video. Now I am really very excited about this one. I have made three videos so far at the moment at the time of this recording, sort of Intro to ml5 and Machine Learning, a video about doing image classification, and a video about doing image classification with realtime images coming in from a webcam. And both of those image classification projects use a pretrained model called MobileNet. So just to pick off where we last left off, here it is. This is the image classification MobileNet with MobileNet example. You can see that I am, today, a ah, come on. I want to be a snorkel. I really feel like a snorkel. I am kind of like an oboe. But we could see here, I think, ah, if I get this ukulele and put it in here, it's going to see it's an acoustic guitar pretty quickly. Let me put this down over here. So here's the thing. We have determined that this model is not good at recognizing certain things. Like it cannot recognize a train whistle. It thinks it's a syringe or an oboe. It cannot recognize my purple water bottle, sponsor of The Coding Train water. It thinks it's a power drill or a microphone. [SINGING] Hello. OK. So what if I forgot what I was doing or why I'm here? What if I want to have this example recognize things that I have here in this room that it doesn't recognize? Could I train it with my own data? So this brings up so many questions, and there's so many different paths we could go down. But the path I want to go down in this video today is something called Transfer Learning. So with the concept of machine learning, I could have a massive database of images and train a model, and label all those images, and train a model based on that data, and show it new images, and it'll tell me what's inside those new images based on what it learned. But I am a person with no massive database of images. So something that I could do is use somebody else's model that already was trained with a massive database of images and just kind of say, eh, I'm going to use these images on top of it. It's not a perfect solution, but it is a quite powerful one that allows you to do certain kinds of things very quickly. In fact, I just came over here apparently to write Transfer Learning. There is a project that I want to show you let me just hit Back here called Teachable Machine. So Teachable Machine is a project made by various a collaboration of many different researchers at Google, led by the Google Creative Lab. And I'm going to run through this in a second. And basically, what I'm going to do, I'm going to introduce the idea of show you Teachable Machine, introduce the idea of transfer learning, talk about how it works in ml5 with MobileNet, we're going to take a break, and then I'm going to come back and actually make the code example. I'm going to make a teachable machine. So let's first just run this and see. So I'm going to skip the tutorial, and I'm going to just open this up. I'm going to zoom in a little bit here. So I should say that Teachable Machine is using a slightly different algorithm behind the scenes than what I'm ultimately going to implement. And I'll kind of talk about maybe the differences between those after I kind of get done with all this. But conceptually, it's exactly the same thing. So right now, I can see there are three categories green purple, orange. So what I'm going to do is I'm going to say so now I'm going to attempt to train the teachable machine with my own images. So I'm going to step out of the frame just for the time being. I'm going to say, Train Green. I'm holding this down, so I'm kind of I'm giving it lots of examples of a ukulele at different kind of angles. Of course, it has my arm in it too. That's a part of what it's learning. Then I'm going to stop. I'm going to put the ukulele down awkwardly. I'm going to grab this train whistle, and I'm going to hit Train Purple. I'm going to give it lots of examples of the train whistle, and then I'm going to let go. And then let's do one more. Oh, I really should have made this one purple. I'm going to train it orange, because it is purple. This is my water bottle. So I'm going to train it with the water bottle a bunch of times. I'm sort of in the frame. I'm going to get out of the frame, and I'm going to let go. All right, so I finished training this teachable machine. If I come into the picture, it actually thinks I'm the water bottle. I'm orange. But look at this. Let me now show it the ukulele. Confidence 99%. Let me now show it the train whistle. Confidence purple 99%. And now let me show it the water bottle. Confidence orange, so you can see this works quite well. If I stand in there, it's kind of confused now. You see, I was standing in a lot of the training images with the water bottle, so it really thinks my train whistle is the water bottle. If I'm not in the picture, it knows it's a train whistle. So this is very important. You have to remember the machine learning system is not learning anything about these particular objects. It's learning about the exact sets of pixels you're showing it to. So if I'm standing in the background with the ukulele every time, and then I'm not in the background anymore, it's going to be confused. We see this in some machine learning models that if you hold something up, no matter what you hold up, it always just says Cell Phone, because there's so many training images of people holding a cell phone, it thinks, well, whatever you're holding, it's got to be a cell phone. So this is the idea, so the wonderful thing about this is it's really fun. There's a really nice interface. It's designed really well. I can have it show different GIFs, play different sounds, but I'm limited to these three categories green, purple, orange. And I want to be able to do something like this in my own interactive experiments. OK, so let's talk about how does it even work. How does this even work? Before we get to the code, let's talk about how it works. All right, so let's talk about how image classification works. We said, or I said, I think, there is something called MobileNet. This is the pretrained model that I've talked about. It has 1,000 image classes, and it was trained on a database of like 15 million images from a database called ImageNet. When we use it, we send our own image unit. We send an image. Maybe it's from a webcam. Maybe it's a PNG. We send it into MobileNet, and then MobileNet gives us back a label and a probability. So maybe it says something like, cat, 90%, bird, 5%, clock, 5%. This is what it gives us back. So how are we going to retrain this model? Well, here's the thing. There's a lot of stuff going on in here, and in order to retrain it, we need to kind of like peel it open a little bit. And the thing that we're going to use to peel it open is something that's built into the ml5 library called a feature extractor. OK, well, internally inside of the MobileNet model, the MobileNet model is running a neural network. You might have heard that term before, and at some point in this video series, we might get more into that. But a neural network is something that has layers. It has multiple layers. Maybe it has Layer 1, Layer 2, Layer 3, and that data is actually being passed into Layer 1. It's being processed, and then sent into Layer 2. It's being processed again. It's being sent to the Layer 3. It's processed again. Those processes there are different kinds of processes. For example, most likely, since we're sending it image data, it's using something called you might have heard this term a convolution. A convolution is actually an image process, which is the same thing that happens like in Photoshop or any kind of image processing utility, where you open up an image and say, hey, let's make it brighter. So a neural network is doing that to the image. It's doing all of these processes over and over and over again to try to reduce the image. This thing's got a lot of pixels in it. Let's process it down to something smaller and down to something smaller, and let's do that many, many, many, many times over multiple layers to eventually get to something, which we can call features. So if I say that the last layer, after it does all these processes, is something called features, and then those features, which exist here, they are then sent into they are then converted sorry through another layer into labels and probabilities. So there's this whole process that's happening inside of MobileNet. The image comes in. It's processed through a convolutional layer, maybe through another convolutional layer, then through some other kind of layer, blah, blah, blah, ends with all these num features, which are really just numbers, a whole lot of numbers. And then those numbers are processed one more time to get probabilities. Well, what if what transfer learning is is it's, hey, let's just delete this part. Let's go into MobileNet and stop right here. Feature extractor let's make a version of the MobileNet model, where we stop right here at the feature extractor. And then we take those features and tr we put our own training images in. We say, take this training image of the ukulele. Send it all the way through. Don't bother to get the label. Don't get acoustic guitar. Just stop here, and say, hey, you know what? These features, these features are 100% ukulele. So we're going to retrain the model to map basically map of the features to our own labels instead of the labels that are previously existed in MobileNet. All right, so what are the features? So here's the thing. In theory, we could this I mean I think in some crazy sort of theoretical sense, we could eliminate all of these layers and just teach a machine to learn that this set of pixels are a cat, and this set of pixels are a bird. And I'll look at this whole other set of pixels. Which set of pixels does it resemble? That's kind of what we're doing, but what you have to remember is that images have so many pixels. And so when you have to compare images pixel by pixel by pixel by pixel, there's so much data. And so actually, this idea of features the idea of features is boiling the essence of an image down to a smaller set of manageable numbers. So in essence, this image, which might have started as like a 512 by 512 you do the math 512 squared number of pixels maybe ends up as 100 features, just 100 numbers. I don't actually remember what it is in MobileNet. We should look it up. And then those numbers are typically just numbers between 0 and 1. This is also often referred to as a vector, meaning a list of numbers. It's the essence it's the numeric essence of that image that you just passed in. And it's been it's learned these features over lots and lots of time of being trained with millions and millions of images. So what we need to do in ml5 is ml5 is a higher level library. We don't have to do all this manipulation ourselves. We basically just say, hey, instead of making an image classifier with MobileNet, we're going to make a feature extractor with MobileNet, and then turn that feature extractor into a classifier, and train it with our own images. So this is what I'm going to do in the next video. I'm actually going to write the code to do exactly this, and I'm going to train it with a few sets of images in here. And actually, what you'll see is what's interesting is you can actually get it to work with things like different facial expressions or different gestures. There's a lot of wonderful possibilities there. [MUSIC PLAYING]
