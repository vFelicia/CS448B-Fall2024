With timestamps:

00:00 - all right this is my third tensorflow
00:02 - chance video and I actually just booked
00:04 - oh I made this list so what have I done
00:07 - so far I have made an introduction to
00:09 - where tensorflow day is sort of fits
00:11 - into the kinds of stuff that I'm working
00:13 - on in thinking about I made an intro
00:16 - video to the idea of what a tensor is
00:19 - and how to make a tensor a variable that
00:21 - stores a tensor in your JavaScript code
00:24 - and now you're really what I want to do
00:26 - is I want to get down to my goal is to
00:29 - get to the point where I'm remaking
00:30 - earlier machine learning coding
00:32 - challenges I did but instead but using
00:35 - tensor flow j s as the basis for them
00:37 - the foundation for them so the things
00:39 - that I think that I need to mention and
00:41 - talk about before I get to that is I
00:43 - need to talk about variables and memory
00:45 - management there's something difference
00:48 - between a variable and a tensor there's
00:49 - they're kind of a similar thing but it
00:51 - has to do with memory management I won't
00:53 - talk about operations so mathematical
00:54 - operations that you can do on these
00:56 - tensors the layers API that's a really
00:59 - that's like the topic I'm the most
01:00 - excited about on this list and so once I
01:02 - can do three more a little quick general
01:05 - tensor flow dot J's videos which are by
01:07 - no means comprehensive as to everything
01:10 - that's in tensor flow js2 a much larger
01:12 - api than what I'm going to cover and
01:14 - also I don't actually know any of this
01:17 - stuff I'm just kind of figuring it out
01:18 - as I go
01:19 - so I'm trying to talk you through me
01:20 - learning it so just in case you're
01:22 - thinking you're watching a video by an
01:23 - expert and then and then I'm gonna get
01:27 - to these coding challenges and then I've
01:29 - teased this before but there's a new
01:32 - library called ml five which is built on
01:34 - top of tensor flow Jas which I'm going
01:36 - to eventually do a lot of tutorials and
01:37 - videos and hopefully bring some guests
01:39 - in also to do stuff with ml 5 ok so back
01:42 - to the computer where I left this off I
01:46 - think if we look at this code that I had
01:48 - is I created a tensor I use tensor 3d
01:53 - because I knew this shape was going to
01:55 - be a rank 3 having three dimensions yes
01:59 - and this is the shape I'm putting
02:01 - integers in it
02:02 - I'm console logging it and inside and we
02:05 - can see the result here now incidentally
02:07 - I think I should also note that if I
02:10 - actually want to look at the data
02:13 - just seeing it the string of it like
02:16 - printed out there then what I want to
02:18 - use is actually the data function oh wow
02:21 - I probably shouldn't have called mine
02:23 - data so let's call it a tensor probably
02:25 - shouldn't call it that either let's call
02:27 - it tense very tense and data now this is
02:31 - gonna be a problem I think let's see
02:33 - what happens here and this is this is
02:34 - gonna relate I'm gonna get to the point
02:36 - here you'll see you'll see look at this
02:39 - a promise oh that's so nice of tensor
02:43 - flow not just to promise me something
02:44 - what are you promising so here's the
02:46 - thing there's something weird going on
02:48 - here and this is actually really the
02:50 - topic of this with variables and memory
02:52 - management in that there something is
02:54 - happening here something is happening
02:56 - right here these values are stored in a
02:59 - plain old array that's in the computer's
03:02 - memory and when we make this tensor out
03:05 - of them that data gets copied on to the
03:08 - computers graphics processing unit the
03:12 - GPU that takes some time and we want to
03:15 - minimize the amount of times we have to
03:17 - copy memory back and forth so this is
03:19 - the thing we have to think about as we
03:20 - build more examples in code
03:22 - this here is doing this asynchronous
03:25 - this this here is doing this asynchrony
03:27 - now I wonder if I could give it a
03:28 - callback I'm just gonna experiment here
03:33 - I think they're only using this thing
03:35 - called promises which I made a video
03:37 - tutorial about this yeah there really
03:38 - need to so what happens if I give it a
03:43 - callback to see like oh can I can I give
03:45 - it a callback there oh wait so let's
03:48 - let's do this and see like oh let's give
03:51 - it a callback and then see if we can
03:52 - look at the stuff that comes out of the
03:54 - callback let's see what happens there no
03:56 - because I have too many parentheses yes
04:00 - too many parentheses no nothing happens
04:03 - there so it's not using and and I'll cos
04:07 - told me I could use oh wait but I all
04:08 - this there's so much yes six and Beyond
04:11 - JavaScript going on here
04:13 - so actually what I want to do is if I
04:17 - say tense dot data I can say then and
04:20 - then is this special function that's
04:22 - part of a promise where it's saying like
04:24 - once the promise has been resolved
04:26 - you've made a promise to me you're going
04:28 - to keep that promise
04:29 - and when that promise comes to roost
04:32 - cockadoodledoo then I can can I just
04:40 - write code in there no I have to make a
04:41 - function I can say console dot log stuff
04:50 - so how it works yeah look at that so
04:52 - there we go we can see this is one way
04:54 - but I actually don't want to deal with
04:56 - any of this so because I want to just so
04:59 - this is something that's going to come
05:00 - up let's try to avoid this and there's
05:04 - also a keyword called a weight that I
05:06 - could potentially use but luckily there
05:10 - is also a function called data sync so
05:13 - if I use the function data sync you're
05:18 - going to see this here what this will do
05:20 - is actually give me all the data back
05:22 - but without without the but-but-but-but
05:29 - block but wait wait for it to be done so
05:31 - now we should be able to say boy I
05:35 - really very tense so you can see here it
05:36 - is and then here is I notice it all came
05:39 - back as a one-dimensional array
05:41 - okay back to recap this is what the
05:44 - tensor is and this is this is the way
05:46 - that we're thinking about the tensor and
05:48 - how the data is stored and the print
05:50 - function allows us to see that very
05:51 - easily in the console if I want access
05:53 - to the data the data sync function will
05:55 - just give me all of that stuff
05:57 - ultimately just as a list of numbers
05:58 - because this idea of the different the
06:01 - shapes and the dimensions is really for
06:03 - you know us as human beings to think
06:05 - about it and store it but ultimately
06:06 - it's just a bunch of numbers so I should
06:08 - also mention that in addition to data
06:10 - sync there is also a get function so if
06:13 - I were to say tense dot get and I were
06:18 - to say 0 we would see I've got the
06:23 - number 80 which is right there and if I
06:25 - were to say 1 I've got the number 41 and
06:28 - if I were to say you know a 29 which
06:31 - would be
06:32 - last one I've got the number 47
06:35 - so get is another way I can start to
06:37 - pull that stuff out Data Sync data and I
06:40 - need to come back I really should add an
06:43 - addendum here which is that in order to
06:44 - support this material that I'm covering
06:46 - at some point I need to come back and
06:49 - make a video on promises and how the
06:53 - arrow syntax is often typically used
06:55 - with promises and this then function so
06:58 - these are something I need to come back
06:59 - and do some additional content on as
07:01 - well as a weight and a sync because if
07:05 - you want to work with this particular
07:08 - library how these new ways of handling
07:12 - synchronous and asynchronous things in
07:15 - JavaScript are kind of key foundational
07:17 - elements so I've got to come back and
07:18 - talk about this I would suspect that
07:20 - funfun function has some nice videos on
07:22 - these topics but I don't know so maybe
07:24 - I'll link to those in this video's
07:25 - description ok but the point of what I
07:29 - was saying is what if what I wanted to
07:31 - do right now is I wanted to change some
07:35 - of the numbers for example I wanted to
07:37 - do the equivalent of saying something
07:38 - like 10 set 0 to 10 in other words I'm
07:44 - sure this doesn't exist but what if what
07:46 - I wanted to do is say like oh whatever
07:48 - number is in the first spot in the
07:50 - tensor I want to change it to something
07:53 - else well I actually can't do that these
07:56 - tensors once you've created them are
07:59 - immutable and that means the values can
08:02 - never ever be changed this is different
08:05 - than this like Const declaration which
08:07 - just means you can't like reassign a
08:09 - variable name but you can might be able
08:11 - to like change the internal data of an
08:12 - object that's different these tensors
08:15 - absolutely cannot be changed so if you
08:19 - need to change the values and you might
08:21 - in a kind of learning system you're
08:23 - building right what if you're storing
08:24 - all the weights of a matrix in a tensor
08:27 - and you want to adjust those weights
08:29 - rather than copying into new tensor to
08:31 - new tensor this is where the concept of
08:33 - a TF dot variable comes in so I can say
08:39 - Const bar oh I shouldn't
08:42 - Const V tense oh boy this is getting
08:46 - really weird TF variable tense I can
08:50 - take a tensor and make it into a
08:53 - variable by passing it through TF dot
08:56 - variable so let's just look at this and
09:01 - sort of see what's there and we whoops
09:05 - us we don't want this set thing and you
09:10 - can see now this looks very similar it
09:12 - has a shape it has a data type but it's
09:15 - now stored differently this is because
09:18 - tensorflow
09:18 - dot j as a kind of low-level library
09:21 - managing web the data using the graphics
09:25 - card intensively really needs to do
09:28 - different things it has to manage the
09:30 - memory differently if the stuff is never
09:32 - going to change versus if it's going to
09:33 - change and so if I go now to tensorflow
09:35 - gjs and I look here at TF variable we
09:39 - can see this is now something that has a
09:43 - new parameter called trainable so in
09:46 - fact you can add this thing called an
09:48 - optimizer to it and adjust it so that's
09:49 - an important thing I wanted to mention
09:51 - and I'm just going to leave it at that
09:53 - for right now if I need to use a TF
09:55 - variable later we'll come back and look
09:57 - at when when I might want to use a TF
09:59 - variable versus just a TF tensor hi so
10:02 - I'm back actually with a weird edit
10:04 - point because I just went down the
10:05 - rabbit hole of trying to figure
10:07 - something out about memory management
10:08 - and I discovered that actually the
10:10 - memory management stuff makes a lot more
10:12 - sense to talk about after I've already
10:16 - looked at operations
10:18 - so let's actually I'm gonna switch this
10:20 - order here and right now in this video
10:23 - that you are watching I'm gonna talk
10:26 - about operations next and then maybe
10:28 - I'll move maybe I'll take a break and
10:29 - move on to memory management in the next
10:31 - video something like that okay so let me
10:33 - come back and talk about operations if I
10:35 - come back to the tension flow digest
10:39 - webpage there's actually a part of the
10:42 - sidebar here part of the API which is
10:45 - all about operations and when I to get
10:47 - my batteries that's what I mean
10:49 - mathematical operations that I want to
10:51 - perform on the tensor itself what if I
10:54 - want to double every number in the
10:55 - or I want to take two tensors and
10:59 - element-wise multiply every number by
11:01 - every other number what if I want to do
11:03 - matrix multiplication between those two
11:05 - tensors now this would probably merit a
11:07 - whole video series about linear algebra
11:10 - and matrix math luckily or unluckily for
11:13 - you I made a whole series about that
11:15 - already so you could pause here and go
11:18 - and watch that I would also refer you to
11:20 - the three blue one brown video series on
11:22 - linear algebra which is excellent so
11:24 - rather than get into the weeds of all of
11:26 - the mathematical pieces themselves I
11:29 - just want to kind of like look at a few
11:30 - and see how you would use these okay
11:33 - so let's for example say I want to use
11:35 - TF add so if I click on that I can see I
11:37 - add two tensors element-wise a plus B
11:41 - what does element-wise mean well what
11:44 - that means just to sort of recap is if I
11:47 - have two matrices a b c d and i have
11:53 - another one
11:54 - e f g h and i want to add them together
11:59 - and i want to see the results
12:02 - element-wise means a plus e b plus F
12:07 - right I just take them ones that are in
12:10 - the same spot and add them together so
12:12 - we could create we could do that right
12:14 - now in our code and I could say I'm
12:17 - gonna call this tensor a and I'm gonna
12:20 - this is a little bit silly but I'm just
12:22 - gonna make a second one that's with the
12:25 - same numbers in it obviously I'm more
12:27 - likely would have two tensors with two
12:30 - different values and then what I want to
12:32 - do is say TF wait how do I let me look
12:35 - at this Oh a add B sorry
12:38 - so what I want to do is I'm gonna say
12:39 - Const C equals a add B which by the way
12:43 - would be exactly the same as saying B
12:46 - dot a da in this case with other
12:48 - operations the order of the matrix the
12:50 - matrix the tensor so I shouldn't say
12:52 - matrice matrix matrices so that really
12:56 - could play a role so if I say a dot B
12:59 - and I were to say a print B print C
13:04 - print and I come back here and go in
13:09 - refresh we're going to see you know
13:12 - here's a here's B they're the same and
13:14 - then every number is doubled basically
13:16 - because I took a plus B so really
13:18 - obvious I should make two different
13:19 - random sets of numbers but this is this
13:21 - is how an operation works and depending
13:24 - on what you're doing you need different
13:27 - mathematical operations so as you can
13:30 - see there are a lot of things we could
13:32 - subtract multiply divide there's maximum
13:34 - minimum modulus power squared difference
13:36 - all need to be interesting to pursue and
13:37 - I might come back and do more videos
13:39 - about particular mathematical op
13:40 - operations but really I think just
13:43 - showing you ad hopefully now you could
13:44 - kind of look at the documentation and
13:46 - see what each one does I think it's
13:48 - worth at least doing there's a lots of
13:50 - other math functionality but I think
13:51 - it's worth looking at matrix
13:52 - multiplication TF gnat mall because this
13:56 - is a really key concept in building a
14:00 - neural network how to do this weighted
14:04 - sum of all of the inputs and all of the
14:06 - weights passed through you really need
14:07 - matrix multiplication if you go back and
14:09 - watch my other videos so let's look at
14:11 - this one so here we can see a dot mat
14:15 - mole so really it's just another
14:17 - function but we're gonna run this an
14:19 - interesting thing here what if I were to
14:22 - try and I don't need to print I'm not
14:25 - gonna bother printing a and B let's just
14:27 - try this right now so I'm going to write
14:29 - this would work a dot multiply and ul B
14:33 - because that's doing element wise a time
14:35 - you know in this case a times B times FC
14:38 - times G let's go back and I'm gonna hit
14:43 - refresh oh it worked so I guess the way
14:51 - okay of course it works I forgot I was
14:53 - doing L then what of course of course it
14:55 - worked I'm doing element wise
14:56 - multiplication I'm trying to demonstrate
14:58 - here that if I actually want to do
15:00 - matrix multiplication which is a totally
15:01 - different thing mat mole is that what
15:05 - was called now there we go oh I love
15:09 - this is like my favorite error this is
15:12 - very simple these are like kinds of
15:14 - areas are gonna run you all the time who
15:15 - saw this before air matmo inputs must be
15:18 - ranked two got ranks three and
15:21 - three when you do matrix multiplication
15:24 - the number of columns in the first
15:27 - matrix a has to match the number of rows
15:32 - in the second one and by the way you
15:38 - can't even have like a this is fully you
15:41 - can't have a rank three again your
15:42 - matrix month so this by the way this
15:44 - whole thing is flawed because this was
15:46 - only going to work
15:47 - so let's if I have a rank two mate
15:50 - tensor or basically a matrix a two
15:52 - dimensional array so now let's get a
15:56 - different error message now
15:59 - error in map so first of all matrix
16:02 - multiplication is only for matrices in
16:05 - two dimensions tensors in two dimensions
16:07 - and then the error here is the inner
16:09 - shapes three and five of the tensors
16:12 - with shapes five three and five three
16:13 - are in transpose so what's this error
16:16 - message about to do matrix
16:20 - multiplication the number of columns in
16:24 - the first one must match the number of
16:28 - rows in the second major so actually in
16:32 - here this would work this would actually
16:34 - work because these are two by two and so
16:37 - the columns here matches matches the
16:39 - rows here and again if you go back to my
16:43 - video on matrix multiplication you'll
16:45 - see why in more detail about this so but
16:47 - just to follow up here what I must do
16:50 - now in order to do Mitra's
16:52 - multiplication as I probably would have
16:54 - shape for a I could transpose one of
16:56 - them but let's do this first so I'm
16:58 - gonna do shape a and then I'm gonna do
17:00 - shape B this is a good time to cover
17:02 - transpose and shape B and now this
17:07 - should give me something there we go
17:09 - there's my new matrix and out of matrix
17:12 - multiplication if I am doing excuse me
17:15 - a five by three multiplied with the
17:17 - three by five I end up getting a five by
17:20 - five matrix which is correct now another
17:23 - way I could have done this by the way is
17:24 - if I use the same shape for both of them
17:29 - if I backed in here I should be able
17:32 - let's do this okay
17:33 - I should be able to say B dot now let's
17:36 - just see what happens if I say B dot
17:37 - transpose
17:40 - No so here's the thing remember these
17:43 - things are immutable
17:44 - so even transposing it I probably have
17:47 - to say constabie equals B transpose and
17:51 - then I can do a dot a dot Matt Moll BB
17:55 - and there we go
17:57 - so what does transpose do with a matrix
18:00 - transpose if I have a two by three
18:07 - matrix we'll take that matrix and
18:11 - transpose the numbers into a three by
18:13 - two matrix so again why do I I'm not
18:16 - doing anything of any value or meaning
18:17 - here if you go back you could probably
18:19 - go back as an exercise if you want an
18:21 - exercise right now go to my toy neural
18:24 - network look at nnj s go through all of
18:28 - the matrix maths that in that and see if
18:30 - you can rewrite that with the tensor
18:33 - flow the TF operations I don't know why
18:35 - you'd want to do that but if you want to
18:37 - just do everything you could possibly do
18:38 - in this world that's something you could
18:40 - try and maybe I'll try to publish
18:42 - something which is like an answer key to
18:43 - that I don't know you could ask me in
18:44 - the comments or somebody could make one
18:46 - I could post it all right so I think
18:48 - this is about where I want to stop right
18:50 - now again this is not meant to be
18:52 - comprehensive I just want to talk
18:54 - through what are the pieces here we know
18:57 - that there are tensors tensors are
19:00 - n-dimensional groups of numbers with we
19:04 - can also those are immutable they can
19:06 - never change if we need them to change
19:09 - we could use this thing called a
19:10 - variable maybe we need to see an actual
19:12 - scenario where we need that variable
19:14 - hopefully that'll come up in one of my
19:15 - videos in the future but you could try
19:17 - it yourself and then we can also perform
19:20 - operations we can perform operations
19:22 - like take these tensors that take this
19:24 - tensor and add this one or double this
19:26 - or find the maximum number in this one
19:28 - and there's plenty plenty more matrix
19:30 - multiplication so I encourage you to
19:31 - explore all of those and maybe I'll come
19:34 - back and go through some of them as we
19:35 - need them but I just want to give you an
19:37 - overview of what's there in the tensor
19:39 - flow jsapi itself so in the next video
19:42 - I'll be talking about something
19:43 - important because I have not been paying
19:45 - attention at all to how whenever I
19:48 - create a tensor I'm using memory in of
19:52 - the computer and sometimes I'm using
19:53 - maybe I'm using memory that's you know
19:55 - in the RAM sometimes using the GPU
19:56 - memory what's going on with all that so
19:59 - I want to in the next video specifically
20:01 - talk about memory management and how to
20:03 - make sure if I'm making all these
20:04 - tensors and doing all these operations
20:06 - how I how to make sure I avoid having a
20:09 - memory leak okay I'll see you in that
20:11 - video
20:16 - [Music]
20:20 - you

Cleaned transcript:

all right this is my third tensorflow chance video and I actually just booked oh I made this list so what have I done so far I have made an introduction to where tensorflow day is sort of fits into the kinds of stuff that I'm working on in thinking about I made an intro video to the idea of what a tensor is and how to make a tensor a variable that stores a tensor in your JavaScript code and now you're really what I want to do is I want to get down to my goal is to get to the point where I'm remaking earlier machine learning coding challenges I did but instead but using tensor flow j s as the basis for them the foundation for them so the things that I think that I need to mention and talk about before I get to that is I need to talk about variables and memory management there's something difference between a variable and a tensor there's they're kind of a similar thing but it has to do with memory management I won't talk about operations so mathematical operations that you can do on these tensors the layers API that's a really that's like the topic I'm the most excited about on this list and so once I can do three more a little quick general tensor flow dot J's videos which are by no means comprehensive as to everything that's in tensor flow js2 a much larger api than what I'm going to cover and also I don't actually know any of this stuff I'm just kind of figuring it out as I go so I'm trying to talk you through me learning it so just in case you're thinking you're watching a video by an expert and then and then I'm gonna get to these coding challenges and then I've teased this before but there's a new library called ml five which is built on top of tensor flow Jas which I'm going to eventually do a lot of tutorials and videos and hopefully bring some guests in also to do stuff with ml 5 ok so back to the computer where I left this off I think if we look at this code that I had is I created a tensor I use tensor 3d because I knew this shape was going to be a rank 3 having three dimensions yes and this is the shape I'm putting integers in it I'm console logging it and inside and we can see the result here now incidentally I think I should also note that if I actually want to look at the data just seeing it the string of it like printed out there then what I want to use is actually the data function oh wow I probably shouldn't have called mine data so let's call it a tensor probably shouldn't call it that either let's call it tense very tense and data now this is gonna be a problem I think let's see what happens here and this is this is gonna relate I'm gonna get to the point here you'll see you'll see look at this a promise oh that's so nice of tensor flow not just to promise me something what are you promising so here's the thing there's something weird going on here and this is actually really the topic of this with variables and memory management in that there something is happening here something is happening right here these values are stored in a plain old array that's in the computer's memory and when we make this tensor out of them that data gets copied on to the computers graphics processing unit the GPU that takes some time and we want to minimize the amount of times we have to copy memory back and forth so this is the thing we have to think about as we build more examples in code this here is doing this asynchronous this this here is doing this asynchrony now I wonder if I could give it a callback I'm just gonna experiment here I think they're only using this thing called promises which I made a video tutorial about this yeah there really need to so what happens if I give it a callback to see like oh can I can I give it a callback there oh wait so let's let's do this and see like oh let's give it a callback and then see if we can look at the stuff that comes out of the callback let's see what happens there no because I have too many parentheses yes too many parentheses no nothing happens there so it's not using and and I'll cos told me I could use oh wait but I all this there's so much yes six and Beyond JavaScript going on here so actually what I want to do is if I say tense dot data I can say then and then is this special function that's part of a promise where it's saying like once the promise has been resolved you've made a promise to me you're going to keep that promise and when that promise comes to roost cockadoodledoo then I can can I just write code in there no I have to make a function I can say console dot log stuff so how it works yeah look at that so there we go we can see this is one way but I actually don't want to deal with any of this so because I want to just so this is something that's going to come up let's try to avoid this and there's also a keyword called a weight that I could potentially use but luckily there is also a function called data sync so if I use the function data sync you're going to see this here what this will do is actually give me all the data back but without without the butbutbutbut block but wait wait for it to be done so now we should be able to say boy I really very tense so you can see here it is and then here is I notice it all came back as a onedimensional array okay back to recap this is what the tensor is and this is this is the way that we're thinking about the tensor and how the data is stored and the print function allows us to see that very easily in the console if I want access to the data the data sync function will just give me all of that stuff ultimately just as a list of numbers because this idea of the different the shapes and the dimensions is really for you know us as human beings to think about it and store it but ultimately it's just a bunch of numbers so I should also mention that in addition to data sync there is also a get function so if I were to say tense dot get and I were to say 0 we would see I've got the number 80 which is right there and if I were to say 1 I've got the number 41 and if I were to say you know a 29 which would be last one I've got the number 47 so get is another way I can start to pull that stuff out Data Sync data and I need to come back I really should add an addendum here which is that in order to support this material that I'm covering at some point I need to come back and make a video on promises and how the arrow syntax is often typically used with promises and this then function so these are something I need to come back and do some additional content on as well as a weight and a sync because if you want to work with this particular library how these new ways of handling synchronous and asynchronous things in JavaScript are kind of key foundational elements so I've got to come back and talk about this I would suspect that funfun function has some nice videos on these topics but I don't know so maybe I'll link to those in this video's description ok but the point of what I was saying is what if what I wanted to do right now is I wanted to change some of the numbers for example I wanted to do the equivalent of saying something like 10 set 0 to 10 in other words I'm sure this doesn't exist but what if what I wanted to do is say like oh whatever number is in the first spot in the tensor I want to change it to something else well I actually can't do that these tensors once you've created them are immutable and that means the values can never ever be changed this is different than this like Const declaration which just means you can't like reassign a variable name but you can might be able to like change the internal data of an object that's different these tensors absolutely cannot be changed so if you need to change the values and you might in a kind of learning system you're building right what if you're storing all the weights of a matrix in a tensor and you want to adjust those weights rather than copying into new tensor to new tensor this is where the concept of a TF dot variable comes in so I can say Const bar oh I shouldn't Const V tense oh boy this is getting really weird TF variable tense I can take a tensor and make it into a variable by passing it through TF dot variable so let's just look at this and sort of see what's there and we whoops us we don't want this set thing and you can see now this looks very similar it has a shape it has a data type but it's now stored differently this is because tensorflow dot j as a kind of lowlevel library managing web the data using the graphics card intensively really needs to do different things it has to manage the memory differently if the stuff is never going to change versus if it's going to change and so if I go now to tensorflow gjs and I look here at TF variable we can see this is now something that has a new parameter called trainable so in fact you can add this thing called an optimizer to it and adjust it so that's an important thing I wanted to mention and I'm just going to leave it at that for right now if I need to use a TF variable later we'll come back and look at when when I might want to use a TF variable versus just a TF tensor hi so I'm back actually with a weird edit point because I just went down the rabbit hole of trying to figure something out about memory management and I discovered that actually the memory management stuff makes a lot more sense to talk about after I've already looked at operations so let's actually I'm gonna switch this order here and right now in this video that you are watching I'm gonna talk about operations next and then maybe I'll move maybe I'll take a break and move on to memory management in the next video something like that okay so let me come back and talk about operations if I come back to the tension flow digest webpage there's actually a part of the sidebar here part of the API which is all about operations and when I to get my batteries that's what I mean mathematical operations that I want to perform on the tensor itself what if I want to double every number in the or I want to take two tensors and elementwise multiply every number by every other number what if I want to do matrix multiplication between those two tensors now this would probably merit a whole video series about linear algebra and matrix math luckily or unluckily for you I made a whole series about that already so you could pause here and go and watch that I would also refer you to the three blue one brown video series on linear algebra which is excellent so rather than get into the weeds of all of the mathematical pieces themselves I just want to kind of like look at a few and see how you would use these okay so let's for example say I want to use TF add so if I click on that I can see I add two tensors elementwise a plus B what does elementwise mean well what that means just to sort of recap is if I have two matrices a b c d and i have another one e f g h and i want to add them together and i want to see the results elementwise means a plus e b plus F right I just take them ones that are in the same spot and add them together so we could create we could do that right now in our code and I could say I'm gonna call this tensor a and I'm gonna this is a little bit silly but I'm just gonna make a second one that's with the same numbers in it obviously I'm more likely would have two tensors with two different values and then what I want to do is say TF wait how do I let me look at this Oh a add B sorry so what I want to do is I'm gonna say Const C equals a add B which by the way would be exactly the same as saying B dot a da in this case with other operations the order of the matrix the matrix the tensor so I shouldn't say matrice matrix matrices so that really could play a role so if I say a dot B and I were to say a print B print C print and I come back here and go in refresh we're going to see you know here's a here's B they're the same and then every number is doubled basically because I took a plus B so really obvious I should make two different random sets of numbers but this is this is how an operation works and depending on what you're doing you need different mathematical operations so as you can see there are a lot of things we could subtract multiply divide there's maximum minimum modulus power squared difference all need to be interesting to pursue and I might come back and do more videos about particular mathematical op operations but really I think just showing you ad hopefully now you could kind of look at the documentation and see what each one does I think it's worth at least doing there's a lots of other math functionality but I think it's worth looking at matrix multiplication TF gnat mall because this is a really key concept in building a neural network how to do this weighted sum of all of the inputs and all of the weights passed through you really need matrix multiplication if you go back and watch my other videos so let's look at this one so here we can see a dot mat mole so really it's just another function but we're gonna run this an interesting thing here what if I were to try and I don't need to print I'm not gonna bother printing a and B let's just try this right now so I'm going to write this would work a dot multiply and ul B because that's doing element wise a time you know in this case a times B times FC times G let's go back and I'm gonna hit refresh oh it worked so I guess the way okay of course it works I forgot I was doing L then what of course of course it worked I'm doing element wise multiplication I'm trying to demonstrate here that if I actually want to do matrix multiplication which is a totally different thing mat mole is that what was called now there we go oh I love this is like my favorite error this is very simple these are like kinds of areas are gonna run you all the time who saw this before air matmo inputs must be ranked two got ranks three and three when you do matrix multiplication the number of columns in the first matrix a has to match the number of rows in the second one and by the way you can't even have like a this is fully you can't have a rank three again your matrix month so this by the way this whole thing is flawed because this was only going to work so let's if I have a rank two mate tensor or basically a matrix a two dimensional array so now let's get a different error message now error in map so first of all matrix multiplication is only for matrices in two dimensions tensors in two dimensions and then the error here is the inner shapes three and five of the tensors with shapes five three and five three are in transpose so what's this error message about to do matrix multiplication the number of columns in the first one must match the number of rows in the second major so actually in here this would work this would actually work because these are two by two and so the columns here matches matches the rows here and again if you go back to my video on matrix multiplication you'll see why in more detail about this so but just to follow up here what I must do now in order to do Mitra's multiplication as I probably would have shape for a I could transpose one of them but let's do this first so I'm gonna do shape a and then I'm gonna do shape B this is a good time to cover transpose and shape B and now this should give me something there we go there's my new matrix and out of matrix multiplication if I am doing excuse me a five by three multiplied with the three by five I end up getting a five by five matrix which is correct now another way I could have done this by the way is if I use the same shape for both of them if I backed in here I should be able let's do this okay I should be able to say B dot now let's just see what happens if I say B dot transpose No so here's the thing remember these things are immutable so even transposing it I probably have to say constabie equals B transpose and then I can do a dot a dot Matt Moll BB and there we go so what does transpose do with a matrix transpose if I have a two by three matrix we'll take that matrix and transpose the numbers into a three by two matrix so again why do I I'm not doing anything of any value or meaning here if you go back you could probably go back as an exercise if you want an exercise right now go to my toy neural network look at nnj s go through all of the matrix maths that in that and see if you can rewrite that with the tensor flow the TF operations I don't know why you'd want to do that but if you want to just do everything you could possibly do in this world that's something you could try and maybe I'll try to publish something which is like an answer key to that I don't know you could ask me in the comments or somebody could make one I could post it all right so I think this is about where I want to stop right now again this is not meant to be comprehensive I just want to talk through what are the pieces here we know that there are tensors tensors are ndimensional groups of numbers with we can also those are immutable they can never change if we need them to change we could use this thing called a variable maybe we need to see an actual scenario where we need that variable hopefully that'll come up in one of my videos in the future but you could try it yourself and then we can also perform operations we can perform operations like take these tensors that take this tensor and add this one or double this or find the maximum number in this one and there's plenty plenty more matrix multiplication so I encourage you to explore all of those and maybe I'll come back and go through some of them as we need them but I just want to give you an overview of what's there in the tensor flow jsapi itself so in the next video I'll be talking about something important because I have not been paying attention at all to how whenever I create a tensor I'm using memory in of the computer and sometimes I'm using maybe I'm using memory that's you know in the RAM sometimes using the GPU memory what's going on with all that so I want to in the next video specifically talk about memory management and how to make sure if I'm making all these tensors and doing all these operations how I how to make sure I avoid having a memory leak okay I'll see you in that video you
