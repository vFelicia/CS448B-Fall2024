With timestamps:

00:00 - (bell dinging)
00:01 - - Hello, and welcome to
another Beginner's Guide
00:03 - to Machine Learning with
ml5.js in JavaScript.
00:06 - So I'm here.
00:07 - It's been a while since I
added a video to this playlist,
00:09 - and a bunch of things
00:10 - about the ml5 library itself have changed.
00:13 - There's a new release, 0.3.1.
00:17 - There is a brand new website,
00:18 - which you can find
right here at ml5js.org.
00:21 - So to some extent, this
video is really an update
00:24 - about the library, but
I'm also going to look
00:26 - at one particular feature,
00:28 - a new feature of the library,
sound classification.
00:31 - The machine learning
model that I'm gonna use
00:33 - in this video is the
Speech Command Recognizer,
00:36 - and this is a model available from Google
00:39 - as part of TensorFlow.js models.
00:42 - Now, so this is a really
important distinction.
00:44 - I am not here to train a sound classifier.
00:48 - I might do that in a future video
00:49 - and show you about how to
apply transfer learning,
00:52 - which is something I did
with images, also to sounds.
00:55 - I just gonna make use
of a freely available,
00:58 - pre-trained machine learning model.
01:00 - Anytime you use one of those things,
01:03 - even in just a playful
and experimental way,
01:05 - which is what I'm doing,
01:06 - it's good to do a little bit of research
01:07 - and take a look at like
well, how was this trained,
01:09 - what the data, what are the considerations
01:13 - around how the data was collected?
01:15 - And so I encourage you to
read through the read me
01:18 - here on GitHub and in particular,
01:21 - to click over and read the original paper
01:23 - about this speech commands model,
01:26 - and there you'll see, if you look,
01:28 - it talks about some of the datasets
01:30 - like Mozilla's Common Voice dataset,
01:32 - 500 hours from 20,000 different people,
01:35 - this LibriSpeech, 1,000
hours of read English speech.
01:40 - I don't know how to say this, TIDY DIGITS,
01:42 - TIDIGITS, T DIGITS,
25,000 digit sequences,
01:46 - which apparently was probably neat, right?
01:48 - It's just like hours
and hours of me reading
01:49 - this random number book
over and over again.
01:52 - But so I encourage you
to check out this paper,
01:54 - and you can also find code
for how to use this model
01:58 - at TensorFlow.js in the tfjs
models, GitHub repo itself.
02:03 - I also want to interrupt
this video for a second
02:05 - to talk about how the sound
classifier actually works.
02:08 - This is kind of a
surprising little tidbit,
02:10 - and I'll come back to this more
02:12 - if at some point I create a video
02:14 - about training your own sound classifier.
02:16 - Now, there different
ways you could do this.
02:17 - This isn't the way to
make a sound classifier,
02:20 - but this is the way that
this particular model works.
02:22 - It's actually shockingly,
02:24 - amazingly doing image classification.
02:26 - So if you image we have this thing
02:29 - that's called a
convolutional neural network.
02:32 - This is the underlying architecture,
02:34 - the structure of that
machine learning model
02:37 - that does the classification.
02:40 - Typically this kind of model is something
02:42 - that we would put images in.
02:44 - Like we might have images of cats.
02:46 - We might have an image of a turtle.
02:50 - That's not really turtle, but whatever.
02:51 - So the idea is that we're
sending these images in
02:54 - and getting back a label
02:57 - and maybe a confidence score.
02:59 - So it's the same idea.
03:00 - The only thing is now
we wanna send in audio
03:04 - and get back a label like up
03:06 - or one and a confidence score.
03:10 - So how would we convert
sound into an image?
03:14 - Now, again, there are other
neural network architectures
03:17 - which you could receive sound data
03:20 - in maybe a more direct fashion,
03:23 - but if you have ever looked
at a graphic equalizer
03:27 - or some type of sound
visualization system,
03:30 - I've made examples like this in p5,
03:32 - you can draw something
that's often referred
03:34 - to as the spectrogram,
03:36 - which is basically a graph
of all the various amplitudes
03:41 - of frequencies, the wave
patterns of the sound itself.
03:45 - So if we took a one second spectrogram
03:50 - and made that into an image,
03:52 - we could then send that image
03:53 - into a convolutional neural network
03:55 - saying that's the image that is produced
03:58 - from the spectrogram of
somebody saying the word, up.
04:02 - So underneath the hood, this
machine learning system,
04:05 - even though it's designed
to work with audio data,
04:08 - it first takes that audio data,
04:10 - converts it into an image
04:11 - and then sends it through
a very similar types
04:14 - of neural network architecture
04:16 - to standard image classification models.
04:18 - And you can read more about
that in that paper itself.
04:22 - However, I'm gonna show you
how to access this model
04:26 - in a quick way with the ml5 library.
04:29 - And this is the new as of
today, which is I dunno.
04:33 - What's today's date?
04:34 - June 13th, 2019 (laughing).
04:38 - I'm gonna show you how to
use this with the ml5 library
04:40 - as it stands today.
04:41 - So I'm gonna click here under reference.
04:43 - One thing you should see,
there's a lot of new features
04:45 - have been added to the ml5 library.
04:47 - I'm gonna come back and do
videos about more of those,
04:49 - but the one I wanna highlight
is sound classifier.
04:52 - So I'm gonna click on this,
04:52 - and for all of the different
functions available in ml5,
04:57 - you'll find a documentation page
04:59 - with some narrative documentation,
05:01 - a little bit of a code snippet
05:02 - and then some written documentation
05:04 - about what the function names are
05:06 - and the various parameters
and things like that.
05:08 - And by the way, I'm
noticing now (laughing).
05:11 - This will hopefully not read.
05:12 - This is like a mistake (laughing).
05:14 - This is documentation that's actually
05:16 - for either Body-Pix or
maybe the U-Net model,
05:19 - which does something
called image segmentation.
05:21 - So we gotta get that fixed.
05:22 - I'm sure many GitHub issues and fixes
05:25 - will be out and done by
the time you see this.
05:27 - So in case you've forgotten
how to use the ml5 library,
05:30 - I'm just gonna show you as it's documented
05:32 - on the ml5 webpage.
05:34 - So first of all, you can
go here to this Quickstart.
05:36 - You can actually just click on this
05:38 - open p5 web editor
sketch with ml5js added.
05:41 - You know what, I'm gonna so that.
05:42 - That's the way I'm gonna do it.
05:44 - But you also could just put a
script tag in your HTML page
05:49 - referencing the current
version of the library,
05:50 - which, as I said, is 0.3.1 as of today,
05:53 - but probably while you're watching it,
05:54 - it will be a higher number.
05:56 - So lemme go and just
open up this link here,
06:00 - and now I'm in the p5 web editor.
06:04 - You could see the name of the
sketch is ml5js boilerplate.
06:08 - Thank you, Joey Lee who's
a contributor to ml5.
06:10 - He's done a ton of work on the website
06:12 - and all of the different features.
06:14 - And oh, this should actually be 3.1.
06:17 - I'm gonna fix that, uh-huh.
06:19 - I'm gonna hit save, and
then I'm gonna rename it
06:22 - to sound classifier.
06:26 - And I am going to then go over here
06:29 - and go to sketch.js,
06:30 - and I'm then I'm gonna run this,
06:32 - and we should see.
06:33 - There we go.
06:33 - So now we know it's working
06:35 - because there's a little console log
06:36 - to log ml5.version.
06:38 - If I hadn't imported the ml5 library,
06:41 - I wouldn't see that, and we see that here.
06:43 - So, what are we gonna do?
06:44 - Let's load the sound classifier.
06:47 - Now, most of the models,
I haven't been using this
06:50 - in my previous videos,
06:51 - most of the models in ml5 are
now actually available to you
06:54 - in preload, meaning you don't
need a callback function.
06:57 - You can just load the model in preload,
06:59 - and it'll be ready by the
time you get to setup.
07:01 - So I'm gonna make a variable
called soundClassifier.
07:05 - In preload, I'm gonna say soundClassifier
07:08 - equals ml5.soundClassifier.
07:11 - Now, I need to tell it
07:12 - what model I want to load.
07:15 - So I need to, in here, put the name
07:18 - of the model I wanna load,
07:19 - and in theory, in the future,
07:21 - there might be a bunch
of different options,
07:23 - different kinds of sound classifiers
07:24 - or maybe a sound classifier
you've trained yourself
07:27 - that you wanna put in there,
07:28 - and I'll come back eventually,
07:30 - show you videos about how to do that.
07:32 - But for right now, I'm just gonna say
07:33 - SpeechCommands,
07:35 - and then I already forgot
what it was called.
07:37 - So I'm gonna go back to the
ml5 website, which is here.
07:40 - I'm gonna go to reference.
07:41 - I'm gonna go to soundClassifier,
07:43 - and I'm looking for it here.
07:44 - So it's SpeechCommands18w.
07:47 - This is a particular model
07:49 - that's been trained on 18 specific words,
07:51 - and you can see what those are.
07:53 - The 10 digits from zero to nine,
07:56 - up, down, left, right, go,
stop, yes, no, that's 18.
08:00 - 10 digits, eight different words.
08:03 - All right, so now I'm gonna go,
08:05 - so it was 18w,
08:08 - and then, once that model is loaded,
08:11 - I need a callback.
08:13 - So I could just say
soundClassifier.Classify,
08:18 - and I might just call it gotResults.
08:20 - So in other words, I'm.
08:21 - Oh, it's not defined, right?
08:22 - So I'm telling the sound
classifier to classify.
08:25 - Now, by default, it's just going to listen
08:28 - to the microphone's audio.
08:30 - Maybe in the future, part
of ml5 will be able to offer
08:33 - hooks to how you can, to connect it
08:35 - to a different audio source,
08:36 - but it's basically just gonna work
08:38 - with the microphone's audio.
08:40 - Then I can write a
function called gotResults,
08:46 - and I'm gonna get rid of the draw loop
08:48 - 'cause I don't need that right now.
08:49 - Lemme just turn off auto refresh
08:51 - so that it doesn't keep refreshing.
08:55 - And then now, if you remember,
08:57 - ml5 employs error first callbacks,
09:01 - meaning the callback function
requires two arguments,
09:05 - an error argument in case
something went wrong,
09:08 - and a data or results
or some other argument
09:10 - where the actual stuff is.
09:12 - So I'm gonna say error,
09:13 - and then I'm gonna say results.
09:15 - And then I could do a little
like basic error handling.
09:17 - I'm just gonna say console.log
09:22 - something went wrong,
09:24 - and then I can also actually
log the error, all right.
09:31 - And then, so now,
09:34 - and then I'm gonna say
console.log(results).
09:38 - So let's see if we get anything.
09:40 - Oh, I have to run it again.
09:43 - And you could ignore this error.
09:44 - Oh, (gasping) something came in!
09:47 - Ready?
09:49 - Up.
09:51 - I just wanna stop and mention
09:52 - that if you're following this along,
09:54 - hopefully your browser
is asking for permission
09:56 - to use the microphone.
09:58 - The reason why that didn't
happen here in this video
10:00 - is because I've already set my browser
10:03 - to allow use of the microphone
on the p5 Web Editor pages,
10:07 - but for security, you can't
just access anybody's microphone
10:10 - from a webpage without the
user giving permission.
10:12 - So hopefully you saw that happen,
10:14 - and if you you didn't,
10:15 - that might be why you run into an error
10:17 - if you haven't given that permission.
10:19 - This is getting a little hard to debug
10:21 - just because so much
stuff is happening here
10:22 - on the console and this huge arrays,
10:24 - but there's actually
something that I missed
10:26 - that I could add here, which
is an options variable.
10:29 - So one of the things I could tell,
10:31 - there's a lot of things
I can set as properties
10:33 - or parameters for how the
sound classifier should work,
10:36 - but there's a very simple one,
10:38 - which I'm gonna just look
it up in the documentation
10:40 - 'cause I don't remember.
10:42 - It's called the probabilityThreshold.
10:44 - I'm actually just gonna
copy-paste this here.
10:46 - What this means is basically
10:49 - the sound classifier is
going to trigger an event.
10:53 - Right now I'm console logging
all of this information
10:56 - about what it thinks it heard
10:58 - based on a confidence
level for how sure it is
11:01 - it heard one of those keywords.
11:02 - And right now, a lot of
those events are triggering
11:05 - because I don't know
11:06 - what the default probability threshold is.
11:08 - Maybe it was .7.
11:09 - Maybe it's .5, but I'm
gonna make that really high.
11:12 - I'm gonna say .95.
11:13 - So it has to have a 95%,
11:16 - the machine learning model
has to calculate a 95%
11:20 - confidence score before it
11:22 - gives the event back to me in ml5.
11:25 - Once I've created that
options variable with .95,
11:27 - I need to pass it into the constructor
11:30 - as the second argument.
11:31 - So now we pass it in there.
11:33 - I'm gonna run the sketch.
11:34 - I'm gonna say the keyword up,
11:36 - and then I'm gonna try
to look into the console
11:38 - to see if that's what came in.
11:42 - Up.
11:48 - And there we go.
11:49 - Look at that!
11:50 - Now other stuff is coming
in, but you saw it there!
11:53 - So rather than kind of
debug with the console,
11:55 - let me actually put what I said
11:58 - onto the webpage itself.
12:00 - Also, to make this easier to see,
12:02 - let me actually console.log(results
index zero label
12:06 - and results index zero,
12:08 - I believe it's called confidence.
12:09 - So rather than have this big
array logging in the console,
12:13 - let me do this.
12:14 - All right, we need to
have a 95% confidence,
12:17 - and I'm gonna run this.
12:20 - Up.
12:23 - Three, four, five,
12:27 - six, seven, eight.
12:30 - I'm quickly adding background color white
12:33 - to the HTML body
12:36 - because what I wanna
then do, just quickly,
12:38 - before I finish this off,
but to finish this off,
12:40 - let me just add a DOM element
using the p5 DOM library.
12:43 - I'm gonna just say resultP
for results paragraph.
12:47 - I'm gonna say resultP equals createP
12:54 - waiting, and then,
right now, I'm gonna say
12:59 - resultP.html.
13:02 - Then I could turn these
results into a string
13:04 - by using a string literal.
13:05 - So back tick and then put curly brackets.
13:09 - Put a colon here and curly brackets
13:12 - and a closed back tick, okay.
13:14 - And let me also say resultP.style,
13:20 - is it font size, font-size,
13:23 - just 32 point so we'll be able to see it.
13:26 - All right, here we go.
13:28 - Ready for this?
13:32 - One, two, five,
13:36 - up, down, left, right.
13:42 - Okay, so (clapping),
13:43 - you could imagine now what
you could do with this.
13:46 - For example, you could control
a game with your voice.
13:49 - And in fact, I'm gonna do that
13:51 - in one of my coding challenge videos.
13:52 - So take a look in this
video's description.
13:54 - I'm gonna do a coding
challenge where I program
13:56 - the Google Dinosaur game,
13:57 - and then I'm gonna add
this sound classifier
14:00 - to have the dinosaur jump
14:01 - except it won't be a dinosaur.
14:02 - It'll be a unicorn,
14:03 - to have the unicorn jump
when I say the keyword, up.
14:06 - All right, thanks for watching
14:07 - this additional ml5 tutorial video
14:09 - about sound classification in the browser.
14:12 - (bell dinging)
14:13 - (energetic dance music)
14:21 - (bell dinging)

Cleaned transcript:

(bell dinging) Hello, and welcome to another Beginner's Guide to Machine Learning with ml5.js in JavaScript. So I'm here. It's been a while since I added a video to this playlist, and a bunch of things about the ml5 library itself have changed. There's a new release, 0.3.1. There is a brand new website, which you can find right here at ml5js.org. So to some extent, this video is really an update about the library, but I'm also going to look at one particular feature, a new feature of the library, sound classification. The machine learning model that I'm gonna use in this video is the Speech Command Recognizer, and this is a model available from Google as part of TensorFlow.js models. Now, so this is a really important distinction. I am not here to train a sound classifier. I might do that in a future video and show you about how to apply transfer learning, which is something I did with images, also to sounds. I just gonna make use of a freely available, pretrained machine learning model. Anytime you use one of those things, even in just a playful and experimental way, which is what I'm doing, it's good to do a little bit of research and take a look at like well, how was this trained, what the data, what are the considerations around how the data was collected? And so I encourage you to read through the read me here on GitHub and in particular, to click over and read the original paper about this speech commands model, and there you'll see, if you look, it talks about some of the datasets like Mozilla's Common Voice dataset, 500 hours from 20,000 different people, this LibriSpeech, 1,000 hours of read English speech. I don't know how to say this, TIDY DIGITS, TIDIGITS, T DIGITS, 25,000 digit sequences, which apparently was probably neat, right? It's just like hours and hours of me reading this random number book over and over again. But so I encourage you to check out this paper, and you can also find code for how to use this model at TensorFlow.js in the tfjs models, GitHub repo itself. I also want to interrupt this video for a second to talk about how the sound classifier actually works. This is kind of a surprising little tidbit, and I'll come back to this more if at some point I create a video about training your own sound classifier. Now, there different ways you could do this. This isn't the way to make a sound classifier, but this is the way that this particular model works. It's actually shockingly, amazingly doing image classification. So if you image we have this thing that's called a convolutional neural network. This is the underlying architecture, the structure of that machine learning model that does the classification. Typically this kind of model is something that we would put images in. Like we might have images of cats. We might have an image of a turtle. That's not really turtle, but whatever. So the idea is that we're sending these images in and getting back a label and maybe a confidence score. So it's the same idea. The only thing is now we wanna send in audio and get back a label like up or one and a confidence score. So how would we convert sound into an image? Now, again, there are other neural network architectures which you could receive sound data in maybe a more direct fashion, but if you have ever looked at a graphic equalizer or some type of sound visualization system, I've made examples like this in p5, you can draw something that's often referred to as the spectrogram, which is basically a graph of all the various amplitudes of frequencies, the wave patterns of the sound itself. So if we took a one second spectrogram and made that into an image, we could then send that image into a convolutional neural network saying that's the image that is produced from the spectrogram of somebody saying the word, up. So underneath the hood, this machine learning system, even though it's designed to work with audio data, it first takes that audio data, converts it into an image and then sends it through a very similar types of neural network architecture to standard image classification models. And you can read more about that in that paper itself. However, I'm gonna show you how to access this model in a quick way with the ml5 library. And this is the new as of today, which is I dunno. What's today's date? June 13th, 2019 (laughing). I'm gonna show you how to use this with the ml5 library as it stands today. So I'm gonna click here under reference. One thing you should see, there's a lot of new features have been added to the ml5 library. I'm gonna come back and do videos about more of those, but the one I wanna highlight is sound classifier. So I'm gonna click on this, and for all of the different functions available in ml5, you'll find a documentation page with some narrative documentation, a little bit of a code snippet and then some written documentation about what the function names are and the various parameters and things like that. And by the way, I'm noticing now (laughing). This will hopefully not read. This is like a mistake (laughing). This is documentation that's actually for either BodyPix or maybe the UNet model, which does something called image segmentation. So we gotta get that fixed. I'm sure many GitHub issues and fixes will be out and done by the time you see this. So in case you've forgotten how to use the ml5 library, I'm just gonna show you as it's documented on the ml5 webpage. So first of all, you can go here to this Quickstart. You can actually just click on this open p5 web editor sketch with ml5js added. You know what, I'm gonna so that. That's the way I'm gonna do it. But you also could just put a script tag in your HTML page referencing the current version of the library, which, as I said, is 0.3.1 as of today, but probably while you're watching it, it will be a higher number. So lemme go and just open up this link here, and now I'm in the p5 web editor. You could see the name of the sketch is ml5js boilerplate. Thank you, Joey Lee who's a contributor to ml5. He's done a ton of work on the website and all of the different features. And oh, this should actually be 3.1. I'm gonna fix that, uhhuh. I'm gonna hit save, and then I'm gonna rename it to sound classifier. And I am going to then go over here and go to sketch.js, and I'm then I'm gonna run this, and we should see. There we go. So now we know it's working because there's a little console log to log ml5.version. If I hadn't imported the ml5 library, I wouldn't see that, and we see that here. So, what are we gonna do? Let's load the sound classifier. Now, most of the models, I haven't been using this in my previous videos, most of the models in ml5 are now actually available to you in preload, meaning you don't need a callback function. You can just load the model in preload, and it'll be ready by the time you get to setup. So I'm gonna make a variable called soundClassifier. In preload, I'm gonna say soundClassifier equals ml5.soundClassifier. Now, I need to tell it what model I want to load. So I need to, in here, put the name of the model I wanna load, and in theory, in the future, there might be a bunch of different options, different kinds of sound classifiers or maybe a sound classifier you've trained yourself that you wanna put in there, and I'll come back eventually, show you videos about how to do that. But for right now, I'm just gonna say SpeechCommands, and then I already forgot what it was called. So I'm gonna go back to the ml5 website, which is here. I'm gonna go to reference. I'm gonna go to soundClassifier, and I'm looking for it here. So it's SpeechCommands18w. This is a particular model that's been trained on 18 specific words, and you can see what those are. The 10 digits from zero to nine, up, down, left, right, go, stop, yes, no, that's 18. 10 digits, eight different words. All right, so now I'm gonna go, so it was 18w, and then, once that model is loaded, I need a callback. So I could just say soundClassifier.Classify, and I might just call it gotResults. So in other words, I'm. Oh, it's not defined, right? So I'm telling the sound classifier to classify. Now, by default, it's just going to listen to the microphone's audio. Maybe in the future, part of ml5 will be able to offer hooks to how you can, to connect it to a different audio source, but it's basically just gonna work with the microphone's audio. Then I can write a function called gotResults, and I'm gonna get rid of the draw loop 'cause I don't need that right now. Lemme just turn off auto refresh so that it doesn't keep refreshing. And then now, if you remember, ml5 employs error first callbacks, meaning the callback function requires two arguments, an error argument in case something went wrong, and a data or results or some other argument where the actual stuff is. So I'm gonna say error, and then I'm gonna say results. And then I could do a little like basic error handling. I'm just gonna say console.log something went wrong, and then I can also actually log the error, all right. And then, so now, and then I'm gonna say console.log(results). So let's see if we get anything. Oh, I have to run it again. And you could ignore this error. Oh, (gasping) something came in! Ready? Up. I just wanna stop and mention that if you're following this along, hopefully your browser is asking for permission to use the microphone. The reason why that didn't happen here in this video is because I've already set my browser to allow use of the microphone on the p5 Web Editor pages, but for security, you can't just access anybody's microphone from a webpage without the user giving permission. So hopefully you saw that happen, and if you you didn't, that might be why you run into an error if you haven't given that permission. This is getting a little hard to debug just because so much stuff is happening here on the console and this huge arrays, but there's actually something that I missed that I could add here, which is an options variable. So one of the things I could tell, there's a lot of things I can set as properties or parameters for how the sound classifier should work, but there's a very simple one, which I'm gonna just look it up in the documentation 'cause I don't remember. It's called the probabilityThreshold. I'm actually just gonna copypaste this here. What this means is basically the sound classifier is going to trigger an event. Right now I'm console logging all of this information about what it thinks it heard based on a confidence level for how sure it is it heard one of those keywords. And right now, a lot of those events are triggering because I don't know what the default probability threshold is. Maybe it was .7. Maybe it's .5, but I'm gonna make that really high. I'm gonna say .95. So it has to have a 95%, the machine learning model has to calculate a 95% confidence score before it gives the event back to me in ml5. Once I've created that options variable with .95, I need to pass it into the constructor as the second argument. So now we pass it in there. I'm gonna run the sketch. I'm gonna say the keyword up, and then I'm gonna try to look into the console to see if that's what came in. Up. And there we go. Look at that! Now other stuff is coming in, but you saw it there! So rather than kind of debug with the console, let me actually put what I said onto the webpage itself. Also, to make this easier to see, let me actually console.log(results index zero label and results index zero, I believe it's called confidence. So rather than have this big array logging in the console, let me do this. All right, we need to have a 95% confidence, and I'm gonna run this. Up. Three, four, five, six, seven, eight. I'm quickly adding background color white to the HTML body because what I wanna then do, just quickly, before I finish this off, but to finish this off, let me just add a DOM element using the p5 DOM library. I'm gonna just say resultP for results paragraph. I'm gonna say resultP equals createP waiting, and then, right now, I'm gonna say resultP.html. Then I could turn these results into a string by using a string literal. So back tick and then put curly brackets. Put a colon here and curly brackets and a closed back tick, okay. And let me also say resultP.style, is it font size, fontsize, just 32 point so we'll be able to see it. All right, here we go. Ready for this? One, two, five, up, down, left, right. Okay, so (clapping), you could imagine now what you could do with this. For example, you could control a game with your voice. And in fact, I'm gonna do that in one of my coding challenge videos. So take a look in this video's description. I'm gonna do a coding challenge where I program the Google Dinosaur game, and then I'm gonna add this sound classifier to have the dinosaur jump except it won't be a dinosaur. It'll be a unicorn, to have the unicorn jump when I say the keyword, up. All right, thanks for watching this additional ml5 tutorial video about sound classification in the browser. (bell dinging) (energetic dance music) (bell dinging)
