With timestamps:

00:00 - oh hello good afternoon welcome to part
00:04 - two of today's coding training lives
00:07 - dream Internet coding show episode thing
00:13 - my name is Dan and I will be here with
00:17 - you for the next approximately one hour
00:19 - and thirty minutes I don't have the
00:22 - YouTube chat going I just realized so
00:24 - let me see if I can pull that up all
00:27 - right hmm I don't see anybody say
00:31 - anything like you see me and you hear me
00:33 - but I assume that live streaming so
00:36 - what's going to happen so first of all
00:37 - welcome I think I already said that
00:39 - okay yes see the third people in the
00:41 - chat this morning I spent some time
00:48 - talking about my new favorite subject
00:52 - linting universal sign for lifting up my
00:57 - phone is okay notification YouTube
01:00 - notification YouTube notification watch
01:05 - now the coding train is notification I
01:08 - probably always get that I don't notice
01:10 - so this morning I talked to Bill about
01:13 - linting which is kind of new I did a
01:15 - little video about github not github
01:19 - sorry get remotes one of which can be a
01:22 - github repository and you're gonna have
01:24 - multiple different the same repository
01:27 - as part of different accounts on github
01:30 - as different remotes boy that was like
01:32 - it was like a thrilling topic that crowd
01:35 - was in front of me they were standing
01:37 - they were cheering was suspenseful they
01:39 - were crying or tears and now it was me
01:42 - with the terminal typing like git remote
01:44 - add but someday we can all dream
01:47 - can't we that I'll be here with my
01:50 - tuxedo and the masses out there will be
01:54 - with their laptops coding along tears
01:56 - because they're so moved by the coding
01:59 - train no no yeah very unlikely
02:03 - now I'm really not sure I think I better
02:08 - stick to what I said I was gonna do
02:10 - because I I know that'll be
02:12 - disappointing but
02:13 - I'm a little bit concerned with the
02:14 - smallish amount of time that I have so
02:16 - I'd like to just get started but let me
02:20 - make an announcement
02:27 - there will be no this is the bad news
02:29 - there will be no coding train next
02:32 - Friday March 16 however the good news is
02:44 - there will be a coding train next
02:49 - Wednesday March 14th which as you may
02:55 - know is pi day 314 so I'm gonna be out
03:00 - of town on Friday
03:01 - I'm not available scratch that I'm just
03:06 - not I'm not gonna be available right hey
03:08 - edit that out never mind good dark fun
03:11 - Friday I'm not available on Friday but I
03:18 - will be yes but I will be on Wednesday
03:24 - and since it's pi day I thought what I
03:25 - would do is do some PI themed coding
03:28 - challenges I was hoping that maybe I
03:30 - could also have somebody I'll come here
03:32 - with like an actual PI and it could open
03:34 - the door and I could get like this one
03:35 - of the pie in the face thing anybody
03:37 - knows a way for me to make that happen
03:38 - I don't think that'll be an excellent
03:40 - thing to do in a livestream on pi day
03:42 - but I'm looking for what I'm looking for
03:45 - our suggestions for things that are sip
03:49 - like phyllotaxis is a good good example
03:52 - of that of like it's a little algorithm
03:53 - it's kind of got it's got pie in it
03:55 - blueberry pie
03:58 - my favorite kind of pie and so some
04:00 - things that I might maybe I know that
04:02 - stand up maths the YouTube channel Matt
04:04 - Parker did an interesting video I think
04:06 - last year about like calculating all the
04:09 - digits of pi maybe I could make a piece
04:12 - of code that does that and displays them
04:13 - in the browser so so I am hey Jen maybe
04:21 - do Python know that does not count
04:24 - Python does not count as something for
04:26 - pi day especially because I I mean
04:29 - actually it does totally does but I
04:30 - don't know if that's gonna happen but
04:32 - I'm sorry I would like to take your
04:34 - suggestions so where can you leave your
04:36 - suggestions for let's do this for now
04:40 - rainbow topics I probably should come up
04:47 - with a better system but I am going to
04:49 - create a new issue pi day coding
04:52 - challenge suggestions please um add your
04:57 - suggestions for a pi day coding
05:03 - challenge in the comments below and you
05:09 - know maybe we can do some kind of like I
05:11 - guess I could do like a maybe I should
05:12 - do a reddit subreddit thread because
05:15 - there is a coding train reddit should we
05:16 - try that I don't know I'm so used to
05:18 - github I'll close this and move it there
05:21 - if somebody has a better suggestion but
05:24 - I'd like to let me just do this right
05:25 - now so you could add your suggestion
05:29 - here on github and then people can like
05:31 - you thumbs up the ones that they like
05:33 - and I can kind of look through and pick
05:34 - them that way you could do a tag on the
05:37 - github topics I could also do that that
05:39 - might be better for me to just do a
05:40 - label yeah let's not do it in the
05:42 - comments let's close this issue
05:44 - nevermind let's use a label I just
05:50 - thought it might be nice to go see them
05:51 - all in one place but with labels you can
05:53 - do that to use a label instead alright
06:03 - the trolls are out in full form today
06:06 - let's quickly quickly lock this
06:09 - conversation that's too heated it's
06:13 - definitely too heated probably and let's
06:20 - add a label oh let's go to issues labels
06:26 - new label pie day challenge for three
06:32 - I'm gonna be very your I'm gonna I'm
06:35 - gonna do this because this is the kind
06:38 - of person I am I'm gonna put my date
06:40 - with the day then the month that's what
06:43 - I'm gonna do and I'm gonna let's get a
06:47 - color here oh that's perfect okay so now
06:51 - you can file an issue
06:54 - and I will label it as pi day like this
07:01 - okay
07:01 - so send me your ID now next up moving
07:06 - right along
07:08 - let's see am I in the correct directory
07:12 - let's just see what's been merged thank
07:14 - you again to me I am Sammy who's been
07:16 - merging some up merging some of these
07:19 - flappy bird pull requests put git pull
07:23 - github master and we got some stuff I
07:28 - think we have some unicorn horn pipes
07:32 - loading loading loading horn filled flip
07:37 - dot PNG mmm looks like we're missing a
07:40 - file let's see what I can figure out
07:45 - their graphics horn body filled horn tip
07:56 - filled horn body filled horn tip fill
08:01 - ooh horn tip filled flips do I need that
08:04 - one paint pant dot ooh horn tip filled
08:08 - horn tip oh those are like I don't know
08:10 - what that is paint.net um these were
08:12 - made by a github user these were made by
08:17 - github user um just wanna k1j Julian Kay
08:27 - 1g Julian King Julian and we can see
08:31 - them here these lovely unicorn horns so
08:36 - I'm a little bit concerned I might
08:37 - really actually just want to replace
08:39 - these with just like drawing rectangles
08:40 - mostly because I'm a little concerned
08:42 - about having the collision I don't want
08:43 - to introduce extra complexity with the
08:46 - collision detection
08:49 - maybe me I am so me maybe
08:58 - whoops get pull get pull I can't like I
09:02 - almost can't do without saying origin I
09:08 - just keep till somebody fixes it and I
09:12 - just keep doing this I'm just kidding
09:14 - just kidding
09:15 - let's take a look here let's look at the
09:17 - pipe where is it being drawn draw this
09:21 - became incredibly came very complicated
09:23 - the good news is I was okay with all
09:25 - this becoming complicated because it's
09:26 - all just in the pipe class and I
09:29 - actually don't really mind if the pipe
09:32 - class is the super complicated thing
09:34 - because what I need to demonstrate will
09:36 - happen can be applied to any game
09:38 - whether the code is very simple or
09:39 - complicated so let me see here what is
09:44 - the image that's missing I'm sure
09:50 - there's a crack team of coders working
09:52 - ok now is fixed okay thank you me I am
09:55 - so we let's there we go looks like we've
10:01 - got that fixed let's go here and there
10:05 - we go so now we have our beautiful
10:09 - flappy bird game so slow on my machine
10:14 - and so let's see about this collision
10:17 - stuff yeah that's make sense but let's
10:19 - see let's see let's see yeah so it's
10:23 - it's it's using the rectangle up to the
10:26 - height of the tip of the unicorn horn I
10:28 - almost look like candy here but now I'm
10:36 - gonna change these two rectangles I
10:37 - can't tolerate it I'm sorry everybody
10:41 - this is I really like these designs and
10:45 - I will maybe come back to them if I
10:51 - don't know if me I am so me while you're
10:53 - watching if you while I'm kind of
10:54 - getting I'm gonna explain the github
10:59 - maybe maybe make it a separate branch or
11:01 - just like push push these unicorn horns
11:03 - into another branch but let's just make
11:05 - these a nice purple green blue rainbow
11:08 - colored rectangles there's also
11:11 - something
11:12 - running really slow and I don't know why
11:16 - because it's not yeah it's running at 30
11:21 - frames per second just reasonable so
11:25 - let's do that if anybody can do a pull
11:26 - request I so cuz I think these are a
11:31 - problem there they're lovely beautiful
11:34 - unicorn horns but I think the collision
11:37 - stuff and yeah they kind of they
11:41 - resemble the poop emoji with but they're
11:43 - pink there's you know there's a lot of
11:44 - there's a lot of it's just gonna I'm
11:46 - just gonna be uncomfortable the entire
11:47 - time okay
11:54 - use of instead of Adam okay let's get
11:58 - let me let's go over to the whiteboard
12:00 - for a minute here and let's do some
12:03 - erasing or let's not do some erasing I'm
12:14 - thinking so I want to talk about neuro I
12:19 - want to talk about neuro evolution and I
12:26 - think that to start talking about neuro
12:28 - evolution it wouldn't be the worst thing
12:30 - in the world for me to have my left over
12:33 - ending diagram from the doodle
12:35 - classification example because one of
12:40 - the key things I want to talk about is
12:43 - the difference between train whistle
12:46 - what about like a train whistle for the
12:50 - pipe here somebody can like right I
12:53 - don't know just make it a dis a
12:55 - rectangle a plain rectangle okay is the
13:01 - chat going on in here okay so what I
13:06 - want to talk about is how back
13:10 - propagation and gradient descent while
13:13 - the sort of standard or probably most
13:16 - well known technique for training the
13:20 - weights of a neural network is not the
13:24 - only technique
13:25 - and so having this as a reference is
13:28 - good and then I need to I'm gonna and
13:30 - then what I'll do is erase this and
13:33 - diagram out how a genetic algorithm can
13:38 - be used to train a neural network okay
13:42 - so I think I'm gonna get started with
13:43 - this right now
13:49 - let's let's take a look here oh let's
13:57 - look at Simon's pull request so here's
14:01 - the thing
14:01 - this is very useful Thank You Simon for
14:03 - this I'm gonna not oh I'm gonna not
14:05 - merge this right now because I want to
14:10 - I'm gonna have to sort of like the
14:12 - mechanics of the way the game works once
14:14 - I'm doing the evolution thing is gonna
14:15 - be pretty different okay okay just push
14:21 - to change bath two pipes great thank you
14:23 - so now I'm gonna do this get pull origin
14:25 - master ah kit pull I just renamed it
14:28 - back to origin and let's go here there
14:34 - we go oh look at those green pipes
14:37 - perfect Oh green pipes everything is
14:41 - right with the world this is very quite
14:47 - hard to play but I'm also probably in
14:51 - the end gonna change this into circles
14:52 - but okay that's good I also let me just
14:54 - see something here yeah now this is
14:59 - great okay perfect okay thank you for
15:05 - that now let us begin
15:15 - I'm trying to think if I want to what am
15:23 - I doing here let's go to so I'm gonna go
15:32 - away from flappy bird for a second
15:45 - let's get rid of this come on computer
15:52 - terminate you okay then I want to open
15:59 - this up and Adam what's going on in here
16:05 - because I changed some stuff at one
16:06 - point and I need the neural network
16:14 - library and that's good XOR so desktop
16:20 - your own network coding train up am I in
16:24 - the wrong I've been in the wrong camera
16:26 - screen all this time again as I always
16:28 - am sorry everybody
16:29 - [Applause]
16:37 - sorry everybody sorry buddy
16:43 - you know I don't know I'm doing my best
16:49 - doing my best okay let's see what do I
16:52 - need now examples XOR let's copy-paste
16:58 - that and let's just do what's called
17:02 - neuro evolution so I'm gonna have an
17:08 - example and I'm gonna I want to do it
17:11 - with XOR but but let's leave that out
17:18 - for right now okay all right
17:24 - and so now I also want to yeah no
17:42 - kidding
17:45 - examples doodle classification oops
17:56 - train alright alright
18:08 - tests pretty good let's get the rainbow
18:13 - here yeah here we go
18:22 - alright so that's gonna be okay so this
18:36 - is weirdly not Chea for when you and
18:39 - everybody watching this is weird this is
18:42 - a new chapter that I so the nature of
18:45 - codebook if I go to the books website I
18:50 - am currently working on a there chapter
18:53 - 9 is all about genetic algorithms and I
18:55 - have a video tutorial series about
18:57 - genetic algorithms chapter 10 is about
19:00 - the basics of neural networks and that
19:02 - the content that's currently in the book
19:04 - at the moment is pretty its from 2012
19:08 - lots happened in the fields machine
19:10 - learning since then you might have
19:11 - noticed and so and so I'm trying to
19:18 - rewrite this chapter and I've been
19:21 - making these video tutorials and
19:23 - examples in this neural network library
19:24 - and working on this other thing called
19:25 - ml v with just built on top of deep
19:27 - learned j yes all of which I am trying
19:30 - to get through and to do more and more
19:31 - stuff with um that I'm currently working
19:34 - on as part of a new chapter 10 and now
19:38 - so chapter 9 being about genetic
19:40 - algorithms I don't I think calling it
19:42 - the evolution of code is sort of silly
19:43 - and I should just call it evolutionary
19:45 - computing or genetic algorithms but that
19:46 - aside chapter 10 about neural networks
19:49 - chapter 11 which does not exist is going
19:56 - to be about using genetic algorithms
19:58 - with neural networks so I'm about to
20:00 - start the first video for chapter 11 and
20:04 - repeat exactly what I just said because
20:06 - I think that would be useful
20:09 - for part of the context because weirdly
20:11 - typically in the past the for all my
20:14 - other nature of code content the book
20:16 - came first and the videos came after the
20:19 - fact and this is a little bit strange
20:20 - that I'm going to make the videos and
20:23 - then hopefully write that content into
20:24 - the book all right so I'm about to get
20:27 - started
20:28 - and I'm will get started I'm just
20:32 - looking at my phone because ah because
20:34 - as I mentioned I have a very exciting
20:36 - guest video that will be released to
20:39 - this channel hopefully next week
20:41 - sometime but certainly sometime in the
20:44 - next week or two and that guest is
20:45 - coming to record at around 5:00 p.m.
20:47 - today so I've got to finish up by then
20:49 - and just in case you were on the fence
20:53 - of whether you wanted to be a patron of
20:55 - the coding train sometimes when
20:56 - especially when I have guests I'll keep
20:58 - a livestream going just to get a back up
21:00 - onto YouTube and I'll let people in the
21:02 - patron group kind of listen in on that
21:04 - as well but don't worry there's no need
21:06 - to be a patron of the coding training
21:07 - the video will be released but if you
21:09 - like a little advanced stuff water no I
21:11 - don't have any water this this this
21:16 - liquid beverage will have to do it's so
21:22 - Slurpee can you hear that like terrible
21:32 - coding train
21:33 - brought to you by anonymous cup of
21:35 - coffee I usually don't drink coffee
21:38 - while I'm live-streaming because it only
21:40 - leads to bad things but every once in a
21:42 - while it's just it's just necessary all
21:50 - right
21:53 - so let's close this out
21:57 - let's leave whoops how would I do no I
22:03 - don't want you to move anything let's
22:05 - leave this here move this out here I'm
22:07 - just getting everything ready come on
22:10 - come on Adam behave I'll do this okay
22:26 - all right here we go oh oh yeah it's
22:30 - fine it's fine I don't have a marker but
22:31 - I will I don't know if I need one this
22:35 - second but it's like a some people have
22:38 - like a lovey or a teddy bear that's
22:41 - their comfort object it helps them sleep
22:43 - at night I have a whiteboard marker it's
22:51 - my comfort object hello welcome to the
22:58 - first video in a new chapter of the book
23:01 - nature of code chapter 11 only strangely
23:05 - chapter 11 does not exist so I'm doing
23:07 - something a little different here where
23:09 - all my previous other nature of code
23:11 - videos that go along with this nature of
23:13 - code book the book was written first
23:15 - came out in 2012 and and this is the
23:21 - current version of it and then I made
23:23 - videos after the fact now what I'm going
23:24 - to do I want so chapter 9 is about
23:27 - genetic algorithms and chapter 10 is
23:29 - about neural networks and I have a set
23:30 - of video tutorials that go along with
23:31 - both of those chapters today I'm going
23:34 - to start talking about something that I
23:36 - want to be in the next edition of the
23:38 - nature of code in Chapter 11 called
23:40 - neuro evolutions so I want to take the
23:42 - idea of a genetic algorithm and a neural
23:44 - network and use them together in a
23:47 - magical way to make wonderful things
23:49 - happen on the screen or or doesn't have
23:53 - to even be on the screen in some other
23:54 - capacity that I can't even imagine right
23:56 - now so what is it that I am going to do
24:00 - so first of all okay
24:02 - so if you are on keyboard if you have
24:05 - watched some of my previous
24:06 - someone might other neural network
24:08 - tutorials you the most recent thing
24:11 - before the recording of this video that
24:12 - I made was a doodle classifier it's kind
24:16 - of the classic machine learning
24:18 - classification example I have some
24:20 - images maybe they're handwritten digits
24:22 - maybe they're doodles of cats and
24:24 - rainbows and unicorns and all that sort
24:26 - of stuff and I want to feed those things
24:28 - into a neural network and I want the
24:30 - neural network to classify them and if
24:32 - you've watched those videos you might
24:35 - have noticed that there's this whole
24:36 - elaborate training process the training
24:40 - process involves making that guess
24:44 - having some labeled correct data and
24:46 - then feeding that and then looking at
24:49 - the error like what is it supposed to be
24:51 - versus what it guessed and feeding that
24:53 - error back through the neural network
24:55 - just timeout for a second okay I really
25:04 - take seriously the at messages to my
25:07 - watch so little notes to me don't look
25:11 - thank you for the kind comments okay
25:14 - come back come back to my momentum so
25:18 - try not to a direct message during the
25:20 - live streams well yeah I probably should
25:23 - set up a separate slack user that just
25:26 - sends the notifications so that regular
25:28 - other messages don't get here all right
25:29 - all right all right let's see here
25:35 - looking at the guest output versus the
25:39 - correct label calculating an error and
25:41 - setting that error backwards through the
25:43 - network through a process known as back
25:45 - propagation where all of the weights are
25:48 - tuned and changed so while this is the
25:51 - most well-known and probably most common
25:54 - and sort of standard technique for
25:56 - training a neural network back
25:58 - propagation with gradient descent very
26:00 - fancy sounding there are many other ways
26:02 - I mean there there's other ways that you
26:04 - can train a neural network one of which
26:06 - is using a genetic algorithm so what if
26:09 - we just threw away all of that calculus
26:12 - math and all of this sort of like error
26:14 - this error that and back propagation
26:17 - and we just said hey I've got an idea
26:19 - why don't I make it's never having one
26:22 - neural network why did I make a thousand
26:24 - of them and I'll try them all maybe some
26:27 - of them will classify image when one
26:28 - will classify images better than another
26:29 - one does maybe I'll keep that one and
26:33 - one just really gets everything wrong
26:34 - maybe I won't keep that one at all and
26:37 - maybe I'll pick from the ones that kind
26:40 - of do well and take those and duplicate
26:43 - them or mix them up to make a new
26:44 - population of neural networks and see
26:46 - how those do and this is the central
26:48 - idea of a genetic algorithm now I might
26:51 - suggest if you want to if you jetta
26:54 - carbs are totally new to you you might
26:56 - want to pause this video right now and
26:57 - go watch a genetic algorithm tutorials
27:00 - if the concept of a neural network is
27:01 - totally new to you you could pause and
27:04 - go watch those tutorials but you could
27:05 - probably also just keep going good I'm
27:08 - gonna I'm gonna cover almost all of this
27:09 - stuff anyway and as I try to sort this
27:12 - out so I'm gonna take a break for a
27:13 - minute I'm gonna erase this whiteboard
27:15 - here what's there right now left over
27:16 - from the doodle classification and then
27:18 - I'm gonna diagram out how a neural
27:21 - network can be trained using a genetic
27:23 - algorithm and then through that diagram
27:26 - I will discover things I need to add to
27:28 - my neural network code base and at some
27:31 - point if all goes according to plan you
27:36 - know I have this particular this was the
27:38 - doodle classifier example where did you
27:41 - see it's classifying my rainbow but what
27:44 - I want to do is take this version of the
27:48 - game flappy code egg train it's not very
27:51 - flappy I guess and see if I can use a
27:54 - neural network that is that evolved to
27:57 - play this particular game so that's
27:59 - going to be the goal of this series and
28:01 - then I have all sorts of other ideas for
28:03 - other types of neuro evolution tutorials
28:06 - I believe this is often also referred to
28:08 - as neat neat algorithm because it's neat
28:12 - neuro evolution of it see here's the
28:15 - thing I was just saying gyro evolution
28:18 - and all the while that could sound so
28:19 - much smarter by saying neuro evolution
28:21 - of augmenting topologies that's totally
28:23 - neat alright be back in a minute
28:28 - um okay I'm looking at the chat
28:35 - everything seems okay so now I need to
28:37 - do is go erase this man this copy is so
28:44 - slurpy alright so let's do some erasing
28:47 - if anyone has any questions I knew there
28:59 - is a reason why I shouldn't Rex this
29:00 - whiteboard for like two weeks
29:01 - I knew my microphone is this like some
29:11 - horrible scratching sound that I'm
29:13 - making you do it's really better if I
29:16 - erase it immediately after I use it
29:22 - anybody has any suggestions maybe
29:24 - there's like a squeegee company and I
29:26 - could hire to come in here just like big
29:29 - this shiny and new
29:31 - spick-and-span you know I said earlier
29:36 - today that my passion in life is
29:40 - indentation and spacing and this is why
29:43 - linting this idea of linting your code
29:46 - is bringing such joy and happiness to me
29:49 - and yet in this moment right now I think
29:53 - I might have discovered that my true
29:55 - passion his whiteboard erasing there's
30:00 - nothing more soothing and meditative
30:07 - it's a good exercise it's good physical
30:10 - therapy for my broken elbow that's
30:12 - pretty much all the way
30:18 - [Music]
30:23 - [Music]
30:28 - just water my hair look nicer
30:34 - no nasty chemicals no cleaning anything
30:37 - just new water paper towel but now isn't
30:41 - so good for the earth but I guess the
30:43 - earth will have to survive means like in
30:45 - paper towels I can get a nice cloth or
30:49 - rag that's more reusable how am i doing
30:55 - how's this whiteboard look to you all
30:57 - clear pretty dirty to be honest with you
31:03 - I don't know what it looks like yeah you
31:05 - can see all those like smudges and
31:07 - things come at you I'm sure you want to
31:15 - edit a highlight reel of be erased
31:18 - important you do what are those like
31:22 - things in the video where like you do it
31:24 - really really fast instead of just doing
31:25 - a jump cut I don't know
31:27 - we've gotta get we gotta we gotta up our
31:29 - coding train game here okay all right
31:37 - [Music]
31:39 - I'm glad to see that the chat has moved
31:41 - on from discussing which coding language
31:43 - is the best to how to effectively clean
31:45 - a whiteboard that's actually not
31:47 - discussing that it's just that's what I
31:48 - wish for we're still like discussing
31:52 - MATLAB versus Pascal versus Java and yes
31:57 - do you think I could have a YouTube
31:59 - channel where I just clean stuff and I'm
32:01 - like happy about it all the time I
32:02 - really like that I think I'm starting a
32:04 - gaming channel because I recently
32:06 - acquired a Nintendo switch my kids and I
32:09 - we play it a lot and I was thinking I
32:11 - don't know maybe I'll make a gaming
32:13 - channel that's what that's all the rage
32:15 - a 44 year olds human who can have a
32:18 - youtube gaming channel people might
32:21 - watch probably not that's a terrible
32:23 - idea
32:23 - okay I don't know how to set that up I
32:27 - gotta like get the output into the input
32:29 - to the output this sort of thing a
32:32 - twitch channel and a little bit afraid
32:34 - of twitch but I am a kind of a twitchy
32:37 - person alright let's see take a second
32:45 - I'm getting a very excellent suggestion
32:47 - in the chat take a second and mark the
32:49 - edges of the whiteboard so you don't
32:51 - draw off the camera but what would be
32:52 - the fun in that
32:53 - that's all we should be living on the
32:55 - edge here don't you think alright let's
32:56 - see let's make this happen
33:02 - yeah I have um I think I'm kind of all
33:05 - better or worse I don't think that I can
33:08 - manage to like do different platforms
33:11 - and I've kind of got the YouTube thing
33:12 - going so that's probably what I'm gonna
33:14 - stick with but I don't know I'm always
33:16 - conflicted about this sort of thing okay
33:18 - so let me use a different color to mark
33:22 - the edges this is actually already
33:26 - marked up here I do actually have some
33:28 - markings already this is Mark's here
33:33 - this is probably more correct and this
33:38 - is mark down here it's not right I could
33:43 - promise I can't see my monitor down down
33:45 - down down down down down there
33:49 - and over here maybe over here this is
33:56 - really this is really the important one
33:57 - this is the one that I mess up all the
33:58 - time
34:00 - can you see that line to be where the
34:03 - tip of my finger is I think I got it
34:05 - right okay
34:05 - now that'll hopefully do okay so I'm
34:13 - just checking the chat doesn't your
34:16 - current streaming setup not take the
34:17 - output it's vegan but it does the output
34:19 - goes into the input here uh-huh
34:21 - it does it totally does and then the
34:23 - input goes back to the output and back
34:24 - into a different input also I'm really
34:26 - input-output things totally working I
34:29 - have all the plugs going from one
34:32 - electronic machine to the other
34:34 - electronic machine and they're connected
34:36 - to the Internet tube that's how I'm
34:38 - live-streaming so I could probably set
34:40 - that tube input-output plug based system
34:43 - up in my place of residence I should
34:47 - really get content what time is it
34:54 - 355 okay okay
34:58 - okay everyone okay so let's work okay
35:06 - now that I have them now that I have a
35:10 - blank whiteboard let me review the steps
35:14 - of a Janek algorithm and think of them
35:16 - in the context of a neural network so
35:18 - the first thing in a genetic algorithm
35:20 - that I need to do is create a population
35:25 - and the population is going to be a
35:29 - whole lot of neural networks neural
35:33 - networks are the individual elements so
35:35 - maybe that population is 100 neural
35:37 - networks - I need to evaluate fitness of
35:48 - neural networks okay so this is kind of
35:52 - like again this is kind of like the
35:53 - setup I know that's kind of getting
35:55 - close to the top there it's the thing
35:57 - that I'm going to do once at the
35:58 - beginning of the program I sort of
35:59 - initialization state
36:02 - and this is this thing that I'm going to
36:03 - do for a loop you know generation after
36:05 - generation in you know in p5 this might
36:08 - be called the draw loop I mean to
36:11 - evaluate the fitness of all the neural
36:13 - networks and then create a new
36:18 - population and the way I will do that is
36:23 - by pick quote-unquote parents based on
36:33 - my handwriting is getting worse and
36:35 - worse over time based on pick parents
36:40 - based on fitness scores maps of
36:46 - probability it's so much room in this
36:49 - direction probability and then I want to
36:53 - apply crossover which is a way if I pick
36:56 - two parents for example I can take half
36:59 - of their so-called
37:00 - digital DNA of one and half of the other
37:03 - or some random amount of one and random
37:05 - out another and combine them into a new
37:07 - entity and then I can apply mutation
37:12 - which would be which is the step of
37:14 - saying hey let me look at the D let me I
37:16 - have this child DNA that is made from
37:18 - two parents let me randomly just change
37:20 - some of it up as if it's spontaneously
37:22 - mutating to continue to have variation
37:24 - in the system so again you could go
37:26 - watch my genetic algorithm tutorials
37:28 - where I describe all this stuff in much
37:30 - greater detail of different techniques
37:32 - and why and how but this is the basic
37:34 - idea but you might remember if you did
37:37 - watch those tutorials that this is kind
37:39 - of like the algorithm and it you know
37:41 - obviously you could change it and be
37:42 - creative with it but it's kind of
37:44 - somewhat of a standard the really tricky
37:46 - thing when you're making your own
37:50 - genetic algorithm and applying it to
37:51 - your own project is as follows number
37:55 - one is this idea of genotype versus
37:59 - phenotype what is that so-called digital
38:04 - DNA the genotype what is the data of
38:08 - that DNA and what is that data do how
38:11 - does it express itself into a system so
38:13 - this is really key in
38:15 - thinking okay with a neural network is
38:16 - somehow the genotype what could be the
38:19 - data so in fact thinking back to my
38:22 - simplest neural network which is just
38:25 - has a - two layers really a hidden layer
38:27 - and output layer the inputs come into
38:29 - the hidden layer they get processed from
38:32 - the hidden to the output they get
38:33 - processed and then we have a final
38:35 - result so the core elements of those
38:39 - layers are weights and biases so all the
38:46 - weight matrices and the bias vectors
38:49 - those things which I described a detail
38:52 - of my neural network tutorials make up
38:54 - the genotype of the neural network the
38:56 - core aspect of it now the phenotype is
38:59 - the expression it's really really what
39:01 - am I using the neural network for so for
39:03 - example the expression of the neural
39:06 - network might be in the game flappy /
39:08 - bird the decision whether to jump or not
39:10 - jump that's the expression that's how
39:12 - it's going to be used applied in a given
39:14 - scenario in a classification example
39:16 - could be it's classifying an image
39:18 - that's how the data from the neural
39:19 - network is going to be used to make a
39:21 - guess based on this image and and and
39:23 - turn it into a string so that's a spec
39:26 - number once we've got that so what that
39:28 - means is when I write the code I need to
39:30 - somehow figure out how to do crossover
39:32 - and mutation with weights and biases I
39:36 - think I can create probably a population
39:38 - of random neural networks that's just
39:40 - gonna be like new neural network new
39:41 - neural network new neural network
39:42 - evaluating the fitness I've got to get
39:44 - to I can pick two random ones but I need
39:47 - to apply crossover mutation and to be
39:49 - honest what I might do it first in my
39:50 - first implementation is not even
39:52 - bothered with crossover and not even
39:54 - bother with picking more than one parent
39:55 - so one technique to simplify the genetic
39:58 - algorithm is just to make copies so I
40:01 - can pick the good ones and make copies
40:02 - of them mutate a little bit and keep
40:04 - going it may not work as effectively as
40:06 - if I use crossover but it'll certainly
40:07 - be easier to code so the other thing
40:10 - that's tricky with with when you're
40:14 - making your own genetic algorithm
40:16 - applying it to your own project is the
40:19 - fitness function question mark witness
40:24 - mark question mark so this is crucial if
40:26 - you don't have a good fitness function
40:28 - this whole selection process this
40:30 - quarter goat natural slice not very
40:32 - natural here it's like digital selection
40:34 - this I'm not gonna be able to
40:36 - distinguish between members of the
40:38 - population that do really well that
40:40 - should be that their digital DNA should
40:42 - be passed down the next generation
40:43 - versus ones that don't so I want a good
40:45 - fitness function that gives me a good
40:47 - range of probabilities and so in this
40:50 - case we could think about the
40:51 - classification it could be okay well
40:52 - this neural network give it 100 images
40:54 - its Fitness is how many of those it
40:57 - classified correctly and we could even
40:59 - go into it deeper and somehow score the
41:01 - fitness in court according to its
41:03 - confidence level about classifying them
41:04 - correctly but that might that might be
41:06 - flawed in some ways also so that's one
41:08 - thing with the flappy bird scenario if
41:10 - we think about the flappy bird game what
41:16 - is the fitness here well the fitness
41:18 - could would simply be the score so I'm a
41:20 - neural network I am a neural network
41:23 - playing floppy coding train now pee pee
41:26 - booboo input/output people my cell like
41:31 - cordial so it could just be like how
41:34 - long am I able to go through this world
41:37 - without running into a pipe so that
41:39 - could be the fitness so I could say hey
41:41 - why don't you thousand of you try
41:43 - playing this game a thousand of you
41:45 - electronic neural network magic machines
41:47 - try playing his game and and your
41:51 - fitness is how long you last before you
41:54 - run into a pipe and so that is the
41:57 - fitness function so we have all the
41:59 - pieces so what do I have already like if
42:02 - I'm going for this flappy bird example I
42:04 - already have the flappy bird game so I
42:07 - have the flappy bird code I have my
42:10 - genetic algorithm examples but
42:12 - ultimately there's not really I don't
42:14 - really have a genetic algorithm library
42:16 - per se so I'm probably gonna have to
42:18 - build the genetic algorithm stuff in the
42:20 - code but I do have a neural network
42:23 - library so I don't have to write I don't
42:27 - have to write the flappy bird game I
42:29 - don't have to write the neural network
42:30 - library however it might make sense for
42:34 - my neural network objects to know about
42:36 - crossover a mutation that might be
42:39 - something that probably should go into
42:40 - the neural net
42:41 - library so that any moment I could say
42:43 - like hey you neural network a new neural
42:45 - network get together make another or hey
42:47 - you neural network mutate yourself so I
42:49 - probably should that's something so
42:51 - that's the first thing I think I'm gonna
42:52 - do in in the next video is add crossover
42:56 - and mutations or maybe just start more
42:58 - simply I'm just gonna start with like a
43:00 - copy function just to kind of get going
43:02 - here a copy function and mutation so
43:06 - those things need to go into the neural
43:07 - network library and then the third thing
43:09 - is I just need to apply the GA so this I
43:12 - really need to do a lot of work to write
43:15 - the genetic algorithm code so I'm gonna
43:16 - start with my flappy bird code import
43:19 - the neural network library add crossover
43:21 - slash copy mutation and then start to
43:25 - implement the idea of a genetic
43:26 - algorithm in this particular program
43:28 - that started with the flappy bird code
43:30 - that imported the journal not look
43:31 - library that my father bought for two
43:33 - zeusie anyway never mind
43:36 - random reference okay because it's like
43:37 - the flappy bird that imported the neural
43:40 - network library that that added the
43:42 - genetic algorithm that there's that
43:44 - there's a song going on there that
43:45 - somebody else will finish for me alright
43:49 - Passover is coming up okay
43:51 - so that's that okay so you've made it to
43:55 - the end of this first video for chapter
43:57 - 11 of the nature of code which doesn't
43:59 - even exist yet but maybe by the time
44:01 - you're watching it oh I'll be so happy
44:02 - if it exists by the time you're watching
44:03 - this and so in the next video I'm going
44:06 - to revisit the neural network library
44:08 - and add functions for copy and mutation
44:10 - I'll see you there
44:13 - alright how we doing everybody
44:28 - okay everybody okay I'm like done okay
44:32 - go home now
44:33 - no echo 34 hello Dan did you have a
44:39 - chance to check my firework program on
44:40 - Twitter I think that I did that was a
44:42 - couple weeks ago I remember seeing it I
44:44 - remember it was beautiful ascend to me
44:46 - again just in case I missed it I
44:47 - recognized her name though echo 34
44:49 - you've made a lot of great contributions
44:50 - thank you Dan many questions about the
44:54 - ethics of neural nets yes I have many
44:58 - questions about the ethics of neural
45:00 - networks itself and I but I so I don't
45:03 - necessarily have questions about the
45:05 - ethics of neural networks I have
45:07 - questions about the ethics of the people
45:09 - who have access and the ability to
45:11 - program neural networks and how those
45:13 - programs are applied in society so
45:16 - hopefully I hope or that the work that
45:21 - I'm doing to provide educational
45:23 - materials about neural networks will
45:25 - provoke that discussion and make it
45:28 - easier and more accessible for
45:31 - understanding of these algorithms to be
45:33 - in the hands of more people and so we
45:38 - can ask the right questions and work
45:40 - together to make sure the power of these
45:42 - algorithms and the way that they can be
45:44 - applied and the datasets that can be
45:45 - used with them is not abused that's what
45:48 - I have to say about that okay I suggest
45:53 - you override the weights when you apply
45:55 - crossover all right so I think that's
46:04 - good so anybody have any comments or
46:06 - questions before oh this is not his
46:10 - computer's not plugged in alright
46:20 - anybody have any close up so let me get
46:23 - set up I guess for the next video again
46:26 - I don't think we're gonna have I just
46:27 - want to be honest here it seems very
46:29 - unlikely at this point that I'm gonna
46:30 - have a finished flappy bird neuro
46:33 - evolution tutorial by the end of today
46:35 - but I certainly this is a project that I
46:37 - intend to
46:39 - I intend to write a chapter for the
46:40 - nature of codebook about it and and yes
46:50 - so don't worry even though I don't
46:51 - finish today I will also mention that if
46:53 - you go to github.com slash Schiffman
46:56 - neural network p5 so you'll notice I
47:01 - have a repository called neural network
47:03 - p5 which says this repository is
47:05 - archived by the owner it has now read
47:06 - only and I wrote this has been
47:08 - deprecated and it links to the coding
47:10 - train neural network Jas the reason why
47:12 - I mentioned this is I started doing this
47:14 - work last year and then decided I wanted
47:17 - to rebuild it again from scratch in
47:19 - video tutorial forum but there are for
47:21 - example the there is a neuro evolution
47:23 - tutorial here example here and let's see
47:27 - if I go to lick click this link this is
47:32 - actually running what I intend now to
47:34 - build and so let's take a look at this
47:37 - so and one thing that I built into this
47:39 - is I can like speed it up and blow away
47:41 - made it very easy to play you notice
47:43 - that so this over time we can see like
47:50 - the all-time high score and what I can
47:52 - do now is I can also say run best so far
47:54 - so this is now going to just show this
47:57 - is the current member of the population
47:58 - that has been that has the best neural
48:02 - network so far to effectively play this
48:04 - game so I probably should have shown
48:05 - this in the video tutorial itself but
48:08 - yeah you know comes next come easy come
48:16 - easy go or something like that alright
48:17 - so you can look at this for reference
48:21 - I think there's flaws and mistakes and
48:23 - weirdness in here but it's something oh
48:24 - I forgot to tweet that I was
48:26 - live-streaming again this is the worst
48:28 - thing that I do while livestream which
48:30 - is that I look at my phone in
48:31 - notifications but sometimes I just want
48:33 - to make sure that my guest which I'm
48:35 - very excited about it's not lost and
48:39 - figuring out where to go okay alright so
48:45 - neat nature of code doodle classifier
48:52 - so let's go to the code here and go to
48:57 - this neural evolution sketch and just
49:03 - make sure it's importing the neural
49:04 - network library in the matrix library
49:06 - and I also need to open up P and let me
49:11 - do something here what changes I just
49:15 - want to like do some get clean up for a
49:17 - second cuz this is gonna go into the
49:19 - repo get status here what did I change
49:25 - so first let me add and commit the let
49:30 - me add neuro evolution and let me say
49:38 - adding new nur neuro evolution tutorial
49:45 - neuro evolution example nothing to see
49:50 - here yet and oops
49:55 - escape alright now let me put get push
50:08 - origin master this and now let me see
50:12 - what's going on here let me say get diff
50:16 - there's some weirdness I don't know what
50:19 - know what's going on here I'm gonna just
50:22 - stash this because I don't want to deal
50:26 - with that and now I think I'm good and I
50:32 - want to have the library code open
50:34 - neural network okay whoa oh right I have
50:41 - the activation function thing this I
50:42 - don't know no so am i call them there or
50:44 - no semicolon there what to do about that
50:47 - look never never decide I guess this is
50:51 - really should have a semicolon there
50:52 - yeah yeah that's alright just want to
50:56 - see what the randomize function does oh
50:58 - that's in the matrix library oh it's
51:00 - really the matrix library that I want to
51:03 - randomize
51:03 - ah I really should change this to a
51:06 - Gaussian distribution so it's the point
51:08 - I need to do that guess I will not worry
51:11 - about it right now boy this has had some
51:16 - nice changes to it oh it has you
51:18 - realizing that wait a second here
51:20 - forgot that that was in there I've had
51:23 - some nice pull requests and also a lot
51:25 - of pull requests that I haven't had a
51:26 - chance to look over and figure out what
51:28 - to do about yeah here's the thing yeah
51:31 - I'm up to date one of the reasons why I
51:33 - haven't merged some of the pull requests
51:35 - which are amazing pull requests is that
51:38 - I feel like what I want to do with
51:43 - crossover and mutation is going to be
51:46 - much simpler to demonstrate with just a
51:49 - single hidden layer than if there were
51:50 - multiple hidden layers and to be honest
51:52 - this sort of flappy bird scenario is so
51:54 - simple it's just a few inputs I'm not
51:57 - gonna do I'm not gonna I could do all
51:58 - the pixels as input that would be
52:00 - interesting actually that would be
52:01 - really I got it I remember to mention
52:03 - that when I get to that but I think it's
52:07 - gonna be easier if there is just a
52:09 - single hidden layer and an output later
52:12 - then I can just do the sort of copying
52:14 - and mutation stuff pretty manually you
52:18 - know I don't know but I like you know I
52:20 - think that's good I think that's a good
52:21 - decision that's a decision I'm making
52:22 - right now
52:24 - [Music]
52:31 - all right sorry I'm looking at the chat
52:35 - um okay yeah I can just know I can just
52:43 - do map right right right all right all
52:45 - right all right it's all right all right
52:49 - okay so so really what I want is some
52:56 - new stuff in the matrix class copy okay
53:01 - I found a wire on the floor can you see
53:14 - this wire look someone's been making
53:18 - some physical computing tutorials in
53:20 - this room that is you can't even see it
53:22 - because of the focus
53:41 - all right hello welcome to part 2 of my
53:53 - neuro evolution series where I am
53:56 - attempting to look at how I can train a
53:59 - neural network or more accurately a
54:01 - population of neural networks with a
54:03 - genetic algorithm I talked about this in
54:06 - the previous video which you could go
54:07 - back and watch if you haven't but the
54:09 - main thing that I want to do in this
54:10 - video is look at okay well if I have a
54:13 - neural network if I have a neural
54:14 - network how can I apply crossover or a
54:18 - mutation to that neural network so that
54:20 - will be the focus of this video I do
54:22 - sort of remember though I did kind of
54:24 - remember that there's a lot of stuff
54:25 - that I haven't talked about yet because
54:27 - one of the reasons why I use the neural
54:29 - network is to be able to give it some
54:30 - inputs and to get some output so that
54:33 - makes sense in the context of the dual
54:35 - classification example probably but may
54:37 - not make sense to you immediately in
54:38 - terms of this idea of a flappy bird game
54:41 - so I will get to all that but right now
54:43 - I'm gonna focus on saying new neural
54:45 - network and you know what I'm gonna
54:47 - stick with copy like is that I'm gonna
54:49 - get to crossover in a future video I'm
54:51 - gonna stick with copy for simplicity and
54:53 - mutation so let's go back let's go over
54:55 - to the code and let's take care of that
54:57 - now let's start by having I'm gonna
55:04 - create a variable called brain and I'm
55:07 - using the p5 I'm using I mean using the
55:10 - p5 GS library though what I'm gonna do
55:12 - right now it's totally unnecessary for
55:14 - but because the flappy bird gained this
55:16 - program in p5 that's going to be helpful
55:17 - later also I use it all the time so I
55:20 - don't need a canvas I'm just gonna say
55:21 - no canvas and I'm gonna say brain equals
55:24 - new neural network now when I create a
55:29 - new neural network object if you
55:30 - remember the three arguments I need to
55:33 - give it and this is just for this
55:34 - particular neural network implementation
55:36 - it'll work differently if you're using
55:38 - you know say like a different machine
55:40 - learning framework like Kerris or
55:42 - somebody else's neural network library
55:44 - maybe you might want to look at deep
55:46 - learned Dutch is I will come back to
55:47 - that in a future video so I need to give
55:51 - it a cert
55:51 - number of inputs well all of a sudden
55:54 - now we're back to that question so let's
55:57 - actually not worry about that right now
55:58 - it's not worry about that question I'm
56:00 - going to go back to that question and
56:01 - I'm just gonna make something up so
56:02 - let's take the XOR example sort of like
56:04 - a trivial example of okay it has two
56:06 - inputs they're either true true true
56:08 - false false true false false so there
56:10 - will be two inputs let's just have a
56:13 - hidden layer with four nodes and one
56:16 - output so we can create some simple
56:18 - neural network and what I want to be
56:20 - able to do is I want to say let a
56:23 - cult-like child equal brain copy like I
56:27 - want to be able to say let me make a
56:29 - copy of that neural network and I also
56:31 - want to say something like child uh
56:33 - mutate so let me take that copy and
56:36 - apply a mutation in it mutation which is
56:40 - something I described more in the
56:41 - genetic algorithm video series so what
56:43 - this means is this neural network class
56:46 - needs to have two new functions it needs
56:48 - to have a copy function and needs to
56:49 - have a mutate function so let's go into
56:52 - the neural network library code this is
56:54 - the class there's a lot of stuff in here
56:56 - I have a way to many videos going
56:58 - through all this code luckily we can
57:00 - kind of ignore all of this and I'm just
57:02 - gonna go down to the bottom here and I'm
57:04 - gonna say something like adding
57:06 - functions for neuro evolution now the
57:12 - truth of the matter is what is it that I
57:14 - really want to copy well if you recall
57:19 - the neural network structure is such
57:22 - that if there are two inputs and four
57:25 - hidden nodes and one output the neural
57:28 - network looks like this it's connections
57:32 - between the inputs and the hidden layer
57:35 - and connections between the hidden layer
57:36 - and the output and these connections the
57:40 - the the sort of dials of the neural
57:42 - network the date of the neural network
57:44 - what controls how the information flows
57:46 - from the inputs and out through the
57:49 - output are all of these weights and
57:51 - these are stored in matrices it's just a
57:54 - whole bunch of numbers so I have a
57:57 - weight matrix which goes from input to
58:00 - hidden I have a weight matrix that goes
58:04 - from hidden
58:05 - to output and with each of these I also
58:07 - have this thing called a bias and if you
58:10 - recall the bias is something you know
58:12 - really all I'm doing in the end is like
58:14 - all of this really boils down to like
58:16 - hey there's a whole bunch of points just
58:17 - fit a line to those points and the
58:19 - plastic to be like move the line a
58:20 - little bit up move the line a little bit
58:21 - down so that's really even though this
58:23 - all seems like fancy magic ultimately
58:25 - that's what it just boils down to in the
58:27 - end so I also have the bias for the
58:30 - hidden and I have the bias for the
58:32 - output I don't know I don't know what
58:34 - the best way to write the notation for
58:36 - this is or yeah so all of these things
58:40 - when I want to copy the neural network
58:42 - what I'm really saying I want to do is
58:44 - copy all this stuff so let me go ahead
58:46 - and so what I need probably is a
58:49 - function inside of all these are all
58:51 - matrix objects I need a function
58:53 - probably in the matrix class to say copy
58:57 - so let's start here and say copy and you
59:03 - know clone could be used and I want to
59:05 - make a deep copy I think not a shallow
59:08 - copy these terms get thrown around a lot
59:10 - in computer science deep versus shallow
59:12 - but I don't want to like just point to
59:14 - the data I want just giving my own
59:16 - version of all of those numbers because
59:18 - I'm gonna mess around with them and I
59:19 - want you to keep your numbers I want to
59:21 - mess with your numbers so instead of
59:22 - just saying like I I'm a new neural
59:23 - network can I just point over to your
59:25 - numbers I really want a whole version of
59:27 - those so and if we go back to here so
59:30 - things that I need to do is I need to
59:33 - keep track of these properties
59:35 - input/output the total input output and
59:38 - hidden nodes so I want to say let let's
59:41 - just put this in here so I want to say
59:42 - let input nodes equal this dot input
59:46 - nodes let hidden nodes equal this dot
59:50 - hidden nodes and let output nodes equal
59:55 - this dot I'll put in you know what this
59:58 - this is very poorly named this is sort
60:01 - of silly what I'm doing but I'm going to
60:02 - do it anyway input nodes this is very
60:07 - this is me this is how I like to code I
60:09 - like to make things as long-winded and
60:11 - as possible so that I can really think
60:13 - it through and explain it like all I'm
60:15 - doing right now is taking the properties
60:17 - of the neural network I wanna cop
60:18 - and put them into local variables why
60:21 - because I want to say neural network
60:24 - copy equals new neural network with what
60:29 - I have a better idea how to do this I
60:34 - have a better idea how to do this so
60:37 - this could work but I have a better idea
60:41 - so actually what I want to do is I kind
60:44 - of just want to say this return new
60:47 - neural network this now you might be
60:52 - asking me like I mean this is this isn't
60:54 - gonna be mad this isn't just gonna work
60:56 - but what I'm sort of realizing here is
60:57 - maybe I don't want to copy everything
60:59 - here what I actually want to do is call
61:02 - the constructor but give it a reference
61:05 - to the existing neural network and then
61:08 - have that constructor instead of
61:09 - creating a new a new wait new weight
61:13 - matrices that are random it'll create
61:15 - weight matrices that are copies of the
61:17 - existing one in other words let me go
61:19 - back up to the constructor and look at
61:24 - this so what if so the constructor gets
61:26 - three things right a B I could just like
61:32 - rename these the parameters of the
61:35 - constructor function for a second and
61:36 - just call them ABC right a being the
61:39 - input nodes B being the hidden nodes C
61:41 - being the output nodes but before I do
61:43 - that what I actually want to say is is a
61:46 - an instance of instance of a neural
61:49 - network else in other words this is how
61:57 - I want to create
61:58 - oh I turn a pause I want to put a
62:06 - preference back that I turned off this
62:09 - morning and when I got to check the time
62:11 - for 2222 packages beautify I just I
62:25 - really like having it beautify on save
62:28 - so I'm so used to that on some
62:31 - put that back okay this here so what I
62:44 - want to do is if if I'm being sent three
62:47 - integers then I want to make the neural
62:50 - network the way I always have however if
62:53 - I'm being sent the first argument and
62:56 - this is this is kind of this is known as
62:57 - overloading typically in a programming
62:59 - language like Java if I had to overload
63:02 - the constructor like there's two
63:03 - different ways I could call the
63:04 - constructor I could give you three
63:05 - numbers to make a brand-new neural
63:07 - network or I could give you a neural
63:08 - network to make the copy of yourself I
63:10 - would write two versions of the
63:12 - constructor but in JavaScript you can
63:14 - only have one version of the constructor
63:15 - but you can kind of check what you're
63:17 - passing in and just to be clear about
63:18 - this let me just make sure this instance
63:20 - of thing is correct so if I were to say
63:22 - let a be a new new neural network 4 4 3
63:28 - 2 just arbitrarily and I could say a
63:32 - instance of its without a capital
63:35 - instance of a string I should get false
63:38 - instance of a neural network I should
63:42 - get true ok so that's right this this of
63:44 - should be lower case though if a is an
63:46 - instance of a neural network then what
63:48 - am i doing
63:49 - then I'm saying this dot input nodes
63:52 - equals a dot input nodes like I can
63:56 - start right here's where if it's not a
63:59 - neural network I'm actually assigning at
64:01 - the numbers that are coming in if it is
64:02 - I can just keep going Oh actually what
64:05 - am i doing I can say this and this
64:09 - should be hidden and now I can say this
64:15 - maybe I should have somebody has a
64:16 - suggestion for how to name these in a
64:18 - better way I just I didn't want to name
64:20 - them hidden input hit in output anymore
64:23 - because a could be either of those
64:25 - things so you know this this may be like
64:29 - to do document what a B C our output
64:37 - nodes that's a little note to myself
64:39 - that I don't like what I've written here
64:40 - and then now let's look at these so I'm
64:43 - not going to need to
64:44 - in demise the weight matrices because
64:46 - I'm just gonna say equals what am I
64:50 - gonna do here a dot weights dot dot copy
64:55 - write what I want to do is say hey my
64:57 - weights are your weights and now my
65:02 - input to hidden weights are your weights
65:04 - and my hidden to output weights are your
65:06 - weights now is this gonna run I don't
65:08 - think so
65:09 - because I probably have to add a copy
65:11 - method to the matrix object but I'm
65:13 - getting somewhere now what else do I
65:15 - need
65:15 - I need the biases so I need to set the
65:20 - biases so the same thing I'm just doing
65:27 - a lot of little like copy/paste stuff
65:29 - here so I need to set the hidden bias
65:30 - values and the output bias values okay
65:34 - so this is me creating this new copy of
65:36 - a previously neural network and then you
65:39 - know right now it looks like learning
65:41 - rate and activation function or at the
65:44 - moment even though I have different
65:45 - activation functions I can kind of write
65:51 - is this default is getting set to
65:52 - sigmoid as default is getting set to 0.1
65:54 - so I probably should copy those as well
65:57 - I'm just gonna just be simple about this
66:00 - right now
66:00 - and just assume that my program is never
66:03 - going to change learning rate or
66:04 - activation function I should that should
66:06 - be a to do to do copy these as well at
66:13 - some point but I don't need to worry
66:14 - about that right now and to be honest
66:15 - the learning rate isn't gonna play a
66:18 - role anymore the learning rate is
66:19 - completely irrelevant the learning rate
66:20 - is specifically a tied to the tied to
66:22 - the gradient descent algorithm which I'm
66:24 - no longer really using with the with the
66:27 - with the genetic algorithm that's what
66:30 - I'm doing now okay we're getting
66:31 - somewhere
66:32 - all right so just out of curiosity
66:34 - remember this is the code I'm making a
66:37 - new neural network and I'm let I know I
66:38 - haven't done mutate yet but is this even
66:41 - working is that was there but did I
66:46 - maybe in some other universe happened to
66:50 - write a copy function already into the
66:52 - matrix class I seriously doubt it but
66:54 - let's see yeah copy is not a function so
66:57 - what this means
66:58 - is I need to also go into the matrix
67:00 - library and I think this this I think is
67:03 - worth having in here that's not just
67:06 - this isn't exclusive to genetic
67:08 - algorithms our Nura evolution like so
67:10 - I'm adding this stuff you know copy and
67:13 - mutate and crossover will be here
67:14 - specifically because of genetic
67:16 - algorithm but the matrix I can the
67:18 - matrix object I can be a little less
67:20 - formal about this so what do I want to
67:21 - do I want to write a function copy and
67:23 - what do I do
67:25 - I'm going to say let m equal a new
67:30 - matrix with this dot rows this dot
67:34 - columns so I create a matrix object with
67:38 - the same number of rows and columns and
67:39 - somebody in the chat I know is going to
67:42 - tell me there's some very fancy way that
67:45 - I could just instantly use some
67:47 - higher-order array function to copy the
67:49 - whole thing over but because I am Who I
67:52 - am I'm gonna say I'm going to write a
67:57 - nice little nested loop I can always
68:00 - refactor this later I just know this is
68:02 - gonna work and I'm going to say m dot
68:05 - data index I index J equals this this
68:10 - dot data index I index J so this is
68:13 - manually me looping through the entire
68:16 - matrix it's a grid of numbers it's all
68:18 - the weights of the connection of the
68:19 - neural network and just manually copying
68:21 - them over one by one and I think this
68:24 - will work I have some breaking news from
68:32 - the chat that I need to mention it's
68:34 - that good but it's not relevant to what
68:35 - I'm doing right now so I will come back
68:36 - to that alright so let's see let me hit
68:39 - refresh there we go
68:40 - so does this work I have two neural
68:44 - networks they both seem to have two for
68:48 - one two for one you know I could go let
68:51 - me look at one of these biases look at
68:53 - these values so this is bias H these are
68:56 - the values can you memorize those can
68:58 - you remember them let's go down here
69:01 - okay nope nope something is wrong so
69:04 - this stuff did not get copied guess what
69:07 - guess what I forgot
69:11 - I forgot something quite important in
69:15 - this function I did not forget to return
69:17 - the thing that's new but in the matrix I
69:21 - forgot to say in the return end so this
69:24 - new matrix that I'm making I've got to
69:26 - actually return it I made the copy you
69:28 - can make the copy you can take the
69:29 - reservation but you can't hold the
69:32 - return of the copy and I'll sign people
69:34 - Seinfeld reference for all of you try
69:35 - should have said that it'd be more
69:36 - interesting if I didn't say that okay so
69:40 - now I can look here and I can see okay
69:43 - look at these numbers memorize these
69:46 - numbers this is the way I actually let
69:48 - me look at I'm just gonna look at one of
69:50 - the biased ones this will be a little
69:51 - bit simpler so here's the bias bias
69:53 - h-has these values memorize them
69:55 - memorize them I could write a nice unit
69:58 - test to actually see if this function
69:59 - works I'll be much better but this has
70:02 - been I ball it 0.20 eight four one five
70:06 - yep these look like the same numbers
70:07 - same numbers say members so I'm gonna
70:09 - just choose at the moment to believe
70:12 - that this worked this is probably a bad
70:15 - idea but this is the reality of what I'm
70:19 - going to do and I'm okay with that
70:21 - so let's move on so we have now
70:23 - implemented copy when I next need to do
70:27 - is implement mutation now I do need in
70:31 - order to do mutation I need to have
70:33 - something called a mutation rate and
70:35 - that mutation rate is essentially a
70:37 - probability of how likely it is for each
70:40 - element of every of sorry each element
70:45 - of every sort of DNA to alter itself
70:48 - randomly when that child DNA is made so
70:51 - what that means in this context is for
70:53 - every single number that's a weight in
70:56 - these matrices for every single number
70:57 - that's in the bias there is a say one
71:00 - percent chance point five percent chance
71:02 - 10 percent chance then I'll pick a new
71:04 - random number I could also with mutation
71:06 - like a nudge to the values so sort of
71:08 - picking an entirely random new number I
71:09 - could like push it a little higher push
71:11 - a little lower but for simplicity right
71:12 - now let's just pick a new random number
71:14 - so what do I need to do I need to go
71:18 - back to neural network J s and I'm going
71:21 - to add a function called mutate
71:25 - what am I gonna do here I'm going to say
71:27 - there are four things that I need to
71:30 - mutate so I'm gonna say let's think
71:33 - about this what are all those things
71:35 - called
71:36 - there's I just got to go back up to here
71:38 - there's weights ih weights H Oh bias H
71:41 - bias Oh
71:42 - so I need to say what I need to do is
71:45 - say weights I'm gonna map a mutation
71:49 - function so remember there's the the V
71:56 - sorry I'm not able to type to talk at
72:00 - the same time so if you recall the
72:03 - matrix the matrix library has a function
72:07 - called map and what it allows you to do
72:12 - it's a little unfortunate that there's
72:14 - also a JavaScript native function called
72:17 - map which I'm using everywhere it allows
72:19 - you to apply a function to every single
72:23 - element of the array so I can pass in a
72:26 - function and apply it to every single
72:28 - element of the right and the function
72:29 - that I'm gonna pass in is mutate so
72:33 - let's write this function now and it's
72:38 - going to get it receives a value but to
72:41 - be honest I don't care about I do care
72:44 - about what the value is
72:45 - if I were planning to nudge it higher
72:47 - and nudge it lower but what I'm going to
72:49 - do right now is I'm just gonna return a
72:51 - random number so let me actually I sort
72:55 - of for that should probably link these
72:57 - better in some way but when I have this
72:59 - function called randomized and this is
73:03 - the kind of random number that I'm
73:05 - asking for so I am going to just return
73:09 - this ah
73:10 - but am I always going oh I do need the
73:12 - Val guess what if I do this it'll
73:14 - completely randomized every single
73:16 - element so this mutation function needs
73:19 - a mutation rate and what I'm going to do
73:22 - is I'm gonna pick a random number if
73:25 - math dot random is greater than or less
73:28 - than the rate right so only so let's say
73:32 - math dot random will give me a number
73:33 - between 0 & 1 so if the mutation rate is
73:37 - 0.1
73:37 - that random number between zero and one
73:39 - will be less than 0.1 ten percent of the
73:41 - time
73:41 - otherwise stick with the same value so
73:46 - this is now the function that I want to
73:48 - apply to every element of all of the
73:51 - weight matrices I want to say hey mutate
73:55 - these weights mutate these weights
73:57 - mutate these biases mutate these biases
73:59 - and perhaps there's a more elegant way
74:00 - to write this and I will consider that
74:01 - all in the future with your many
74:03 - comments and pull requests and
74:04 - complaints I look forward to them but
74:07 - this is what I'm gonna leave it at right
74:08 - now so let's see now if what I can do if
74:12 - I go back to this particular program and
74:14 - I say child now just out of curiosity
74:17 - I'm gonna say child mutate one so I'm
74:20 - giving it a mutation rate of 1 which
74:22 - means everything should be completely
74:24 - random and what I'm gonna do just as a
74:27 - kind of like a way of testing actually
74:29 - is I'm gonna change this to I'm gonna
74:32 - multiply it by like a thousand just cuz
74:34 - I want to see like totally different
74:36 - numbers to see that this is working so
74:39 - I'm gonna go back and I'm gonna refresh
74:41 - the code weights I H is not defined oh
74:43 - right I forgot about that this dot that
74:45 - won't surprise any of you you've
74:47 - probably been saying this in the chat
74:49 - all along there we go
74:51 - so let's take a look so here we have
74:53 - once again the bias ease there all
74:56 - reasonable numbers between negative 1
74:58 - and 1 which is how I started the neural
75:00 - network and now if I look at the child
75:05 - neural network and I look at the bias
75:07 - ease we can see yep so that mutated now
75:09 - let's change the mutation rate let's
75:12 - change it to 0.5 so we have kind of I
75:15 - mean we're not going to get exactly 50%
75:16 - of them because it's supposed to be
75:17 - random but we'll least see some of them
75:19 - didn't mutate and some of them did so
75:21 - let's change now the mutation rate to
75:24 - 0.5 just to see that this is working
75:27 - this is instead of me writing unit tests
75:29 - manual unit testing let's look here we
75:32 - can see wonderful here's some original
75:35 - values and now let's go down to here in
75:39 - the child object and let's look at the
75:42 - bias ease again and we can see hey it
75:45 - actually worked out exactly as planned
75:47 - it mutated two of them and did
75:50 - you take two of them which is what you
75:52 - know most commonly with a 50%
75:53 - probability we're gonna see and I
75:55 - suspect that if I go into the output
75:57 - bias though the alibis just has one
76:00 - value it didn't get mutated right
76:02 - because this is such a simple little
76:03 - neural network if we go into these
76:04 - weights we can see you know three out of
76:07 - four got mutated I think it's working so
76:09 - we are in good shape here we now and so
76:14 - I went up probably the actual mutation
76:15 - rate I'm going to want to use something
76:17 - like 1% because I want to do it pretty
76:19 - rarely but we now have the ability to
76:22 - both copy a neural network again as the
76:25 - Nexus if you're watching this and the
76:27 - future videos of this tutorial aren't
76:29 - released yet or if you don't feel like
76:30 - watching them just yet
76:31 - try as an exercise to yourself go and
76:33 - implement crossover how could instead of
76:35 - instead of copy could you create a new
76:37 - neural network that's a mixture of all
76:40 - of these weight matrices that would be a
76:41 - really interesting thing to try I will
76:43 - do that hopefully in a future video so
76:45 - I've got copy I've got mutation I've got
76:47 - the flappy bird game I've got the neural
76:49 - network library I've added cross the
76:51 - room mutation so we're ready now we're
76:53 - actually ready to implement the genetic
76:55 - algorithm I'm going to say this twice
76:56 - I'm someone in the chat pointed out that
76:59 - the neat algorithm neural evolution
77:02 - augmented topology things refers to a
77:04 - very specific implementation of neuro
77:08 - evolution in a specific paper and
77:10 - honestly being much more informal about
77:11 - this here so technically this probably
77:13 - isn't the neat algorithm and maybe I'll
77:15 - mention that the beginning of the next
77:16 - video - just to emphasize it a bit more
77:18 - alright thanks for watching and I will
77:20 - see you when I continue all right let me
77:30 - look at the time where are we time was
77:33 - it's 4:40 I think I should stop because
77:35 - the next aspect of this I think I should
77:44 - stop because I don't have very much time
77:50 - left and it's definitely going to take
77:56 - longer than 20 minutes to finish a neuro
78:00 - evolution
78:01 - let me just check some things he
78:11 - let me just check some things here I'm
78:14 - just looking to make sure I don't have
78:15 - any important messages that I need to
78:17 - deal with right now
78:18 - and I don't seem to okay I do see that
78:22 - some people have suggested pi day to
78:24 - calling challenges which I'm excited
78:25 - about
78:28 - so what have I done so far today I have
78:33 - done one video about linting which needs
78:37 - a follow-up I have done one video about
78:39 - remote maybe I could do my load bites
78:46 - pull request this could fail this is
78:49 - probably gonna fail horribly because I
78:51 - mean more time for this yeah it's a bad
79:02 - idea
79:02 - III want it I I let me just say what I
79:04 - was thinking about doing I think it
79:06 - better for me to just it's not good for
79:08 - me to try to do something I have a very
79:09 - limited amount of time especially
79:11 - anything that's complicated that this
79:13 - will be way more complicated but I want
79:15 - to go to up what I want what I was what
79:17 - is thinking about doing that I think
79:19 - would make a useful video tutorial and
79:21 - so let me add a comment here about this
79:23 - at least load bytes so so this by the
79:34 - way is a github issue that I opened for
79:37 - the process for the open source project
79:39 - p5.js which is a project that I'm very
79:41 - much involved with a wonderful community
79:43 - and open source project where I created
79:47 - a version of the load bytes function to
79:49 - load a binary file to a JavaScript
79:51 - program which doesn't at present exist
79:56 - but doesn't at present yeah
80:00 - which doesn't at present exists in p5 if
80:03 - you go and look in the p5 source code
80:04 - will say like to be implemented and so
80:07 - some people gave me some feedback here
80:09 - and I just keep it's been on my list
80:11 - like every day oh let me pull request
80:13 - this and then I thought oh it would be
80:14 - useful to actually make this
80:16 - contribution to p5.js in a video
80:18 - tutorial but I also feel like oh what am
80:21 - I ever gonna get to that so let me make
80:22 - a note here and just say FYI I am
80:27 - writing this while live-streaming I
80:32 - would like to make a video tutorial
80:36 - where I walk through the steps of how to
80:41 - contribute this in enhancement but I did
80:50 - not get to it today I I think this would
80:55 - be useful so I am holding off on
81:03 - submitting until another recording
81:06 - session so this looks right okay so
81:21 - wonderful so out of that note there
81:24 - [Music]
81:27 - thank you to everyone who has been
81:31 - monitoring the YouTube chat today I it's
81:33 - not an easy thing to do and I really
81:35 - appreciate it it's very important to me
81:37 - that the community that participates in
81:39 - the channel is kind and welcoming to
81:42 - everyone and so as always if you're
81:45 - running into any issues during any of
81:48 - the live streams please send me a tweet
81:49 - or an email you probably can figure out
81:50 - a way to get in contact with me I'm
81:52 - happy to do anything I can to help so I
81:58 - think what I'll do is so I'm gonna go in
82:00 - about five or ten minutes I'm happy to
82:05 - maybe take a few questions let me
82:10 - actually also I don't think anything
82:15 - that I've done has broken any of my
82:16 - existing examples wouldn't it be nice to
82:18 - have unit testing here to see but I'm
82:21 - going to say get add and did I say to
82:25 - everything that I've done
82:29 - and I'm gonna say get commit oh let me
82:33 - do something this is my new thing that I
82:35 - do that I like git commit and Adam
82:39 - editor wait
82:41 - so let me do this I want to make a video
82:44 - about this but yes this is what I want
82:49 - so what I'm doing is I'm associating
82:52 - Adam as my core text editor with git and
82:56 - so I'm going to do this and the weight
82:59 - is there because when I say commit I
83:02 - want it to wait to make the commit till
83:04 - I finish typing in Adam so now if I say
83:06 - git commit it'll open up the message in
83:11 - adamant actually I'm gonna I'm gonna do
83:15 - ctrl C because what is the thing where I
83:17 - say - V is a - V or verbose somebody
83:20 - must know where it I should really just
83:25 - make this a video right now 15 minutes I
83:31 - I just like when I have a nugget of
83:34 - something I think people find it better
83:36 - if it's in the right place and it's an
83:38 - edited chunk as opposed to a random
83:40 - thing all of this is a bad idea and the
83:46 - chat of being told that ass usually Adam
83:48 - for this is a bad idea
83:48 - guess what yeah it's all a bad idea we
83:51 - should all just go back relax by the
83:53 - fire with our friends and read a book
83:55 - frankly you know playing the violin is
83:57 - something that I did for 12 years and I
83:59 - approached should spend more time doing
84:00 - that what is it for both nobody's
84:04 - answered my question I was oh you
84:11 - already is asking when is the next ITP
84:12 - show I think it's May 15th and 16th you
84:15 - should come to it in person but I will
84:16 - plan on doing a live stream from it
84:18 - again I guess I could just try - B yeah
84:25 - that was it shows me all the discs look
84:27 - at that boom boom boom
84:30 - I like that it's good okay so alright
84:34 - why not
84:39 - all right use a sensible editor like VI
84:44 - but i but but but but but
84:49 - oh nice lollipops you can't really do
84:55 - that it takes time and googling all your
84:57 - questions you need a violin coach am i
84:59 - you know I can't tell with you but
85:02 - whether I'm doing alright fine I'm not
85:03 - gonna do my video about commit right now
85:05 - you guys have thrown me off I'm gonna
85:07 - just use Adam because that's the way I
85:09 - like it and I would say commit - V so
85:12 - what I like about this is it opens up
85:13 - the text editor where I can write my
85:15 - commit message adding and it'll actually
85:18 - even like do stuff like hey start that
85:19 - with a capital adding adding code from
85:26 - March 8th live stream on neuro evolution
85:33 - because it doesn't like that I went so
85:35 - far on one line or too many yeah
85:38 - adding code for March 8th live stream
85:41 - this here is the start of code I need
85:47 - for my neuro evolution examples so far I
85:58 - guess it wants me to so far I have only
86:01 - implemented copy and I've only
86:11 - implemented copy and mutate I I still
86:17 - need to do to do crossover these I don't
86:30 - know I feel like that I don't know where
86:32 - the period should go I'm having a
86:33 - grammar moment the period I thought
86:35 - should go inside but then the print
86:36 - these threw me off ah I still need in
86:39 - lacrosse I'll have an ant ear how about
86:42 - this okay met up I still need to do
86:49 - crossover
86:50 - and and implement the GA itself of
86:58 - course for for for reference the live
87:06 - stream is here live stream is here so
87:13 - how do I get YouTube the coding train
87:18 - yeah I really got to go actually cuz and
87:21 - so if I go to code and train live
87:23 - this will be the York this URL will go
87:28 - on forever and so this I think is a nice
87:31 - thoughtful commit message that explains
87:33 - everything
87:34 - I am now going to hit close and it added
87:41 - that is it March 8th hope it's part its
87:43 - virgin I mean nobody noticed that
87:47 - there's like an ax men thing right get
87:50 - commit amend wonderful thing ever it
87:58 - just happened this would have been so
88:01 - good for me to make this into a video
88:02 - tutorial you all of you who shamed me
88:05 - for using Adam as my text editor you are
88:07 - depriving the world of a nice standalone
88:09 - tutorial I'm gonna tweet a link to this
88:13 - time code in this video for people to
88:15 - see and find adding code for March 9th
88:17 - live stream and I'm going to close that
88:20 - and there we go
88:24 - get log - 1 we can see here's my message
88:28 - and now I could say git push origin
88:31 - master and we will go to the toy neural
88:40 - network and I will look at the commits
88:44 - and look see ix is right oh it's running
88:47 - some tests I forgot that I have oh I
88:49 - should have pushed it I forgot
88:51 - I meant to push it to a different branch
88:53 - and then merge it oh well at least the
88:57 - test passed and we can see look see look
88:59 - at this this is the world we should be
89:02 - living in where people write thought
89:04 - full and kind commit messages the date
89:11 - is right I know the dates below the
89:13 - dates in there but it's fine
89:14 - all right I've got to go I even just got
89:16 - a calendar warning I've got to read some
89:23 - and I'm just reminder that I'll be back
89:25 - on march 14th for pi day submit your
89:29 - coding challenge pi day coding challenge
89:31 - ideas - rainbow topics link in the
89:34 - description below 7705 2050 490 1961
89:40 - five hundred fifty eighty four thousand
89:42 - seventy one three thousand five hundred
89:43 - seventeen twenty eight thousand nine
89:45 - hundred fourteen forty eight thousand
89:46 - seven or sixty to seventy two thousand
89:47 - nine fifty to ninety six thousand thirty
89:48 - seven alright I read some random numbers
89:51 - and I'm gonna put this song on goodbye
89:55 - everyone but it now has a different
89:57 - commit date to its creation date or
89:59 - something oh boy I don't know I don't
90:01 - know just I can't get it right to
90:04 - everybody it is what it is
90:07 - thank you to Darius cos Amy who gave me
90:12 - a lot of great tips on open source stuff
90:15 - in the last couple weeks I'm working by
90:16 - the way on an idea for a course about
90:18 - contributing to open source that I
90:20 - wanted to go and teach next year and
90:21 - then why you have a github repository
90:23 - with ideas in it it's a point out
90:25 - puppies you could find it probably few
90:27 - want but I'm soliciting contributions
90:29 - and ideas for that so and I will
90:32 - continue to make more video tutorials
90:36 - I'll be back next Wednesday there will
90:38 - be a surprise for you because I am going
90:40 - to shortly record a guest video for the
90:44 - coding train channel that will come out
90:45 - sometime in the next couple weeks so
90:47 - stay tuned for that and I'll see you all
90:54 - [Music]
90:56 - I'm still here looking at my phone now
91:02 - [Music]
91:12 - alright everybody I'm gonna hit stop
91:17 - screaming now so I probably won't return
91:22 - to the neuro evolution thing next week
91:23 - that'll come two weeks from now so the
91:25 - next live stream there also be a guest
91:27 - two weeks from now I mean I have a lot
91:28 - of guests scheduled that I'm excited
91:30 - about like three or four so far coming
91:31 - up okay next live stream Wednesday March
91:35 - 14th probably in the morning of my time
91:37 - and then March whatever it is like 23rd
91:41 - I think is the next Friday
91:42 - alright bye everyone

Cleaned transcript:

oh hello good afternoon welcome to part two of today's coding training lives dream Internet coding show episode thing my name is Dan and I will be here with you for the next approximately one hour and thirty minutes I don't have the YouTube chat going I just realized so let me see if I can pull that up all right hmm I don't see anybody say anything like you see me and you hear me but I assume that live streaming so what's going to happen so first of all welcome I think I already said that okay yes see the third people in the chat this morning I spent some time talking about my new favorite subject linting universal sign for lifting up my phone is okay notification YouTube notification YouTube notification watch now the coding train is notification I probably always get that I don't notice so this morning I talked to Bill about linting which is kind of new I did a little video about github not github sorry get remotes one of which can be a github repository and you're gonna have multiple different the same repository as part of different accounts on github as different remotes boy that was like it was like a thrilling topic that crowd was in front of me they were standing they were cheering was suspenseful they were crying or tears and now it was me with the terminal typing like git remote add but someday we can all dream can't we that I'll be here with my tuxedo and the masses out there will be with their laptops coding along tears because they're so moved by the coding train no no yeah very unlikely now I'm really not sure I think I better stick to what I said I was gonna do because I I know that'll be disappointing but I'm a little bit concerned with the smallish amount of time that I have so I'd like to just get started but let me make an announcement there will be no this is the bad news there will be no coding train next Friday March 16 however the good news is there will be a coding train next Wednesday March 14th which as you may know is pi day 314 so I'm gonna be out of town on Friday I'm not available scratch that I'm just not I'm not gonna be available right hey edit that out never mind good dark fun Friday I'm not available on Friday but I will be yes but I will be on Wednesday and since it's pi day I thought what I would do is do some PI themed coding challenges I was hoping that maybe I could also have somebody I'll come here with like an actual PI and it could open the door and I could get like this one of the pie in the face thing anybody knows a way for me to make that happen I don't think that'll be an excellent thing to do in a livestream on pi day but I'm looking for what I'm looking for our suggestions for things that are sip like phyllotaxis is a good good example of that of like it's a little algorithm it's kind of got it's got pie in it blueberry pie my favorite kind of pie and so some things that I might maybe I know that stand up maths the YouTube channel Matt Parker did an interesting video I think last year about like calculating all the digits of pi maybe I could make a piece of code that does that and displays them in the browser so so I am hey Jen maybe do Python know that does not count Python does not count as something for pi day especially because I I mean actually it does totally does but I don't know if that's gonna happen but I'm sorry I would like to take your suggestions so where can you leave your suggestions for let's do this for now rainbow topics I probably should come up with a better system but I am going to create a new issue pi day coding challenge suggestions please um add your suggestions for a pi day coding challenge in the comments below and you know maybe we can do some kind of like I guess I could do like a maybe I should do a reddit subreddit thread because there is a coding train reddit should we try that I don't know I'm so used to github I'll close this and move it there if somebody has a better suggestion but I'd like to let me just do this right now so you could add your suggestion here on github and then people can like you thumbs up the ones that they like and I can kind of look through and pick them that way you could do a tag on the github topics I could also do that that might be better for me to just do a label yeah let's not do it in the comments let's close this issue nevermind let's use a label I just thought it might be nice to go see them all in one place but with labels you can do that to use a label instead alright the trolls are out in full form today let's quickly quickly lock this conversation that's too heated it's definitely too heated probably and let's add a label oh let's go to issues labels new label pie day challenge for three I'm gonna be very your I'm gonna I'm gonna do this because this is the kind of person I am I'm gonna put my date with the day then the month that's what I'm gonna do and I'm gonna let's get a color here oh that's perfect okay so now you can file an issue and I will label it as pi day like this okay so send me your ID now next up moving right along let's see am I in the correct directory let's just see what's been merged thank you again to me I am Sammy who's been merging some up merging some of these flappy bird pull requests put git pull github master and we got some stuff I think we have some unicorn horn pipes loading loading loading horn filled flip dot PNG mmm looks like we're missing a file let's see what I can figure out their graphics horn body filled horn tip filled horn body filled horn tip fill ooh horn tip filled flips do I need that one paint pant dot ooh horn tip filled horn tip oh those are like I don't know what that is paint.net um these were made by a github user these were made by github user um just wanna k1j Julian Kay 1g Julian King Julian and we can see them here these lovely unicorn horns so I'm a little bit concerned I might really actually just want to replace these with just like drawing rectangles mostly because I'm a little concerned about having the collision I don't want to introduce extra complexity with the collision detection maybe me I am so me maybe whoops get pull get pull I can't like I almost can't do without saying origin I just keep till somebody fixes it and I just keep doing this I'm just kidding just kidding let's take a look here let's look at the pipe where is it being drawn draw this became incredibly came very complicated the good news is I was okay with all this becoming complicated because it's all just in the pipe class and I actually don't really mind if the pipe class is the super complicated thing because what I need to demonstrate will happen can be applied to any game whether the code is very simple or complicated so let me see here what is the image that's missing I'm sure there's a crack team of coders working ok now is fixed okay thank you me I am so we let's there we go looks like we've got that fixed let's go here and there we go so now we have our beautiful flappy bird game so slow on my machine and so let's see about this collision stuff yeah that's make sense but let's see let's see let's see yeah so it's it's it's using the rectangle up to the height of the tip of the unicorn horn I almost look like candy here but now I'm gonna change these two rectangles I can't tolerate it I'm sorry everybody this is I really like these designs and I will maybe come back to them if I don't know if me I am so me while you're watching if you while I'm kind of getting I'm gonna explain the github maybe maybe make it a separate branch or just like push push these unicorn horns into another branch but let's just make these a nice purple green blue rainbow colored rectangles there's also something running really slow and I don't know why because it's not yeah it's running at 30 frames per second just reasonable so let's do that if anybody can do a pull request I so cuz I think these are a problem there they're lovely beautiful unicorn horns but I think the collision stuff and yeah they kind of they resemble the poop emoji with but they're pink there's you know there's a lot of there's a lot of it's just gonna I'm just gonna be uncomfortable the entire time okay use of instead of Adam okay let's get let me let's go over to the whiteboard for a minute here and let's do some erasing or let's not do some erasing I'm thinking so I want to talk about neuro I want to talk about neuro evolution and I think that to start talking about neuro evolution it wouldn't be the worst thing in the world for me to have my left over ending diagram from the doodle classification example because one of the key things I want to talk about is the difference between train whistle what about like a train whistle for the pipe here somebody can like right I don't know just make it a dis a rectangle a plain rectangle okay is the chat going on in here okay so what I want to talk about is how back propagation and gradient descent while the sort of standard or probably most well known technique for training the weights of a neural network is not the only technique and so having this as a reference is good and then I need to I'm gonna and then what I'll do is erase this and diagram out how a genetic algorithm can be used to train a neural network okay so I think I'm gonna get started with this right now let's let's take a look here oh let's look at Simon's pull request so here's the thing this is very useful Thank You Simon for this I'm gonna not oh I'm gonna not merge this right now because I want to I'm gonna have to sort of like the mechanics of the way the game works once I'm doing the evolution thing is gonna be pretty different okay okay just push to change bath two pipes great thank you so now I'm gonna do this get pull origin master ah kit pull I just renamed it back to origin and let's go here there we go oh look at those green pipes perfect Oh green pipes everything is right with the world this is very quite hard to play but I'm also probably in the end gonna change this into circles but okay that's good I also let me just see something here yeah now this is great okay perfect okay thank you for that now let us begin I'm trying to think if I want to what am I doing here let's go to so I'm gonna go away from flappy bird for a second let's get rid of this come on computer terminate you okay then I want to open this up and Adam what's going on in here because I changed some stuff at one point and I need the neural network library and that's good XOR so desktop your own network coding train up am I in the wrong I've been in the wrong camera screen all this time again as I always am sorry everybody sorry everybody sorry buddy you know I don't know I'm doing my best doing my best okay let's see what do I need now examples XOR let's copypaste that and let's just do what's called neuro evolution so I'm gonna have an example and I'm gonna I want to do it with XOR but but let's leave that out for right now okay all right and so now I also want to yeah no kidding examples doodle classification oops train alright alright tests pretty good let's get the rainbow here yeah here we go alright so that's gonna be okay so this is weirdly not Chea for when you and everybody watching this is weird this is a new chapter that I so the nature of codebook if I go to the books website I am currently working on a there chapter 9 is all about genetic algorithms and I have a video tutorial series about genetic algorithms chapter 10 is about the basics of neural networks and that the content that's currently in the book at the moment is pretty its from 2012 lots happened in the fields machine learning since then you might have noticed and so and so I'm trying to rewrite this chapter and I've been making these video tutorials and examples in this neural network library and working on this other thing called ml v with just built on top of deep learned j yes all of which I am trying to get through and to do more and more stuff with um that I'm currently working on as part of a new chapter 10 and now so chapter 9 being about genetic algorithms I don't I think calling it the evolution of code is sort of silly and I should just call it evolutionary computing or genetic algorithms but that aside chapter 10 about neural networks chapter 11 which does not exist is going to be about using genetic algorithms with neural networks so I'm about to start the first video for chapter 11 and repeat exactly what I just said because I think that would be useful for part of the context because weirdly typically in the past the for all my other nature of code content the book came first and the videos came after the fact and this is a little bit strange that I'm going to make the videos and then hopefully write that content into the book all right so I'm about to get started and I'm will get started I'm just looking at my phone because ah because as I mentioned I have a very exciting guest video that will be released to this channel hopefully next week sometime but certainly sometime in the next week or two and that guest is coming to record at around 500 p.m. today so I've got to finish up by then and just in case you were on the fence of whether you wanted to be a patron of the coding train sometimes when especially when I have guests I'll keep a livestream going just to get a back up onto YouTube and I'll let people in the patron group kind of listen in on that as well but don't worry there's no need to be a patron of the coding training the video will be released but if you like a little advanced stuff water no I don't have any water this this this liquid beverage will have to do it's so Slurpee can you hear that like terrible coding train brought to you by anonymous cup of coffee I usually don't drink coffee while I'm livestreaming because it only leads to bad things but every once in a while it's just it's just necessary all right so let's close this out let's leave whoops how would I do no I don't want you to move anything let's leave this here move this out here I'm just getting everything ready come on come on Adam behave I'll do this okay all right here we go oh oh yeah it's fine it's fine I don't have a marker but I will I don't know if I need one this second but it's like a some people have like a lovey or a teddy bear that's their comfort object it helps them sleep at night I have a whiteboard marker it's my comfort object hello welcome to the first video in a new chapter of the book nature of code chapter 11 only strangely chapter 11 does not exist so I'm doing something a little different here where all my previous other nature of code videos that go along with this nature of code book the book was written first came out in 2012 and and this is the current version of it and then I made videos after the fact now what I'm going to do I want so chapter 9 is about genetic algorithms and chapter 10 is about neural networks and I have a set of video tutorials that go along with both of those chapters today I'm going to start talking about something that I want to be in the next edition of the nature of code in Chapter 11 called neuro evolutions so I want to take the idea of a genetic algorithm and a neural network and use them together in a magical way to make wonderful things happen on the screen or or doesn't have to even be on the screen in some other capacity that I can't even imagine right now so what is it that I am going to do so first of all okay so if you are on keyboard if you have watched some of my previous someone might other neural network tutorials you the most recent thing before the recording of this video that I made was a doodle classifier it's kind of the classic machine learning classification example I have some images maybe they're handwritten digits maybe they're doodles of cats and rainbows and unicorns and all that sort of stuff and I want to feed those things into a neural network and I want the neural network to classify them and if you've watched those videos you might have noticed that there's this whole elaborate training process the training process involves making that guess having some labeled correct data and then feeding that and then looking at the error like what is it supposed to be versus what it guessed and feeding that error back through the neural network just timeout for a second okay I really take seriously the at messages to my watch so little notes to me don't look thank you for the kind comments okay come back come back to my momentum so try not to a direct message during the live streams well yeah I probably should set up a separate slack user that just sends the notifications so that regular other messages don't get here all right all right all right let's see here looking at the guest output versus the correct label calculating an error and setting that error backwards through the network through a process known as back propagation where all of the weights are tuned and changed so while this is the most wellknown and probably most common and sort of standard technique for training a neural network back propagation with gradient descent very fancy sounding there are many other ways I mean there there's other ways that you can train a neural network one of which is using a genetic algorithm so what if we just threw away all of that calculus math and all of this sort of like error this error that and back propagation and we just said hey I've got an idea why don't I make it's never having one neural network why did I make a thousand of them and I'll try them all maybe some of them will classify image when one will classify images better than another one does maybe I'll keep that one and one just really gets everything wrong maybe I won't keep that one at all and maybe I'll pick from the ones that kind of do well and take those and duplicate them or mix them up to make a new population of neural networks and see how those do and this is the central idea of a genetic algorithm now I might suggest if you want to if you jetta carbs are totally new to you you might want to pause this video right now and go watch a genetic algorithm tutorials if the concept of a neural network is totally new to you you could pause and go watch those tutorials but you could probably also just keep going good I'm gonna I'm gonna cover almost all of this stuff anyway and as I try to sort this out so I'm gonna take a break for a minute I'm gonna erase this whiteboard here what's there right now left over from the doodle classification and then I'm gonna diagram out how a neural network can be trained using a genetic algorithm and then through that diagram I will discover things I need to add to my neural network code base and at some point if all goes according to plan you know I have this particular this was the doodle classifier example where did you see it's classifying my rainbow but what I want to do is take this version of the game flappy code egg train it's not very flappy I guess and see if I can use a neural network that is that evolved to play this particular game so that's going to be the goal of this series and then I have all sorts of other ideas for other types of neuro evolution tutorials I believe this is often also referred to as neat neat algorithm because it's neat neuro evolution of it see here's the thing I was just saying gyro evolution and all the while that could sound so much smarter by saying neuro evolution of augmenting topologies that's totally neat alright be back in a minute um okay I'm looking at the chat everything seems okay so now I need to do is go erase this man this copy is so slurpy alright so let's do some erasing if anyone has any questions I knew there is a reason why I shouldn't Rex this whiteboard for like two weeks I knew my microphone is this like some horrible scratching sound that I'm making you do it's really better if I erase it immediately after I use it anybody has any suggestions maybe there's like a squeegee company and I could hire to come in here just like big this shiny and new spickandspan you know I said earlier today that my passion in life is indentation and spacing and this is why linting this idea of linting your code is bringing such joy and happiness to me and yet in this moment right now I think I might have discovered that my true passion his whiteboard erasing there's nothing more soothing and meditative it's a good exercise it's good physical therapy for my broken elbow that's pretty much all the way just water my hair look nicer no nasty chemicals no cleaning anything just new water paper towel but now isn't so good for the earth but I guess the earth will have to survive means like in paper towels I can get a nice cloth or rag that's more reusable how am i doing how's this whiteboard look to you all clear pretty dirty to be honest with you I don't know what it looks like yeah you can see all those like smudges and things come at you I'm sure you want to edit a highlight reel of be erased important you do what are those like things in the video where like you do it really really fast instead of just doing a jump cut I don't know we've gotta get we gotta we gotta up our coding train game here okay all right I'm glad to see that the chat has moved on from discussing which coding language is the best to how to effectively clean a whiteboard that's actually not discussing that it's just that's what I wish for we're still like discussing MATLAB versus Pascal versus Java and yes do you think I could have a YouTube channel where I just clean stuff and I'm like happy about it all the time I really like that I think I'm starting a gaming channel because I recently acquired a Nintendo switch my kids and I we play it a lot and I was thinking I don't know maybe I'll make a gaming channel that's what that's all the rage a 44 year olds human who can have a youtube gaming channel people might watch probably not that's a terrible idea okay I don't know how to set that up I gotta like get the output into the input to the output this sort of thing a twitch channel and a little bit afraid of twitch but I am a kind of a twitchy person alright let's see take a second I'm getting a very excellent suggestion in the chat take a second and mark the edges of the whiteboard so you don't draw off the camera but what would be the fun in that that's all we should be living on the edge here don't you think alright let's see let's make this happen yeah I have um I think I'm kind of all better or worse I don't think that I can manage to like do different platforms and I've kind of got the YouTube thing going so that's probably what I'm gonna stick with but I don't know I'm always conflicted about this sort of thing okay so let me use a different color to mark the edges this is actually already marked up here I do actually have some markings already this is Mark's here this is probably more correct and this is mark down here it's not right I could promise I can't see my monitor down down down down down down down there and over here maybe over here this is really this is really the important one this is the one that I mess up all the time can you see that line to be where the tip of my finger is I think I got it right okay now that'll hopefully do okay so I'm just checking the chat doesn't your current streaming setup not take the output it's vegan but it does the output goes into the input here uhhuh it does it totally does and then the input goes back to the output and back into a different input also I'm really inputoutput things totally working I have all the plugs going from one electronic machine to the other electronic machine and they're connected to the Internet tube that's how I'm livestreaming so I could probably set that tube inputoutput plug based system up in my place of residence I should really get content what time is it 355 okay okay okay everyone okay so let's work okay now that I have them now that I have a blank whiteboard let me review the steps of a Janek algorithm and think of them in the context of a neural network so the first thing in a genetic algorithm that I need to do is create a population and the population is going to be a whole lot of neural networks neural networks are the individual elements so maybe that population is 100 neural networks I need to evaluate fitness of neural networks okay so this is kind of like again this is kind of like the setup I know that's kind of getting close to the top there it's the thing that I'm going to do once at the beginning of the program I sort of initialization state and this is this thing that I'm going to do for a loop you know generation after generation in you know in p5 this might be called the draw loop I mean to evaluate the fitness of all the neural networks and then create a new population and the way I will do that is by pick quoteunquote parents based on my handwriting is getting worse and worse over time based on pick parents based on fitness scores maps of probability it's so much room in this direction probability and then I want to apply crossover which is a way if I pick two parents for example I can take half of their socalled digital DNA of one and half of the other or some random amount of one and random out another and combine them into a new entity and then I can apply mutation which would be which is the step of saying hey let me look at the D let me I have this child DNA that is made from two parents let me randomly just change some of it up as if it's spontaneously mutating to continue to have variation in the system so again you could go watch my genetic algorithm tutorials where I describe all this stuff in much greater detail of different techniques and why and how but this is the basic idea but you might remember if you did watch those tutorials that this is kind of like the algorithm and it you know obviously you could change it and be creative with it but it's kind of somewhat of a standard the really tricky thing when you're making your own genetic algorithm and applying it to your own project is as follows number one is this idea of genotype versus phenotype what is that socalled digital DNA the genotype what is the data of that DNA and what is that data do how does it express itself into a system so this is really key in thinking okay with a neural network is somehow the genotype what could be the data so in fact thinking back to my simplest neural network which is just has a two layers really a hidden layer and output layer the inputs come into the hidden layer they get processed from the hidden to the output they get processed and then we have a final result so the core elements of those layers are weights and biases so all the weight matrices and the bias vectors those things which I described a detail of my neural network tutorials make up the genotype of the neural network the core aspect of it now the phenotype is the expression it's really really what am I using the neural network for so for example the expression of the neural network might be in the game flappy / bird the decision whether to jump or not jump that's the expression that's how it's going to be used applied in a given scenario in a classification example could be it's classifying an image that's how the data from the neural network is going to be used to make a guess based on this image and and and turn it into a string so that's a spec number once we've got that so what that means is when I write the code I need to somehow figure out how to do crossover and mutation with weights and biases I think I can create probably a population of random neural networks that's just gonna be like new neural network new neural network new neural network evaluating the fitness I've got to get to I can pick two random ones but I need to apply crossover mutation and to be honest what I might do it first in my first implementation is not even bothered with crossover and not even bother with picking more than one parent so one technique to simplify the genetic algorithm is just to make copies so I can pick the good ones and make copies of them mutate a little bit and keep going it may not work as effectively as if I use crossover but it'll certainly be easier to code so the other thing that's tricky with with when you're making your own genetic algorithm applying it to your own project is the fitness function question mark witness mark question mark so this is crucial if you don't have a good fitness function this whole selection process this quarter goat natural slice not very natural here it's like digital selection this I'm not gonna be able to distinguish between members of the population that do really well that should be that their digital DNA should be passed down the next generation versus ones that don't so I want a good fitness function that gives me a good range of probabilities and so in this case we could think about the classification it could be okay well this neural network give it 100 images its Fitness is how many of those it classified correctly and we could even go into it deeper and somehow score the fitness in court according to its confidence level about classifying them correctly but that might that might be flawed in some ways also so that's one thing with the flappy bird scenario if we think about the flappy bird game what is the fitness here well the fitness could would simply be the score so I'm a neural network I am a neural network playing floppy coding train now pee pee booboo input/output people my cell like cordial so it could just be like how long am I able to go through this world without running into a pipe so that could be the fitness so I could say hey why don't you thousand of you try playing this game a thousand of you electronic neural network magic machines try playing his game and and your fitness is how long you last before you run into a pipe and so that is the fitness function so we have all the pieces so what do I have already like if I'm going for this flappy bird example I already have the flappy bird game so I have the flappy bird code I have my genetic algorithm examples but ultimately there's not really I don't really have a genetic algorithm library per se so I'm probably gonna have to build the genetic algorithm stuff in the code but I do have a neural network library so I don't have to write I don't have to write the flappy bird game I don't have to write the neural network library however it might make sense for my neural network objects to know about crossover a mutation that might be something that probably should go into the neural net library so that any moment I could say like hey you neural network a new neural network get together make another or hey you neural network mutate yourself so I probably should that's something so that's the first thing I think I'm gonna do in in the next video is add crossover and mutations or maybe just start more simply I'm just gonna start with like a copy function just to kind of get going here a copy function and mutation so those things need to go into the neural network library and then the third thing is I just need to apply the GA so this I really need to do a lot of work to write the genetic algorithm code so I'm gonna start with my flappy bird code import the neural network library add crossover slash copy mutation and then start to implement the idea of a genetic algorithm in this particular program that started with the flappy bird code that imported the journal not look library that my father bought for two zeusie anyway never mind random reference okay because it's like the flappy bird that imported the neural network library that that added the genetic algorithm that there's that there's a song going on there that somebody else will finish for me alright Passover is coming up okay so that's that okay so you've made it to the end of this first video for chapter 11 of the nature of code which doesn't even exist yet but maybe by the time you're watching it oh I'll be so happy if it exists by the time you're watching this and so in the next video I'm going to revisit the neural network library and add functions for copy and mutation I'll see you there alright how we doing everybody okay everybody okay I'm like done okay go home now no echo 34 hello Dan did you have a chance to check my firework program on Twitter I think that I did that was a couple weeks ago I remember seeing it I remember it was beautiful ascend to me again just in case I missed it I recognized her name though echo 34 you've made a lot of great contributions thank you Dan many questions about the ethics of neural nets yes I have many questions about the ethics of neural networks itself and I but I so I don't necessarily have questions about the ethics of neural networks I have questions about the ethics of the people who have access and the ability to program neural networks and how those programs are applied in society so hopefully I hope or that the work that I'm doing to provide educational materials about neural networks will provoke that discussion and make it easier and more accessible for understanding of these algorithms to be in the hands of more people and so we can ask the right questions and work together to make sure the power of these algorithms and the way that they can be applied and the datasets that can be used with them is not abused that's what I have to say about that okay I suggest you override the weights when you apply crossover all right so I think that's good so anybody have any comments or questions before oh this is not his computer's not plugged in alright anybody have any close up so let me get set up I guess for the next video again I don't think we're gonna have I just want to be honest here it seems very unlikely at this point that I'm gonna have a finished flappy bird neuro evolution tutorial by the end of today but I certainly this is a project that I intend to I intend to write a chapter for the nature of codebook about it and and yes so don't worry even though I don't finish today I will also mention that if you go to github.com slash Schiffman neural network p5 so you'll notice I have a repository called neural network p5 which says this repository is archived by the owner it has now read only and I wrote this has been deprecated and it links to the coding train neural network Jas the reason why I mentioned this is I started doing this work last year and then decided I wanted to rebuild it again from scratch in video tutorial forum but there are for example the there is a neuro evolution tutorial here example here and let's see if I go to lick click this link this is actually running what I intend now to build and so let's take a look at this so and one thing that I built into this is I can like speed it up and blow away made it very easy to play you notice that so this over time we can see like the alltime high score and what I can do now is I can also say run best so far so this is now going to just show this is the current member of the population that has been that has the best neural network so far to effectively play this game so I probably should have shown this in the video tutorial itself but yeah you know comes next come easy come easy go or something like that alright so you can look at this for reference I think there's flaws and mistakes and weirdness in here but it's something oh I forgot to tweet that I was livestreaming again this is the worst thing that I do while livestream which is that I look at my phone in notifications but sometimes I just want to make sure that my guest which I'm very excited about it's not lost and figuring out where to go okay alright so neat nature of code doodle classifier so let's go to the code here and go to this neural evolution sketch and just make sure it's importing the neural network library in the matrix library and I also need to open up P and let me do something here what changes I just want to like do some get clean up for a second cuz this is gonna go into the repo get status here what did I change so first let me add and commit the let me add neuro evolution and let me say adding new nur neuro evolution tutorial neuro evolution example nothing to see here yet and oops escape alright now let me put get push origin master this and now let me see what's going on here let me say get diff there's some weirdness I don't know what know what's going on here I'm gonna just stash this because I don't want to deal with that and now I think I'm good and I want to have the library code open neural network okay whoa oh right I have the activation function thing this I don't know no so am i call them there or no semicolon there what to do about that look never never decide I guess this is really should have a semicolon there yeah yeah that's alright just want to see what the randomize function does oh that's in the matrix library oh it's really the matrix library that I want to randomize ah I really should change this to a Gaussian distribution so it's the point I need to do that guess I will not worry about it right now boy this has had some nice changes to it oh it has you realizing that wait a second here forgot that that was in there I've had some nice pull requests and also a lot of pull requests that I haven't had a chance to look over and figure out what to do about yeah here's the thing yeah I'm up to date one of the reasons why I haven't merged some of the pull requests which are amazing pull requests is that I feel like what I want to do with crossover and mutation is going to be much simpler to demonstrate with just a single hidden layer than if there were multiple hidden layers and to be honest this sort of flappy bird scenario is so simple it's just a few inputs I'm not gonna do I'm not gonna I could do all the pixels as input that would be interesting actually that would be really I got it I remember to mention that when I get to that but I think it's gonna be easier if there is just a single hidden layer and an output later then I can just do the sort of copying and mutation stuff pretty manually you know I don't know but I like you know I think that's good I think that's a good decision that's a decision I'm making right now all right sorry I'm looking at the chat um okay yeah I can just know I can just do map right right right all right all right all right it's all right all right okay so so really what I want is some new stuff in the matrix class copy okay I found a wire on the floor can you see this wire look someone's been making some physical computing tutorials in this room that is you can't even see it because of the focus all right hello welcome to part 2 of my neuro evolution series where I am attempting to look at how I can train a neural network or more accurately a population of neural networks with a genetic algorithm I talked about this in the previous video which you could go back and watch if you haven't but the main thing that I want to do in this video is look at okay well if I have a neural network if I have a neural network how can I apply crossover or a mutation to that neural network so that will be the focus of this video I do sort of remember though I did kind of remember that there's a lot of stuff that I haven't talked about yet because one of the reasons why I use the neural network is to be able to give it some inputs and to get some output so that makes sense in the context of the dual classification example probably but may not make sense to you immediately in terms of this idea of a flappy bird game so I will get to all that but right now I'm gonna focus on saying new neural network and you know what I'm gonna stick with copy like is that I'm gonna get to crossover in a future video I'm gonna stick with copy for simplicity and mutation so let's go back let's go over to the code and let's take care of that now let's start by having I'm gonna create a variable called brain and I'm using the p5 I'm using I mean using the p5 GS library though what I'm gonna do right now it's totally unnecessary for but because the flappy bird gained this program in p5 that's going to be helpful later also I use it all the time so I don't need a canvas I'm just gonna say no canvas and I'm gonna say brain equals new neural network now when I create a new neural network object if you remember the three arguments I need to give it and this is just for this particular neural network implementation it'll work differently if you're using you know say like a different machine learning framework like Kerris or somebody else's neural network library maybe you might want to look at deep learned Dutch is I will come back to that in a future video so I need to give it a cert number of inputs well all of a sudden now we're back to that question so let's actually not worry about that right now it's not worry about that question I'm going to go back to that question and I'm just gonna make something up so let's take the XOR example sort of like a trivial example of okay it has two inputs they're either true true true false false true false false so there will be two inputs let's just have a hidden layer with four nodes and one output so we can create some simple neural network and what I want to be able to do is I want to say let a cultlike child equal brain copy like I want to be able to say let me make a copy of that neural network and I also want to say something like child uh mutate so let me take that copy and apply a mutation in it mutation which is something I described more in the genetic algorithm video series so what this means is this neural network class needs to have two new functions it needs to have a copy function and needs to have a mutate function so let's go into the neural network library code this is the class there's a lot of stuff in here I have a way to many videos going through all this code luckily we can kind of ignore all of this and I'm just gonna go down to the bottom here and I'm gonna say something like adding functions for neuro evolution now the truth of the matter is what is it that I really want to copy well if you recall the neural network structure is such that if there are two inputs and four hidden nodes and one output the neural network looks like this it's connections between the inputs and the hidden layer and connections between the hidden layer and the output and these connections the the the sort of dials of the neural network the date of the neural network what controls how the information flows from the inputs and out through the output are all of these weights and these are stored in matrices it's just a whole bunch of numbers so I have a weight matrix which goes from input to hidden I have a weight matrix that goes from hidden to output and with each of these I also have this thing called a bias and if you recall the bias is something you know really all I'm doing in the end is like all of this really boils down to like hey there's a whole bunch of points just fit a line to those points and the plastic to be like move the line a little bit up move the line a little bit down so that's really even though this all seems like fancy magic ultimately that's what it just boils down to in the end so I also have the bias for the hidden and I have the bias for the output I don't know I don't know what the best way to write the notation for this is or yeah so all of these things when I want to copy the neural network what I'm really saying I want to do is copy all this stuff so let me go ahead and so what I need probably is a function inside of all these are all matrix objects I need a function probably in the matrix class to say copy so let's start here and say copy and you know clone could be used and I want to make a deep copy I think not a shallow copy these terms get thrown around a lot in computer science deep versus shallow but I don't want to like just point to the data I want just giving my own version of all of those numbers because I'm gonna mess around with them and I want you to keep your numbers I want to mess with your numbers so instead of just saying like I I'm a new neural network can I just point over to your numbers I really want a whole version of those so and if we go back to here so things that I need to do is I need to keep track of these properties input/output the total input output and hidden nodes so I want to say let let's just put this in here so I want to say let input nodes equal this dot input nodes let hidden nodes equal this dot hidden nodes and let output nodes equal this dot I'll put in you know what this this is very poorly named this is sort of silly what I'm doing but I'm going to do it anyway input nodes this is very this is me this is how I like to code I like to make things as longwinded and as possible so that I can really think it through and explain it like all I'm doing right now is taking the properties of the neural network I wanna cop and put them into local variables why because I want to say neural network copy equals new neural network with what I have a better idea how to do this I have a better idea how to do this so this could work but I have a better idea so actually what I want to do is I kind of just want to say this return new neural network this now you might be asking me like I mean this is this isn't gonna be mad this isn't just gonna work but what I'm sort of realizing here is maybe I don't want to copy everything here what I actually want to do is call the constructor but give it a reference to the existing neural network and then have that constructor instead of creating a new a new wait new weight matrices that are random it'll create weight matrices that are copies of the existing one in other words let me go back up to the constructor and look at this so what if so the constructor gets three things right a B I could just like rename these the parameters of the constructor function for a second and just call them ABC right a being the input nodes B being the hidden nodes C being the output nodes but before I do that what I actually want to say is is a an instance of instance of a neural network else in other words this is how I want to create oh I turn a pause I want to put a preference back that I turned off this morning and when I got to check the time for 2222 packages beautify I just I really like having it beautify on save so I'm so used to that on some put that back okay this here so what I want to do is if if I'm being sent three integers then I want to make the neural network the way I always have however if I'm being sent the first argument and this is this is kind of this is known as overloading typically in a programming language like Java if I had to overload the constructor like there's two different ways I could call the constructor I could give you three numbers to make a brandnew neural network or I could give you a neural network to make the copy of yourself I would write two versions of the constructor but in JavaScript you can only have one version of the constructor but you can kind of check what you're passing in and just to be clear about this let me just make sure this instance of thing is correct so if I were to say let a be a new new neural network 4 4 3 2 just arbitrarily and I could say a instance of its without a capital instance of a string I should get false instance of a neural network I should get true ok so that's right this this of should be lower case though if a is an instance of a neural network then what am i doing then I'm saying this dot input nodes equals a dot input nodes like I can start right here's where if it's not a neural network I'm actually assigning at the numbers that are coming in if it is I can just keep going Oh actually what am i doing I can say this and this should be hidden and now I can say this maybe I should have somebody has a suggestion for how to name these in a better way I just I didn't want to name them hidden input hit in output anymore because a could be either of those things so you know this this may be like to do document what a B C our output nodes that's a little note to myself that I don't like what I've written here and then now let's look at these so I'm not going to need to in demise the weight matrices because I'm just gonna say equals what am I gonna do here a dot weights dot dot copy write what I want to do is say hey my weights are your weights and now my input to hidden weights are your weights and my hidden to output weights are your weights now is this gonna run I don't think so because I probably have to add a copy method to the matrix object but I'm getting somewhere now what else do I need I need the biases so I need to set the biases so the same thing I'm just doing a lot of little like copy/paste stuff here so I need to set the hidden bias values and the output bias values okay so this is me creating this new copy of a previously neural network and then you know right now it looks like learning rate and activation function or at the moment even though I have different activation functions I can kind of write is this default is getting set to sigmoid as default is getting set to 0.1 so I probably should copy those as well I'm just gonna just be simple about this right now and just assume that my program is never going to change learning rate or activation function I should that should be a to do to do copy these as well at some point but I don't need to worry about that right now and to be honest the learning rate isn't gonna play a role anymore the learning rate is completely irrelevant the learning rate is specifically a tied to the tied to the gradient descent algorithm which I'm no longer really using with the with the with the genetic algorithm that's what I'm doing now okay we're getting somewhere all right so just out of curiosity remember this is the code I'm making a new neural network and I'm let I know I haven't done mutate yet but is this even working is that was there but did I maybe in some other universe happened to write a copy function already into the matrix class I seriously doubt it but let's see yeah copy is not a function so what this means is I need to also go into the matrix library and I think this this I think is worth having in here that's not just this isn't exclusive to genetic algorithms our Nura evolution like so I'm adding this stuff you know copy and mutate and crossover will be here specifically because of genetic algorithm but the matrix I can the matrix object I can be a little less formal about this so what do I want to do I want to write a function copy and what do I do I'm going to say let m equal a new matrix with this dot rows this dot columns so I create a matrix object with the same number of rows and columns and somebody in the chat I know is going to tell me there's some very fancy way that I could just instantly use some higherorder array function to copy the whole thing over but because I am Who I am I'm gonna say I'm going to write a nice little nested loop I can always refactor this later I just know this is gonna work and I'm going to say m dot data index I index J equals this this dot data index I index J so this is manually me looping through the entire matrix it's a grid of numbers it's all the weights of the connection of the neural network and just manually copying them over one by one and I think this will work I have some breaking news from the chat that I need to mention it's that good but it's not relevant to what I'm doing right now so I will come back to that alright so let's see let me hit refresh there we go so does this work I have two neural networks they both seem to have two for one two for one you know I could go let me look at one of these biases look at these values so this is bias H these are the values can you memorize those can you remember them let's go down here okay nope nope something is wrong so this stuff did not get copied guess what guess what I forgot I forgot something quite important in this function I did not forget to return the thing that's new but in the matrix I forgot to say in the return end so this new matrix that I'm making I've got to actually return it I made the copy you can make the copy you can take the reservation but you can't hold the return of the copy and I'll sign people Seinfeld reference for all of you try should have said that it'd be more interesting if I didn't say that okay so now I can look here and I can see okay look at these numbers memorize these numbers this is the way I actually let me look at I'm just gonna look at one of the biased ones this will be a little bit simpler so here's the bias bias hhas these values memorize them memorize them I could write a nice unit test to actually see if this function works I'll be much better but this has been I ball it 0.20 eight four one five yep these look like the same numbers same numbers say members so I'm gonna just choose at the moment to believe that this worked this is probably a bad idea but this is the reality of what I'm going to do and I'm okay with that so let's move on so we have now implemented copy when I next need to do is implement mutation now I do need in order to do mutation I need to have something called a mutation rate and that mutation rate is essentially a probability of how likely it is for each element of every of sorry each element of every sort of DNA to alter itself randomly when that child DNA is made so what that means in this context is for every single number that's a weight in these matrices for every single number that's in the bias there is a say one percent chance point five percent chance 10 percent chance then I'll pick a new random number I could also with mutation like a nudge to the values so sort of picking an entirely random new number I could like push it a little higher push a little lower but for simplicity right now let's just pick a new random number so what do I need to do I need to go back to neural network J s and I'm going to add a function called mutate what am I gonna do here I'm going to say there are four things that I need to mutate so I'm gonna say let's think about this what are all those things called there's I just got to go back up to here there's weights ih weights H Oh bias H bias Oh so I need to say what I need to do is say weights I'm gonna map a mutation function so remember there's the the V sorry I'm not able to type to talk at the same time so if you recall the matrix the matrix library has a function called map and what it allows you to do it's a little unfortunate that there's also a JavaScript native function called map which I'm using everywhere it allows you to apply a function to every single element of the array so I can pass in a function and apply it to every single element of the right and the function that I'm gonna pass in is mutate so let's write this function now and it's going to get it receives a value but to be honest I don't care about I do care about what the value is if I were planning to nudge it higher and nudge it lower but what I'm going to do right now is I'm just gonna return a random number so let me actually I sort of for that should probably link these better in some way but when I have this function called randomized and this is the kind of random number that I'm asking for so I am going to just return this ah but am I always going oh I do need the Val guess what if I do this it'll completely randomized every single element so this mutation function needs a mutation rate and what I'm going to do is I'm gonna pick a random number if math dot random is greater than or less than the rate right so only so let's say math dot random will give me a number between 0 & 1 so if the mutation rate is 0.1 that random number between zero and one will be less than 0.1 ten percent of the time otherwise stick with the same value so this is now the function that I want to apply to every element of all of the weight matrices I want to say hey mutate these weights mutate these weights mutate these biases mutate these biases and perhaps there's a more elegant way to write this and I will consider that all in the future with your many comments and pull requests and complaints I look forward to them but this is what I'm gonna leave it at right now so let's see now if what I can do if I go back to this particular program and I say child now just out of curiosity I'm gonna say child mutate one so I'm giving it a mutation rate of 1 which means everything should be completely random and what I'm gonna do just as a kind of like a way of testing actually is I'm gonna change this to I'm gonna multiply it by like a thousand just cuz I want to see like totally different numbers to see that this is working so I'm gonna go back and I'm gonna refresh the code weights I H is not defined oh right I forgot about that this dot that won't surprise any of you you've probably been saying this in the chat all along there we go so let's take a look so here we have once again the bias ease there all reasonable numbers between negative 1 and 1 which is how I started the neural network and now if I look at the child neural network and I look at the bias ease we can see yep so that mutated now let's change the mutation rate let's change it to 0.5 so we have kind of I mean we're not going to get exactly 50% of them because it's supposed to be random but we'll least see some of them didn't mutate and some of them did so let's change now the mutation rate to 0.5 just to see that this is working this is instead of me writing unit tests manual unit testing let's look here we can see wonderful here's some original values and now let's go down to here in the child object and let's look at the bias ease again and we can see hey it actually worked out exactly as planned it mutated two of them and did you take two of them which is what you know most commonly with a 50% probability we're gonna see and I suspect that if I go into the output bias though the alibis just has one value it didn't get mutated right because this is such a simple little neural network if we go into these weights we can see you know three out of four got mutated I think it's working so we are in good shape here we now and so I went up probably the actual mutation rate I'm going to want to use something like 1% because I want to do it pretty rarely but we now have the ability to both copy a neural network again as the Nexus if you're watching this and the future videos of this tutorial aren't released yet or if you don't feel like watching them just yet try as an exercise to yourself go and implement crossover how could instead of instead of copy could you create a new neural network that's a mixture of all of these weight matrices that would be a really interesting thing to try I will do that hopefully in a future video so I've got copy I've got mutation I've got the flappy bird game I've got the neural network library I've added cross the room mutation so we're ready now we're actually ready to implement the genetic algorithm I'm going to say this twice I'm someone in the chat pointed out that the neat algorithm neural evolution augmented topology things refers to a very specific implementation of neuro evolution in a specific paper and honestly being much more informal about this here so technically this probably isn't the neat algorithm and maybe I'll mention that the beginning of the next video just to emphasize it a bit more alright thanks for watching and I will see you when I continue all right let me look at the time where are we time was it's 440 I think I should stop because the next aspect of this I think I should stop because I don't have very much time left and it's definitely going to take longer than 20 minutes to finish a neuro evolution let me just check some things he let me just check some things here I'm just looking to make sure I don't have any important messages that I need to deal with right now and I don't seem to okay I do see that some people have suggested pi day to calling challenges which I'm excited about so what have I done so far today I have done one video about linting which needs a followup I have done one video about remote maybe I could do my load bites pull request this could fail this is probably gonna fail horribly because I mean more time for this yeah it's a bad idea III want it I I let me just say what I was thinking about doing I think it better for me to just it's not good for me to try to do something I have a very limited amount of time especially anything that's complicated that this will be way more complicated but I want to go to up what I want what I was what is thinking about doing that I think would make a useful video tutorial and so let me add a comment here about this at least load bytes so so this by the way is a github issue that I opened for the process for the open source project p5.js which is a project that I'm very much involved with a wonderful community and open source project where I created a version of the load bytes function to load a binary file to a JavaScript program which doesn't at present exist but doesn't at present yeah which doesn't at present exists in p5 if you go and look in the p5 source code will say like to be implemented and so some people gave me some feedback here and I just keep it's been on my list like every day oh let me pull request this and then I thought oh it would be useful to actually make this contribution to p5.js in a video tutorial but I also feel like oh what am I ever gonna get to that so let me make a note here and just say FYI I am writing this while livestreaming I would like to make a video tutorial where I walk through the steps of how to contribute this in enhancement but I did not get to it today I I think this would be useful so I am holding off on submitting until another recording session so this looks right okay so wonderful so out of that note there thank you to everyone who has been monitoring the YouTube chat today I it's not an easy thing to do and I really appreciate it it's very important to me that the community that participates in the channel is kind and welcoming to everyone and so as always if you're running into any issues during any of the live streams please send me a tweet or an email you probably can figure out a way to get in contact with me I'm happy to do anything I can to help so I think what I'll do is so I'm gonna go in about five or ten minutes I'm happy to maybe take a few questions let me actually also I don't think anything that I've done has broken any of my existing examples wouldn't it be nice to have unit testing here to see but I'm going to say get add and did I say to everything that I've done and I'm gonna say get commit oh let me do something this is my new thing that I do that I like git commit and Adam editor wait so let me do this I want to make a video about this but yes this is what I want so what I'm doing is I'm associating Adam as my core text editor with git and so I'm going to do this and the weight is there because when I say commit I want it to wait to make the commit till I finish typing in Adam so now if I say git commit it'll open up the message in adamant actually I'm gonna I'm gonna do ctrl C because what is the thing where I say V is a V or verbose somebody must know where it I should really just make this a video right now 15 minutes I I just like when I have a nugget of something I think people find it better if it's in the right place and it's an edited chunk as opposed to a random thing all of this is a bad idea and the chat of being told that ass usually Adam for this is a bad idea guess what yeah it's all a bad idea we should all just go back relax by the fire with our friends and read a book frankly you know playing the violin is something that I did for 12 years and I approached should spend more time doing that what is it for both nobody's answered my question I was oh you already is asking when is the next ITP show I think it's May 15th and 16th you should come to it in person but I will plan on doing a live stream from it again I guess I could just try B yeah that was it shows me all the discs look at that boom boom boom I like that it's good okay so alright why not all right use a sensible editor like VI but i but but but but but oh nice lollipops you can't really do that it takes time and googling all your questions you need a violin coach am i you know I can't tell with you but whether I'm doing alright fine I'm not gonna do my video about commit right now you guys have thrown me off I'm gonna just use Adam because that's the way I like it and I would say commit V so what I like about this is it opens up the text editor where I can write my commit message adding and it'll actually even like do stuff like hey start that with a capital adding adding code from March 8th live stream on neuro evolution because it doesn't like that I went so far on one line or too many yeah adding code for March 8th live stream this here is the start of code I need for my neuro evolution examples so far I guess it wants me to so far I have only implemented copy and I've only implemented copy and mutate I I still need to do to do crossover these I don't know I feel like that I don't know where the period should go I'm having a grammar moment the period I thought should go inside but then the print these threw me off ah I still need in lacrosse I'll have an ant ear how about this okay met up I still need to do crossover and and implement the GA itself of course for for for reference the live stream is here live stream is here so how do I get YouTube the coding train yeah I really got to go actually cuz and so if I go to code and train live this will be the York this URL will go on forever and so this I think is a nice thoughtful commit message that explains everything I am now going to hit close and it added that is it March 8th hope it's part its virgin I mean nobody noticed that there's like an ax men thing right get commit amend wonderful thing ever it just happened this would have been so good for me to make this into a video tutorial you all of you who shamed me for using Adam as my text editor you are depriving the world of a nice standalone tutorial I'm gonna tweet a link to this time code in this video for people to see and find adding code for March 9th live stream and I'm going to close that and there we go get log 1 we can see here's my message and now I could say git push origin master and we will go to the toy neural network and I will look at the commits and look see ix is right oh it's running some tests I forgot that I have oh I should have pushed it I forgot I meant to push it to a different branch and then merge it oh well at least the test passed and we can see look see look at this this is the world we should be living in where people write thought full and kind commit messages the date is right I know the dates below the dates in there but it's fine all right I've got to go I even just got a calendar warning I've got to read some and I'm just reminder that I'll be back on march 14th for pi day submit your coding challenge pi day coding challenge ideas rainbow topics link in the description below 7705 2050 490 1961 five hundred fifty eighty four thousand seventy one three thousand five hundred seventeen twenty eight thousand nine hundred fourteen forty eight thousand seven or sixty to seventy two thousand nine fifty to ninety six thousand thirty seven alright I read some random numbers and I'm gonna put this song on goodbye everyone but it now has a different commit date to its creation date or something oh boy I don't know I don't know just I can't get it right to everybody it is what it is thank you to Darius cos Amy who gave me a lot of great tips on open source stuff in the last couple weeks I'm working by the way on an idea for a course about contributing to open source that I wanted to go and teach next year and then why you have a github repository with ideas in it it's a point out puppies you could find it probably few want but I'm soliciting contributions and ideas for that so and I will continue to make more video tutorials I'll be back next Wednesday there will be a surprise for you because I am going to shortly record a guest video for the coding train channel that will come out sometime in the next couple weeks so stay tuned for that and I'll see you all I'm still here looking at my phone now alright everybody I'm gonna hit stop screaming now so I probably won't return to the neuro evolution thing next week that'll come two weeks from now so the next live stream there also be a guest two weeks from now I mean I have a lot of guests scheduled that I'm excited about like three or four so far coming up okay next live stream Wednesday March 14th probably in the morning of my time and then March whatever it is like 23rd I think is the next Friday alright bye everyone
