With timestamps:

00:00 - hi guys welcome back to our new course
00:02 - on pandas this is gonna be an
00:05 - introductory video so let's get started
00:09 - so pandas is a python Library used for
00:12 - working with data sets it has functions
00:15 - for the following task first of all we
00:19 - have data analysis so data analysis is
00:22 - the practice of working with data to
00:25 - glean useful information which can then
00:28 - be used to make informed decisions so in
00:31 - data analysis basically we try to find
00:34 - out patterns from data so we extract
00:38 - meaningful information and we use that
00:41 - information in future to make meaningful
00:45 - and informed decisions the second is
00:48 - cleansing data cleansing or data
00:51 - cleaning is the process of detecting and
00:54 - correcting corrupt or inaccurate records
00:57 - from a record set table or a database
01:00 - and it refers to identifying incomplete
01:03 - incorrect inaccurate or irrelevant parts
01:07 - of the data and then replacing modifying
01:10 - or deleting the dirty or course data so
01:13 - in cleansing what we do is we correct
01:17 - the wrongly
01:18 - process data for example when you are
01:23 - making a data set there must be some
01:25 - values missing in a record that may not
01:28 - be available so we just need to clean
01:31 - them for example we have a row with
01:34 - empty values so we have to replace that
01:37 - row because we can't work with raw data
01:41 - data needs to be in a meaningful form to
01:44 - extract patterns from the data and the
01:47 - last part is data exploration
01:50 - it is very similar to initial data
01:52 - analysis whereby a data analyst uses
01:55 - visual exploration to understand what is
01:58 - in a data set and then characteristics
02:00 - of theta rather than the traditional
02:03 - data management systems so in data
02:05 - analysis we try to use the functions to
02:09 - find out patterns like the mean the
02:11 - median or something like that but in
02:13 - data exploration we draw
02:16 - colorful plots or charts to analyze the
02:20 - data the process of analysis and
02:23 - exploration are usually performed
02:25 - together armed together known as Eda or
02:29 - explorate free data analysis so it
02:33 - includes the data Exploration with the
02:35 - help of charts or plots as well as with
02:38 - the help of functions so it is called as
02:41 - Eda or exploratory data analysis
02:44 - whenever we have any project the first
02:47 - part is
02:49 - to perform the ede or the exploratory
02:53 - data analysis and then Keening because
02:56 - if we know what is the problem in data
03:00 - then only we can key in the data so
03:03 - these are three important functions that
03:07 - are facilitated through pandas or with
03:10 - the help of functions that are available
03:12 - in the
03:14 - module pandas of python now let's move
03:17 - on to our next question why even use
03:20 - pandas so pandas has a lot of other
03:24 - functionalities as well we'll go through
03:26 - some of them first one is analysis of
03:29 - big data so what is Big Data Big Data
03:33 - refers to the data sets that are too
03:35 - large or complex to be to be dealt with
03:37 - by traditional data processing
03:39 - application software
03:41 - the size of big data is very large and
03:44 - the computational power required is very
03:47 - high so big data can't be analyzed on
03:52 - your systems with the help of
03:54 - traditional software we need newer
03:58 - software to analyze big data so pandas
04:01 - can help with analysis of Big Data as
04:04 - well the second is pandas can clean
04:08 - messy data sets and make them readable
04:10 - and relevant so already discussed it can
04:14 - help in cleansing or cleaning of
04:16 - data the third one is it has a lot of
04:19 - use in data science
04:21 - as already discussed data science is a
04:24 - branch of computer science where we
04:25 - study how to store use and analyze data
04:29 - for deriving information from it
04:31 - so pandas has a bigger role to play in
04:35 - the field of data science
04:37 - so what is difference between data and
04:40 - information
04:41 - data refers to Raw facts
04:44 - so it's usually in unprocessed form
04:47 - while information
04:49 - has proper context it means that it is
04:52 - in a process form and we can directly
04:56 - extract patterns from information
05:00 - now let's move on to our next topic that
05:03 - is what else can pandas do so what are
05:07 - some additional functionalities offered
05:09 - by pandas so first one is calculation of
05:12 - correlation among different keys or
05:15 - columns or features so what is
05:18 - correlation in statistics correlation or
05:21 - dependence is any statistical
05:23 - relationship whether causal or not
05:26 - between two random variables or
05:28 - bivariate data
05:30 - so in simple word it means that how will
05:33 - one variable will be affected when the
05:36 - other one is changed
05:38 - the second one is calculation of average
05:41 - values from the data set calculation of
05:44 - the minimum and the maximum values of a
05:47 - column in a data set deletion of fruits
05:51 - that are not relevant or continual
05:53 - values so this comes under data
05:56 - cleansing the second and third points
05:58 - come under the data analysis and
06:02 - correlation is a very important aspect
06:05 - because whenever we do feature
06:08 - engineering we have to do correlation
06:10 - analysis to find out which features are
06:15 - correlated and what is the extent of
06:17 - correlation and we neglect
06:20 - the features which have a direct
06:22 - correlation or a perfect correlation
06:24 - because it doesn't make sense as the
06:28 - both the features
06:30 - are giving us the same information so
06:32 - why bother to take multiple features if
06:35 - one feature is sufficient
06:37 - so in today's lecture we covered the
06:40 - basics of pandas and the basic
06:42 - functionalities of this module offered
06:45 - by python
06:47 - hi guys welcome back to our course on
06:49 - pandas in our last lecture I introduce
06:52 - you to the python module pandas and
06:56 - today I'm going to teach you how to get
06:58 - started with pandas that is how to
07:01 - install import and to some basic
07:04 - operations with thunder so let's get
07:07 - started
07:09 - so first of all let's discuss the
07:11 - installation of pandas
07:15 - so installation of
07:18 - pandas
07:25 - if you already have
07:28 - the PIP package
07:32 - as well as the python installed on your
07:35 - system
07:37 - so generally we prefer to do all the
07:40 - installations with either the PIP
07:42 - installer package or the condom so
07:45 - either it's pip install pandas or
07:49 - conda install pandas
07:52 - so if you have Python and pip package
07:54 - already installed then installation of
07:57 - pandas is very easy
08:00 - so you just need to write the command
08:04 - pip install pandas so install it using
08:08 - the command
08:10 - that is
08:12 - pep
08:15 - install
08:17 - pandas
08:20 - so now I'll run it so it says
08:22 - requirement already satisfied because
08:24 - I've already installed but it may take
08:27 - some time in your system because you
08:30 - would be running it in the command line
08:33 - also uh the
08:36 - Anaconda package comes with the
08:39 - pre-installed pandas module so if you're
08:42 - using jupyter notebook and you have the
08:45 - whole Anaconda package then you don't
08:47 - need to install it explicitly it comes
08:51 - already installed with Anaconda package
08:54 - so if this fails in your command line
08:58 - and if you are using some other IDE you
09:02 - can use a distribution like Anaconda
09:04 - spider
09:05 - Etc that comes pre-installed with pandas
09:09 - now let's import the pandas module
09:16 - so we just need to write import
09:20 - so just simply write import
09:24 - pandas
09:27 - so we have imported pandas successfully
09:30 - but most of the type we import our
09:34 - modules as an alias because we can't
09:37 - write a
09:40 - big word every time we are using some
09:42 - functions so we just import with a short
09:45 - term or an alias so import pandas as PD
09:49 - and PD is the Alias here
09:52 - now let's
09:54 - check the version
09:57 - so how to check
10:00 - what version are you using you just
10:02 - simply need to write print
10:06 - PD dot underscore underscore version
10:11 - underscore underscore
10:14 - so it says 1.4.2 I'm using notebooks so
10:19 - I don't need to write print explicitly
10:22 - but if you're using some other
10:25 - IDE then this will not work you have to
10:28 - write print
10:30 - PD dots underscore underscore version
10:33 - underscore underscore as whole if I just
10:36 - write PD dot version it works here but
10:39 - it won't work on any other IDE
10:43 - it works for Notebook environment only
10:47 - now we'll create a data frame so data
10:50 - frame is a very important
10:52 - part of pandas module because this is
10:55 - mostly used
10:57 - in projects so data frame we'll discuss
11:01 - it in later in detail that what is the
11:04 - data frame and how it works how to add
11:08 - rows or columns to data frame what is
11:10 - difference between data frame and a
11:13 - normal table
11:16 - so right now I'm just creating a data
11:18 - frame so let the name be my data set and
11:22 - I'm using a dictionary
11:24 - to create a data frame
11:28 - so there are a lot of methods to create
11:30 - a data frame you can pass less numpy
11:33 - arrays or dictionaries to create data
11:37 - frames so I'm using a dictionary here I
11:40 - have to give values as key value pairs
11:44 - so my keys are cards and values and my
11:49 - values are BMW skoda as one list and one
11:53 - and two as another list now I'll just
11:55 - write print
11:59 - my data set
12:07 - so I'm going to print this dictionary
12:09 - first
12:13 - and then I'm going to create a data
12:16 - frame so my VAR or any data frame name
12:20 - is equal to PD dot data frame
12:24 - please note that data frame is key
12:27 - sensitive so first
12:31 - alphabet is capital
12:34 - my gaps log has some problems
12:37 - so it's PD dot data frame and pass the
12:41 - dictionary
12:43 - so dictionary name is my data set and
12:45 - shift enter to run
12:51 - so it's giving an error because I've not
12:54 - added a comma in
12:57 - the elements
12:59 - so now it will work
13:01 - so this is our dictionary and this is
13:04 - our data frame
13:07 - so you can see the difference
13:10 - so in today's lecture I taught you how
13:14 - to get started with pandas hi guys
13:17 - welcome back to our course on pandas in
13:19 - today's lecture we are going to cover
13:21 - the series data type in pandas
13:25 - so let's get started
13:27 - so the first question is
13:30 - what is a series
13:38 - so we know that pandas is usually
13:41 - preferred for data analysis
13:46 - so we know that
13:48 - Anders
13:50 - is primarily
13:52 - used for
13:57 - analysis of data
13:59 - pandas is a module which is primarily
14:02 - used for analysis of data and it has two
14:05 - major data types the first one being a
14:09 - pandas CDs and the other one being a
14:13 - data free
14:14 - so pandas supports two main data types
14:20 - first one being series data type and the
14:24 - other one being data Frame data type
14:27 - usually we use data frames because
14:33 - a table or in data is in form of rows
14:38 - and columns but series is very important
14:41 - biggest data frame is nothing but a
14:44 - bunch of series clustered together
14:46 - so now our original question is
14:49 - what is a series
14:54 - so we know that pandas has two main data
14:58 - types series and data frame
15:01 - and what is a series so series is
15:05 - nothing but like a column in a table
15:10 - so Panda series is like
15:13 - a column
15:16 - in a table and when you combine a lot of
15:20 - different columns it becomes a table and
15:24 - in this case a data frame so Panda
15:28 - series is like a column in a table and
15:30 - it is a one dimensional array
15:33 - holding data off
15:35 - any type
15:38 - so Panda series is just like a 1D array
15:42 - but now you must be wondering then what
15:44 - is the difference between a 1D array and
15:47 - a panda series
15:49 - we'll get on to that topic first let us
15:52 - see
15:53 - that which data types can be used to
15:57 - create a panda Series so we can convert
15:59 - a python list a numpy array or a
16:03 - dictionary into a series
16:07 - so first of all
16:10 - we'll
16:13 - try to make a series from a python list
16:18 - so first of all I'm going to import
16:20 - pandas as PD and import numpy as NP now
16:25 - I'll not use anything from numpy but
16:28 - maybe in between I may need to use it so
16:32 - I'm importing
16:34 - numpy as well and I have created a
16:37 - python list
16:39 - that is my list
16:42 - let me just create it in a new cell
16:45 - yeah I've created a list and now I need
16:48 - to create a series so I'll write a is
16:52 - equal to PD dot series and SS capital
16:55 - and I need to pass my list that is my
16:59 - list only so data is equal to my list
17:04 - I'll just print
17:06 - and this is the
17:08 - pandas series
17:16 - now let me just create
17:20 - a series
17:22 - from
17:23 - numpy array so first of all I need to
17:26 - create a numpy array and I have already
17:30 - discussed how to create a numpy array in
17:33 - the numpy module so you can refer that
17:35 - module if you don't know how to create a
17:38 - numpy array
17:40 - so now I'll just
17:42 - write the same syntax and I just need to
17:45 - pass the numpy array instead of list
17:48 - I'll print it again so I got the same
17:50 - result
17:59 - now
18:01 - we covered
18:03 - the topics
18:05 - of
18:06 - creating a series
18:08 - from a numpy array as well as a python
18:12 - list
18:22 - so we covered these two topics now let's
18:26 - move on to the topic of creating a
18:29 - series from a dictionary or a python
18:33 - dictionary
18:34 - so you know that in dictionary we have
18:36 - the entries as key value pairs
18:40 - but first we'll try to understand the
18:43 - difference between
18:47 - an array
18:49 - and a series
18:52 - and then we'll proceed to create a
18:54 - series from a dictionary and understand
18:57 - the difference
18:58 - that how it is different from creating
19:02 - a series from a list or a
19:06 - so first let's try to understand the
19:09 - difference between a series and an array
19:12 - so in an array we have numeric indexes
19:18 - so to refer to the first element we'll
19:21 - use the zeros index to refer to the
19:24 - second element we'll use
19:27 - the index 1. and in general to refer to
19:32 - the K plus one element we need to use
19:36 - the kth index
19:40 - so a series
19:42 - doesn't necessarily have numeric indexes
19:46 - it can have access labels meaning that
19:50 - it can be indexed by a label instead of
19:54 - numeric values so instead of 0 1 2 I can
19:59 - use Alpha Beta gamma a b c or anything I
20:03 - like or any
20:06 - word like ABC for 10 x y z for 20
20:12 - def for 30. so a series can have access
20:16 - labels meaning that it can be indexed by
20:19 - a label instead of numeric value but by
20:22 - default it has a numeric indexing it
20:26 - means that if I don't pass a value for
20:28 - the labels the default
20:32 - label will be a numeric value like 0 1 2
20:36 - and so on
20:37 - so now
20:38 - we'll create a series
20:43 - using
20:47 - a dictionary
20:50 - so now I'll create a series using a
20:53 - dictionary and we'll see what is
20:56 - different here
20:58 - so first of all I need to create a
21:00 - dictionary
21:04 - so let there be dictionary with the name
21:07 - e and it has element
21:10 - Anna
21:12 - and
21:14 - let's add one more element after we give
21:17 - the values for this
21:20 - key
21:21 - so let Anna has the value 10 and Jasper
21:25 - has the value 20 and Nikita has the
21:29 - value 30 so Anna Jasper and Nikita are
21:32 - the keys while 10 20 and 30 are the
21:35 - values
21:37 - so now create a series so w is equal to
21:42 - PD dot CDs and I'll pass this dictionary
21:45 - and I'll print it
21:48 - so Anna Jasper and Nikita here act as
21:51 - the labels for the series and 20 10 20
21:55 - 30 are the values so in today's lecture
21:58 - we covered
22:01 - the concept of pandas series
22:05 - that will be all for today
22:07 - hi guys welcome back to our course on
22:10 - pandas today we are going to continue
22:12 - our discussion on pandas series
22:15 - so let's get started
22:17 - in our last video we covered
22:21 - how to create a panda series from a list
22:24 - and an array as well as what is the
22:28 - difference between
22:30 - a series and an array
22:33 - so we'll continue our discussion on
22:35 - labels and indexing today
22:38 - so now our task is to take the data and
22:42 - label values as separate lists and
22:44 - create a panda series
22:46 - in our last lecture we use a dictionary
22:51 - for the values as well as the labels
22:55 - because the keys can act as label values
22:59 - and the values can act as
23:03 - the entries
23:05 - so now our task is to take the data and
23:09 - label values as
23:11 - separate list
23:15 - and create a pandas
23:18 - series
23:24 - so first of all we are going to import
23:28 - partners
23:30 - so import pandas as PD
23:34 - and import numpy
23:41 - as NP
23:43 - now we'll take two less one for the data
23:48 - and one for the label so first of all
23:50 - let's take labels as a b
23:53 - comma BC on
23:57 - CD
23:58 - so we have defined the list for the
24:01 - labels now we have to define the list
24:03 - for the data so let's data Z equal to
24:08 - Let It Be numeric so 1 comma 2 comma
24:12 - 3. now we'll just write
24:17 - the series name or Q is equal to PD dot
24:21 - series and will pass two parameters
24:24 - first one being the data
24:28 - so data is equal to data Z
24:32 - and the second one being
24:35 - the index so index is equal to
24:39 - labeled so in last case we just passed a
24:42 - dictionary and automatically the index
24:46 - or the labels and data was assigned but
24:49 - in this case we have to pass
24:51 - data and index so here we can see that
24:54 - it doesn't have a numeric index it has
24:57 - index as
25:00 - the value of labels that we have passed
25:03 - now let's see what if we don't pass
25:07 - the index values
25:10 - so it's not a brainer if we don't pass
25:13 - index values they will be taken as the
25:16 - default numeric values starting from
25:19 - zeros
25:20 - so it will be 0 1 2 and so on now let me
25:24 - just print the capital Q so it's 0 1 and
25:27 - 2. so if you don't pass
25:30 - the index it automatically will take
25:34 - numerical values of numeric values
25:38 - now
25:39 - let's move on
25:43 - [Music]
25:44 - let me just give you an assignment now
25:47 - your assignment is to take a list and a
25:50 - numpy array as an input and create a
25:53 - series
25:56 - please do all the assignments it helpful
26:00 - and it will help you learn more
26:12 - now let's move on
26:15 - to choosing an index to access a
26:19 - particular value
26:20 - so we'll use an index to access a value
26:23 - let me just access the value 2 here
26:26 - so I can write
26:29 - capital Q
26:33 - so to access
26:35 - the value 2 in
26:38 - capital Q first so I'll just write
26:42 - capital Q
26:46 - off
26:47 - 1. I'll just
26:51 - run this cell and I got to
26:53 - and to access
26:55 - to
26:57 - from small q I can write
27:01 - q and
27:03 - in bracket I have to specify the label
27:06 - that is
27:07 - BC so I got to as result so in today's
27:12 - lecture we continued our discussion on
27:15 - labels on indexing hi guys welcome back
27:18 - to our course on pandas in today's
27:20 - lecture we are going to cover some
27:22 - operations on pandas Series so let's get
27:26 - started
27:27 - so we are going to cover some basic
27:29 - operations such as sum and difference
27:34 - so operations on CDs
27:37 - first of all let me
27:41 - declare or Define two series
27:44 - so that I can perform operations on them
27:48 - so let series 1 or scr1 is equal to PD
27:52 - dot series
27:54 - and let me just pass a list and
27:57 - labels
27:59 - so I have to pass the labels
28:03 - as a list
28:09 - so PD dot series
28:12 - list and
28:13 - the list of labels as index
28:16 - similarly I have to declare the second
28:20 - series
28:23 - so let the elements p2345 and the index
28:28 - be a little different this time
28:30 - so let it be
28:33 - B c d
28:37 - and E
28:43 - so the indexes are not same or the
28:46 - labels are not same
28:48 - let me just import
28:50 - pandas as PD
28:52 - don't forget to do this sometimes I do
28:55 - forget because I think that it's
28:57 - continuation of previous notebook but
29:00 - I'm making a new notebook every time so
29:02 - I have to declare it
29:04 - or import again
29:06 - now let's just perform the additions so
29:08 - scr1 Plus acr2
29:13 - so I'll just run this cell and I got
29:17 - the sum it shows not a number for label
29:21 - e and label e because there are no
29:24 - values corresponding to these
29:27 - enables in the other list
29:29 - for example label a is only in series 1
29:33 - and there is no label a in series 2 so
29:35 - their sum is not a number similarly for
29:38 - the label e
29:40 - also we can use the add method
29:46 - to perform the sum
29:49 - so we can also use the add
29:53 - method
29:56 - to
29:58 - perform the sum or for addition
30:03 - now I'll demonstrate the use of add
30:06 - method
30:07 - so I just have to write scr1
30:12 - dot add
30:16 - scr 2 in the bracket I'll run this cell
30:23 - will get the same result
30:28 - now we should
30:31 - observe that whether it causes the
30:34 - change in series 1 or not
30:37 - so I'll just print the series 1 scr1 so
30:40 - no change
30:44 - so add method doesn't cause any change
30:46 - in the original series it just performs
30:49 - the addition and returns a new series
30:52 - so no change in
30:54 - series one now let's see
30:57 - what's the difference between addition
31:01 - using add method and the normal add
31:04 - operator so in this case we can also
31:07 - pass
31:09 - the default fill value
31:11 - so we observed that there was no nothing
31:14 - corresponding to label a in series 2 but
31:18 - we can give a default value in this case
31:21 - so the label
31:23 - a will be filled by 0 in series 2. so
31:29 - someone
31:31 - plus 0 will give us 1 and not not a
31:35 - number so this is the difference we can
31:38 - give the default field value if there is
31:41 - no corresponding label in the other
31:43 - series
31:44 - similarly we can perform the subtraction
31:48 - using the subtract operator
31:51 - the minus operator as well as using
31:56 - the
31:58 - sub method
32:00 - so this is your assignment that you have
32:02 - to perform subtraction using the minus
32:05 - operator and Dot
32:08 - sub method and demonstrate the code and
32:12 - the output as well
32:13 - from today's lecture we covered some
32:16 - basic operations related to pandas
32:20 - series
32:21 - hi guys welcome back to our course on
32:24 - pandas in our last lecture we concluded
32:27 - our discussion on series and today we
32:30 - are going to start data frames so let's
32:33 - get started
32:35 - so our first question is what are data
32:39 - frames so data frames are the workhouse
32:42 - of pandas
32:45 - so they are directly inspired by our
32:48 - programming language and then nothing
32:51 - but a bunch of series objects put
32:55 - together
32:56 - so series is one dimension we just
33:00 - cluster together a bunch of series and
33:05 - it becomes a data offering
33:08 - so data frames are the workhouse of
33:11 - pandas that directly inspired by the r
33:14 - programming language and we can think of
33:17 - data frame as a bunch of series objects
33:21 - put together because series is one
33:23 - dimension and when you combine a group
33:27 - or a bunch of Series so it becomes a
33:30 - tabular structure that is a data frame
33:34 - now let's see how to create a data frame
33:40 - so first of all we have to import
33:43 - numpy and pandas
33:46 - by Alias so import
33:50 - numpy as NP
33:54 - unimport Partners as PD
33:59 - now we'll run the cell
34:02 - also in this case I am going to use
34:07 - the rant n
34:09 - method of numpy
34:12 - to create
34:16 - a 2d array which I'll will pass
34:20 - to data frames so I I'll import that
34:23 - also
34:25 - from numpy dot random input run in I'll
34:30 - create a data frame with the name DF so
34:33 - DF is equal to PD dot data frame and in
34:37 - this case we have to pass a 2d array
34:40 - so
34:42 - instead of passing a hole to the array
34:45 - I'll just write run in 5 comma 4 because
34:48 - this will return me a 2d array and I
34:52 - have to pass a list of index as well as
34:56 - a list of columns because a data frame
34:59 - has indexes as well as column values or
35:03 - keys
35:04 - so I'll just write
35:07 - a b c d e dot split because this will
35:11 - return me a list
35:12 - I can either write split and I can pass
35:16 - comma or I can just give normal spaces
35:20 - so a b c d e and by default the split is
35:25 - along the
35:26 - blank or the white space
35:28 - similarly for the columns I'll pass
35:32 - w x y z
35:34 - and Dot split because this will also
35:36 - return me a list
35:38 - of four elements and 4 into 5 is 20
35:43 - which is the
35:46 - dimension of the data
35:49 - so this is the resultant data frame
35:55 - now I'll try to access a row directly
36:00 - to access
36:02 - a row or an index in a series I can
36:06 - write DF and in bracket pass the index
36:10 - or the label so DFA this will work when
36:14 - I use it in a series but will it work in
36:18 - a data free let's check
36:21 - so DF of
36:23 - a where a is the label this is giving me
36:26 - an error so this helps us to deduce the
36:31 - fact that we can't directly access a row
36:34 - using a label in case of a data frame
36:38 - so this gives us an error we'll discuss
36:42 - the use of dot Loc
36:45 - and Dot iloc later for selecting row
36:52 - offer data free
36:54 - so we'll discuss these topics in
36:56 - upcoming videos
36:58 - but
36:59 - there is one more Point here
37:03 - we can't
37:05 - access the rows directly but we can
37:08 - select the columns directly
37:10 - so DF of
37:12 - eggs
37:13 - X where X is the column label
37:16 - will give me the column values so I'll
37:20 - just write DF of X where X is the column
37:24 - label and it returns me a series
37:29 - because
37:30 - data frame is nothing but a bunch of
37:33 - series
37:35 - put together
37:37 - now let's select multiple columns to
37:39 - multiple columns
37:41 - we have to pass
37:43 - a list of columns
37:49 - so you need to pass a list of columns so
37:53 - X comma y or Y
37:56 - comma Z or anything so there's an error
38:00 - in syntax let me just Rectify it
38:03 - and then I'll run
38:10 - so this is the correct syntax and this
38:13 - is the result please note that you have
38:15 - to pass a list of column labels and not
38:19 - just labels separated by commas
38:22 - now let's prove that data frame is just
38:26 - a bunch of series objects
38:32 - so to prove that data frame is just a
38:35 - bunch of series you can use the type
38:38 - method and select a column
38:41 - so if you can prove that every column of
38:44 - a data frame is the CDs then it's
38:48 - proven that data frame is just a bunch
38:51 - of series objects combined so I'll just
38:54 - write type and in bracket I'll pass DF
38:58 - and I'll pass DS
39:00 - W column so it gives me
39:05 - pandas.co.series.series so it's a series
39:07 - object
39:08 - so in today's lecture I introduce you to
39:11 - the data Frame data type of pandas
39:15 - hi guys welcome back to our course on
39:17 - pandas today we'll continue our
39:20 - discussion on data frames and I'll teach
39:22 - you how to add a column to a data frame
39:25 - or how to remove a column from a data
39:28 - frame
39:29 - so let's get started
39:32 - so our topic is
39:34 - adding
39:35 - and removing columns
39:41 - adding and removing columns
39:44 - to Slash from a
39:47 - data frame
39:51 - so first of all we are going to create a
39:53 - data frame
39:55 - so our first step is to create a data
39:58 - frame
40:00 - so I'll first do the necessary Imports
40:03 - so import pandas as PD import
40:08 - numpy as
40:11 - NP
40:12 - and from numpy dot random
40:20 - I need to do one more import before I
40:22 - can create a data frame
40:26 - let me just write the instruction for
40:29 - the data frame first then I'll add that
40:32 - command
40:33 - so DF is equal to PD dot data frame
40:37 - Rand in 4 comma 4 will create a matrix
40:42 - of Dimension 4 cross 4 and we'll pass it
40:46 - to the data frame along with we have to
40:48 - pass the index
40:51 - so we'll pass
40:53 - ABCD dot split it will return a list
40:56 - containing four values and similarly
40:59 - we'll pass the columns
41:01 - w x y z
41:06 - and also We'll add dot split because it
41:09 - will also return a list
41:11 - containing four values
41:14 - now I'll run it but I have to do the
41:18 - necessary import first so from
41:23 - numpy
41:25 - Dot
41:28 - random
41:31 - import Rand and because run n requires
41:34 - this import
41:36 - or you can use
41:38 - numpy dot random dot Rand and instead of
41:41 - only Rand in so it will also work so now
41:44 - it's giving an edit because so
41:49 - word columns is not spelled correctly so
41:52 - let me just
41:54 - change it
41:57 - yeah now I have created a data frame
42:00 - successfully now just print it
42:04 - so this is our 4 cross 4 data frame
42:08 - now I'll try to
42:10 - add a new column to the data frame with
42:13 - the name ZN so DF of c n or DF
42:19 - bracket
42:20 - square bracket and the column name or
42:23 - the name of the new column is equal to
42:28 - then I have to pass the values because
42:30 - every column has four values I have to
42:33 - pass four values now let's display the
42:37 - modified data frame so here we can see
42:39 - that a new column has been added
42:41 - successfully
42:44 - initially we had W X Y Z now we have w x
42:47 - y z and z in so we we have added a new
42:51 - column
42:52 - successfully
42:59 - now let's remove
43:01 - the column
43:04 - so generally we use a method known as
43:09 - drop
43:12 - to remove a column so I'll write TF dot
43:15 - drop and I'll pass the column name
43:19 - and also the axis because whenever we
43:23 - deal with columns we are dealing with
43:25 - axis equal to one if we don't pass this
43:28 - axis equal to 1 it will give you an
43:30 - error
43:31 - so this is the modified data frame when
43:35 - ZN has been removed but it will not make
43:39 - changes in the original data frame it
43:42 - just returns a new data frame
43:45 - so is it actually deleted no it's not
43:48 - when you display DF you still have
43:52 - Z in so it's
43:54 - false
43:56 - in place by default what is in place let
43:59 - me illustrate in place true or in place
44:02 - false while performing an operation
44:05 - means that do you want to make changes
44:08 - to the original data frame or not if in
44:11 - place is false then no changes are made
44:15 - to the initial data frame and when you
44:18 - where you want to make changes you have
44:20 - to pass in place is equal to true now
44:24 - when I'll make this change it will
44:27 - remove it permanently it's giving an
44:31 - error because
44:35 - the name is the n not CE yeah now it has
44:39 - been removed permanently so when you
44:43 - specify in place is equal to true it
44:46 - makes changes to the initial data frame
44:48 - only
44:49 - so in today's lecture we covered how to
44:52 - create new columns and how to delete
44:55 - columns from a data frame
44:58 - hi guys welcome back to our course on
45:00 - pandas in today's lecture we are going
45:03 - to continue our discussion on data
45:06 - frames and I'll teach you how to select
45:09 - rows of a data frame so let's get
45:12 - started
45:13 - so today's topic is
45:16 - selection
45:19 - of rows
45:21 - offer
45:23 - data frame
45:25 - so this is the topic
45:27 - first let me just import
45:30 - so import pandas as PD
45:34 - import
45:36 - numpy as NP
45:39 - and from
45:42 - numpy dot random
45:46 - so from numpy
45:51 - dot random
45:54 - import
45:56 - [Music]
45:57 - run n
45:59 - we are using this to create an array a
46:03 - 2d array you can directly pass a 2d
46:06 - array but that will just make the code a
46:09 - little bit
46:10 - complicated
46:12 - not complicated but it won't look very
46:15 - clean
46:17 - so that's why we are creating a 2d array
46:20 - with help of Rand in
46:22 - method
46:25 - so let me just create a data frame
46:27 - and I have to pass
46:29 - index value as well as the column values
46:33 - so let the index be e and B and the
46:38 - column B
46:40 - C and D
46:42 - so I have to pass these as lists and I
46:46 - have a 2 cross 2
46:48 - Matrix that will create a two cross two
46:51 - data frame with the index a b and column
46:54 - values
46:55 - C and D
47:00 - so I have
47:04 - created
47:08 - a data frame
47:12 - so this is our result in data frame
47:16 - now
47:17 - let's just try to
47:21 - select a column first
47:24 - so we have selected the column C
47:27 - so
47:29 - we can conclude that
47:32 - we can select a column directly from a
47:35 - data frame
47:37 - now let's try to select a row let it be
47:40 - a
47:41 - so d f of e so this is giving us an
47:45 - error
47:47 - so we can conclude that we'll get an
47:52 - error
47:55 - while trying to select a row directly
47:58 - so error
47:59 - while trying to
48:03 - select a row
48:06 - directly also all the necessary notebook
48:10 - links will be given this time so you can
48:13 - just refer to the notebooks if you are
48:16 - not able to implement something I'll
48:18 - share a GitHub link
48:20 - so selecting of rows
48:25 - is therefore different from
48:29 - selection of columns or from selecting
48:32 - columns
48:34 - so selection of rows and selection of
48:37 - columns are done by different methods
48:41 - and selection of
48:44 - rows can be done using two methods
48:48 - first one is dot loc method and in this
48:52 - we have to pass the label
48:55 - so dot loc and we have to pass the label
48:59 - or label is passed and the other method
49:02 - is iloc method
49:05 - and in this case index is passed
49:09 - so there are two methods for selection
49:11 - of rows first method is dot loc and the
49:15 - other method is dot iloc in the first
49:18 - one label is passed and then the second
49:20 - one index is passed let me just write
49:24 - df.loc a
49:26 - so here we can directly pass the label
49:29 - and we'll get the row
49:32 - and in iloc method we have to pass
49:36 - the
49:37 - index
49:39 - so the index of e is 0 so
49:44 - it gives us the first row
49:48 - similarly we can select the second row
49:51 - using
49:53 - locb or using iloc
49:56 - one
50:08 - now
50:11 - let's see how to select
50:13 - a single cell
50:16 - because we have selected rows we have
50:19 - selected columns but now we want to
50:21 - select a single cell
50:24 - so I can write
50:28 - DF Dot
50:32 - Loc
50:34 - and I have to pass e comma
50:38 - fee
50:39 - so it selects the first row and the
50:43 - first column
50:44 - or the first cell
50:47 - now let's select the value
50:55 - 2.424
50:56 - so what is the code that you're going to
51:00 - write to select
51:01 - this particular cell so it is the
51:06 - second row and the second column or the
51:09 - column
51:11 - D and the row P so d f b comma d
51:15 - but we have to write
51:18 - d f Dot
51:20 - Loc
51:25 - because we are passing the labels so we
51:28 - have to write Loc
51:35 - or we can write DF Dot iloc
51:39 - one comma 1.
51:42 - so it is first
51:48 - index column wise as well as row wise
51:52 - because 0 and 1 and 0 and 1 so first
51:56 - now let's select subset of a data frame
52:03 - so in this case we are going to select
52:06 - the rows A and B but only one of the
52:09 - columns
52:10 - so I'll write df.loc
52:14 - and I'll pass both the rows
52:18 - and only one column
52:23 - oh it gives me an error because the
52:26 - mixing is not allowed you can't mix if
52:30 - you are using dot loc then you have to
52:33 - use all the
52:35 - labels only you can't use indexes
52:44 - now let's try using this this also gives
52:46 - us an error because if you are passing a
52:50 - single element you don't need to pass it
52:53 - as a list
52:58 - also
52:59 - it won't work if you pass two of the
53:02 - elements as less in both the cases
53:11 - so this is the wrong syntax
53:13 - also if you do mixing of any kind
53:18 - that is also wrong
53:27 - so this also gives an error now let's
53:29 - see
53:31 - what happens if I pass both the row
53:33 - values instead of arrow and a column
53:35 - value so there was an error but now I
53:39 - passed
53:40 - a row value and a column value so it's
53:42 - correct
53:44 - also now I'm going to pass a list as
53:48 - the row value and only one column value
53:52 - so this is the result
53:54 - so that's the correct method
54:01 - also note that
54:05 - mixing of
54:08 - labels and indexes is not allowed
54:14 - or not supported so in today's lecture
54:17 - we covered selection of rows of a data
54:21 - free hi guys welcome back to our course
54:24 - on pandas in today's lecture we are
54:27 - going to continue and hopefully conclude
54:30 - our discussion on data frames and today
54:33 - we'll discuss conditional selection in
54:36 - data frames so let's get started
54:39 - so the topic for today is
54:42 - conditional
54:44 - selection
54:48 - so conditional selection is an important
54:51 - feature of pandas in which we select
54:55 - cells or
54:58 - sub data frames or subsets of data
55:02 - frames or rows or columns using bracket
55:06 - notations in a way which is very similar
55:10 - to numpy
55:12 - so the use of bracket notation is very
55:16 - important
55:22 - also we know that a conditional operator
55:26 - always returns a true or a false value
55:30 - because every condition that we are
55:33 - checking is either true or false so it
55:37 - makes sense that a conditional operator
55:39 - or any conditional operator returns a
55:43 - true or a false value now let's
55:45 - demonstrate
55:47 - these Concepts using code
55:50 - first of all let me just import all the
55:53 - necessary modules or functions
55:58 - so from num by dot random
56:01 - import ran in we are using rant n for
56:06 - creating a 2d array and then passing it
56:09 - to create a data frame
56:11 - so let's first create a data frame and
56:13 - then I'll demonstrate the concept of
56:17 - conditional selection
56:20 - so we are writing
56:23 - the statement to create a data frame
56:29 - Let It Be 3 cross 3 data frame
56:33 - so I have to write Rand in 3 cross 3 or
56:38 - 3 comma 3 because we don't write cross
56:40 - here we write 3 comma 3 and I am going
56:43 - to pass the index values as
56:46 - let it be a comma B comma C
56:51 - so the index values are
56:53 - a comma B comma C and we have to pass
56:56 - this as a list or I can also write ABC
57:00 - with space dot split because that also
57:03 - Returns the list
57:05 - or I can write directly and let the
57:08 - column values p d e and f now I've
57:12 - created a data frame
57:14 - so this is our data frame as visible on
57:17 - the screen
57:19 - now what if we want
57:26 - to select under a given condition so DF
57:29 - greater than
57:31 - 1.
57:33 - and let's execute the statement so it
57:36 - gives us true and false values so all
57:39 - the values which are greater than 1
57:42 - corresponding to those we get true
57:44 - otherwise we get false now what if we
57:47 - want the values which are greater than 1
57:50 - and not just true or false we actually
57:53 - want to print the values
57:55 - so we'll write DS and in square bracket
57:58 - not in
58:01 - this
58:04 - round bracket in square bracket we'll
58:07 - write d f
58:08 - and d s greater than 1.
58:12 - so it will print the value which is
58:15 - greater than 1 so that is 1.24 and so on
58:19 - and it will return not a number for all
58:23 - the other entries now what if we want to
58:27 - check
58:31 - a particular column
58:34 - only
58:45 - so we'll write DF and we'll write in
58:49 - square bracket DF of d
58:56 - greater than 0. so all the rows in which
59:02 - the value of column D is greater than 0
59:05 - will be displayed that's all the rows so
59:08 - we don't see any difference now let's
59:10 - change it to 1.
59:12 - so now only first row is
59:14 - displayed because only in first row the
59:17 - value of column D is greater than 1 that
59:20 - is 1.24
59:21 - now what if we want a particular
59:28 - cell or I should not frame it like this
59:32 - I should frame it like
59:33 - to select particular rows and particular
59:37 - columns so it can be a cell or it can be
59:41 - a subset of the data frame so we'll
59:44 - write DF DF of D greater than 1
59:48 - and
59:50 - a
59:52 - so it is giving me an error
59:58 - because I have to pass
60:02 - a
60:04 - column value
60:18 - so in this case we got the result
60:23 - so it is streamed like this what if we
60:27 - want to
60:28 - a particular column so we are selecting
60:31 - a particular column and a particular row
60:35 - or a set of columns and rows so it's
60:37 - either a cell or a
60:40 - sub data frame so there's a little
60:43 - ambiguity here so please take care of
60:45 - that
60:46 - so in today's lecture we covered
60:50 - conditional selection
60:52 - in data frames
60:54 - so we conclude the topic of data frames
60:58 - hi guys welcome back to our course on
61:01 - numpy in our last lecture we concluded
61:04 - our discussion on data frames and
61:06 - today's lecture we are going to see how
61:10 - to read and view data using the
61:13 - functions offered by the pandas module
61:16 - of python so let's get started
61:20 - so today's topic is
61:22 - reading and viewing data from CSV and
61:27 - Json files
61:29 - so these are two important or the two
61:32 - major file formats
61:34 - in which data is available
61:37 - so we have two data sets here first is
61:40 - the iris data set and the Json version
61:45 - will download
61:47 - the two versions of the iris data set
61:50 - the first is the Json version and the
61:52 - second is the CSV version
61:58 - so let me make sure that it isn't the
62:01 - same
62:02 - directory as our notebook is going to be
62:06 - otherwise it will give us an error when
62:08 - we'll try to read data from these files
62:11 - so it is
62:12 - ids.csv and Iris dot Json
62:16 - this data set is publicly available on
62:20 - both the versions can be downloaded from
62:22 - Kiki
62:25 - now let's move back to our notebook
62:29 - that is reading and viewing data from
62:31 - CSV and Json files
62:36 - so first we are going to import pandas
62:39 - as PD
62:40 - and now we'll create a data frame so DF
62:44 - is equal to PD dot read
62:48 - CSV please note that read CSV is the
62:51 - function which is used to read
62:55 - CSV files and extract data frames from
62:58 - the CSP files now we'll just print the
63:02 - data frame
63:04 - so it gives us the first five entries
63:08 - and the last five entries let me just
63:10 - write DF and execute it so it's in a
63:14 - better format so it contains 150 rows
63:17 - and six columns
63:18 - but what if we want to display all the
63:22 - rules like all 150 rows I want to
63:24 - display then I'll convert it to string
63:27 - using
63:28 - the two string method so I'll write DF
63:31 - dot to underscore string function
63:36 - so it will give me the whole
63:42 - data set
63:44 - so this is the data set and all the 153
63:49 - entries have been displayed
63:51 - now
63:54 - let me just print the first five entries
63:57 - of the data set so I'll write DF dot
63:59 - head so head method here displays the
64:03 - first five entries similarly the tail
64:07 - method displays the last five entries by
64:10 - default and this can be changed by
64:13 - passing the parameters
64:15 - so it displays the last five entries of
64:18 - the data frame
64:22 - now we can have some basic info by using
64:26 - the info method so it tells us
64:29 - about The Columns of the data set and
64:32 - the normal count in the column
64:35 - so all the 150 entries in the data set
64:39 - are not null
64:42 - when we check if we call them so this
64:45 - data set is probably clean
64:48 - also note that the names of all the
64:51 - columns can be
64:53 - extracted using the keys method
64:57 - so we'll write DF dot keys and we'll
64:59 - execute
65:00 - so we got all the column names as a list
65:04 - now as already discussed we can pass
65:09 - arguments To The Head and the tail
65:12 - function we want to change the number of
65:15 - entries so here if I want to display the
65:18 - first 15 entries I have to pass 15 in
65:20 - the head similarly if I want to display
65:23 - the last 15 entries I have to write DF
65:26 - dot tail and in bracket as to pass 15.
65:31 - so I have displayed the first 15 as well
65:34 - as the last 15 entries
65:36 - now let's see how to read Json files so
65:40 - for that we have a function called as
65:42 - read Json so I'll write PD dot read Json
65:49 - so I have to write pd.3 Json and I have
65:52 - to pass the filing
65:56 - so it is iris.json
66:00 - so Iris dot Chi Zone
66:04 - so this is the theme data set just to
66:07 - format is different also note that Json
66:10 - files are generally used to deal with
66:12 - big data
66:15 - all the other functions will remain same
66:17 - like the head tail info
66:21 - everything everything will remain just
66:24 - the same
66:28 - just note that the format is different
66:32 - so the function to read will be
66:35 - different but the data set will be dealt
66:38 - with the same functions so in today's
66:40 - lecture we covered how to read and view
66:43 - data from csb and Json hi guys welcome
66:46 - back to our course on pandas in today's
66:49 - lecture we are going to do correlation
66:52 - analysis using the function offered by
66:55 - pandas module of python so let's get
66:59 - started
67:00 - so as already discussed and the
67:03 - introductory lecture in statistics
67:06 - correlation or dependence is any
67:09 - statistical relationship whether causal
67:12 - or not between two random variables
67:16 - or pivariate data
67:18 - so in simpler words it tells us how
67:22 - to different columns or different
67:25 - features
67:27 - or different keys in a data frame or a
67:31 - data set are related to each other
67:35 - so correlation
67:37 - in simpler words is used to find
67:40 - relationship among the features or among
67:44 - the columns
67:45 - or among the keys
67:48 - so this is more of a formal definition
67:51 - correlation or dependence is any
67:54 - statistical relationship whether causal
67:57 - or not between two random variables or
68:00 - by varied data
68:04 - we can see that correlation is used to
68:08 - find relationship
68:11 - among the features
68:16 - now a great aspect of the pandas module
68:21 - is the curve method or the correlation
68:24 - method in short
68:26 - we say core or core or anything it
68:30 - doesn't matter until you write the
68:33 - correct
68:34 - thing
68:35 - when you are coding so a great aspect of
68:38 - pandas module is the core method
68:41 - so it can calculate the correlation
68:44 - between each column of the data set
68:48 - so the core method
68:51 - calculates
68:53 - the relationship
68:56 - between
68:58 - each column
69:02 - of the data set
69:06 - now I'll demonstrate
69:10 - the use of
69:12 - core method
69:14 - so first of all let me just do the
69:17 - necessary import so import pandas
69:20 - as PD
69:23 - now I'm going to
69:25 - extract a data frame
69:28 - from a CSV file
69:31 - so PD dot read CSV and let the file name
69:35 - be Iris dot CSV
69:38 - this is the same file that we discussed
69:40 - in our previous lecture now I just need
69:43 - to write DF Dot code and it will return
69:47 - me a new
69:49 - table or we can say a data frame
69:54 - with the correlation values
69:59 - you can read more about the correlation
70:01 - analysis in statistics but let me just
70:07 - tell you in brief how to read this table
70:11 - so the core method ignores the column
70:14 - which are not numeric
70:16 - so it had
70:19 - six columns I think one was about the
70:22 - species so it was not included in this
70:24 - table because the core method ignores
70:27 - the column which are not numeric
70:30 - and the result of core method is a table
70:36 - consisting of
70:38 - numeric values
70:44 - also if You observe carefully you'll
70:48 - notice that all the values in this table
70:51 - vary between
70:53 - -1 and plus 1.
70:59 - so the negative or the minus values
71:03 - denote a negative correlation
71:07 - so if one variable increases the other
71:10 - one will decrease
71:11 - so this is the meaning of the negative
71:13 - correlation
71:15 - and the positive values
71:18 - are plus denotes a positive correlation
71:22 - it means that if one variable will
71:25 - increase the other will increase as well
71:35 - now our next question is what is a good
71:39 - correlation
71:41 - so when we have a numeric value of 0.6
71:45 - or minus 0.6
71:48 - that denotes a good correlation either
71:51 - positive or negative like positive 0.6
71:55 - will denote that it has a good positive
71:59 - correlation
72:01 - and a negative value will indicate that
72:03 - it has a good negative correlation
72:15 - another important thing to note is
72:18 - the presence of a perfect correlation
72:23 - so if you are having a correlation value
72:26 - of plus 1 it denotes a perfect
72:30 - correlation and note that every feature
72:34 - is perfectly correlated to itself
72:39 - for example petal length
72:42 - is perfectly correlated to Petal length
72:47 - it's not important that to have a
72:50 - perfect correlation we observe the
72:53 - correlational same features we can have
72:57 - perfect correlation in
73:00 - different features as well but if the
73:04 - features are same then they must be
73:06 - perfectly correlated
73:13 - now let's see
73:15 - the correlation in form of a heat map
73:19 - we'll discuss more about heat maps and
73:22 - other plots in the map plotlet and c
73:25 - bond module I'm going to give you a
73:28 - brief intro only
73:33 - so we are going to observe this
73:36 - correlation and form of a heat map
73:50 - for that I need to import c bond as SNS
73:55 - and mat.lib Dot pipelot as PLT
73:59 - also note that if you don't have these
74:03 - modules
74:04 - installed in your
74:06 - system you can just skip
74:10 - because I'm going to teach this in
74:12 - future playlists as well I'll write SNS
74:16 - Dot
74:17 - heat map and I'll pass
74:20 - the table or the data frame that is DF
74:24 - Dot
74:25 - core
74:26 - so I'll write df.com
74:31 - so it gives me a heat map
74:34 - note that light value means a positive
74:38 - correlation and darker color means a
74:41 - negative correlation let me just print
74:44 - the correlation values as well and
74:46 - change this
74:49 - color combination to much of something
74:53 - cool warm and a not is equal to true now
74:57 - I have the correlation values as well as
75:01 - a different color for better
75:04 - visibility
75:06 - so today's lecture was all about
75:08 - correlation analysis using
75:12 - pandas
75:15 - also we'll discuss more about plotting
75:18 - in future playlists hi guys welcome back
75:21 - to your course on pandas in today's
75:23 - lecture I'm going to introduce you to
75:26 - the concept of data cleaning in pandas
75:32 - it is one of the most important stages
75:34 - in data preprocessing so let's get
75:38 - started
75:39 - so today's topic is data cleansing or
75:43 - more popularly known as data keyning
75:48 - so data cleaning means fixing bad data
75:51 - in our data set and bad data could be of
75:55 - many types so we'll discuss
75:59 - what can and cannot be considered as
76:03 - bad data
76:05 - so you have a clearer picture in your
76:08 - mind
76:10 - that what comprises of bad data
76:13 - so bad data could be
76:16 - data in wrong format
76:20 - empty cells
76:23 - let me just make a new
76:26 - cell for all this
76:28 - just wait a second so bad data could be
76:31 - data in wrong format
76:37 - empty cells
76:41 - wrong data
76:45 - duplicates that you have the same
76:48 - entries many times
76:51 - so all this comprises of
76:54 - bad data and we need to clean or we need
76:58 - to remove this kind of data from the
77:01 - data set or data frame
77:03 - so first of all we'll discuss how to
77:05 - deal with empty cells
77:10 - so first of all let me just import
77:13 - pandas as PD
77:20 - so I'm going to import pandas as PD
77:25 - and I'll
77:28 - read a data frame
77:31 - so DF is equal to read CSV
77:34 - and I'll pass Iris dot CSV
77:38 - now let me just
77:43 - sorry it's PD dot read so
77:47 - let me just display the first five
77:49 - entries of the data set
77:52 - so you can see that data is already
77:54 - clean
77:56 - so I need to add some
77:59 - other columns
78:03 - so that it is not clean anymore and then
78:05 - I'll demonstrate how can we actually fix
78:10 - the data set so like it would be DF of
78:13 - additional data is equal to
78:17 - not a number
78:21 - or this gives me an error because
78:25 - any n can't directly be used with python
78:29 - so there's a number function for it
78:32 - we'll discuss it later
78:35 - for now let it be none
78:42 - so I'll change it
78:43 - to
78:45 - none just give me one second
78:49 - yeah so let me just change it to none
78:52 - so it's still
78:57 - the same data set but we have added an
79:00 - additional
79:02 - column that is none so all the entries
79:05 - are none so you can
79:07 - observe that these columns makes
79:10 - absolutely no sense
79:13 - if it were present or not so this is
79:16 - kind
79:17 - of an example of bad cells
79:22 - now this column contains only none
79:25 - values so the right choice would be to
79:28 - remove this entire column
79:31 - which has none values now there's also
79:34 - method to remove the rules that contain
79:39 - not a number of values or null values so
79:42 - there's a little ambiguity here this
79:45 - difference between none and not a number
79:47 - or any n here so new DS is equal to DF
79:52 - dot drop any this will remove all the
79:56 - rows containing
79:59 - none values
80:03 - so in the new data frame is empty
80:06 - because every
80:08 - true contains a none or any n value
80:12 - so it will remove all the rows because
80:15 - of that column so this method is not a
80:18 - perfect choice here we should remove the
80:22 - column itself other than removing all
80:25 - the rows containing none or in in values
80:38 - so if I display the head of the new data
80:41 - frame it is also empty
80:44 - so
80:45 - hi guys welcome back to our course on
80:47 - pandas in our last lecture we started
80:51 - the topic data cleaning and I
80:54 - demonstrated how to remove
80:58 - a column and how to remove rows with
81:02 - none or not a number values today we'll
81:06 - see how to replace the null values with
81:09 - the mean or median and other simple
81:12 - methods of data keyning so let's get
81:15 - started
81:20 - so
81:23 - this is continuation of the previous
81:26 - video
81:27 - I'm just gonna
81:29 - make this data a little more dirty by
81:34 - adding an additional column
81:37 - which has
81:40 - empty values
81:48 - now I'm gonna replace the empty values
81:53 - by
81:55 - a number
81:57 - so this is one of the methods of data
82:00 - cleaning so replace the empty values by
82:04 - a number
82:11 - so we'll write DF dot fill any and we'll
82:14 - just provide that number by which we
82:16 - want to fill
82:18 - the
82:20 - none values this is to be used if all
82:23 - the rows
82:24 - have a value in common like they have
82:27 - one two three in common or we are adding
82:30 - a city for a particular set of people
82:35 - who live in same city so all people will
82:38 - have the city value as same
82:41 - so this is to be used only when
82:43 - all the rows share a same value now I'm
82:47 - going to add two more
82:49 - columns in which the values are none
82:54 - so that I can demonstrate a few more
82:57 - concepts of
83:00 - data cleaning
83:05 - now you must have observed that
83:08 - we
83:09 - failed additional data column with value
83:12 - one two three but when I displayed the
83:14 - data set again
83:16 - the changes were not committed so to
83:20 - commit the changes we always need to
83:22 - specify in place as true so I'll write
83:26 - fill any or fill the none or the null
83:30 - values or the NN values in the column A
83:34 - add 1
83:35 - and in base is true so that when I
83:38 - display the data set again
83:40 - the changes have been committed
83:44 - now I'll just drop
83:47 - one of the columns
83:49 - because I have a lot of columns with
83:52 - none value
83:54 - that I may not require so one of the
83:56 - simple method as Illustrated before is
83:59 - to just drop the column
84:04 - so this is a very long and documented
84:07 - error and there is an error because I
84:10 - have forgot to mention the access always
84:14 - remember that when you are dropping a
84:16 - column you have to pass the axis equal
84:19 - to one
84:21 - and then you have to execute it
84:25 - so now I'll just
84:27 - run
84:29 - this cell again
84:38 - so I'll run
84:40 - the cell again and it works perfectly
84:44 - now
84:45 - so I'll display the data set again so
84:49 - the empty column add with no values has
84:52 - been removed
84:54 - now I'll teach you
84:57 - how to
85:01 - replace
85:03 - the rows which have none values with the
85:07 - mean of other values in that column
85:12 - because the rows have
85:14 - null values some of the rows might have
85:19 - null or none or any in values for a
85:22 - column
85:25 - in our case the column is absolutely
85:28 - empty so let me just fill some of the
85:31 - values in a column
85:32 - and then I'll tell you how to fill all
85:35 - the other values in the column using
85:37 - mean median Etc
85:44 - so let me just change the
85:48 - column values
85:55 - for some of the rows
86:01 - so you can see that I have replaced
86:07 - one value of this column additional data
86:10 - with three
86:13 - and I have
86:14 - also
86:16 - accidentally change the value of another
86:19 - column to 4 but it won't matter let me
86:22 - just correct it
86:24 - and select the right column yeah now I
86:27 - have replaced two of the values by three
86:29 - and four now
86:31 - I'll replace all the other none or null
86:35 - or not a number of values in this column
86:38 - with the mean of these two values
86:41 - so X is equal to
86:47 - DF additional mean or additional data
86:51 - dot mean
86:59 - so X is the mean head and I'll write
87:07 - d f
87:09 - and the column name is additional data
87:12 - and I'll write
87:14 - fill
87:16 - dot fill
87:21 - n a and I'll pass X which is the mean in
87:25 - place is equal to
87:27 - true
87:29 - now I'll display the data frames so all
87:32 - the other values have been replaced with
87:34 - mean of these values now I've left with
87:37 - just one column which has all the none
87:39 - values let me just drop it simply
87:43 - so I'll write DF dot drop
87:46 - and pass the column name X is equal to 1
87:50 - and in place is equal to
87:53 - true now the data is key in again
87:56 - also I'll demonstrate some of
87:59 - other ways to handle null data in a
88:02 - specific lecture which will be dedicated
88:05 - to handling of null values so in today's
88:08 - lecture we conclude our discussion on
88:12 - data cleaning
88:14 - hi guys welcome back to our course on
88:16 - pandas in today's lecture I'm going to
88:18 - teach you how to visualize data using
88:22 - inbuilt functions of pandas so let's get
88:25 - started
88:26 - so I am going to cover the use of plot
88:29 - method
88:31 - to drop plots using pandas
88:35 - so the title is pandas for plotting
88:39 - and
88:43 - an ID only I'm not going to discuss it
88:46 - in detail I'm just giving you an idea
88:51 - so pandas uses the plot method
88:54 - to create plots
89:05 - also to visualize the plots we can use
89:09 - Pi plot which a sub module
89:12 - of the matplot library
89:17 - so I'm using notebook so I don't need to
89:21 - use this
89:22 - Pi plot or I don't need to import the
89:25 - pipelot but if you're using some other
89:27 - IDE it is
89:30 - vital it's important it's absolutely
89:33 - compulsory otherwise you won't be able
89:35 - to visualize the plots
89:39 - so first of all I am going to import
89:43 - pandas as PD
89:49 - and I will also
89:51 - import matplotlib Dot pipelot
89:58 - as pld
90:01 - so our first job now is to read
90:05 - the
90:07 - CSV file and extract the data frame or
90:10 - the data set from the CSV file I'll use
90:13 - the same Iris data set
90:16 - so I'll write PD dot Tweed CSV and it
90:18 - will pass the name iris.csv
90:22 - now I'll display
90:24 - some entries
90:27 - so this is our data set or our data
90:30 - frame now let me just draw a scatter
90:32 - plot we'll discuss two plots first is a
90:35 - scatter plot and the other one is the
90:37 - histogram
90:39 - so DF Dot Plot kind is equal to scatter
90:46 - and
90:48 - I have to pass the X and Y values
90:50 - because in a scatter plot we plot points
90:55 - and we have to pass the x coordinate as
90:58 - well as the y coordinate so let the x
91:01 - coordinate be petal length in centimeter
91:05 - and the y coordinate B
91:08 - length in centimeter
91:13 - so scatter plot can also be used for
91:15 - correlation analysis so we can see that
91:18 - how petal length and supple length are
91:21 - correlated using a scatter plot
91:24 - so it is also important so see as petal
91:29 - length increases after three the simple
91:32 - length also increases there is some
91:35 - correlation although it is not a very
91:37 - good correlation because for the values
91:41 - from 1 and 2 the plot is kind of messy
91:45 - but after three it shows a positive
91:48 - correlation
91:50 - although it doesn't seem a very good
91:52 - correlation
91:55 - we are using notebooks so we don't need
91:57 - to write PLT dot show but it will be
92:01 - required if you use some other
92:04 - IDE
92:06 - for executing this code
92:10 - now let's move on to our next plot
92:13 - so let's draw a histogram now
92:20 - so histogram is kind of a frequency plot
92:26 - so we have a variable and its frequency
92:32 - so let the variable be simple length in
92:35 - centimeters and
92:38 - the y-axis will give the frequency of
92:41 - that particular sepuland that how many
92:44 - flowers have that particular length of
92:48 - sepals so I'll write simple length
92:51 - centimeters
92:54 - and I'll
92:56 - write Dot Plot kind is equal to
93:01 - hist
93:03 - or histogram
93:06 - so this is our histogram
93:11 - so in today's lecture we covered how to
93:15 - draw plots using pandas and the data
93:20 - visualization abilities of pandas
93:24 - that will be all for today this video is
93:27 - brought to you by programming knowledge
93:29 - please like comment share subscribe and
93:32 - hit the Bell button for updates and stay
93:35 - tuned with us for next lecture thank you
93:39 - hi guys welcome back to our course on
93:41 - pandas in today's lecture I am going to
93:44 - discuss the merging of data frames in
93:47 - pandas so let's get started
93:52 - so the merge function is used for
93:56 - merging of data sets and pandas or we
94:00 - can see that the merge function
94:03 - facilitates the merging of data sets
94:10 - so there are three terms here merging
94:13 - joining and concatenating
94:16 - so we'll also try to understand the
94:19 - difference between these three so the
94:22 - first is merging and it is done by the
94:25 - merge function so the function merge
94:29 - allows us to merge data frames together
94:32 - using similar logic as merging SQL
94:36 - tables together
94:38 - so this merging is same as joins in SQL
94:42 - if you can recall the inner join outer
94:46 - join
94:47 - left join right join that is same as
94:51 - this merging
94:54 - so the merge function allows us to merge
94:56 - data frames together using a similar
94:59 - logic as merging the SQL tables together
95:04 - if you haven't
95:06 - studied joining an SQL I
95:09 - request you to find some good video on
95:13 - SQL joins and go through it now I'll
95:16 - create a data frame
95:19 - in this case I'm going to create two
95:21 - data frames left and right then I am
95:24 - going to demonstrate inner join
95:28 - or the inner merge outer merge
95:31 - same as SQL joins inner outer left and
95:36 - right let me just create two data frames
95:40 - real quick
95:42 - this gave me an error because I forgot
95:45 - to import pandas as PD so this is our
95:49 - left
95:50 - data frame or the data frame one in a
95:53 - similar manner let's create the second
95:55 - data frame
95:57 - we just change some values in the
95:59 - original data frame
96:01 - and that's it
96:07 - so create two data frames first and then
96:10 - try to perform different time soft
96:13 - merging
96:14 - and merging is similar as SQL joint so
96:19 - there are generally four kinds of
96:21 - merging
96:24 - and also note that
96:26 - for merging we require
96:29 - keys
96:31 - that is very important
96:34 - so this is our right data set and
96:37 - we have one more data set that is left
96:40 - these are the two data sets
96:44 - now I'll write
96:48 - PD dot merge
96:51 - and I'll pass both the data sets
96:56 - that is left and right
96:59 - final pass on
97:04 - equal to
97:08 - key one
97:10 - comma
97:12 - key to
97:15 - submerging is performed on keys
97:19 - that is very important
97:21 - so this is the inner merging or inner
97:26 - SQL join
97:28 - so please take care of this
97:31 - that merging is same as SQL joins and is
97:36 - performed on keys so this is the inner
97:40 - join
97:41 - if we want to perform the outer join
97:43 - then we have to specify
97:46 - how is equal to Outer
97:56 - and we have to also specify on q1 comma
98:00 - key2
98:05 - so that was in a join and this is outer
98:08 - join similarly to perform right and left
98:12 - join we just need to specify how is
98:14 - equal to left or right
98:18 - so in a similar manner we can perform
98:21 - left and right join
98:32 - also for more information
98:35 - please refer SQL joints so in today's
98:39 - lecture we covered merging of data sets
98:44 - in pandas
98:50 - that will be all for today this video is
98:52 - brought to you by programming knowledge
98:54 - please like comment share subscribe and
98:57 - hit the Bell button for updates and stay
99:00 - tuned with us for next lecture thank you
99:04 - hi guys welcome back to our course on
99:07 - pandas today I'm going to discuss the
99:10 - joining of data sets or data frames with
99:14 - the help of functions offered by pandas
99:17 - so let's get started
99:19 - so the topic for today is joining so
99:23 - joining is a convenient method for
99:25 - combining The Columns of two potentially
99:28 - differently index data frames into a
99:32 - single result data frame
99:34 - so we have two different data frames and
99:39 - we are combining the columns of two data
99:42 - frames which have different indexes and
99:46 - we get one data frame as a result so
99:50 - this is the official definition now
99:53 - let's try to understand the difference
99:55 - between joining and merging so joining
99:59 - is performed on indexes
100:03 - and merging is performed on keys so this
100:07 - is one and only major difference between
100:11 - joining and merging of data sets or data
100:15 - frames and panels we are concerned with
100:18 - indexes
100:21 - so now I'll demonstrate how joining
100:24 - works
100:29 - but let me just complete
100:33 - this statement so joining is same as
100:36 - merging but performed on indexes
100:41 - instead of
100:45 - keys
100:47 - K1 and K2
100:50 - so the
100:51 - keys of the dictionary are a and b and a
100:56 - contains the value
100:58 - as well as 8 0 a 1 and E2 and the key B
101:03 - contain the value in a list b0 P1 and B2
101:08 - and the index contains
101:19 - so in a similar manner will create
101:24 - a second data frame
101:27 - whose name is right and will make some
101:30 - changes in the columns as well as the
101:33 - index
101:34 - also I forgot to import pandas as PD so
101:38 - I got an edit but I've corrected it so
101:41 - this is our left data frame
101:43 - and in a similar manner I'll create the
101:46 - second data frame
101:48 - so let me just copy and paste whole
101:50 - thing and I'll change the name
101:52 - and I also change the key values as well
101:58 - so it's c c not C1 and C2
102:04 - or C not C2 C3 or whatever you want to
102:09 - have
102:10 - it doesn't actually matter
102:12 - so the keys in this case are C and D
102:15 - containing lists as c0 C2 C3 and d0 D2
102:20 - D3 and the index values
102:24 - now
102:29 - pay attention that joining is done on
102:33 - indexes rather than keys so now I'll
102:36 - write left dot join and I'll pass
102:39 - right
102:40 - so this is the inner join
102:43 - similarly we can
102:46 - exhibit the outer join I'll just need to
102:49 - write left dot join
102:53 - and I'll pass
102:55 - right and I'll pass how is equal to
103:00 - outer
103:01 - so this is the result of the outer join
103:07 - in a similar manner we can perform
103:11 - left and right jaw
103:13 - so this is the result of left join and
103:18 - the result of
103:22 - right join
103:23 - so in today's lecture we discuss joining
103:27 - of data frame in pandas
103:33 - that's all about joining
103:36 - that will be all for today this video is
103:39 - brought to you by programming knowledge
103:40 - please like comment share subscribe and
103:43 - hit the Bell button for updates and stay
103:46 - tuned with us for next lecture thank you
103:50 - hi guys welcome back to our course on
103:53 - pandas today I'm going to cover the
103:55 - concatenation of data frames in pandas
104:00 - so let's get started
104:02 - so the topic for today is concatenation
104:08 - so concatenation basically means gluing
104:11 - together data frames
104:13 - also note that Dimensions should match
104:16 - along the axis we are concatenating on
104:20 - so it is different from joining what we
104:24 - are basically doing is we are just
104:26 - gluing two data frames together
104:29 - and please take care of Dimensions while
104:32 - concatenating
104:37 - so let me just complete this sentence
104:40 - Dimensions should match along the axis
104:45 - we are
104:46 - concatenating on
104:54 - also to perform concatenation we have a
104:58 - special method
105:01 - so the use of concat method is done
105:07 - fourth concatenation
105:10 - so he used PD dot concat method
105:15 - and pass the list of data frames you
105:18 - want to concatenate
105:32 - so let me just create a data frame real
105:35 - quick and I also need to do the
105:37 - necessary import
105:39 - that is pandas but let me just first
105:42 - Define the First Data frame
105:48 - so let me move on uh so by mistake I
105:52 - went back now have to do it all over
105:55 - again
105:56 - so let me just do it again quickly
105:59 - so I have to import pandas as PD
106:07 - and I'll run this cell again
106:11 - and the cell again
106:14 - so now
106:15 - I'll just display the First Data frame
106:19 - and in a similar manner I'll create the
106:21 - second data frame
106:23 - so df2
106:26 - this time and the
106:29 - columns will remain the same
106:32 - The Columns will be the same a b c and d
106:35 - but the indexes will vary so the index
106:39 - is
106:40 - should be in continuation here when I'll
106:43 - am concatenate along the axis 0.
106:48 - the only thing that we need to keep in
106:51 - mind while concatenation is the indexes
106:55 - and the values
106:57 - if you don't have
106:59 - access to values or indexes
107:03 - so the result of concatenation can be a
107:06 - little messy when you concatenate along
107:08 - the higher axis
107:10 - but we are concatenating
107:13 - on axis 0 right now so for the First
107:16 - Data frame the Nexus are 0 1 2 and 3 and
107:19 - for the second data frame I'll specify
107:22 - the indexes as
107:24 - four five six and seven
107:27 - so I'll make the necessary changes
107:31 - let me make some changes in the data as
107:34 - well
107:38 - the data can be redundant also
107:42 - but to make it look better I'm just
107:45 - making some changes in data as well
107:48 - but we can have redundant data we just
107:51 - need to change the indexes
107:53 - so let the indexes be
107:55 - 4 5 6 and 7.
107:59 - now I'll just run this cell and I'll
108:02 - display the second data frame that is
108:05 - in continuation of the First Data frame
108:09 - now write PD Dot concat and I'll pass
108:13 - both the data frames
108:18 - so this gave me an error because always
108:21 - remember that you have to pass the data
108:24 - frames in form of a list
108:27 - so please don't forget
108:29 - indentation or the documentation because
108:32 - different methods or functions take
108:35 - different
108:36 - kind of references or arguments
108:40 - so make sure to pass a list of data free
108:45 - now let's concatenate
108:50 - along the axis 1.
108:56 - so
108:57 - in this case we have to specify access
109:00 - is equal to one rest of the procedure is
109:03 - same so I'll write PD Dot concat and
109:06 - I'll pass the list of data frames as
109:09 - well as I'll specify the axis is equal
109:12 - to
109:13 - 1.
109:15 - so we have
109:17 - concatenated along the columns or along
109:22 - the axis 1.
109:27 - so we have concatenated along the
109:30 - columns also note that we got a lot of
109:33 - null or nnn values so make sure you have
109:38 - all the necessary values for
109:40 - concatenation if you are doing
109:42 - concatenation along the axis 1.
109:49 - so make sure you have all the values
109:53 - for concatenation otherwise it will give
109:56 - n a n or null in the results
110:07 - so in today's lecture we covered the
110:10 - concatenation of data frames and pandas
110:23 - that will be all for today this video is
110:26 - brought to you by programming knowledge
110:27 - please like comment share subscribe and
110:30 - hit the Bell button for updates and stay
110:33 - tuned with us for next lecture thank you
110:37 - hi guys welcome back to our course on
110:39 - pandas in today's lecture I'm going to
110:42 - teach you how to combine multiple
110:44 - conditions using selection and how to
110:47 - extract information about unique
110:50 - elements from a data frame so let's get
110:53 - started
110:55 - so today's topic is
110:57 - selection and information on unique
111:01 - elements
111:05 - first of all our job is to create a data
111:08 - frame
111:10 - so create a data frame first
111:14 - so I'm going to import
111:17 - pandas as PD
111:23 - and I'll create a data frame so DF is
111:27 - equal to PD dot data frame
111:31 - so in this case I am not going to use
111:33 - the Run n method I'm just simply passing
111:37 - the
111:39 - column wise values
111:42 - in form of a dictionary
111:53 - so in a similar manner you have to
111:55 - create a data frame
112:05 - with three columns
112:24 - so this is our third column
112:27 - and
112:28 - executed
112:31 - so we have created a data frame and this
112:34 - is the resultant data frame
112:37 - now
112:39 - under selection
112:46 - so we select from data frame using
112:48 - criteria from multiple columns
112:52 - so we'll combine multiple conditions
112:55 - using operator such as and
112:59 - and or
113:03 - so we can select from a data frame using
113:07 - right area from multiple columns
113:18 - so basically here we are combining
113:21 - multiple conditions using and operator
113:38 - so I'll write new data frame is equal to
113:43 - data frame and in bracket
113:46 - I'll specify the conditions 1 so DF
113:53 - column 1 greater than 2
113:58 - and
113:59 - other condition
114:03 - so I'll write add
114:04 - DF column 2 equal to equal to
114:08 - triple four
114:10 - or four hundred forty four
114:20 - I'll execute this
114:22 - I'll display
114:25 - so it is giving me an empty data frame
114:29 - yeah this is because our initially data
114:32 - frame should have three columns so
114:34 - there's something wrong
114:36 - in our Declaration of a data frame
114:39 - so let's check
114:41 - because it should have three column here
114:43 - we have two column oh I have not given
114:46 - the correct name to the third column Let
114:49 - It Be column three now let's execute
114:51 - execute this again now we have three
114:54 - columns now this condition will work so
114:57 - we have
114:59 - a row
115:01 - here so this row had column 2 equal to
115:05 - equal to
115:08 - 444 and the column 1 was greater than 2.
115:12 - so this is similar to the conditional
115:15 - selection
115:17 - we discussed earlier
115:19 - only difference is we are combining
115:22 - several conditions
115:31 - now we'll see how to get information on
115:35 - unique values
115:37 - column
115:50 - so first we'll try to identify all the
115:55 - unique elements of a column
115:57 - and obtain them in form of a list so for
116:00 - that purpose we have to use the unique
116:03 - method so I'll write DF and the target
116:06 - column Dot
116:08 - unique
116:09 - so column 2 has three unique values
116:13 - 444 555 and 666.
116:17 - in a similar manner we can calculate the
116:20 - count of unique elements of a column
116:25 - so for that I have to write DF Target
116:28 - column Dot
116:31 - n unique
116:36 - and I'll execute it so it has three
116:40 - unique elements
116:41 - first one gives us the unique element
116:43 - and the second one gives us the count of
116:48 - the unique elements
116:50 - or the total number of unique ellipse
116:52 - now our next objective is to
116:56 - calculate how many times the unique
116:58 - values appear in a column so what is
117:01 - frequency of each unique value
117:03 - so for that we have a method called as
117:06 - value counts
117:08 - so here triple four occurred two times
117:11 - triple five occurred one time and triple
117:14 - six
117:15 - awkward only one time so in today's
117:17 - lecture we covered
117:20 - unique elements and conditional
117:24 - selection
117:25 - that will be all for today this video is
117:28 - brought to you by programming knowledge
117:29 - please like comment share subscribe and
117:32 - hit the Bell button for updates and stay
117:35 - tuned with us for next lecture thank you
117:39 - hi guys welcome back to your course on
117:41 - pandas in today's lecture I'm going to
117:44 - demonstrate the use of apply method of
117:48 - pandas so let's get started
117:52 - so the apply method is used for applying
117:55 - functions on columns so we can
117:59 - select a particular column apply a
118:02 - function on all the values of that
118:05 - column and store the result somewhere
118:07 - else like in a new column
118:09 - or in a new list somewhere
118:13 - so apply method is used to broadcast a
118:17 - function to the columns that is every
118:20 - value of that particular column will be
118:24 - treated with the function that is passed
118:27 - in the apply method
118:32 - so first of all let us read a data frame
118:38 - so let's feed a data frame first
118:44 - then I am going to import pandas as PD
118:55 - so now I'll write DF is equal to PD Dot
119:00 - read
119:02 - CSV
119:04 - and I'll pass the
119:07 - file name that is Iris dot CSV
119:13 - I'll display some of the
119:16 - rows of the data frame
119:20 - so this is basically our data frame
119:23 - now let's create a new column that
119:27 - stores the length of species name so the
119:31 - last column is the species and we have
119:34 - to count the number of
119:37 - characters
119:38 - in the species name for every row
119:42 - so create a new column that stores the
119:45 - length of species name so the procedure
119:48 - is very simple we have to create a new
119:50 - column so DF of new column name or
119:54 - anything let it be Lin
119:57 - species
119:59 - or dense species name
120:03 - is equal to DF
120:05 - of
120:07 - species because we are applying this
120:09 - function on
120:10 - the column species and we are applying
120:14 - the function that is length
120:17 - so this needs not to be in
120:20 - commas or removing
120:22 - so dot apply length
120:26 - now I'll display the data frame and it
120:28 - has a new column that is length of
120:32 - species name and you can count it
120:34 - manually also it's one two three four
120:36 - five six seven eight nine ten eleven and
120:39 - for the last
120:41 - true it is 14.
120:44 - now
120:46 - let's create a function
120:51 - that takes a number
120:55 - and Returns the number
120:58 - multiplied by 2.
121:08 - so create a function that takes a number
121:10 - and Returns the number multiplied by 2
121:15 - and our second
121:16 - job is that
121:19 - using apply method
121:25 - so using apply method
121:29 - create a new column
121:36 - Dot contains
121:42 - the doubled
121:50 - cepulant
121:55 - so we have to create a function
121:58 - that takes a number and returns a number
122:00 - multiplied by two and we have to apply
122:02 - it
122:03 - to the column that is
122:06 - supplement centimeter so I'm creating a
122:11 - function first
122:12 - so this is the function
122:14 - Let its name be times 2 and it returns
122:17 - number multiplied by two now d s of
122:23 - sample length
122:27 - 2 art supplement doubled is equal to DF
122:31 - supplement centimeter
122:38 - dot apply
122:43 - times 2.
122:51 - I'll execute it
122:56 - and now we have a new
123:00 - version of the data frame containing an
123:03 - additional
123:04 - column that contains
123:06 - the double
123:08 - length
123:10 - so apply method can be used to broadcast
123:13 - inbuilt or user defined functions
123:22 - so it can be used to broadcast
123:24 - in build functions as well and user
123:28 - defined functions as well
123:30 - also we can use a combination of apply
123:34 - method as well as the Lambda function
123:52 - so today's lecture was all about the
123:56 - application of apply method
123:59 - in pandas
124:08 - that will be all for today this video is
124:10 - brought to you by programming knowledge
124:12 - please like comment share subscribe and
124:15 - hit the Bell button for updates and stay
124:18 - tuned with us for next lecture thank you
124:22 - hi guys welcome back to our course on
124:24 - pandas in today's lecture we'll discuss
124:27 - some methods functions or operations of
124:31 - pandas to get information about data
124:34 - frames
124:35 - so let's get started
124:37 - short topic is basic operations or
124:41 - functions
124:44 - so we'll discuss some methods or
124:47 - functions of pandas to get information
124:50 - about the data frames
124:52 - and these functions are very useful
124:55 - during data analysis
125:08 - so use of methods last functions of
125:10 - pandas to get info about
125:14 - data frames
125:17 - so I'll import pandas as PD
125:23 - then I'll
125:25 - read a data frame from the iris file
125:29 - as usual
125:31 - so DF is equal to PD dot read CSV and
125:35 - I'll pass Iris dot CSV
125:38 - and this is our data frame
125:42 - now I'll just get some basic info about
125:46 - this data frame using
125:49 - DF dot info
125:52 - so this is the application of info
125:55 - method or function so it gives us the
125:59 - information about
126:01 - the column names
126:03 - the number of null values and their
126:05 - count
126:08 - to this we have already discussed in a
126:11 - previous lecture
126:12 - now let's move on
126:16 - so how to get the names of all the keys
126:21 - of a data frame for that we can write DF
126:24 - dot keys
126:26 - and it will display a list of all the
126:30 - keys
126:33 - moving on to the description
126:37 - we can write DF dot describe so it will
126:41 - tell us the count the mean the standard
126:44 - deviation the minimum and the maximum
126:47 - value and the quartile distribution
126:51 - of each and every column
126:55 - provided that it is a numeric column
126:57 - like for the species it's showing
127:00 - nothing so that is missing here
127:03 - so DF dot describe can give us the
127:06 - description of each and every column
127:13 - another method to fetch all the keys is
127:17 - using DF dot columns so it also returns
127:20 - us a list of all the column names
127:23 - it is similar to DF dot keys
127:27 - so this is cmf DF dot Keys function
127:36 - now we also have a function to fetch all
127:41 - the indexes
127:44 - so it will not return a list of indexes
127:46 - but rather it Returns the start value
127:50 - the stop value and the step size
127:52 - in today's lecture we covered some
127:54 - functions to get information on data
127:57 - frames that will be all for today this
128:00 - video is brought to you by programming
128:02 - knowledge please like comment share
128:04 - subscribe and hit the Bell button for
128:06 - updates and stay tuned with us for next
128:10 - lecture thank you
128:12 - hi guys welcome back to our course on
128:15 - pandas in today's lecture we are going
128:17 - to cover the topics of sorting and
128:20 - ordering using
128:22 - pandas so let's get started
128:26 - so topic for today is sorting and
128:30 - ordering
128:35 - so sorting and ordering
128:38 - first I'm going to import pandas as PD
128:43 - so import pandas
128:46 - as PD
128:50 - our next job is to read a CSP file on
128:54 - extract the data set from the file
128:58 - so read a
129:01 - CSV file and extract a data frame
129:09 - for that I am going to write DF or the
129:13 - variable or anything is equal to PD dot
129:16 - read CSV and the name of the file is
129:20 - Iris dot CSV
129:24 - so this is our data frame
129:27 - so let's sort in ascending order
129:32 - by
129:34 - simple length
129:37 - so sorting
129:38 - in
129:40 - ascending order
129:46 - by
129:49 - sepulence so here we have sorting
129:53 - as well as ordering so order is
129:57 - ascending on
129:59 - the variable is
130:01 - simple length
130:03 - so now I'm going to write d f Dot
130:06 - sort underscore values function and it
130:11 - takes a parameter that is by in which we
130:14 - have to specify the column name
130:17 - so we want to
130:19 - sort by supplement so I'll pass
130:21 - supplements centimeter
130:25 - and it will return me the sorted
130:29 - data frame
130:33 - also note that it does not cause any
130:37 - change in the original data frame
130:41 - see
130:43 - there's no change in the original data
130:46 - frame so we can say that by default
130:49 - sorting is
130:52 - not in place
130:55 - so in place is false by default
131:00 - if we want permanent changes we have to
131:03 - specify the in place as
131:06 - true
131:21 - now let's sort by Petland
131:25 - so
131:26 - the syntax will be same DF
131:30 - dot sort values Pi is equal to Petal and
131:34 - centimeter
131:36 - so this is quite easy to do so let's
131:39 - sort by petal and now and write DF dot
131:42 - sort values
131:47 - and by
131:51 - is equal to Petal length centimeters
132:05 - and I'll run it so this also returns a
132:09 - data frame but does not cause any change
132:12 - in the original data frame until and
132:15 - unless we specified in place as true
132:19 - so that's all about
132:23 - sorting
132:25 - and ordering
132:28 - the values or the entries in a data
132:31 - frame
132:33 - that will be all for today this video is
132:35 - brought to you by programming knowledge
132:37 - please like comment share subscribe and
132:40 - hit the Bell button for updates and stay
132:43 - tuned with us for next lecture thank you
132:47 - hi guys welcome back to our course on
132:50 - pandas in today's lecture I am going to
132:52 - demonstrate the use of tail keyword for
132:55 - deletion of columns from a data frame so
133:00 - let's get started
133:01 - let me just create a new notebook
133:04 - first and rename it
133:07 - so let me just change the name to Dell
133:11 - keyword
133:15 - so the topic for today is Dell keyword
133:19 - and it's used for dropping The Columns
133:22 - of a data frame
133:25 - so Dell keyword can be used for removing
133:29 - a column
133:33 - earlier I demonstrated the use of drop
133:36 - method for removal of a column now I'll
133:39 - teach you how to remove a column using
133:42 - the Dell keyword
133:46 - so let me just import
133:50 - pandas as PD and let me just
133:53 - read a CSV file and extract the data
133:56 - frame from that particular file
134:03 - so this is our data frame let me just
134:06 - create a new column real quick
134:09 - so let's
134:11 - add a column
134:14 - so DF
134:17 - of blank
134:20 - blank is equal to
134:24 - none or anything so it's giving an error
134:29 - because it can't be directly used we
134:31 - have to write
134:32 - numpy Dot N A N I think so because it is
134:36 - in numpy and not in Python
134:38 - NN is not a data type supported by
134:41 - python so let it be blank or white space
134:44 - or anything
134:46 - and let me just
134:49 - display the data frames now I have a
134:52 - column with the blank values
134:54 - or empty values
134:58 - now let me just
135:02 - conventionally
135:04 - remove this column
135:08 - so conventionally we used the drop
135:11 - method so I'll write DF dot drop and
135:14 - I'll pass
135:17 - the column name
135:19 - axis is equal to one
135:27 - so this is the conventional method and
135:30 - it does not
135:31 - cause any change in the original data
135:34 - set unless you specify in place as true
135:39 - so no change
135:44 - in the original data frame
135:46 - until and unless we specify in place
135:51 - true
136:04 - so it won't commit any changes until you
136:07 - specify in place as true so pass DFT
136:10 - drop
136:12 - the column name
136:14 - axis is equal to 1
136:17 - and in place is equal to true
136:30 - so now it has been removed
136:33 - now let's see
136:35 - how to use the Dell keyword to achieve
136:38 - the same result so let me just add the
136:41 - column
136:42 - once again so DF of Planck is equal to
136:46 - none this time
136:49 - and display the data set so this is the
136:52 - extra column with none values now I'll
136:54 - just simply write Del DS and the column
136:58 - name in square bracket and it has been
137:01 - deleted from the original data set as
137:03 - well we don't need to specify in place
137:06 - true or something like that
137:08 - so deleted
137:10 - successfully
137:12 - without passing additional parameters
137:14 - like access
137:16 - or
137:19 - specifying the in place as true
137:22 - so in today's lecture we covered the use
137:25 - of tail keyword
137:27 - for removing columns from a data frame
137:34 - that will be all for today this video is
137:37 - brought to you by programming knowledge
137:38 - please like comment share subscribe and
137:42 - hit the Bell button for updates and stay
137:44 - tuned with us for next lecture thank you
137:48 - hi guys welcome back to our course on
137:51 - pandas in today's lecture we'll learn
137:54 - how to deal with
137:56 - not a number or non-values in a data set
138:01 - we have already discussed some of it in
138:03 - the data cleaning portion of this
138:06 - playlist but we'll
138:08 - cover this topic in a finer detail today
138:12 - so let's get started
138:15 - so our topic is
138:17 - dealing with
138:19 - Nan
138:21 - values in a data set
138:30 - so that's the topic for today
138:34 - so first job is always to create or
138:38 - import a data set
138:39 - will create a data set
138:41 - instead of importing it
138:47 - let the data frame name be df1
138:51 - in this case
138:52 - and we'll pass a dictionary here to
138:56 - create a data frame
138:58 - so let me just do it quickly
139:06 - so it has two columns that is a and b
139:10 - elements of column A are gonna be a 0 a
139:15 - 1 and A2 while the elements of column B
139:19 - are gonna be b0 B1 and B2
139:24 - so B 0 comma B1 and B2
139:31 - and our index gonna be
139:34 - 0 1 and 2.
139:38 - so I have to pass the index the index is
139:40 - equal to list 0 comma 1 comma 2.
139:45 - now
139:47 - I'm gonna
139:50 - run this thing there's an error I forgot
139:52 - to import
139:54 - let's import
139:57 - import panels as PD
140:04 - as PD
140:06 - and
140:08 - df1 is equal to PD dot data frame
140:21 - so this is our first data frame now
140:24 - similarly create a second data frame
140:29 - just change some of the values
140:40 - so second data frame
140:42 - can be created in a similar manner by
140:45 - changing a few values
140:47 - what we are actually gonna do is we are
140:50 - waiting two data frames and we are gonna
140:53 - concatenate it along
140:57 - the axis one so that we have some of the
141:01 - values as
141:02 - nen
141:04 - that's how we are gonna achieve some
141:06 - alien values we can also directly insert
141:09 - some of the Nan values using
141:12 - numpy dot Nan but here to make it a
141:16 - little more complex I'm using
141:19 - concatenation
141:22 - so I've concatenated both the data sets
141:25 - or the data frames along the axis one so
141:28 - now I have one two three four five six
141:31 - six into two twelve twelve and in values
141:35 - now our first
141:37 - job is to check for
141:39 - null values or any n values in this case
141:43 - so checking for null values
141:46 - so we can check for null values by is
141:49 - null
141:50 - method or function so I'll write Q dot
141:54 - is null and all the null values
141:58 - will return true while other values will
142:01 - return false so we'll
142:04 - get a sort of data frame as a result and
142:07 - the entries corresponding to the null
142:09 - values will be true and corresponding to
142:13 - not null values will be false
142:19 - now
142:21 - we can see how to drop all the rows with
142:25 - null values so I had already discussed
142:28 - this just write me Q dot drop any so all
142:31 - the rows have been deleted because all
142:34 - the rows contain
142:36 - null value
142:39 - so drop n a removes the rows
142:42 - with null values this we have already
142:46 - discussed in data cleaning
142:49 - but I thought it would be great if I
142:52 - explain it in detail here
142:55 - so by default drop
142:57 - any or drop null is false
143:01 - in place
143:02 - so it won't cause any change and the
143:05 - original data set unless you specify in
143:08 - place is equal to true
143:10 - now we Are Gonna Fill
143:14 - the null values
143:16 - for that we have fill
143:19 - any method and I'm gonna pass whatever
143:23 - number or string I want null values to
143:26 - be replaced with
143:28 - now all the null values will be replaced
143:33 - with the string fill so here we can see
143:37 - the result
143:39 - now moving on
143:49 - so if we want to assign any n value
143:55 - to some variable directly
143:58 - we can't do it using the python
144:01 - functions or the python Concepts we have
144:05 - to go for numpy
144:07 - so I'm gonna import and I'm by as NP
144:13 - and I'm gonna write
144:16 - Q of Nu
144:19 - is equal to NP Dot N A N
144:22 - this is how we assign an in value to a
144:25 - row or a column
144:27 - you can't directly write Q Nu is equal
144:29 - to Indian it won't work
144:32 - so now you know how to
144:35 - directly
144:37 - assign
144:40 - a row red
144:42 - n in value
144:49 - so in today's lecture we covered how to
144:52 - deal with null or NN values in a data
144:56 - frame
144:59 - that will be all for today this video is
145:01 - brought to you by programming knowledge
145:03 - please like comment share subscribe and
145:06 - hit the Bell button for updates and stay
145:09 - tuned with us for next lecture thank you
145:13 - hi guys welcome back to our course on
145:16 - pandas in today's lecture I'm going to
145:19 - teach you about
145:21 - pivot tables so let's get started
145:25 - so what is a pivot table so pivot table
145:30 - it's nothing but a multi-index table
145:34 - so earlier we had just only one index
145:38 - or label here we can have multiple and
145:41 - we can sort them
145:43 - so we are basically trying to create a
145:46 - multi-index table from a normal data
145:49 - frame this is what we are gonna do in
145:52 - this video
145:57 - so pivot table
146:00 - simplifies the presentation of data in a
146:04 - data frame
146:05 - where we have redundant a pivot valid
146:09 - for a particular let me just important
146:11 - so let's now I'm going to first create a
146:15 - dictionary
146:16 - in order and then we will create
146:20 - so let the dictionary b a b c and d
146:24 - respectively
146:30 - so A and B have redundant
146:34 - so A and B will act as the
146:38 - multiple indexes
146:55 - so let the elements of a b
146:59 - so
147:00 - bar
147:02 - redundant and elements p
147:07 - 1 and 2 for
147:09 - the column B also redundant
147:38 - so we have created a dictionary now
147:41 - let's create a data frame so DF is equal
147:44 - to PD dot data frame
147:46 - and I'll pass the dictionary
147:48 - so this is the resultant data frame
147:52 - now I have to create a pivot table
147:55 - so let's create a pivot table
147:59 - so in order to create a pivot table
148:05 - I have to write DF dot pivot table
148:08 - within underscore separation and I'll
148:11 - pass values is equal to D because it
148:15 - represents the data index or the
148:18 - multi-index is represented by a and b
148:24 - and let's represent the columns by C so
148:28 - the columns is equal to
148:31 - Phi so the columns I X and Y
148:35 - now this is the resultant
148:38 - data frame or pivot table or a pivot
148:41 - data frame
148:43 - so in today's lecture I covered how to
148:47 - create a pivot table
148:49 - from a normal data frame
148:56 - hence we have created a pivot table
148:58 - successfully that will be all for today
149:00 - this video is brought to you by
149:02 - programming knowledge please like
149:04 - comment share subscribe and hit the Bell
149:07 - button for updates and stay tuned with
149:10 - us for next lecture thank you
149:13 - hi guys welcome back to our course on
149:16 - pandas this is gonna be the last Theory
149:19 - lecture
149:20 - I'm gonna teach you how to read data
149:24 - from HTML files so let's get started
149:29 - so today's topic is
149:37 - reading data from HTML files
149:42 - so we have to install HTML lib
149:47 - lxml and
149:50 - beautiful soup
149:53 - before we can actually read data from
149:56 - HTML files so these are the commands for
149:59 - installation that is conda install lxml
150:04 - conda install HTML lib 5 or HTML5 lib
150:09 - and conda install
150:12 - beautiful soup
150:14 - 4.
150:17 - so I have installed
150:20 - after installation you have to restart
150:23 - the restart notebook
150:26 - so just close it
150:29 - close your browser
150:31 - open the Anaconda navigate again and
150:34 - launch the notebook again
150:43 - so I have launched the notebook
150:45 - successfully now I just need to look for
150:47 - that particular file
150:49 - search in desktop
150:52 - so I'll just go to
151:01 - the file was reading HTML Dot ipin
151:06 - now I just need to import
151:09 - pandas aspd
151:16 - so import pandas as PD and then I just
151:20 - need to write DS is equal to PD Dot
151:24 - read HTML it's similar to reading from
151:27 - CSV or
151:28 - Json I just need to change read
151:31 - underscore HTML and I have to pass
151:36 - the link of the file
151:42 - and you can process and manipulate the
151:46 - data frame in the similar way as we
151:48 - manipulated any other data frame so this
151:51 - is gonna be your assignment you have to
151:54 - look for an HTML file that contains a
151:58 - database and you have to extract that
152:01 - database from the HTML file
152:05 - that will be all for today this video is
152:08 - brought to you by programming knowledge
152:09 - please like comment share subscribe and
152:12 - hit the Bell button for updates and stay
152:15 - tuned with us for next lecture thank you

Cleaned transcript:

hi guys welcome back to our new course on pandas this is gonna be an introductory video so let's get started so pandas is a python Library used for working with data sets it has functions for the following task first of all we have data analysis so data analysis is the practice of working with data to glean useful information which can then be used to make informed decisions so in data analysis basically we try to find out patterns from data so we extract meaningful information and we use that information in future to make meaningful and informed decisions the second is cleansing data cleansing or data cleaning is the process of detecting and correcting corrupt or inaccurate records from a record set table or a database and it refers to identifying incomplete incorrect inaccurate or irrelevant parts of the data and then replacing modifying or deleting the dirty or course data so in cleansing what we do is we correct the wrongly process data for example when you are making a data set there must be some values missing in a record that may not be available so we just need to clean them for example we have a row with empty values so we have to replace that row because we can't work with raw data data needs to be in a meaningful form to extract patterns from the data and the last part is data exploration it is very similar to initial data analysis whereby a data analyst uses visual exploration to understand what is in a data set and then characteristics of theta rather than the traditional data management systems so in data analysis we try to use the functions to find out patterns like the mean the median or something like that but in data exploration we draw colorful plots or charts to analyze the data the process of analysis and exploration are usually performed together armed together known as Eda or explorate free data analysis so it includes the data Exploration with the help of charts or plots as well as with the help of functions so it is called as Eda or exploratory data analysis whenever we have any project the first part is to perform the ede or the exploratory data analysis and then Keening because if we know what is the problem in data then only we can key in the data so these are three important functions that are facilitated through pandas or with the help of functions that are available in the module pandas of python now let's move on to our next question why even use pandas so pandas has a lot of other functionalities as well we'll go through some of them first one is analysis of big data so what is Big Data Big Data refers to the data sets that are too large or complex to be to be dealt with by traditional data processing application software the size of big data is very large and the computational power required is very high so big data can't be analyzed on your systems with the help of traditional software we need newer software to analyze big data so pandas can help with analysis of Big Data as well the second is pandas can clean messy data sets and make them readable and relevant so already discussed it can help in cleansing or cleaning of data the third one is it has a lot of use in data science as already discussed data science is a branch of computer science where we study how to store use and analyze data for deriving information from it so pandas has a bigger role to play in the field of data science so what is difference between data and information data refers to Raw facts so it's usually in unprocessed form while information has proper context it means that it is in a process form and we can directly extract patterns from information now let's move on to our next topic that is what else can pandas do so what are some additional functionalities offered by pandas so first one is calculation of correlation among different keys or columns or features so what is correlation in statistics correlation or dependence is any statistical relationship whether causal or not between two random variables or bivariate data so in simple word it means that how will one variable will be affected when the other one is changed the second one is calculation of average values from the data set calculation of the minimum and the maximum values of a column in a data set deletion of fruits that are not relevant or continual values so this comes under data cleansing the second and third points come under the data analysis and correlation is a very important aspect because whenever we do feature engineering we have to do correlation analysis to find out which features are correlated and what is the extent of correlation and we neglect the features which have a direct correlation or a perfect correlation because it doesn't make sense as the both the features are giving us the same information so why bother to take multiple features if one feature is sufficient so in today's lecture we covered the basics of pandas and the basic functionalities of this module offered by python hi guys welcome back to our course on pandas in our last lecture I introduce you to the python module pandas and today I'm going to teach you how to get started with pandas that is how to install import and to some basic operations with thunder so let's get started so first of all let's discuss the installation of pandas so installation of pandas if you already have the PIP package as well as the python installed on your system so generally we prefer to do all the installations with either the PIP installer package or the condom so either it's pip install pandas or conda install pandas so if you have Python and pip package already installed then installation of pandas is very easy so you just need to write the command pip install pandas so install it using the command that is pep install pandas so now I'll run it so it says requirement already satisfied because I've already installed but it may take some time in your system because you would be running it in the command line also uh the Anaconda package comes with the preinstalled pandas module so if you're using jupyter notebook and you have the whole Anaconda package then you don't need to install it explicitly it comes already installed with Anaconda package so if this fails in your command line and if you are using some other IDE you can use a distribution like Anaconda spider Etc that comes preinstalled with pandas now let's import the pandas module so we just need to write import so just simply write import pandas so we have imported pandas successfully but most of the type we import our modules as an alias because we can't write a big word every time we are using some functions so we just import with a short term or an alias so import pandas as PD and PD is the Alias here now let's check the version so how to check what version are you using you just simply need to write print PD dot underscore underscore version underscore underscore so it says 1.4.2 I'm using notebooks so I don't need to write print explicitly but if you're using some other IDE then this will not work you have to write print PD dots underscore underscore version underscore underscore as whole if I just write PD dot version it works here but it won't work on any other IDE it works for Notebook environment only now we'll create a data frame so data frame is a very important part of pandas module because this is mostly used in projects so data frame we'll discuss it in later in detail that what is the data frame and how it works how to add rows or columns to data frame what is difference between data frame and a normal table so right now I'm just creating a data frame so let the name be my data set and I'm using a dictionary to create a data frame so there are a lot of methods to create a data frame you can pass less numpy arrays or dictionaries to create data frames so I'm using a dictionary here I have to give values as key value pairs so my keys are cards and values and my values are BMW skoda as one list and one and two as another list now I'll just write print my data set so I'm going to print this dictionary first and then I'm going to create a data frame so my VAR or any data frame name is equal to PD dot data frame please note that data frame is key sensitive so first alphabet is capital my gaps log has some problems so it's PD dot data frame and pass the dictionary so dictionary name is my data set and shift enter to run so it's giving an error because I've not added a comma in the elements so now it will work so this is our dictionary and this is our data frame so you can see the difference so in today's lecture I taught you how to get started with pandas hi guys welcome back to our course on pandas in today's lecture we are going to cover the series data type in pandas so let's get started so the first question is what is a series so we know that pandas is usually preferred for data analysis so we know that Anders is primarily used for analysis of data pandas is a module which is primarily used for analysis of data and it has two major data types the first one being a pandas CDs and the other one being a data free so pandas supports two main data types first one being series data type and the other one being data Frame data type usually we use data frames because a table or in data is in form of rows and columns but series is very important biggest data frame is nothing but a bunch of series clustered together so now our original question is what is a series so we know that pandas has two main data types series and data frame and what is a series so series is nothing but like a column in a table so Panda series is like a column in a table and when you combine a lot of different columns it becomes a table and in this case a data frame so Panda series is like a column in a table and it is a one dimensional array holding data off any type so Panda series is just like a 1D array but now you must be wondering then what is the difference between a 1D array and a panda series we'll get on to that topic first let us see that which data types can be used to create a panda Series so we can convert a python list a numpy array or a dictionary into a series so first of all we'll try to make a series from a python list so first of all I'm going to import pandas as PD and import numpy as NP now I'll not use anything from numpy but maybe in between I may need to use it so I'm importing numpy as well and I have created a python list that is my list let me just create it in a new cell yeah I've created a list and now I need to create a series so I'll write a is equal to PD dot series and SS capital and I need to pass my list that is my list only so data is equal to my list I'll just print and this is the pandas series now let me just create a series from numpy array so first of all I need to create a numpy array and I have already discussed how to create a numpy array in the numpy module so you can refer that module if you don't know how to create a numpy array so now I'll just write the same syntax and I just need to pass the numpy array instead of list I'll print it again so I got the same result now we covered the topics of creating a series from a numpy array as well as a python list so we covered these two topics now let's move on to the topic of creating a series from a dictionary or a python dictionary so you know that in dictionary we have the entries as key value pairs but first we'll try to understand the difference between an array and a series and then we'll proceed to create a series from a dictionary and understand the difference that how it is different from creating a series from a list or a so first let's try to understand the difference between a series and an array so in an array we have numeric indexes so to refer to the first element we'll use the zeros index to refer to the second element we'll use the index 1. and in general to refer to the K plus one element we need to use the kth index so a series doesn't necessarily have numeric indexes it can have access labels meaning that it can be indexed by a label instead of numeric values so instead of 0 1 2 I can use Alpha Beta gamma a b c or anything I like or any word like ABC for 10 x y z for 20 def for 30. so a series can have access labels meaning that it can be indexed by a label instead of numeric value but by default it has a numeric indexing it means that if I don't pass a value for the labels the default label will be a numeric value like 0 1 2 and so on so now we'll create a series using a dictionary so now I'll create a series using a dictionary and we'll see what is different here so first of all I need to create a dictionary so let there be dictionary with the name e and it has element Anna and let's add one more element after we give the values for this key so let Anna has the value 10 and Jasper has the value 20 and Nikita has the value 30 so Anna Jasper and Nikita are the keys while 10 20 and 30 are the values so now create a series so w is equal to PD dot CDs and I'll pass this dictionary and I'll print it so Anna Jasper and Nikita here act as the labels for the series and 20 10 20 30 are the values so in today's lecture we covered the concept of pandas series that will be all for today hi guys welcome back to our course on pandas today we are going to continue our discussion on pandas series so let's get started in our last video we covered how to create a panda series from a list and an array as well as what is the difference between a series and an array so we'll continue our discussion on labels and indexing today so now our task is to take the data and label values as separate lists and create a panda series in our last lecture we use a dictionary for the values as well as the labels because the keys can act as label values and the values can act as the entries so now our task is to take the data and label values as separate list and create a pandas series so first of all we are going to import partners so import pandas as PD and import numpy as NP now we'll take two less one for the data and one for the label so first of all let's take labels as a b comma BC on CD so we have defined the list for the labels now we have to define the list for the data so let's data Z equal to Let It Be numeric so 1 comma 2 comma 3. now we'll just write the series name or Q is equal to PD dot series and will pass two parameters first one being the data so data is equal to data Z and the second one being the index so index is equal to labeled so in last case we just passed a dictionary and automatically the index or the labels and data was assigned but in this case we have to pass data and index so here we can see that it doesn't have a numeric index it has index as the value of labels that we have passed now let's see what if we don't pass the index values so it's not a brainer if we don't pass index values they will be taken as the default numeric values starting from zeros so it will be 0 1 2 and so on now let me just print the capital Q so it's 0 1 and 2. so if you don't pass the index it automatically will take numerical values of numeric values now let's move on let me just give you an assignment now your assignment is to take a list and a numpy array as an input and create a series please do all the assignments it helpful and it will help you learn more now let's move on to choosing an index to access a particular value so we'll use an index to access a value let me just access the value 2 here so I can write capital Q so to access the value 2 in capital Q first so I'll just write capital Q off 1. I'll just run this cell and I got to and to access to from small q I can write q and in bracket I have to specify the label that is BC so I got to as result so in today's lecture we continued our discussion on labels on indexing hi guys welcome back to our course on pandas in today's lecture we are going to cover some operations on pandas Series so let's get started so we are going to cover some basic operations such as sum and difference so operations on CDs first of all let me declare or Define two series so that I can perform operations on them so let series 1 or scr1 is equal to PD dot series and let me just pass a list and labels so I have to pass the labels as a list so PD dot series list and the list of labels as index similarly I have to declare the second series so let the elements p2345 and the index be a little different this time so let it be B c d and E so the indexes are not same or the labels are not same let me just import pandas as PD don't forget to do this sometimes I do forget because I think that it's continuation of previous notebook but I'm making a new notebook every time so I have to declare it or import again now let's just perform the additions so scr1 Plus acr2 so I'll just run this cell and I got the sum it shows not a number for label e and label e because there are no values corresponding to these enables in the other list for example label a is only in series 1 and there is no label a in series 2 so their sum is not a number similarly for the label e also we can use the add method to perform the sum so we can also use the add method to perform the sum or for addition now I'll demonstrate the use of add method so I just have to write scr1 dot add scr 2 in the bracket I'll run this cell will get the same result now we should observe that whether it causes the change in series 1 or not so I'll just print the series 1 scr1 so no change so add method doesn't cause any change in the original series it just performs the addition and returns a new series so no change in series one now let's see what's the difference between addition using add method and the normal add operator so in this case we can also pass the default fill value so we observed that there was no nothing corresponding to label a in series 2 but we can give a default value in this case so the label a will be filled by 0 in series 2. so someone plus 0 will give us 1 and not not a number so this is the difference we can give the default field value if there is no corresponding label in the other series similarly we can perform the subtraction using the subtract operator the minus operator as well as using the sub method so this is your assignment that you have to perform subtraction using the minus operator and Dot sub method and demonstrate the code and the output as well from today's lecture we covered some basic operations related to pandas series hi guys welcome back to our course on pandas in our last lecture we concluded our discussion on series and today we are going to start data frames so let's get started so our first question is what are data frames so data frames are the workhouse of pandas so they are directly inspired by our programming language and then nothing but a bunch of series objects put together so series is one dimension we just cluster together a bunch of series and it becomes a data offering so data frames are the workhouse of pandas that directly inspired by the r programming language and we can think of data frame as a bunch of series objects put together because series is one dimension and when you combine a group or a bunch of Series so it becomes a tabular structure that is a data frame now let's see how to create a data frame so first of all we have to import numpy and pandas by Alias so import numpy as NP unimport Partners as PD now we'll run the cell also in this case I am going to use the rant n method of numpy to create a 2d array which I'll will pass to data frames so I I'll import that also from numpy dot random input run in I'll create a data frame with the name DF so DF is equal to PD dot data frame and in this case we have to pass a 2d array so instead of passing a hole to the array I'll just write run in 5 comma 4 because this will return me a 2d array and I have to pass a list of index as well as a list of columns because a data frame has indexes as well as column values or keys so I'll just write a b c d e dot split because this will return me a list I can either write split and I can pass comma or I can just give normal spaces so a b c d e and by default the split is along the blank or the white space similarly for the columns I'll pass w x y z and Dot split because this will also return me a list of four elements and 4 into 5 is 20 which is the dimension of the data so this is the resultant data frame now I'll try to access a row directly to access a row or an index in a series I can write DF and in bracket pass the index or the label so DFA this will work when I use it in a series but will it work in a data free let's check so DF of a where a is the label this is giving me an error so this helps us to deduce the fact that we can't directly access a row using a label in case of a data frame so this gives us an error we'll discuss the use of dot Loc and Dot iloc later for selecting row offer data free so we'll discuss these topics in upcoming videos but there is one more Point here we can't access the rows directly but we can select the columns directly so DF of eggs X where X is the column label will give me the column values so I'll just write DF of X where X is the column label and it returns me a series because data frame is nothing but a bunch of series put together now let's select multiple columns to multiple columns we have to pass a list of columns so you need to pass a list of columns so X comma y or Y comma Z or anything so there's an error in syntax let me just Rectify it and then I'll run so this is the correct syntax and this is the result please note that you have to pass a list of column labels and not just labels separated by commas now let's prove that data frame is just a bunch of series objects so to prove that data frame is just a bunch of series you can use the type method and select a column so if you can prove that every column of a data frame is the CDs then it's proven that data frame is just a bunch of series objects combined so I'll just write type and in bracket I'll pass DF and I'll pass DS W column so it gives me pandas.co.series.series so it's a series object so in today's lecture I introduce you to the data Frame data type of pandas hi guys welcome back to our course on pandas today we'll continue our discussion on data frames and I'll teach you how to add a column to a data frame or how to remove a column from a data frame so let's get started so our topic is adding and removing columns adding and removing columns to Slash from a data frame so first of all we are going to create a data frame so our first step is to create a data frame so I'll first do the necessary Imports so import pandas as PD import numpy as NP and from numpy dot random I need to do one more import before I can create a data frame let me just write the instruction for the data frame first then I'll add that command so DF is equal to PD dot data frame Rand in 4 comma 4 will create a matrix of Dimension 4 cross 4 and we'll pass it to the data frame along with we have to pass the index so we'll pass ABCD dot split it will return a list containing four values and similarly we'll pass the columns w x y z and also We'll add dot split because it will also return a list containing four values now I'll run it but I have to do the necessary import first so from numpy Dot random import Rand and because run n requires this import or you can use numpy dot random dot Rand and instead of only Rand in so it will also work so now it's giving an edit because so word columns is not spelled correctly so let me just change it yeah now I have created a data frame successfully now just print it so this is our 4 cross 4 data frame now I'll try to add a new column to the data frame with the name ZN so DF of c n or DF bracket square bracket and the column name or the name of the new column is equal to then I have to pass the values because every column has four values I have to pass four values now let's display the modified data frame so here we can see that a new column has been added successfully initially we had W X Y Z now we have w x y z and z in so we we have added a new column successfully now let's remove the column so generally we use a method known as drop to remove a column so I'll write TF dot drop and I'll pass the column name and also the axis because whenever we deal with columns we are dealing with axis equal to one if we don't pass this axis equal to 1 it will give you an error so this is the modified data frame when ZN has been removed but it will not make changes in the original data frame it just returns a new data frame so is it actually deleted no it's not when you display DF you still have Z in so it's false in place by default what is in place let me illustrate in place true or in place false while performing an operation means that do you want to make changes to the original data frame or not if in place is false then no changes are made to the initial data frame and when you where you want to make changes you have to pass in place is equal to true now when I'll make this change it will remove it permanently it's giving an error because the name is the n not CE yeah now it has been removed permanently so when you specify in place is equal to true it makes changes to the initial data frame only so in today's lecture we covered how to create new columns and how to delete columns from a data frame hi guys welcome back to our course on pandas in today's lecture we are going to continue our discussion on data frames and I'll teach you how to select rows of a data frame so let's get started so today's topic is selection of rows offer data frame so this is the topic first let me just import so import pandas as PD import numpy as NP and from numpy dot random so from numpy dot random import run n we are using this to create an array a 2d array you can directly pass a 2d array but that will just make the code a little bit complicated not complicated but it won't look very clean so that's why we are creating a 2d array with help of Rand in method so let me just create a data frame and I have to pass index value as well as the column values so let the index be e and B and the column B C and D so I have to pass these as lists and I have a 2 cross 2 Matrix that will create a two cross two data frame with the index a b and column values C and D so I have created a data frame so this is our result in data frame now let's just try to select a column first so we have selected the column C so we can conclude that we can select a column directly from a data frame now let's try to select a row let it be a so d f of e so this is giving us an error so we can conclude that we'll get an error while trying to select a row directly so error while trying to select a row directly also all the necessary notebook links will be given this time so you can just refer to the notebooks if you are not able to implement something I'll share a GitHub link so selecting of rows is therefore different from selection of columns or from selecting columns so selection of rows and selection of columns are done by different methods and selection of rows can be done using two methods first one is dot loc method and in this we have to pass the label so dot loc and we have to pass the label or label is passed and the other method is iloc method and in this case index is passed so there are two methods for selection of rows first method is dot loc and the other method is dot iloc in the first one label is passed and then the second one index is passed let me just write df.loc a so here we can directly pass the label and we'll get the row and in iloc method we have to pass the index so the index of e is 0 so it gives us the first row similarly we can select the second row using locb or using iloc one now let's see how to select a single cell because we have selected rows we have selected columns but now we want to select a single cell so I can write DF Dot Loc and I have to pass e comma fee so it selects the first row and the first column or the first cell now let's select the value 2.424 so what is the code that you're going to write to select this particular cell so it is the second row and the second column or the column D and the row P so d f b comma d but we have to write d f Dot Loc because we are passing the labels so we have to write Loc or we can write DF Dot iloc one comma 1. so it is first index column wise as well as row wise because 0 and 1 and 0 and 1 so first now let's select subset of a data frame so in this case we are going to select the rows A and B but only one of the columns so I'll write df.loc and I'll pass both the rows and only one column oh it gives me an error because the mixing is not allowed you can't mix if you are using dot loc then you have to use all the labels only you can't use indexes now let's try using this this also gives us an error because if you are passing a single element you don't need to pass it as a list also it won't work if you pass two of the elements as less in both the cases so this is the wrong syntax also if you do mixing of any kind that is also wrong so this also gives an error now let's see what happens if I pass both the row values instead of arrow and a column value so there was an error but now I passed a row value and a column value so it's correct also now I'm going to pass a list as the row value and only one column value so this is the result so that's the correct method also note that mixing of labels and indexes is not allowed or not supported so in today's lecture we covered selection of rows of a data free hi guys welcome back to our course on pandas in today's lecture we are going to continue and hopefully conclude our discussion on data frames and today we'll discuss conditional selection in data frames so let's get started so the topic for today is conditional selection so conditional selection is an important feature of pandas in which we select cells or sub data frames or subsets of data frames or rows or columns using bracket notations in a way which is very similar to numpy so the use of bracket notation is very important also we know that a conditional operator always returns a true or a false value because every condition that we are checking is either true or false so it makes sense that a conditional operator or any conditional operator returns a true or a false value now let's demonstrate these Concepts using code first of all let me just import all the necessary modules or functions so from num by dot random import ran in we are using rant n for creating a 2d array and then passing it to create a data frame so let's first create a data frame and then I'll demonstrate the concept of conditional selection so we are writing the statement to create a data frame Let It Be 3 cross 3 data frame so I have to write Rand in 3 cross 3 or 3 comma 3 because we don't write cross here we write 3 comma 3 and I am going to pass the index values as let it be a comma B comma C so the index values are a comma B comma C and we have to pass this as a list or I can also write ABC with space dot split because that also Returns the list or I can write directly and let the column values p d e and f now I've created a data frame so this is our data frame as visible on the screen now what if we want to select under a given condition so DF greater than 1. and let's execute the statement so it gives us true and false values so all the values which are greater than 1 corresponding to those we get true otherwise we get false now what if we want the values which are greater than 1 and not just true or false we actually want to print the values so we'll write DS and in square bracket not in this round bracket in square bracket we'll write d f and d s greater than 1. so it will print the value which is greater than 1 so that is 1.24 and so on and it will return not a number for all the other entries now what if we want to check a particular column only so we'll write DF and we'll write in square bracket DF of d greater than 0. so all the rows in which the value of column D is greater than 0 will be displayed that's all the rows so we don't see any difference now let's change it to 1. so now only first row is displayed because only in first row the value of column D is greater than 1 that is 1.24 now what if we want a particular cell or I should not frame it like this I should frame it like to select particular rows and particular columns so it can be a cell or it can be a subset of the data frame so we'll write DF DF of D greater than 1 and a so it is giving me an error because I have to pass a column value so in this case we got the result so it is streamed like this what if we want to a particular column so we are selecting a particular column and a particular row or a set of columns and rows so it's either a cell or a sub data frame so there's a little ambiguity here so please take care of that so in today's lecture we covered conditional selection in data frames so we conclude the topic of data frames hi guys welcome back to our course on numpy in our last lecture we concluded our discussion on data frames and today's lecture we are going to see how to read and view data using the functions offered by the pandas module of python so let's get started so today's topic is reading and viewing data from CSV and Json files so these are two important or the two major file formats in which data is available so we have two data sets here first is the iris data set and the Json version will download the two versions of the iris data set the first is the Json version and the second is the CSV version so let me make sure that it isn't the same directory as our notebook is going to be otherwise it will give us an error when we'll try to read data from these files so it is ids.csv and Iris dot Json this data set is publicly available on both the versions can be downloaded from Kiki now let's move back to our notebook that is reading and viewing data from CSV and Json files so first we are going to import pandas as PD and now we'll create a data frame so DF is equal to PD dot read CSV please note that read CSV is the function which is used to read CSV files and extract data frames from the CSP files now we'll just print the data frame so it gives us the first five entries and the last five entries let me just write DF and execute it so it's in a better format so it contains 150 rows and six columns but what if we want to display all the rules like all 150 rows I want to display then I'll convert it to string using the two string method so I'll write DF dot to underscore string function so it will give me the whole data set so this is the data set and all the 153 entries have been displayed now let me just print the first five entries of the data set so I'll write DF dot head so head method here displays the first five entries similarly the tail method displays the last five entries by default and this can be changed by passing the parameters so it displays the last five entries of the data frame now we can have some basic info by using the info method so it tells us about The Columns of the data set and the normal count in the column so all the 150 entries in the data set are not null when we check if we call them so this data set is probably clean also note that the names of all the columns can be extracted using the keys method so we'll write DF dot keys and we'll execute so we got all the column names as a list now as already discussed we can pass arguments To The Head and the tail function we want to change the number of entries so here if I want to display the first 15 entries I have to pass 15 in the head similarly if I want to display the last 15 entries I have to write DF dot tail and in bracket as to pass 15. so I have displayed the first 15 as well as the last 15 entries now let's see how to read Json files so for that we have a function called as read Json so I'll write PD dot read Json so I have to write pd.3 Json and I have to pass the filing so it is iris.json so Iris dot Chi Zone so this is the theme data set just to format is different also note that Json files are generally used to deal with big data all the other functions will remain same like the head tail info everything everything will remain just the same just note that the format is different so the function to read will be different but the data set will be dealt with the same functions so in today's lecture we covered how to read and view data from csb and Json hi guys welcome back to our course on pandas in today's lecture we are going to do correlation analysis using the function offered by pandas module of python so let's get started so as already discussed and the introductory lecture in statistics correlation or dependence is any statistical relationship whether causal or not between two random variables or pivariate data so in simpler words it tells us how to different columns or different features or different keys in a data frame or a data set are related to each other so correlation in simpler words is used to find relationship among the features or among the columns or among the keys so this is more of a formal definition correlation or dependence is any statistical relationship whether causal or not between two random variables or by varied data we can see that correlation is used to find relationship among the features now a great aspect of the pandas module is the curve method or the correlation method in short we say core or core or anything it doesn't matter until you write the correct thing when you are coding so a great aspect of pandas module is the core method so it can calculate the correlation between each column of the data set so the core method calculates the relationship between each column of the data set now I'll demonstrate the use of core method so first of all let me just do the necessary import so import pandas as PD now I'm going to extract a data frame from a CSV file so PD dot read CSV and let the file name be Iris dot CSV this is the same file that we discussed in our previous lecture now I just need to write DF Dot code and it will return me a new table or we can say a data frame with the correlation values you can read more about the correlation analysis in statistics but let me just tell you in brief how to read this table so the core method ignores the column which are not numeric so it had six columns I think one was about the species so it was not included in this table because the core method ignores the column which are not numeric and the result of core method is a table consisting of numeric values also if You observe carefully you'll notice that all the values in this table vary between 1 and plus 1. so the negative or the minus values denote a negative correlation so if one variable increases the other one will decrease so this is the meaning of the negative correlation and the positive values are plus denotes a positive correlation it means that if one variable will increase the other will increase as well now our next question is what is a good correlation so when we have a numeric value of 0.6 or minus 0.6 that denotes a good correlation either positive or negative like positive 0.6 will denote that it has a good positive correlation and a negative value will indicate that it has a good negative correlation another important thing to note is the presence of a perfect correlation so if you are having a correlation value of plus 1 it denotes a perfect correlation and note that every feature is perfectly correlated to itself for example petal length is perfectly correlated to Petal length it's not important that to have a perfect correlation we observe the correlational same features we can have perfect correlation in different features as well but if the features are same then they must be perfectly correlated now let's see the correlation in form of a heat map we'll discuss more about heat maps and other plots in the map plotlet and c bond module I'm going to give you a brief intro only so we are going to observe this correlation and form of a heat map for that I need to import c bond as SNS and mat.lib Dot pipelot as PLT also note that if you don't have these modules installed in your system you can just skip because I'm going to teach this in future playlists as well I'll write SNS Dot heat map and I'll pass the table or the data frame that is DF Dot core so I'll write df.com so it gives me a heat map note that light value means a positive correlation and darker color means a negative correlation let me just print the correlation values as well and change this color combination to much of something cool warm and a not is equal to true now I have the correlation values as well as a different color for better visibility so today's lecture was all about correlation analysis using pandas also we'll discuss more about plotting in future playlists hi guys welcome back to your course on pandas in today's lecture I'm going to introduce you to the concept of data cleaning in pandas it is one of the most important stages in data preprocessing so let's get started so today's topic is data cleansing or more popularly known as data keyning so data cleaning means fixing bad data in our data set and bad data could be of many types so we'll discuss what can and cannot be considered as bad data so you have a clearer picture in your mind that what comprises of bad data so bad data could be data in wrong format empty cells let me just make a new cell for all this just wait a second so bad data could be data in wrong format empty cells wrong data duplicates that you have the same entries many times so all this comprises of bad data and we need to clean or we need to remove this kind of data from the data set or data frame so first of all we'll discuss how to deal with empty cells so first of all let me just import pandas as PD so I'm going to import pandas as PD and I'll read a data frame so DF is equal to read CSV and I'll pass Iris dot CSV now let me just sorry it's PD dot read so let me just display the first five entries of the data set so you can see that data is already clean so I need to add some other columns so that it is not clean anymore and then I'll demonstrate how can we actually fix the data set so like it would be DF of additional data is equal to not a number or this gives me an error because any n can't directly be used with python so there's a number function for it we'll discuss it later for now let it be none so I'll change it to none just give me one second yeah so let me just change it to none so it's still the same data set but we have added an additional column that is none so all the entries are none so you can observe that these columns makes absolutely no sense if it were present or not so this is kind of an example of bad cells now this column contains only none values so the right choice would be to remove this entire column which has none values now there's also method to remove the rules that contain not a number of values or null values so there's a little ambiguity here this difference between none and not a number or any n here so new DS is equal to DF dot drop any this will remove all the rows containing none values so in the new data frame is empty because every true contains a none or any n value so it will remove all the rows because of that column so this method is not a perfect choice here we should remove the column itself other than removing all the rows containing none or in in values so if I display the head of the new data frame it is also empty so hi guys welcome back to our course on pandas in our last lecture we started the topic data cleaning and I demonstrated how to remove a column and how to remove rows with none or not a number values today we'll see how to replace the null values with the mean or median and other simple methods of data keyning so let's get started so this is continuation of the previous video I'm just gonna make this data a little more dirty by adding an additional column which has empty values now I'm gonna replace the empty values by a number so this is one of the methods of data cleaning so replace the empty values by a number so we'll write DF dot fill any and we'll just provide that number by which we want to fill the none values this is to be used if all the rows have a value in common like they have one two three in common or we are adding a city for a particular set of people who live in same city so all people will have the city value as same so this is to be used only when all the rows share a same value now I'm going to add two more columns in which the values are none so that I can demonstrate a few more concepts of data cleaning now you must have observed that we failed additional data column with value one two three but when I displayed the data set again the changes were not committed so to commit the changes we always need to specify in place as true so I'll write fill any or fill the none or the null values or the NN values in the column A add 1 and in base is true so that when I display the data set again the changes have been committed now I'll just drop one of the columns because I have a lot of columns with none value that I may not require so one of the simple method as Illustrated before is to just drop the column so this is a very long and documented error and there is an error because I have forgot to mention the access always remember that when you are dropping a column you have to pass the axis equal to one and then you have to execute it so now I'll just run this cell again so I'll run the cell again and it works perfectly now so I'll display the data set again so the empty column add with no values has been removed now I'll teach you how to replace the rows which have none values with the mean of other values in that column because the rows have null values some of the rows might have null or none or any in values for a column in our case the column is absolutely empty so let me just fill some of the values in a column and then I'll tell you how to fill all the other values in the column using mean median Etc so let me just change the column values for some of the rows so you can see that I have replaced one value of this column additional data with three and I have also accidentally change the value of another column to 4 but it won't matter let me just correct it and select the right column yeah now I have replaced two of the values by three and four now I'll replace all the other none or null or not a number of values in this column with the mean of these two values so X is equal to DF additional mean or additional data dot mean so X is the mean head and I'll write d f and the column name is additional data and I'll write fill dot fill n a and I'll pass X which is the mean in place is equal to true now I'll display the data frames so all the other values have been replaced with mean of these values now I've left with just one column which has all the none values let me just drop it simply so I'll write DF dot drop and pass the column name X is equal to 1 and in place is equal to true now the data is key in again also I'll demonstrate some of other ways to handle null data in a specific lecture which will be dedicated to handling of null values so in today's lecture we conclude our discussion on data cleaning hi guys welcome back to our course on pandas in today's lecture I'm going to teach you how to visualize data using inbuilt functions of pandas so let's get started so I am going to cover the use of plot method to drop plots using pandas so the title is pandas for plotting and an ID only I'm not going to discuss it in detail I'm just giving you an idea so pandas uses the plot method to create plots also to visualize the plots we can use Pi plot which a sub module of the matplot library so I'm using notebook so I don't need to use this Pi plot or I don't need to import the pipelot but if you're using some other IDE it is vital it's important it's absolutely compulsory otherwise you won't be able to visualize the plots so first of all I am going to import pandas as PD and I will also import matplotlib Dot pipelot as pld so our first job now is to read the CSV file and extract the data frame or the data set from the CSV file I'll use the same Iris data set so I'll write PD dot Tweed CSV and it will pass the name iris.csv now I'll display some entries so this is our data set or our data frame now let me just draw a scatter plot we'll discuss two plots first is a scatter plot and the other one is the histogram so DF Dot Plot kind is equal to scatter and I have to pass the X and Y values because in a scatter plot we plot points and we have to pass the x coordinate as well as the y coordinate so let the x coordinate be petal length in centimeter and the y coordinate B length in centimeter so scatter plot can also be used for correlation analysis so we can see that how petal length and supple length are correlated using a scatter plot so it is also important so see as petal length increases after three the simple length also increases there is some correlation although it is not a very good correlation because for the values from 1 and 2 the plot is kind of messy but after three it shows a positive correlation although it doesn't seem a very good correlation we are using notebooks so we don't need to write PLT dot show but it will be required if you use some other IDE for executing this code now let's move on to our next plot so let's draw a histogram now so histogram is kind of a frequency plot so we have a variable and its frequency so let the variable be simple length in centimeters and the yaxis will give the frequency of that particular sepuland that how many flowers have that particular length of sepals so I'll write simple length centimeters and I'll write Dot Plot kind is equal to hist or histogram so this is our histogram so in today's lecture we covered how to draw plots using pandas and the data visualization abilities of pandas that will be all for today this video is brought to you by programming knowledge please like comment share subscribe and hit the Bell button for updates and stay tuned with us for next lecture thank you hi guys welcome back to our course on pandas in today's lecture I am going to discuss the merging of data frames in pandas so let's get started so the merge function is used for merging of data sets and pandas or we can see that the merge function facilitates the merging of data sets so there are three terms here merging joining and concatenating so we'll also try to understand the difference between these three so the first is merging and it is done by the merge function so the function merge allows us to merge data frames together using similar logic as merging SQL tables together so this merging is same as joins in SQL if you can recall the inner join outer join left join right join that is same as this merging so the merge function allows us to merge data frames together using a similar logic as merging the SQL tables together if you haven't studied joining an SQL I request you to find some good video on SQL joins and go through it now I'll create a data frame in this case I'm going to create two data frames left and right then I am going to demonstrate inner join or the inner merge outer merge same as SQL joins inner outer left and right let me just create two data frames real quick this gave me an error because I forgot to import pandas as PD so this is our left data frame or the data frame one in a similar manner let's create the second data frame we just change some values in the original data frame and that's it so create two data frames first and then try to perform different time soft merging and merging is similar as SQL joint so there are generally four kinds of merging and also note that for merging we require keys that is very important so this is our right data set and we have one more data set that is left these are the two data sets now I'll write PD dot merge and I'll pass both the data sets that is left and right final pass on equal to key one comma key to submerging is performed on keys that is very important so this is the inner merging or inner SQL join so please take care of this that merging is same as SQL joins and is performed on keys so this is the inner join if we want to perform the outer join then we have to specify how is equal to Outer and we have to also specify on q1 comma key2 so that was in a join and this is outer join similarly to perform right and left join we just need to specify how is equal to left or right so in a similar manner we can perform left and right join also for more information please refer SQL joints so in today's lecture we covered merging of data sets in pandas that will be all for today this video is brought to you by programming knowledge please like comment share subscribe and hit the Bell button for updates and stay tuned with us for next lecture thank you hi guys welcome back to our course on pandas today I'm going to discuss the joining of data sets or data frames with the help of functions offered by pandas so let's get started so the topic for today is joining so joining is a convenient method for combining The Columns of two potentially differently index data frames into a single result data frame so we have two different data frames and we are combining the columns of two data frames which have different indexes and we get one data frame as a result so this is the official definition now let's try to understand the difference between joining and merging so joining is performed on indexes and merging is performed on keys so this is one and only major difference between joining and merging of data sets or data frames and panels we are concerned with indexes so now I'll demonstrate how joining works but let me just complete this statement so joining is same as merging but performed on indexes instead of keys K1 and K2 so the keys of the dictionary are a and b and a contains the value as well as 8 0 a 1 and E2 and the key B contain the value in a list b0 P1 and B2 and the index contains so in a similar manner will create a second data frame whose name is right and will make some changes in the columns as well as the index also I forgot to import pandas as PD so I got an edit but I've corrected it so this is our left data frame and in a similar manner I'll create the second data frame so let me just copy and paste whole thing and I'll change the name and I also change the key values as well so it's c c not C1 and C2 or C not C2 C3 or whatever you want to have it doesn't actually matter so the keys in this case are C and D containing lists as c0 C2 C3 and d0 D2 D3 and the index values now pay attention that joining is done on indexes rather than keys so now I'll write left dot join and I'll pass right so this is the inner join similarly we can exhibit the outer join I'll just need to write left dot join and I'll pass right and I'll pass how is equal to outer so this is the result of the outer join in a similar manner we can perform left and right jaw so this is the result of left join and the result of right join so in today's lecture we discuss joining of data frame in pandas that's all about joining that will be all for today this video is brought to you by programming knowledge please like comment share subscribe and hit the Bell button for updates and stay tuned with us for next lecture thank you hi guys welcome back to our course on pandas today I'm going to cover the concatenation of data frames in pandas so let's get started so the topic for today is concatenation so concatenation basically means gluing together data frames also note that Dimensions should match along the axis we are concatenating on so it is different from joining what we are basically doing is we are just gluing two data frames together and please take care of Dimensions while concatenating so let me just complete this sentence Dimensions should match along the axis we are concatenating on also to perform concatenation we have a special method so the use of concat method is done fourth concatenation so he used PD dot concat method and pass the list of data frames you want to concatenate so let me just create a data frame real quick and I also need to do the necessary import that is pandas but let me just first Define the First Data frame so let me move on uh so by mistake I went back now have to do it all over again so let me just do it again quickly so I have to import pandas as PD and I'll run this cell again and the cell again so now I'll just display the First Data frame and in a similar manner I'll create the second data frame so df2 this time and the columns will remain the same The Columns will be the same a b c and d but the indexes will vary so the index is should be in continuation here when I'll am concatenate along the axis 0. the only thing that we need to keep in mind while concatenation is the indexes and the values if you don't have access to values or indexes so the result of concatenation can be a little messy when you concatenate along the higher axis but we are concatenating on axis 0 right now so for the First Data frame the Nexus are 0 1 2 and 3 and for the second data frame I'll specify the indexes as four five six and seven so I'll make the necessary changes let me make some changes in the data as well the data can be redundant also but to make it look better I'm just making some changes in data as well but we can have redundant data we just need to change the indexes so let the indexes be 4 5 6 and 7. now I'll just run this cell and I'll display the second data frame that is in continuation of the First Data frame now write PD Dot concat and I'll pass both the data frames so this gave me an error because always remember that you have to pass the data frames in form of a list so please don't forget indentation or the documentation because different methods or functions take different kind of references or arguments so make sure to pass a list of data free now let's concatenate along the axis 1. so in this case we have to specify access is equal to one rest of the procedure is same so I'll write PD Dot concat and I'll pass the list of data frames as well as I'll specify the axis is equal to 1. so we have concatenated along the columns or along the axis 1. so we have concatenated along the columns also note that we got a lot of null or nnn values so make sure you have all the necessary values for concatenation if you are doing concatenation along the axis 1. so make sure you have all the values for concatenation otherwise it will give n a n or null in the results so in today's lecture we covered the concatenation of data frames and pandas that will be all for today this video is brought to you by programming knowledge please like comment share subscribe and hit the Bell button for updates and stay tuned with us for next lecture thank you hi guys welcome back to our course on pandas in today's lecture I'm going to teach you how to combine multiple conditions using selection and how to extract information about unique elements from a data frame so let's get started so today's topic is selection and information on unique elements first of all our job is to create a data frame so create a data frame first so I'm going to import pandas as PD and I'll create a data frame so DF is equal to PD dot data frame so in this case I am not going to use the Run n method I'm just simply passing the column wise values in form of a dictionary so in a similar manner you have to create a data frame with three columns so this is our third column and executed so we have created a data frame and this is the resultant data frame now under selection so we select from data frame using criteria from multiple columns so we'll combine multiple conditions using operator such as and and or so we can select from a data frame using right area from multiple columns so basically here we are combining multiple conditions using and operator so I'll write new data frame is equal to data frame and in bracket I'll specify the conditions 1 so DF column 1 greater than 2 and other condition so I'll write add DF column 2 equal to equal to triple four or four hundred forty four I'll execute this I'll display so it is giving me an empty data frame yeah this is because our initially data frame should have three columns so there's something wrong in our Declaration of a data frame so let's check because it should have three column here we have two column oh I have not given the correct name to the third column Let It Be column three now let's execute execute this again now we have three columns now this condition will work so we have a row here so this row had column 2 equal to equal to 444 and the column 1 was greater than 2. so this is similar to the conditional selection we discussed earlier only difference is we are combining several conditions now we'll see how to get information on unique values column so first we'll try to identify all the unique elements of a column and obtain them in form of a list so for that purpose we have to use the unique method so I'll write DF and the target column Dot unique so column 2 has three unique values 444 555 and 666. in a similar manner we can calculate the count of unique elements of a column so for that I have to write DF Target column Dot n unique and I'll execute it so it has three unique elements first one gives us the unique element and the second one gives us the count of the unique elements or the total number of unique ellipse now our next objective is to calculate how many times the unique values appear in a column so what is frequency of each unique value so for that we have a method called as value counts so here triple four occurred two times triple five occurred one time and triple six awkward only one time so in today's lecture we covered unique elements and conditional selection that will be all for today this video is brought to you by programming knowledge please like comment share subscribe and hit the Bell button for updates and stay tuned with us for next lecture thank you hi guys welcome back to your course on pandas in today's lecture I'm going to demonstrate the use of apply method of pandas so let's get started so the apply method is used for applying functions on columns so we can select a particular column apply a function on all the values of that column and store the result somewhere else like in a new column or in a new list somewhere so apply method is used to broadcast a function to the columns that is every value of that particular column will be treated with the function that is passed in the apply method so first of all let us read a data frame so let's feed a data frame first then I am going to import pandas as PD so now I'll write DF is equal to PD Dot read CSV and I'll pass the file name that is Iris dot CSV I'll display some of the rows of the data frame so this is basically our data frame now let's create a new column that stores the length of species name so the last column is the species and we have to count the number of characters in the species name for every row so create a new column that stores the length of species name so the procedure is very simple we have to create a new column so DF of new column name or anything let it be Lin species or dense species name is equal to DF of species because we are applying this function on the column species and we are applying the function that is length so this needs not to be in commas or removing so dot apply length now I'll display the data frame and it has a new column that is length of species name and you can count it manually also it's one two three four five six seven eight nine ten eleven and for the last true it is 14. now let's create a function that takes a number and Returns the number multiplied by 2. so create a function that takes a number and Returns the number multiplied by 2 and our second job is that using apply method so using apply method create a new column Dot contains the doubled cepulant so we have to create a function that takes a number and returns a number multiplied by two and we have to apply it to the column that is supplement centimeter so I'm creating a function first so this is the function Let its name be times 2 and it returns number multiplied by two now d s of sample length 2 art supplement doubled is equal to DF supplement centimeter dot apply times 2. I'll execute it and now we have a new version of the data frame containing an additional column that contains the double length so apply method can be used to broadcast inbuilt or user defined functions so it can be used to broadcast in build functions as well and user defined functions as well also we can use a combination of apply method as well as the Lambda function so today's lecture was all about the application of apply method in pandas that will be all for today this video is brought to you by programming knowledge please like comment share subscribe and hit the Bell button for updates and stay tuned with us for next lecture thank you hi guys welcome back to our course on pandas in today's lecture we'll discuss some methods functions or operations of pandas to get information about data frames so let's get started short topic is basic operations or functions so we'll discuss some methods or functions of pandas to get information about the data frames and these functions are very useful during data analysis so use of methods last functions of pandas to get info about data frames so I'll import pandas as PD then I'll read a data frame from the iris file as usual so DF is equal to PD dot read CSV and I'll pass Iris dot CSV and this is our data frame now I'll just get some basic info about this data frame using DF dot info so this is the application of info method or function so it gives us the information about the column names the number of null values and their count to this we have already discussed in a previous lecture now let's move on so how to get the names of all the keys of a data frame for that we can write DF dot keys and it will display a list of all the keys moving on to the description we can write DF dot describe so it will tell us the count the mean the standard deviation the minimum and the maximum value and the quartile distribution of each and every column provided that it is a numeric column like for the species it's showing nothing so that is missing here so DF dot describe can give us the description of each and every column another method to fetch all the keys is using DF dot columns so it also returns us a list of all the column names it is similar to DF dot keys so this is cmf DF dot Keys function now we also have a function to fetch all the indexes so it will not return a list of indexes but rather it Returns the start value the stop value and the step size in today's lecture we covered some functions to get information on data frames that will be all for today this video is brought to you by programming knowledge please like comment share subscribe and hit the Bell button for updates and stay tuned with us for next lecture thank you hi guys welcome back to our course on pandas in today's lecture we are going to cover the topics of sorting and ordering using pandas so let's get started so topic for today is sorting and ordering so sorting and ordering first I'm going to import pandas as PD so import pandas as PD our next job is to read a CSP file on extract the data set from the file so read a CSV file and extract a data frame for that I am going to write DF or the variable or anything is equal to PD dot read CSV and the name of the file is Iris dot CSV so this is our data frame so let's sort in ascending order by simple length so sorting in ascending order by sepulence so here we have sorting as well as ordering so order is ascending on the variable is simple length so now I'm going to write d f Dot sort underscore values function and it takes a parameter that is by in which we have to specify the column name so we want to sort by supplement so I'll pass supplements centimeter and it will return me the sorted data frame also note that it does not cause any change in the original data frame see there's no change in the original data frame so we can say that by default sorting is not in place so in place is false by default if we want permanent changes we have to specify the in place as true now let's sort by Petland so the syntax will be same DF dot sort values Pi is equal to Petal and centimeter so this is quite easy to do so let's sort by petal and now and write DF dot sort values and by is equal to Petal length centimeters and I'll run it so this also returns a data frame but does not cause any change in the original data frame until and unless we specified in place as true so that's all about sorting and ordering the values or the entries in a data frame that will be all for today this video is brought to you by programming knowledge please like comment share subscribe and hit the Bell button for updates and stay tuned with us for next lecture thank you hi guys welcome back to our course on pandas in today's lecture I am going to demonstrate the use of tail keyword for deletion of columns from a data frame so let's get started let me just create a new notebook first and rename it so let me just change the name to Dell keyword so the topic for today is Dell keyword and it's used for dropping The Columns of a data frame so Dell keyword can be used for removing a column earlier I demonstrated the use of drop method for removal of a column now I'll teach you how to remove a column using the Dell keyword so let me just import pandas as PD and let me just read a CSV file and extract the data frame from that particular file so this is our data frame let me just create a new column real quick so let's add a column so DF of blank blank is equal to none or anything so it's giving an error because it can't be directly used we have to write numpy Dot N A N I think so because it is in numpy and not in Python NN is not a data type supported by python so let it be blank or white space or anything and let me just display the data frames now I have a column with the blank values or empty values now let me just conventionally remove this column so conventionally we used the drop method so I'll write DF dot drop and I'll pass the column name axis is equal to one so this is the conventional method and it does not cause any change in the original data set unless you specify in place as true so no change in the original data frame until and unless we specify in place true so it won't commit any changes until you specify in place as true so pass DFT drop the column name axis is equal to 1 and in place is equal to true so now it has been removed now let's see how to use the Dell keyword to achieve the same result so let me just add the column once again so DF of Planck is equal to none this time and display the data set so this is the extra column with none values now I'll just simply write Del DS and the column name in square bracket and it has been deleted from the original data set as well we don't need to specify in place true or something like that so deleted successfully without passing additional parameters like access or specifying the in place as true so in today's lecture we covered the use of tail keyword for removing columns from a data frame that will be all for today this video is brought to you by programming knowledge please like comment share subscribe and hit the Bell button for updates and stay tuned with us for next lecture thank you hi guys welcome back to our course on pandas in today's lecture we'll learn how to deal with not a number or nonvalues in a data set we have already discussed some of it in the data cleaning portion of this playlist but we'll cover this topic in a finer detail today so let's get started so our topic is dealing with Nan values in a data set so that's the topic for today so first job is always to create or import a data set will create a data set instead of importing it let the data frame name be df1 in this case and we'll pass a dictionary here to create a data frame so let me just do it quickly so it has two columns that is a and b elements of column A are gonna be a 0 a 1 and A2 while the elements of column B are gonna be b0 B1 and B2 so B 0 comma B1 and B2 and our index gonna be 0 1 and 2. so I have to pass the index the index is equal to list 0 comma 1 comma 2. now I'm gonna run this thing there's an error I forgot to import let's import import panels as PD as PD and df1 is equal to PD dot data frame so this is our first data frame now similarly create a second data frame just change some of the values so second data frame can be created in a similar manner by changing a few values what we are actually gonna do is we are waiting two data frames and we are gonna concatenate it along the axis one so that we have some of the values as nen that's how we are gonna achieve some alien values we can also directly insert some of the Nan values using numpy dot Nan but here to make it a little more complex I'm using concatenation so I've concatenated both the data sets or the data frames along the axis one so now I have one two three four five six six into two twelve twelve and in values now our first job is to check for null values or any n values in this case so checking for null values so we can check for null values by is null method or function so I'll write Q dot is null and all the null values will return true while other values will return false so we'll get a sort of data frame as a result and the entries corresponding to the null values will be true and corresponding to not null values will be false now we can see how to drop all the rows with null values so I had already discussed this just write me Q dot drop any so all the rows have been deleted because all the rows contain null value so drop n a removes the rows with null values this we have already discussed in data cleaning but I thought it would be great if I explain it in detail here so by default drop any or drop null is false in place so it won't cause any change and the original data set unless you specify in place is equal to true now we Are Gonna Fill the null values for that we have fill any method and I'm gonna pass whatever number or string I want null values to be replaced with now all the null values will be replaced with the string fill so here we can see the result now moving on so if we want to assign any n value to some variable directly we can't do it using the python functions or the python Concepts we have to go for numpy so I'm gonna import and I'm by as NP and I'm gonna write Q of Nu is equal to NP Dot N A N this is how we assign an in value to a row or a column you can't directly write Q Nu is equal to Indian it won't work so now you know how to directly assign a row red n in value so in today's lecture we covered how to deal with null or NN values in a data frame that will be all for today this video is brought to you by programming knowledge please like comment share subscribe and hit the Bell button for updates and stay tuned with us for next lecture thank you hi guys welcome back to our course on pandas in today's lecture I'm going to teach you about pivot tables so let's get started so what is a pivot table so pivot table it's nothing but a multiindex table so earlier we had just only one index or label here we can have multiple and we can sort them so we are basically trying to create a multiindex table from a normal data frame this is what we are gonna do in this video so pivot table simplifies the presentation of data in a data frame where we have redundant a pivot valid for a particular let me just important so let's now I'm going to first create a dictionary in order and then we will create so let the dictionary b a b c and d respectively so A and B have redundant so A and B will act as the multiple indexes so let the elements of a b so bar redundant and elements p 1 and 2 for the column B also redundant so we have created a dictionary now let's create a data frame so DF is equal to PD dot data frame and I'll pass the dictionary so this is the resultant data frame now I have to create a pivot table so let's create a pivot table so in order to create a pivot table I have to write DF dot pivot table within underscore separation and I'll pass values is equal to D because it represents the data index or the multiindex is represented by a and b and let's represent the columns by C so the columns is equal to Phi so the columns I X and Y now this is the resultant data frame or pivot table or a pivot data frame so in today's lecture I covered how to create a pivot table from a normal data frame hence we have created a pivot table successfully that will be all for today this video is brought to you by programming knowledge please like comment share subscribe and hit the Bell button for updates and stay tuned with us for next lecture thank you hi guys welcome back to our course on pandas this is gonna be the last Theory lecture I'm gonna teach you how to read data from HTML files so let's get started so today's topic is reading data from HTML files so we have to install HTML lib lxml and beautiful soup before we can actually read data from HTML files so these are the commands for installation that is conda install lxml conda install HTML lib 5 or HTML5 lib and conda install beautiful soup 4. so I have installed after installation you have to restart the restart notebook so just close it close your browser open the Anaconda navigate again and launch the notebook again so I have launched the notebook successfully now I just need to look for that particular file search in desktop so I'll just go to the file was reading HTML Dot ipin now I just need to import pandas aspd so import pandas as PD and then I just need to write DS is equal to PD Dot read HTML it's similar to reading from CSV or Json I just need to change read underscore HTML and I have to pass the link of the file and you can process and manipulate the data frame in the similar way as we manipulated any other data frame so this is gonna be your assignment you have to look for an HTML file that contains a database and you have to extract that database from the HTML file that will be all for today this video is brought to you by programming knowledge please like comment share subscribe and hit the Bell button for updates and stay tuned with us for next lecture thank you
