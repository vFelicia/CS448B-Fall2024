With timestamps:

00:09 - hi and welcome to real-world Python
00:11 - declining projects by Packt publishing
00:15 - my name is kuba konschak and i have been
00:17 - programming since 1995
00:20 - since then programming is my passion and
00:23 - I really enjoy doing it and I've been
00:26 - programming in a lot of different
00:27 - programming language and I you know
00:30 - built a lot of different kind of systems
00:32 - and since around 2006 I've started to
00:37 - build a pretty complex Python systems
00:40 - and what got me really into machine
00:43 - learning and deep learning specifically
00:45 - is that I started to work with a start
00:49 - of that you know they wanted to predict
00:51 - future promises of a real estate based
00:54 - on corn prices that they have in the
00:57 - database and they got me this task of
01:00 - doing all of the research and trying to
01:03 - figure out how we can implement and
01:05 - unfortunately I didn't have enough time
01:07 - and to really deep into this problem and
01:11 - I simply couldn't do that so then I
01:13 - decided that I want to really get into
01:16 - machine learning and understand how it
01:18 - works make it really simple for myself
01:20 - and also for hunters so this is the goal
01:23 - of this course to make machine learning
01:26 - and deep learning specifically really
01:28 - accessible for you so let's go through
01:31 - the course overview so in this course
01:34 - you'll work with a variety of machine
01:38 - learning problems and in each product
01:41 - you will learn a distinctive set of
01:44 - tools and how to solve a specific
01:46 - problem of different kind of data and
01:49 - with different kind of deepening methods
01:51 - and we'll start with section 1 when I
01:55 - want to give you the essential
01:57 - informations about deep learning so you
02:00 - can get started really quickly so this
02:03 - section is all about getting you up and
02:06 - running with deep learning tools really
02:09 - quickly and also I will show you how can
02:11 - set up all of the necessary tools really
02:14 - quickly so you can start working with
02:17 - our project straightaway then in section
02:20 - 2 this is our first project and we'll
02:23 - start with forecasting how much people
02:26 - will travel by L
02:28 - next year based on past data that we
02:31 - have and in this project you will learn
02:34 - the basics of building your models and
02:36 - training and testing your models this
02:39 - will be just a very gentle introduction
02:41 - to how to work with deep learning
02:43 - methods and by the way you will learn
02:46 - how to work with time series data and
02:49 - with supervised learning problem then in
02:52 - section 3 we'll switch to different kind
02:55 - of data and in this section we'll focus
02:58 - on working with text and with conversion
03:01 - networks and here we will learn how to
03:04 - do what we call a sentiment analysis
03:07 - which basically means that you want to
03:09 - recognize the emotional feel of a text
03:12 - and here we'll be using a short text or
03:16 - you know the tweets to do that you will
03:19 - learn how to prepare your data to work
03:21 - with convulsion neural networks and text
03:24 - now how to build those models that can
03:26 - tell if a piece of text is negative or
03:29 - not then after that we will switch to
03:33 - different kind of data as well because
03:36 - we'll start with working with images
03:38 - we'll still work with commercial neural
03:40 - networks but the hair will learn how to
03:43 - detect smiles in images and this is a
03:47 - very common and interesting problem and
03:49 - you will learn pretty much how to design
03:52 - the model of convolutional neural
03:54 - network for this type of problems and
03:56 - how to purge that and also how to
03:59 - prepare your data in a way that your
04:02 - training will be quick which is
04:03 - essential in this section at any in
04:06 - section 5 will end with a really
04:09 - changing problem which is predicting
04:11 - stock prices using long short term
04:13 - memory networks and this is changing for
04:16 - various reasons and you know in this
04:19 - section we'll explore all that and how
04:21 - we can solve those problems to actually
04:24 - get some results so I hope that you are
04:26 - excited and one more thing let's go
04:30 - through some requirements and that we
04:33 - have to meet before you can get into
04:34 - this course so the main requirement is
04:37 - that you have to have some point and
04:39 - experience you don't have to be an
04:41 - expert
04:42 - we basically in this course we're using
04:44 - a whole bunch of a penis we don't use
04:47 - any advanced point and programming
04:49 - tricks so you know the only thing that
04:52 - you need to do here or to have here that
04:55 - you have to have some basic part and
04:57 - experience and of course if you have
04:59 - machine learning knowledge or experience
05:01 - that's a big plus but I don't assume
05:04 - that Trump discourse and I do my best
05:07 - introduce and fin from the ground up as
05:09 - much as I can in a tone that we have in
05:12 - this course so I hope that you're
05:14 - excited about this course I am and you
05:18 - know let's get started so let's start
05:21 - with opening our preparation script or
05:24 - to be exact let's run our preparation
05:27 - script is located under prep dot apply
05:29 - in the source directory for this section
05:32 - and it will help us understand how our
05:35 - source data looks like and how do we
05:38 - actually need to prepare it so let's
05:41 - first look at our roll values so here we
05:45 - have on the left we have years and then
05:48 - for interview we have a number of
05:50 - passengers karatbar airlines and this is
05:54 - pretty straightforward
05:56 - data set and main question you might
05:59 - have right now is okay why we can
06:02 - actually use this data directly and the
06:05 - answer is because our new network don't
06:08 - know what it means and need to tell it
06:11 - what it means so how we can do that
06:13 - convert this data and phrase our problem
06:17 - into the way our new network looks at
06:20 - problems our new network look at
06:23 - problems as a series of observations
06:26 - when you have x and y and X depends on Y
06:31 - in some way it looks at a list of those
06:34 - mappings between those two values or
06:36 - more but in our case we need to
06:39 - correlate the number of passengers
06:41 - carried in some way I'll tell you how in
06:45 - a minute and then new network can learn
06:48 - those dependencies between those two
06:50 - values or a list of two values
06:53 - so we need to give our network some kind
06:56 - of observation or experience that it can
06:59 - learn on so how we can do that
07:04 - to generate to erase or lists in one
07:08 - array going to provide a list of actual
07:13 - predictions and then in the second list
07:16 - we need to provide what those first
07:19 - predictions are based on so we have x
07:21 - and y and here on the right we have the
07:25 - values for each here and we started
07:28 - moving 1973 here and this is just for
07:31 - illustration let's look at those values
07:34 - so on the right we have four 1970 three
07:38 - four and so on we have number of
07:41 - passengers carat and we can create an X
07:45 - which is values that we base our
07:48 - prediction on for each year we can
07:52 - assume that each year's value depends on
07:55 - previous year's value so here for 1974
08:00 - we can assume that the vibe for 1974
08:04 - somehow correlates to the value from
08:08 - 1973 the only problem is that with 1973
08:13 - which is our first year we don't have
08:16 - any data for previews here that we can
08:19 - use here so here the previews data will
08:23 - be empty and we'll have to exclude this
08:25 - particular data point from our X&Y
08:29 - because you know we don't have any way
08:33 - to base our prediction on so this is the
08:37 - main idea behind the data preparation we
08:39 - need to construct this x and y arrays
08:42 - and the values from X will correlate for
08:47 - advice from Y and the resulting values
08:51 - will be just X list values which are the
08:56 - values that we base our predictions on
08:58 - and Y are the actual predictions and
09:02 - just try to grasp it it's a very simple
09:05 - idea and in both cases we have this
09:09 - number of passengers carrot they are
09:12 - just phrased in different
09:14 - and let's look now at the code that we
09:17 - used to generate those values let's open
09:20 - our prep dot PI scripts our preparation
09:24 - script it's located in the source
09:27 - directory for this section so we are
09:30 - mainly interested in get underscored
09:33 - data function where all the magic
09:35 - happens and we are generating those
09:38 - values in four stages the first stage is
09:42 - to read our source data I'll talk about
09:45 - the details of our source data a moment
09:48 - but here we using pandas just read the
09:52 - values from our CSV file and CSV file is
09:55 - a text file and it's a common separated
09:57 - value file after we've done that then we
10:01 - need to convert our values from this
10:04 - file to a numerical values so here we
10:08 - have values and years those two arrays
10:10 - we are producing using get row XY the
10:15 - next step is to create our X array
10:18 - because values in this case are our
10:21 - wiring those are our predictions and
10:25 - then we need to have files that match
10:27 - those predictions or advice that we base
10:30 - our predictions on so those will be the
10:33 - X we need to generate those X values and
10:36 - here we call them past values and when
10:40 - you look at the data once more when you
10:44 - look at it side by side you can see that
10:46 - those two arrays X is to slightly
10:49 - shifted Y array
10:52 - so when you look at it this value did
10:55 - the value number number two let's say or
10:57 - one when we talk in terms of Python the
11:01 - race match exactly the value from Y the
11:06 - first value or the zero value and this
11:08 - goes on and on
11:12 - to generate X we just shift y el a
11:14 - little bit so we can look at get on the
11:18 - score of ppl it's well-documented and we
11:21 - generate those values in pretty standard
11:24 - the last step is to return this value so
11:28 - we have fears we need those to produce
11:31 - our graphs our charts then when you
11:36 - return our X values and then our Y
11:40 - bodies
11:41 - so those are two arrays that we are most
11:44 - interested in and not is that we are
11:47 - filtering the first and the first row in
11:51 - those arrays and this is one again
11:54 - because we don't have data for 1972 or
12:00 - past data any data that we can base a
12:05 - prediction for 1973 that's why we need
12:08 - to filter that out because we don't have
12:10 - data for this particular year okay so
12:14 - let's now talk hold it about our source
12:17 - data so our source data is a CSV file in
12:23 - and it looks like this and it will be
12:26 - located in the source directory for this
12:28 - section in data directory under airline
12:32 - data don't CSV can always download this
12:36 - data set just by googling World Bank
12:40 - airline travel this is usually how I do
12:43 - it and then the first result our
12:46 - transport bus engine current is the
12:49 - right result and when you go to World
12:52 - Bank's website for this data set you can
12:56 - download it by clicking on the right
12:59 - under download under CSV and to use this
13:03 - newly downloaded data I need to put it
13:06 - into the data folder in the source
13:09 - directory for this section I need to
13:11 - name it to airline indict a dot CSV okay
13:15 - that's it for now
13:17 - I hope that was useful
13:20 - so let's look at our projects plan so
13:22 - the first step and this is very
13:24 - important step is to prepare our data
13:26 - and the problem that we're trying to
13:28 - solve here is called sentiment analysis
13:31 - in machine learning lingo and this step
13:34 - is very important because you will see
13:36 - how when you prepare data in a different
13:39 - way you will get a different accuracy of
13:42 - our model so this step is really pretty
13:45 - important then we will learn how to
13:49 - build convolutional neural network for
13:51 - text classification there are a couple
13:53 - of things you need to be aware about
13:54 - that we'll use so called Wharton
13:58 - Manning's here recent invention that
14:01 - allow us to get a really really nice
14:05 - results then we will train and test our
14:09 - model with data that we've prepared to
14:11 - find the best model that we can get and
14:15 - here you'll learn which metrics are
14:19 - important what you should look for when
14:21 - training and testing your model to get
14:23 - the best model that you can get and the
14:26 - last step is to actually use this model
14:28 - to detect those tweets and then we'll
14:32 - explore what we can do even more to
14:35 - improve the performance of our model so
14:39 - let's look first at our data set that
14:43 - we'll be using
14:44 - we'll be using a data set from smile
14:47 - smile D project and it's open source
14:50 - mile detector and it uses a traditional
14:53 - methods to detect smiles and what we are
14:57 - interested most is inside this
14:59 - repository teh github repository we are
15:03 - most interested in smiles directory so
15:07 - when you go to smile smile D repository
15:11 - you can click on the smiles directory
15:14 - and we have two directories negatives
15:17 - and positives and here we have those
15:20 - images that we'll be using to train our
15:22 - network so how we can install them so to
15:26 - install them you have to download the
15:28 - whole repository so just go to the main
15:32 - page as you
15:33 - can see here and you can click on clone
15:36 - or download on the right when you click
15:38 - on this button can choose download zip
15:41 - when you download this repository can
15:45 - unzip it and you have to copy those two
15:49 - directories from Smiles directory
15:52 - negatives and positive inside the data
15:54 - directory inside source directory for
15:57 - this section and I've already done that
16:00 - here you can see the source directory
16:04 - for this section and we have negatives
16:07 - and positives and that's it so let's
16:10 - look at how our source data looks like
16:13 - so in negatives and positives we have
16:17 - faces of people that are basically
16:20 - smiling and having different facial
16:23 - expression
16:23 - so in positives we have a variety of
16:26 - people basically most of the time
16:30 - smiling so this is our positives data
16:33 - set and then we also have negatives data
16:38 - set and here we have different kind of
16:40 - facial expression it's not smiling so
16:43 - even now we have negative and positive
16:45 - it will fit our project perfectly and
16:48 - one thing to notice here in those two
16:51 - data set or in this one that is data set
16:53 - and in negatives and positives is that
16:56 - here we have imbalance because we have
16:59 - around 10000 negative facial expressions
17:03 - and 4000 positive ones but we can work
17:08 - with that but we have to make sure that
17:10 - we blend together those two data set so
17:13 - our network will Train effectively so
17:17 - let's now look at our output and as
17:21 - usually at this step we need to generate
17:23 - the right x and y's so what will be x in
17:27 - our case X will be the images that we
17:30 - have and we have to encode those images
17:33 - in the right way
17:34 - so here I've opened the terminal and run
17:37 - our preparation script which is in prep
17:39 - dot pi and here we have just the first
17:42 - image from X this is already encoded
17:46 - so each value that you can see here is
17:49 - the value of the pixel from each image
17:52 - which is converted and I will talk about
17:54 - it in a moment
17:55 - notice how this each value has its
17:58 - separate array and this is most often
18:02 - confusing for a lot of beginners that
18:05 - each value of this pixel have to be
18:07 - wrapped inside a separate list or array
18:11 - so this is our X and we are using our
18:16 - input for our network will be Xin
18:19 - convolutional neural network our first
18:21 - layer is the two-dimensional convolution
18:24 - layer and the input for this layer is 32
18:29 - bar 32 images in our case we have 32
18:34 - rows and 32 combs that we need to
18:36 - provide data in this kind of shape and
18:39 - we also have to provide the number of
18:42 - channels and here we're using the black
18:45 - and white images so we can encode the
18:49 - black and white just with one value so
18:51 - we'll be using s channels will use one
18:54 - and we generate those valleys and we put
18:58 - them inside our preparation script and
19:01 - then our training script will pick them
19:03 - up and configure our network in the
19:05 - right way so this is our X what will be
19:09 - our Y so here we have our Y this is
19:14 - again the first encoded class and here
19:17 - we have just two classes we have not
19:20 - smiling as zero ends minus one and here
19:25 - will be when things will be differently
19:28 - than in previous section because we'll
19:30 - be using a soft max activation function
19:35 - in the last layer and that means that we
19:37 - can use more than two categories and we
19:41 - will still just use two categories here
19:44 - but I want to show you how you can use
19:47 - multiple categories here
19:50 - soft mint function and here we have to
19:53 - encode our data in a different way we
19:55 - have to encode them each of those
19:57 - classes as an array or as a vector and
20:00 - here we have just two classes so we'll
20:04 - you know the encoding will look like 4 0
20:07 - - 0 1 0 and for smiling we have 0 1 and
20:13 - this is the only difference the
20:16 - predictions will be pretty much the same
20:17 - like previous section and we have even
20:21 - more options with when we using soft map
20:24 - when we doing predictions and we'll talk
20:26 - about in a moment so let's go through
20:29 - the code here here I want to highlight
20:32 - bits and pieces that I required and also
20:35 - bits and pieces that allow us to process
20:39 - data quicker because this is very
20:41 - important when you're working with
20:42 - convolution your networks and images and
20:45 - with images in the U networks in general
20:48 - because images contains all the data and
20:51 - you want to make sure that we have
20:53 - enough data for training but at the same
20:56 - time our data is small enough so we can
20:59 - work with it in an effective way so
21:02 - we'll start with our preparation script
21:05 - it's located in our source directory for
21:08 - this section it's in prep dot PI and our
21:11 - main function as usual is get underscore
21:15 - data and here where all the magic
21:17 - happens we starting with loading the
21:21 - data the negative data and the positive
21:23 - data from our directories and then for
21:27 - each of those data generating the
21:29 - appropriate label and we doing this
21:32 - first for negative samples and then for
21:36 - positive ones after that we need to
21:38 - merge them together join them together
21:41 - both X's and Y's this is how we do it
21:45 - here and you were using a lot of numpy
21:49 - operations because they are really quick
21:51 - and they work well with images or arrays
21:54 - that we converted from images
21:59 - then after that we hit optimization step
22:02 - so by default I got 64-bit values for
22:06 - those images that get converted into a
22:09 - race this can work with our new networks
22:12 - but it turns out that our new networks
22:15 - in our case we don't need that much data
22:19 - 64-bit values they can have a broader
22:23 - range that 32-bit values but they are
22:25 - larger it will take more time time to
22:28 - train our network so here what we are
22:30 - actually doing is that we're just
22:33 - converting both of those values from X&Y
22:35 - to 32 bits and this will make our
22:38 - training much more quicker then the next
22:42 - step is to define the shape of our data
22:45 - and here we have our input data or 64 by
22:50 - 64 images black and white images or grey
22:55 - images and here we're just providing
22:58 - this information for our training script
23:01 - when it will define the layers it needs
23:04 - those kind of information and then we
23:06 - also need to provide those kind of
23:09 - information about the number of classes
23:11 - for our training script when it define
23:14 - the last layer with the softmax
23:16 - activation function and then we need to
23:20 - encode our classes in the right way and
23:23 - this is the you know the last output
23:25 - that I've showed you from the
23:26 - preparation script and here we're just
23:29 - turning our classes into those factors
23:31 - those arrays this is just the
23:33 - requirement of the softmax function then
23:36 - after that to do something with
23:39 - inequality of negative and positive
23:41 - samples and if you don't do that if you
23:44 - have imbalance between two data set your
23:47 - training will be much more slower here
23:50 - we can just use a pretty standard numpy
23:53 - the function is to just mix up the
23:56 - indexes indexes will be just distributed
23:59 - randomly when we do that we can just
24:02 - rewrite those indexes for both X and Y's
24:05 - in the same way so they can both match
24:09 - and not is that in this gait data script
24:13 - we don't split into training and testing
24:16 - sets because we'll later on in the
24:20 - training script will let care us take
24:22 - care of it the curse will automatically
24:25 - do that for us
24:26 - so this is the main idea behind
24:29 - preparation of the data and let's look
24:32 - at our load data function because this
24:35 - is where we have another set of
24:37 - optimizations so let's first look at our
24:40 - load data function so load data just
24:44 - goes through each image and turn it into
24:46 - array and it mainly does that using
24:49 - image to array function so let's look at
24:53 - this image to a rake function so here it
24:59 - is it's right at the beginning of
25:00 - preparation Square and there are two
25:03 - things that you need to know about this
25:05 - particular function so first it will
25:08 - quite easily turn the images into arrays
25:11 - using I am read from signcut image
25:15 - package this is the required step this
25:18 - is how you can get those values from the
25:22 - images and turn them just to an ordinary
25:25 - erase but after that there is this very
25:29 - important down sample step and this is
25:33 - when the next step comes in this is the
25:35 - optimization step this is where we turn
25:38 - our 64 by 64 images into 32 by 32 but
25:44 - without losing any important
25:46 - informations and this will allow us to
25:49 - really much quicker working with the
25:52 - network much quicker and still get the
25:55 - high quality that we're looking for last
25:59 - step is that once load data creates all
26:03 - those arrays for each image it will at
26:06 - the end run Prem underscore array all of
26:09 - those images turn into a race this
26:12 - function does two things first thing is
26:15 - that by default the pixel values are in
26:18 - the range from 0 to 255 and this range
26:23 - values doesn't really work with you
26:25 - network especially when you're working
26:27 - with images because we have thought of
26:30 - values here so the good idea is to
26:32 - actually convert each value to a
26:36 - different range to a range of 0 0 to 1.0
26:40 - and this works much much more better so
26:44 - here we can using a pretty standard dump
26:47 - on the operation and just turn this data
26:51 - into a numpy array and divide it white
26:55 - 254 5.0 which is the maximum value that
27:00 - we can get in those values and this will
27:03 - turn those ranges around to 0 0 to 1.0
27:09 - and you can still work without this
27:13 - conversion but it will be much much more
27:15 - small and this is not a required step in
27:20 - an absolute sense but it will make
27:22 - training much more faster and the last
27:25 - step is to turn those single values for
27:28 - each value that we have it's separated
27:31 - list or separate array this is the stuff
27:34 - that I've showed you when I run the
27:36 - preparation script and here we have a
27:39 - quick way of going about it using numpy
27:42 - we can just do that on the whole data
27:44 - set really easily and that's it when it
27:48 - comes to data preparation
27:51 - so we'll start with downloaded data from
27:54 - Yahoo Finance and this is freely
27:56 - available data and we'll be downloading
27:59 - it and using it with this project so the
28:02 - first step to do that is that you need
28:04 - to go to finance dot yahoo.com and then
28:07 - you need to type in the name of the
28:09 - company that you are interested in in
28:12 - this project we'll be using the
28:13 - historical stock prices data for Tesla
28:17 - so Candace turnip in Tesla here and once
28:21 - we've done it we've got all sorts of
28:23 - informations here but what we are most
28:25 - interested in is the historical data tab
28:29 - let's click on that and when you do that
28:31 - then on the left hand side you have time
28:34 - periods drop down menu time range of the
28:37 - data that you are interested in and
28:40 - we'll be using about a one year period
28:44 - data for this particle project when you
28:47 - choose the time range you can click on
28:50 - on done then click on apply and then you
28:55 - can download your data in CSV form and
28:59 - once you download it you have to put it
29:02 - into the data directory in the source
29:04 - directory for this section under stock
29:07 - underscore prices dot CSV so the data
29:12 - that i've downloaded this is the
29:14 - historical stock prices for Tesla and we
29:18 - have a couple of variables here and
29:21 - we'll be using the same ideas that we've
29:24 - used in section two to predict the
29:27 - amount of people that will travel by air
29:29 - in the next year but the difference
29:32 - between section two in this section is
29:35 - that we'll be using multiple variables
29:38 - to do that so we'll be using a couple of
29:41 - variables that are related to prices so
29:44 - our data comprised of rows in each row
29:47 - we have this information about the
29:49 - prices of a stock for Tesla the price is
29:53 - in US Dollars and we have the price for
29:56 - one share our variables that we'll be
29:59 - using is open this is the stock market
30:01 - opens this particular one
30:04 - stock-exchange open this is the Dupre's
30:06 - that we starting with then we have hi
30:09 - hi is the maximum price that this
30:12 - particular got in this particle day then
30:15 - we have lo this is the lowest price and
30:17 - then we have closed price when the stock
30:21 - market closes and then we also have
30:24 - adjusted close and we are not really
30:27 - interested in this one because in our
30:30 - case it will be the exact same as the
30:32 - closing price and then you also have
30:34 - volume this is the number of shares that
30:36 - were traded in a given day so we'll be
30:39 - using all those variables
30:41 - apart from adjacent clothes and data
30:43 - will be used in all of those to predict
30:46 - the closing price for a given day so how
30:49 - we can do that we'll use the exact same
30:52 - thinking that we've used in section 2 so
30:55 - let's say that we want to predict
30:57 - closing price for this particle day so
31:01 - we can say that we can use all the
31:03 - values from previous day to predict this
31:06 - price okay
31:07 - and as you can see the thinking behind
31:09 - it is exactly like in section 1 we just
31:12 - you know phrasing problem as supervised
31:15 - learning problem that's pretty much it
31:18 - so we need to convert our data so it can
31:20 - match this thinking we need to create
31:23 - the x and y so let's look at the output
31:26 - of our preparation script to understand
31:29 - also how we can do that and also how we
31:33 - need to format our data so we can use it
31:35 - with our network so the first step is to
31:39 - do the shifting and we can do that with
31:41 - different ways in section 2 I gave you
31:43 - some idea that how we can do that the
31:46 - other idea is that we can when you've
31:48 - got the data for x and y so x is the
31:52 - variables the matrix that we're using
31:54 - for prediction that we will base our
31:56 - prediction on and the y is the actual
31:59 - prediction so when you have those two
32:02 - lists in Perl you can easily achieve
32:06 - this the right formatting way that you
32:09 - can phrase a problem as supervised
32:11 - learning problem just by shifting
32:15 - the Y upwards it means simply not here
32:19 - and getting value from the first element
32:22 - from Y yeah I knew just discard it right
32:25 - because we are interested in basing our
32:28 - prediction on previews days of metrics
32:33 - so our first step is to just get the
32:36 - data from our CSV file and crate those x
32:38 - and y's and without any shifted so
32:41 - here's how it looks like this is the our
32:44 - X this is not shifted right and you have
32:47 - our Y this is the closing price then
32:50 - after we've shifted that we live in the
32:53 - X alone and we're just moving the Y from
32:57 - in the next day to the present day so
33:00 - this is how it looks shifted so three
33:02 - four four goes up and we can see that
33:07 - those values are just going up those are
33:10 - closing prices are going up right and it
33:14 - always takes a little bit of time to get
33:17 - your head around those kind of ideas but
33:19 - you know the idea is would be simple and
33:22 - I've got a lot of comments for you in
33:24 - the actual code so you can read those
33:26 - I've got some examples there so and I'm
33:29 - pretty sure that it will help you
33:31 - understand what's going on
33:33 - so once we've shifted the Y and we
33:35 - phrased our XY z-- in the right way as a
33:38 - supervised learning problem we need to
33:41 - also shape it so that our network will
33:44 - accept that and those long short term
33:47 - memory networks they work with sequences
33:51 - our data looks like that after this
33:54 - shifting this is the just one element
33:57 - from the data so here we have just a
33:59 - list of those arrays or lists of lists
34:02 - of those metrics right and then in the
34:07 - second in the Y array or list we have
34:10 - just those values of those closing
34:12 - prices values
34:14 - to convert that into the format that all
34:17 - network will accept so our network needs
34:20 - another dimension another list because
34:24 - it assumed that it doesn't necessarily
34:27 - have to work with just one sequence and
34:30 - you can imagine that we can add more
34:32 - sequences or more data from more days in
34:36 - the past and here we're just using one
34:38 - day from the past the data from the past
34:41 - so here we just need to add one
34:44 - dimension which means that we need to
34:46 - just embrace or you know put those
34:49 - variables into one more list and that's
34:54 - it when we've done it our long
34:56 - short-term memory network will accept
34:58 - our data so let's have a look at our
35:02 - data actually when you run this script
35:05 - it will show us what we will be working
35:07 - with so here's just a plot of the
35:11 - closing promise for our data so as you
35:14 - can see in blue we have our training set
35:17 - and in orange we have our test set we've
35:20 - split at 8020 so 80% of data is training
35:24 - set and 20% is our test set as you can
35:27 - see this is not an easy to model data
35:29 - because we have lot of peaks and valleys
35:32 - okay
35:33 - so we'll be working with this data so
35:36 - let's just quickly have a look at our
35:38 - preparation script to just give you some
35:40 - idea how you can get around it and it's
35:43 - van twenty percent is from section two
35:46 - but let's just go through it quickly so
35:49 - we're starting with our main function
35:51 - which is get underscore data and this is
35:54 - our main starting point and then we also
35:56 - have our prep underscore data this is d
35:59 - tap when we add in this one more
36:02 - dimension to our data set so let's just
36:04 - get started with get underscore data so
36:07 - what we're basically doing here is that
36:09 - we're using pandas to read our CSV file
36:13 - then we just get in the right columns
36:17 - from the data and discarding those that
36:19 - we don't need so we don't think the data
36:21 - column and we don't need the adjusted
36:24 - column so this is what we're doing in
36:26 - get underscore row XY
36:29 - and then after that we shift in this why
36:31 - right we're shifting it upwards or
36:34 - backwards it depending how you look at
36:37 - it so this is what we do we've get
36:39 - underscore VPL so notice here that we
36:42 - live in X alone and we just shift in y
36:47 - and then after that we just discard in
36:50 - the last data at the bottom because we
36:53 - don't have any data there because we've
36:55 - shifted why so we don't have data at the
36:58 - bottom as well so this is what we
37:01 - basically do here and all of those
37:03 - functions are pretty well-documented so
37:06 - feel free to go through the minutes and
37:08 - what's going on and you can also go
37:11 - through the section to again if you're
37:14 - in doubt and explain it a little bit
37:16 - differently and deeply so feel free to
37:19 - check out section two for preparation
37:21 - data if you still feel that something is
37:25 - unclear

Cleaned transcript:

hi and welcome to realworld Python declining projects by Packt publishing my name is kuba konschak and i have been programming since 1995 since then programming is my passion and I really enjoy doing it and I've been programming in a lot of different programming language and I you know built a lot of different kind of systems and since around 2006 I've started to build a pretty complex Python systems and what got me really into machine learning and deep learning specifically is that I started to work with a start of that you know they wanted to predict future promises of a real estate based on corn prices that they have in the database and they got me this task of doing all of the research and trying to figure out how we can implement and unfortunately I didn't have enough time and to really deep into this problem and I simply couldn't do that so then I decided that I want to really get into machine learning and understand how it works make it really simple for myself and also for hunters so this is the goal of this course to make machine learning and deep learning specifically really accessible for you so let's go through the course overview so in this course you'll work with a variety of machine learning problems and in each product you will learn a distinctive set of tools and how to solve a specific problem of different kind of data and with different kind of deepening methods and we'll start with section 1 when I want to give you the essential informations about deep learning so you can get started really quickly so this section is all about getting you up and running with deep learning tools really quickly and also I will show you how can set up all of the necessary tools really quickly so you can start working with our project straightaway then in section 2 this is our first project and we'll start with forecasting how much people will travel by L next year based on past data that we have and in this project you will learn the basics of building your models and training and testing your models this will be just a very gentle introduction to how to work with deep learning methods and by the way you will learn how to work with time series data and with supervised learning problem then in section 3 we'll switch to different kind of data and in this section we'll focus on working with text and with conversion networks and here we will learn how to do what we call a sentiment analysis which basically means that you want to recognize the emotional feel of a text and here we'll be using a short text or you know the tweets to do that you will learn how to prepare your data to work with convulsion neural networks and text now how to build those models that can tell if a piece of text is negative or not then after that we will switch to different kind of data as well because we'll start with working with images we'll still work with commercial neural networks but the hair will learn how to detect smiles in images and this is a very common and interesting problem and you will learn pretty much how to design the model of convolutional neural network for this type of problems and how to purge that and also how to prepare your data in a way that your training will be quick which is essential in this section at any in section 5 will end with a really changing problem which is predicting stock prices using long short term memory networks and this is changing for various reasons and you know in this section we'll explore all that and how we can solve those problems to actually get some results so I hope that you are excited and one more thing let's go through some requirements and that we have to meet before you can get into this course so the main requirement is that you have to have some point and experience you don't have to be an expert we basically in this course we're using a whole bunch of a penis we don't use any advanced point and programming tricks so you know the only thing that you need to do here or to have here that you have to have some basic part and experience and of course if you have machine learning knowledge or experience that's a big plus but I don't assume that Trump discourse and I do my best introduce and fin from the ground up as much as I can in a tone that we have in this course so I hope that you're excited about this course I am and you know let's get started so let's start with opening our preparation script or to be exact let's run our preparation script is located under prep dot apply in the source directory for this section and it will help us understand how our source data looks like and how do we actually need to prepare it so let's first look at our roll values so here we have on the left we have years and then for interview we have a number of passengers karatbar airlines and this is pretty straightforward data set and main question you might have right now is okay why we can actually use this data directly and the answer is because our new network don't know what it means and need to tell it what it means so how we can do that convert this data and phrase our problem into the way our new network looks at problems our new network look at problems as a series of observations when you have x and y and X depends on Y in some way it looks at a list of those mappings between those two values or more but in our case we need to correlate the number of passengers carried in some way I'll tell you how in a minute and then new network can learn those dependencies between those two values or a list of two values so we need to give our network some kind of observation or experience that it can learn on so how we can do that to generate to erase or lists in one array going to provide a list of actual predictions and then in the second list we need to provide what those first predictions are based on so we have x and y and here on the right we have the values for each here and we started moving 1973 here and this is just for illustration let's look at those values so on the right we have four 1970 three four and so on we have number of passengers carat and we can create an X which is values that we base our prediction on for each year we can assume that each year's value depends on previous year's value so here for 1974 we can assume that the vibe for 1974 somehow correlates to the value from 1973 the only problem is that with 1973 which is our first year we don't have any data for previews here that we can use here so here the previews data will be empty and we'll have to exclude this particular data point from our X&Y because you know we don't have any way to base our prediction on so this is the main idea behind the data preparation we need to construct this x and y arrays and the values from X will correlate for advice from Y and the resulting values will be just X list values which are the values that we base our predictions on and Y are the actual predictions and just try to grasp it it's a very simple idea and in both cases we have this number of passengers carrot they are just phrased in different and let's look now at the code that we used to generate those values let's open our prep dot PI scripts our preparation script it's located in the source directory for this section so we are mainly interested in get underscored data function where all the magic happens and we are generating those values in four stages the first stage is to read our source data I'll talk about the details of our source data a moment but here we using pandas just read the values from our CSV file and CSV file is a text file and it's a common separated value file after we've done that then we need to convert our values from this file to a numerical values so here we have values and years those two arrays we are producing using get row XY the next step is to create our X array because values in this case are our wiring those are our predictions and then we need to have files that match those predictions or advice that we base our predictions on so those will be the X we need to generate those X values and here we call them past values and when you look at the data once more when you look at it side by side you can see that those two arrays X is to slightly shifted Y array so when you look at it this value did the value number number two let's say or one when we talk in terms of Python the race match exactly the value from Y the first value or the zero value and this goes on and on to generate X we just shift y el a little bit so we can look at get on the score of ppl it's welldocumented and we generate those values in pretty standard the last step is to return this value so we have fears we need those to produce our graphs our charts then when you return our X values and then our Y bodies so those are two arrays that we are most interested in and not is that we are filtering the first and the first row in those arrays and this is one again because we don't have data for 1972 or past data any data that we can base a prediction for 1973 that's why we need to filter that out because we don't have data for this particular year okay so let's now talk hold it about our source data so our source data is a CSV file in and it looks like this and it will be located in the source directory for this section in data directory under airline data don't CSV can always download this data set just by googling World Bank airline travel this is usually how I do it and then the first result our transport bus engine current is the right result and when you go to World Bank's website for this data set you can download it by clicking on the right under download under CSV and to use this newly downloaded data I need to put it into the data folder in the source directory for this section I need to name it to airline indict a dot CSV okay that's it for now I hope that was useful so let's look at our projects plan so the first step and this is very important step is to prepare our data and the problem that we're trying to solve here is called sentiment analysis in machine learning lingo and this step is very important because you will see how when you prepare data in a different way you will get a different accuracy of our model so this step is really pretty important then we will learn how to build convolutional neural network for text classification there are a couple of things you need to be aware about that we'll use so called Wharton Manning's here recent invention that allow us to get a really really nice results then we will train and test our model with data that we've prepared to find the best model that we can get and here you'll learn which metrics are important what you should look for when training and testing your model to get the best model that you can get and the last step is to actually use this model to detect those tweets and then we'll explore what we can do even more to improve the performance of our model so let's look first at our data set that we'll be using we'll be using a data set from smile smile D project and it's open source mile detector and it uses a traditional methods to detect smiles and what we are interested most is inside this repository teh github repository we are most interested in smiles directory so when you go to smile smile D repository you can click on the smiles directory and we have two directories negatives and positives and here we have those images that we'll be using to train our network so how we can install them so to install them you have to download the whole repository so just go to the main page as you can see here and you can click on clone or download on the right when you click on this button can choose download zip when you download this repository can unzip it and you have to copy those two directories from Smiles directory negatives and positive inside the data directory inside source directory for this section and I've already done that here you can see the source directory for this section and we have negatives and positives and that's it so let's look at how our source data looks like so in negatives and positives we have faces of people that are basically smiling and having different facial expression so in positives we have a variety of people basically most of the time smiling so this is our positives data set and then we also have negatives data set and here we have different kind of facial expression it's not smiling so even now we have negative and positive it will fit our project perfectly and one thing to notice here in those two data set or in this one that is data set and in negatives and positives is that here we have imbalance because we have around 10000 negative facial expressions and 4000 positive ones but we can work with that but we have to make sure that we blend together those two data set so our network will Train effectively so let's now look at our output and as usually at this step we need to generate the right x and y's so what will be x in our case X will be the images that we have and we have to encode those images in the right way so here I've opened the terminal and run our preparation script which is in prep dot pi and here we have just the first image from X this is already encoded so each value that you can see here is the value of the pixel from each image which is converted and I will talk about it in a moment notice how this each value has its separate array and this is most often confusing for a lot of beginners that each value of this pixel have to be wrapped inside a separate list or array so this is our X and we are using our input for our network will be Xin convolutional neural network our first layer is the twodimensional convolution layer and the input for this layer is 32 bar 32 images in our case we have 32 rows and 32 combs that we need to provide data in this kind of shape and we also have to provide the number of channels and here we're using the black and white images so we can encode the black and white just with one value so we'll be using s channels will use one and we generate those valleys and we put them inside our preparation script and then our training script will pick them up and configure our network in the right way so this is our X what will be our Y so here we have our Y this is again the first encoded class and here we have just two classes we have not smiling as zero ends minus one and here will be when things will be differently than in previous section because we'll be using a soft max activation function in the last layer and that means that we can use more than two categories and we will still just use two categories here but I want to show you how you can use multiple categories here soft mint function and here we have to encode our data in a different way we have to encode them each of those classes as an array or as a vector and here we have just two classes so we'll you know the encoding will look like 4 0 0 1 0 and for smiling we have 0 1 and this is the only difference the predictions will be pretty much the same like previous section and we have even more options with when we using soft map when we doing predictions and we'll talk about in a moment so let's go through the code here here I want to highlight bits and pieces that I required and also bits and pieces that allow us to process data quicker because this is very important when you're working with convolution your networks and images and with images in the U networks in general because images contains all the data and you want to make sure that we have enough data for training but at the same time our data is small enough so we can work with it in an effective way so we'll start with our preparation script it's located in our source directory for this section it's in prep dot PI and our main function as usual is get underscore data and here where all the magic happens we starting with loading the data the negative data and the positive data from our directories and then for each of those data generating the appropriate label and we doing this first for negative samples and then for positive ones after that we need to merge them together join them together both X's and Y's this is how we do it here and you were using a lot of numpy operations because they are really quick and they work well with images or arrays that we converted from images then after that we hit optimization step so by default I got 64bit values for those images that get converted into a race this can work with our new networks but it turns out that our new networks in our case we don't need that much data 64bit values they can have a broader range that 32bit values but they are larger it will take more time time to train our network so here what we are actually doing is that we're just converting both of those values from X&Y to 32 bits and this will make our training much more quicker then the next step is to define the shape of our data and here we have our input data or 64 by 64 images black and white images or grey images and here we're just providing this information for our training script when it will define the layers it needs those kind of information and then we also need to provide those kind of information about the number of classes for our training script when it define the last layer with the softmax activation function and then we need to encode our classes in the right way and this is the you know the last output that I've showed you from the preparation script and here we're just turning our classes into those factors those arrays this is just the requirement of the softmax function then after that to do something with inequality of negative and positive samples and if you don't do that if you have imbalance between two data set your training will be much more slower here we can just use a pretty standard numpy the function is to just mix up the indexes indexes will be just distributed randomly when we do that we can just rewrite those indexes for both X and Y's in the same way so they can both match and not is that in this gait data script we don't split into training and testing sets because we'll later on in the training script will let care us take care of it the curse will automatically do that for us so this is the main idea behind preparation of the data and let's look at our load data function because this is where we have another set of optimizations so let's first look at our load data function so load data just goes through each image and turn it into array and it mainly does that using image to array function so let's look at this image to a rake function so here it is it's right at the beginning of preparation Square and there are two things that you need to know about this particular function so first it will quite easily turn the images into arrays using I am read from signcut image package this is the required step this is how you can get those values from the images and turn them just to an ordinary erase but after that there is this very important down sample step and this is when the next step comes in this is the optimization step this is where we turn our 64 by 64 images into 32 by 32 but without losing any important informations and this will allow us to really much quicker working with the network much quicker and still get the high quality that we're looking for last step is that once load data creates all those arrays for each image it will at the end run Prem underscore array all of those images turn into a race this function does two things first thing is that by default the pixel values are in the range from 0 to 255 and this range values doesn't really work with you network especially when you're working with images because we have thought of values here so the good idea is to actually convert each value to a different range to a range of 0 0 to 1.0 and this works much much more better so here we can using a pretty standard dump on the operation and just turn this data into a numpy array and divide it white 254 5.0 which is the maximum value that we can get in those values and this will turn those ranges around to 0 0 to 1.0 and you can still work without this conversion but it will be much much more small and this is not a required step in an absolute sense but it will make training much more faster and the last step is to turn those single values for each value that we have it's separated list or separate array this is the stuff that I've showed you when I run the preparation script and here we have a quick way of going about it using numpy we can just do that on the whole data set really easily and that's it when it comes to data preparation so we'll start with downloaded data from Yahoo Finance and this is freely available data and we'll be downloading it and using it with this project so the first step to do that is that you need to go to finance dot yahoo.com and then you need to type in the name of the company that you are interested in in this project we'll be using the historical stock prices data for Tesla so Candace turnip in Tesla here and once we've done it we've got all sorts of informations here but what we are most interested in is the historical data tab let's click on that and when you do that then on the left hand side you have time periods drop down menu time range of the data that you are interested in and we'll be using about a one year period data for this particle project when you choose the time range you can click on on done then click on apply and then you can download your data in CSV form and once you download it you have to put it into the data directory in the source directory for this section under stock underscore prices dot CSV so the data that i've downloaded this is the historical stock prices for Tesla and we have a couple of variables here and we'll be using the same ideas that we've used in section two to predict the amount of people that will travel by air in the next year but the difference between section two in this section is that we'll be using multiple variables to do that so we'll be using a couple of variables that are related to prices so our data comprised of rows in each row we have this information about the prices of a stock for Tesla the price is in US Dollars and we have the price for one share our variables that we'll be using is open this is the stock market opens this particular one stockexchange open this is the Dupre's that we starting with then we have hi hi is the maximum price that this particular got in this particle day then we have lo this is the lowest price and then we have closed price when the stock market closes and then we also have adjusted close and we are not really interested in this one because in our case it will be the exact same as the closing price and then you also have volume this is the number of shares that were traded in a given day so we'll be using all those variables apart from adjacent clothes and data will be used in all of those to predict the closing price for a given day so how we can do that we'll use the exact same thinking that we've used in section 2 so let's say that we want to predict closing price for this particle day so we can say that we can use all the values from previous day to predict this price okay and as you can see the thinking behind it is exactly like in section 1 we just you know phrasing problem as supervised learning problem that's pretty much it so we need to convert our data so it can match this thinking we need to create the x and y so let's look at the output of our preparation script to understand also how we can do that and also how we need to format our data so we can use it with our network so the first step is to do the shifting and we can do that with different ways in section 2 I gave you some idea that how we can do that the other idea is that we can when you've got the data for x and y so x is the variables the matrix that we're using for prediction that we will base our prediction on and the y is the actual prediction so when you have those two lists in Perl you can easily achieve this the right formatting way that you can phrase a problem as supervised learning problem just by shifting the Y upwards it means simply not here and getting value from the first element from Y yeah I knew just discard it right because we are interested in basing our prediction on previews days of metrics so our first step is to just get the data from our CSV file and crate those x and y's and without any shifted so here's how it looks like this is the our X this is not shifted right and you have our Y this is the closing price then after we've shifted that we live in the X alone and we're just moving the Y from in the next day to the present day so this is how it looks shifted so three four four goes up and we can see that those values are just going up those are closing prices are going up right and it always takes a little bit of time to get your head around those kind of ideas but you know the idea is would be simple and I've got a lot of comments for you in the actual code so you can read those I've got some examples there so and I'm pretty sure that it will help you understand what's going on so once we've shifted the Y and we phrased our XY z in the right way as a supervised learning problem we need to also shape it so that our network will accept that and those long short term memory networks they work with sequences our data looks like that after this shifting this is the just one element from the data so here we have just a list of those arrays or lists of lists of those metrics right and then in the second in the Y array or list we have just those values of those closing prices values to convert that into the format that all network will accept so our network needs another dimension another list because it assumed that it doesn't necessarily have to work with just one sequence and you can imagine that we can add more sequences or more data from more days in the past and here we're just using one day from the past the data from the past so here we just need to add one dimension which means that we need to just embrace or you know put those variables into one more list and that's it when we've done it our long shortterm memory network will accept our data so let's have a look at our data actually when you run this script it will show us what we will be working with so here's just a plot of the closing promise for our data so as you can see in blue we have our training set and in orange we have our test set we've split at 8020 so 80% of data is training set and 20% is our test set as you can see this is not an easy to model data because we have lot of peaks and valleys okay so we'll be working with this data so let's just quickly have a look at our preparation script to just give you some idea how you can get around it and it's van twenty percent is from section two but let's just go through it quickly so we're starting with our main function which is get underscore data and this is our main starting point and then we also have our prep underscore data this is d tap when we add in this one more dimension to our data set so let's just get started with get underscore data so what we're basically doing here is that we're using pandas to read our CSV file then we just get in the right columns from the data and discarding those that we don't need so we don't think the data column and we don't need the adjusted column so this is what we're doing in get underscore row XY and then after that we shift in this why right we're shifting it upwards or backwards it depending how you look at it so this is what we do we've get underscore VPL so notice here that we live in X alone and we just shift in y and then after that we just discard in the last data at the bottom because we don't have any data there because we've shifted why so we don't have data at the bottom as well so this is what we basically do here and all of those functions are pretty welldocumented so feel free to go through the minutes and what's going on and you can also go through the section to again if you're in doubt and explain it a little bit differently and deeply so feel free to check out section two for preparation data if you still feel that something is unclear
