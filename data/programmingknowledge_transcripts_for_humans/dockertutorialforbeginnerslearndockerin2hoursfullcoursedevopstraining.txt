With timestamps:

00:02 - hello this is moonshot mohamed and
00:06 - welcome to the docker for beginners
00:07 - course this is an introduction to
00:10 - application containerization with docker
00:12 - for the absolute beginners made by code
00:15 - cloud for the programming knowledge
00:17 - audience thank you for joining and I
00:20 - hope you enjoyed the course hello and
00:26 - welcome to the docker for beginners
00:28 - course
00:28 - my name is moonshot monolith and I will
00:31 - be your instructor for this course I'm a
00:34 - DevOps and cloud trainer at code cloud
00:36 - comm which is an interactive hands-on
00:38 - online learning platform I've been
00:41 - working in the industry as a consultant
00:43 - for over thirteen years and have helped
00:45 - hundreds of thousands of students learn
00:48 - technology in a fun and interactive way
00:50 - in this course you will learn docker
00:53 - through a series of lectures that use
00:55 - animation illustration and some fun
00:58 - analogies that simplify complex concepts
01:01 - with demos that will show you how to
01:03 - install and get started with docker and
01:05 - most importantly we have hands-on labs
01:08 - that you can access right in your
01:10 - browser I will explain more about it in
01:12 - a bit but first let's look at the
01:15 - objectives of this course in this course
01:17 - we first try to understand what
01:19 - containers are what docker is and why
01:22 - you might need it and what it can do for
01:24 - you we will see how to run a docker
01:26 - container how to build your own docker
01:28 - image we will see networking in docker
01:31 - and how to use docker compose what
01:33 - docker registry is how to deploy your
01:35 - own private registry and we then look at
01:38 - some of these concepts in debt and we
01:40 - try to understand how docker really
01:42 - works under the hood you look at docker
01:45 - for Windows and Mac before finally
01:47 - getting a basic introduction to
01:49 - container orchestration tools like dr.
01:52 - Swann and kubernetes here's a quick note
01:55 - about hands-on labs first of all to
01:57 - complete this course you don't have to
01:59 - set up your own labs well you may set it
02:01 - up if you wish to if you wish to have
02:04 - your own environment and we have a demo
02:06 - as well but as part of this course we
02:08 - provide real labs that you can access
02:11 - right in your browser
02:12 - anywhere any
02:13 - and as many times as you want the labs
02:16 - give you instant access to a terminal to
02:19 - a docker host and an accompanying quiz
02:21 - portal the quiz portal asks a set of
02:24 - questions such as exploring the
02:26 - environment and gathering information or
02:28 - you might be asked to perform an action
02:30 - such as run docker container the quiz
02:33 - portal then validates your work and
02:35 - gives you feedback instantly every
02:38 - lecture in this course is accompanied by
02:40 - such challenging interactive quizzes
02:42 - that makes learning docker a fun
02:45 - activity so I hope you're as thrilled as
02:48 - I am to get started so let us begin
02:52 - [Music]
02:56 - we're going to start by looking at a
02:58 - high-level overview on why you need
03:00 - docker and what it can do for you let me
03:03 - start by sharing how I got introduced to
03:05 - Locker in one of my previous projects I
03:08 - had this requirement to set up an
03:10 - end-to-end application stack including
03:12 - various different technologies like a
03:14 - web server using node.js and a database
03:17 - such as MongoDB and a messaging system
03:20 - like Redis and an orchestration tool
03:22 - like ansible we had a lot of issues
03:24 - developing this application stack with
03:27 - all these different components first of
03:29 - all their compatibility with the
03:31 - underlying OS was an issue we had to
03:33 - ensure that all these different services
03:36 - were compatible with the version of OS
03:38 - we were planning to use there have been
03:40 - times when certain version of these
03:43 - services were not compatible with the OS
03:45 - and we've had to go back and look at
03:47 - different OS that was compatible with
03:49 - all of these different services secondly
03:52 - we had to check the compatibility
03:54 - between these services and the libraries
03:56 - and dependencies on the OS we've had
03:59 - issues where one service requires one
04:02 - version of a dependent library whereas
04:04 - another service requires another version
04:06 - the architecture of our application
04:09 - changed over time we've had to upgrade
04:11 - to newer versions of these components or
04:13 - change the database etc and every time
04:16 - something changed we had to go through
04:19 - the same process of checking
04:21 - compatibility between these various
04:22 - components and the underlying
04:25 - infrastructure this
04:27 - compatibility matrix issue is usually
04:29 - referred to as the matrix from hell
04:32 - next every time we had a new developer
04:35 - on board we found it really difficult to
04:38 - set up a new environment the new
04:40 - developers had to follow a large set of
04:42 - instructions and run hundreds of
04:44 - commands to finally set up their
04:46 - environment we had to make sure they
04:48 - were using the right operating system
04:50 - the right versions of each of these
04:51 - components and each developer had to set
04:54 - all that up by himself each time we also
04:57 - had different development tests and
04:59 - production environments one developer
05:02 - may be comfortable using one OS and the
05:04 - others may be comfortable using another
05:06 - one and so we couldn't guarantee that
05:08 - the application that we were building
05:10 - would run the same way in different
05:12 - environments and so all of this made our
05:15 - life in developing building and shipping
05:18 - the application really difficult so I
05:22 - needed something that could help us with
05:24 - the compatibility issue and something
05:26 - that will allow us to modify or change
05:28 - these components without affecting the
05:30 - other components and even modify the
05:32 - underlying operating systems as required
05:35 - and that search landed me on docker with
05:39 - docker I was able to run each component
05:42 - in a separate container with its own
05:44 - dependencies and its own libraries all
05:48 - on the same VM and the OS but within
05:51 - separate environments or containers we
05:53 - just had to build the docker
05:55 - configuration once and all our
05:57 - developers could now get started with a
05:59 - simple docker run command a respective
06:02 - of what the underlying operating system
06:03 - layer on all they needed to do was to
06:06 - make sure they had doc are installed on
06:08 - their systems so what are containers
06:10 - containers are completely isolated
06:13 - environments as in they can have their
06:15 - own processes for services their own
06:17 - network interfaces their own mounts just
06:20 - like washing machines except they all
06:22 - share the same OS kernel we will look at
06:25 - what that means in a bit but it's also
06:27 - important to note that containers are
06:29 - not new with docker containers have
06:31 - existed for about 10 years now and some
06:34 - of the different types of containers are
06:35 - Alexei Alexei Alexei FS etc
06:38 - docker utilizes Alex
06:41 - containers setting up these container
06:43 - environments is hard as they're very
06:45 - low-level
06:45 - and that is where docker offers a high
06:47 - level tool with several powerful
06:50 - functionalities making it really easy
06:52 - for end-users like us to understand how
06:55 - docker works let us revisit some basic
06:58 - concepts of operating systems first if
07:00 - you look at operating systems like
07:02 - Ubuntu Fedora Susi air scent OS they all
07:05 - consist of two things an OS kernel and a
07:09 - set of software the OS kernel is
07:11 - responsible for interacting with the
07:13 - underlying hardware while the OS kernel
07:15 - remains the same which is Linux in this
07:18 - case it's the software above it that
07:20 - makes these operating systems different
07:22 - this software may consist of a different
07:25 - user interface drivers compilers file
07:27 - managers developer tools etc so you have
07:30 - a common Linux kernel shared across all
07:33 - races and some custom software that
07:36 - differentiate operating systems from
07:38 - each other we said earlier that docker
07:41 - containers share the underlying kernel
07:43 - so what does that actually mean
07:44 - sharing the kernel let's say we have a
07:47 - system with an Ubuntu OS with docker
07:50 - installed on it docker can run any
07:52 - flavor of OS on top of it as long as
07:55 - they are all based on the same kernel in
07:57 - this case Linux if the underlying OS is
08:01 - Ubuntu docker can run a container based
08:04 - on another distribution like debian
08:06 - fedora SUSE or Sint OS each docker
08:09 - container only has the additional
08:11 - software that we just talked about in
08:14 - the previous slide that makes these
08:15 - operating systems different and docker
08:18 - utilizes the underlying kernel of the
08:20 - docker host which works with all OSS
08:23 - above so what is an OS that do not share
08:26 - the same kernel as this Windows and so
08:29 - you won't be able to run a Windows based
08:32 - container on a docker host with Linux on
08:34 - it for that you will require a docker on
08:37 - a Windows server now it is when I say
08:40 - this that most of my students go hey
08:43 - hold on there that's not true and they
08:46 - installed are on Windows run a container
08:47 - based on Linux and go see it's possible
08:50 - well when you install docker on Windows
08:53 - and run
08:54 - a Linux container on Windows you're not
08:57 - really running a Linux container on
08:59 - Windows Windows runs a Linux container
09:01 - on a Linux virtual machine under the
09:04 - hoods so it's really Linux container on
09:06 - Linux virtual machine on Windows we
09:09 - discuss more about this on the docker on
09:12 - Windows or Mac later during this course
09:15 - now you might ask
09:17 - isn't that a disadvantage then not being
09:19 - able to run another kernel on the OS the
09:23 - answer is no because unlike hypervisors
09:25 - docker is not meant to virtualize and
09:28 - run different operating systems and
09:29 - kernels on the same hardware the main
09:32 - purpose of docker is to package and
09:34 - container as applications and to ship
09:37 - them and to run them anywhere any times
09:40 - as many times as you want so that brings
09:43 - us to the differences between virtual
09:45 - machines and containers something that
09:47 - we tend to do is specially those from a
09:49 - virtualization background as you can see
09:52 - on the right in case of docker we have
09:54 - the underlying hardware infrastructure
09:57 - and then the OS and then docker
09:59 - installed on the OS docker then manages
10:02 - the containers that run with libraries
10:04 - and dependencies alone in case of
10:06 - virtual machines we have the hypervisor
10:08 - like ESX on the hardware and then the
10:12 - virtual machines on them as you can see
10:14 - each virtual machine has its own OS
10:16 - inside it then the dependencies and then
10:19 - the application the overhead causes
10:22 - higher utilization of underlying
10:24 - resources as there are multiple virtual
10:26 - operating systems and kernels running
10:28 - the virtual machines also consume higher
10:31 - disk space as each VM is heavy and is
10:34 - usually in gigabytes in size whereas
10:36 - docker containers are lightweight and
10:38 - are usually in megabytes in size this
10:41 - allows docker containers to boot up
10:43 - faster usually in a matter of seconds
10:45 - whereas VMs as we know takes minutes to
10:48 - boot up as it needs to boot up the
10:50 - entire operating system it is also
10:52 - important to note that docker has less
10:55 - isolation as more resources are shared
10:57 - between the containers like the kernel
10:59 - whereas VMs have complete isolation from
11:02 - each other since VMs don't rely on the
11:05 - underlying OS or kernel you can run
11:07 - different types of application
11:08 - occasions built on different OSS such as
11:10 - Linux based or windows-based apps on the
11:13 - same paralyzer so those are some
11:15 - differences between the two now having
11:18 - said that it's not an either container
11:20 - or virtual machine situation its
11:23 - containers and virtual machines now when
11:26 - you have large environments with
11:27 - thousands of application containers
11:29 - running on thousands of dog or hosts you
11:32 - will often see containers provisioned on
11:34 - virtual docker hosts that way we can
11:37 - utilize the advantages of both
11:39 - technologies we can use the benefits of
11:41 - virtualization to easily provision or
11:43 - decommission docker House serves as
11:45 - required at the same time make use of
11:47 - the benefits of docker to easily
11:50 - provision applications and quickly scale
11:52 - them as required but remember that in
11:55 - this case we will not be provisioning
11:57 - that many virtual machines as we used to
12:00 - before because earlier we provisioned a
12:04 - virtual machine for each application
12:06 - now you might provision a virtual
12:08 - machine for hundreds or thousands of
12:10 - containers so how is it done there are
12:13 - lots of containerized versions of
12:15 - applications readily available as of
12:18 - today so most organizations have their
12:20 - products containerized and available in
12:22 - a public dock or repository called
12:25 - docker hub or docker store for example
12:28 - you can find images of most common
12:30 - operating systems databases and other
12:32 - services and tools once you identify the
12:35 - images you need and you install docker
12:37 - on your hosts bringing up an application
12:40 - is as easy as running a docker run
12:43 - command with the name of the image in
12:45 - this case running a docker run ansible
12:47 - command will run an instance of ansible
12:49 - on the docker host similarly run an
12:51 - instance of MongoDB Redis and nodejs
12:54 - using the docker run command if we need
12:57 - to run multiple instances of the web
12:59 - service simply add as many instances as
13:02 - you need and configure a load balancer
13:04 - of some kind in the front in case one of
13:07 - the instances were to fail simply
13:10 - destroy that instance and launch anyone
13:12 - there are other solutions available for
13:15 - handling such cases that we will look at
13:17 - later during this course and for now
13:20 - don't focus too much on the command
13:22 - we'll get to that in a bit we've been
13:25 - talking about images and containers
13:28 - let's understand the difference between
13:29 - the two an image is a package or a
13:33 - template just like a VM template that
13:35 - you might have worked with in the
13:36 - virtualization world it is used to
13:38 - create one or more containers containers
13:41 - are running instances of images that are
13:44 - isolated and have their own environments
13:47 - and set of processors as we have seen
13:49 - before a lot of products have been
13:51 - derived already in case you cannot find
13:54 - what you're looking for you could create
13:56 - your own image and push it to docker hub
13:58 - repository making it available for
14:00 - public so if you look at it
14:03 - traditionally developers developed
14:05 - applications then they hand it over to
14:08 - ops team to deploy and manage it in
14:10 - production environments they do that by
14:13 - providing a set of instructions such as
14:15 - information about how the hosts must be
14:17 - set up what prerequisites are to be
14:19 - installed on the host and how the
14:20 - dependencies are to be configured etc
14:23 - since the ops team did not really
14:25 - develop the application on their own
14:27 - then struggle with setting it up when
14:30 - the hidden issue they worked with the
14:31 - developers to resolve it with docker the
14:35 - developers and operations teams work
14:37 - hand-in-hand to transform the guide into
14:40 - a docker file with both of their
14:42 - requirements this docker file is then
14:45 - used to create an image for their
14:47 - applications this image can now run on
14:50 - any host with docker installed on it and
14:52 - is guaranteed to run the same way
14:54 - everywhere so the ops team can now
14:57 - simply use the image to deploy the
14:58 - application since the image was already
15:01 - working when the developer built it and
15:03 - operations are have not modified it it
15:06 - continues to work the same way when
15:08 - deployed in production and that's one
15:11 - example of how a tool like docker
15:13 - contributes to the DevOps culture well
15:17 - that's it for now and in the upcoming
15:19 - lecture we will look at how to get
15:21 - started with docker
15:23 - [Music]
15:27 - we will now see how to get started with
15:29 - docker now docker has two editions the
15:33 - Community Edition and the Enterprise
15:35 - Edition the Community Edition is the set
15:38 - of free docker products the Enterprise
15:40 - Edition is the certified and supported
15:43 - container platform that comes with
15:45 - enterprise add-ons like the image
15:47 - management image security Universal
15:50 - control plane for managing and
15:52 - orchestrating container runtimes but of
15:54 - course these come with a price we will
15:57 - discuss more about container
15:59 - orchestration later in this course and
16:00 - along with some alternatives for now we
16:04 - will go ahead with the Community Edition
16:06 - the Community Edition is available on
16:09 - Linux Mac Windows or on cloud platforms
16:13 - like anti Bleus or either in the
16:16 - upcoming demo we will take a look at how
16:18 - to install and get started with docker
16:20 - on a Linux system now if you are on Mac
16:24 - or Windows you have two options either
16:27 - install a Linux VM using VirtualBox or
16:30 - some kind of virtualization platform and
16:33 - then follow along with the upcoming demo
16:35 - which is really the most easiest way to
16:37 - get started with docker the second
16:40 - option is to install docker Desktop for
16:43 - Mac or the docker desktop for Windows
16:45 - which are native applications so if that
16:48 - is really what you want like check out
16:50 - the docker for Mac and the windows
16:52 - sections towards the end of this course
16:54 - and then head back here once you are all
16:57 - set up
16:58 - we will now head over to a demo and we
17:01 - will take a look at how to install
17:03 - docker on a Linux machine
17:07 - in this demo we look at how to install
17:10 - and get started with docker first of all
17:13 - identify a system physical or virtual
17:15 - machine or laptop that has a supported
17:18 - operating system in my case I have an
17:21 - Ubuntu VM go to doctor comm and click on
17:27 - get docker
17:29 - you will be taken to the docker engine
17:31 - Community Edition page that is the free
17:35 - version that we are after from the
17:38 - left-hand menu select your system type I
17:41 - choose Linux in my case and then select
17:45 - your OS flavor
17:47 - I choose Ubuntu read through the
17:50 - prerequisites and requirements your abun
17:53 - to system must be 64-bit and one of
17:56 - these supported versions like disco
17:59 - cosmic Bionic or decennial in my case I
18:02 - have a bionic version to confirm view
18:06 - the Etsy release file next uninstall any
18:13 - older version if one exists so let's
18:16 - just make sure that there is none on my
18:19 - host so I'll just copy and paste that
18:21 - command and I confirm that there are no
18:25 - older version that exists on my system
18:28 - the next step is to set up repository
18:31 - and install the software now there are
18:34 - two ways to go about this the first is
18:37 - using the package manager by first
18:40 - updating the repository using the
18:42 - apt-get update command then installing
18:45 - the prerequisite packages and then
18:47 - adding Dockers of facial GPG keys and
18:50 - then installing docker but I'm not going
18:52 - to go that route there is an easier way
18:55 - if you scroll all the way to the bottom
18:58 - you will find the instructions to
19:00 - install docker using the convenience
19:02 - script it's a script that automates the
19:06 - entire installation process and works on
19:08 - most operating systems run the first
19:11 - command to download a copy of the script
19:14 - and then run the second command to
19:16 - execute the script to install docker
19:19 - automatically give it a few minutes to
19:21 - complete the installation the
19:31 - installation is now successful that has
19:33 - now checked the version of docker using
19:36 - the darker version command we've
19:38 - installed version 19.0 3.1 we will now
19:43 - run a simple container to ensure
19:45 - everything is working as expected for
19:48 - this head over to docker hub at hub
19:51 - docker calm here you will find a list of
19:54 - the most popular docker images like
19:56 - engine eggs MongoDB Alpine nodejs Redis
20:01 - etc let's search for a fun image called
20:04 - we'll say we'll say is Dockers version
20:08 - of cows a which is basically a simple
20:10 - application that trends a cow saying
20:14 - something in this case it happens to be
20:16 - a whale copy the docker run command
20:20 - given here remember to add a sudo and we
20:24 - will change the message to hello world
20:42 - I'm running this command docker pulls
20:44 - the image of the whales II application
20:47 - from docker hub and runs it and we have
20:50 - our avail saying hello great we're all
20:55 - set remember for the purpose of this
20:59 - course you don't really need to set up a
21:01 - docker system on your own we provide
21:03 - hands-on labs that you will get access
21:05 - to but if you wish to experiment on your
21:08 - own and follow along feel free to do so
21:10 - [Music]
21:14 - we now look at some of the docker
21:16 - commands at the end of this lecture you
21:18 - will go through a hands-on quiz where
21:21 - you will practice working with these
21:22 - commands let's start by looking at
21:25 - docker run command the docker run
21:27 - command is used to run a container from
21:30 - an image running the docker run nginx
21:32 - command will run an instance of the
21:34 - nginx
21:35 - application from the docker host if it
21:37 - already exists if the image is not
21:40 - present on the host it will go out to
21:42 - docker hub and pull the image down but
21:45 - this is only done the first time for the
21:48 - subsequent executions the same image
21:50 - will be reused the docker PS command
21:54 - lists all running containers and some
21:56 - basic information about them such as the
21:59 - container ID the name of the image we
22:01 - used to run the containers the current
22:03 - status and the name of the container
22:05 - each container automatically gets a
22:08 - random ID and name created for it by
22:11 - docker which in this case is silly Samet
22:14 - to see all containers running or not use
22:18 - the - a option this outputs all running
22:21 - as well as previously stopped or exited
22:24 - containers will talk about the command
22:26 - and port fields shown in this output
22:28 - later in this course for now let's just
22:31 - focus on the basic command to stop a
22:34 - running container use the Tucker stop
22:36 - command but you must provide either the
22:38 - container ID or the continue name in the
22:41 - stop command if you're not sure of the
22:43 - name run the docker PS command to get it
22:46 - on success you will see the name printed
22:48 - out and running docker PS again will
22:51 - show no running containers running
22:53 - docker PS - a
22:55 - however shows the container silly summit
22:58 - and that it is in an accident states a
23:01 - few seconds ago now what if we don't
23:04 - want this container lying around
23:06 - consuming space what if we want to get
23:08 - rid of it for good use the docker RM
23:12 - command to remove a stopped or exited
23:14 - container permanently if it prints the
23:18 - name back we're good
23:19 - run the docker PS command again to
23:21 - verify that it's no longer present good
23:24 - but what about the nginx image that was
23:27 - downloaded at first we're not using that
23:29 - anymore so how do we get rid of that
23:32 - image but first how do we see a list of
23:35 - images present on our hosts run the
23:38 - docker images command to see a list of
23:40 - available images and their sizes on our
23:43 - hosts we have four images the nginx
23:46 - Redis Ubuntu and Alpine we will talk
23:49 - about tags later in this course when we
23:51 - discuss about images to remove an image
23:55 - that you no longer plan to use run the
23:58 - docker RM I command remember you must
24:01 - ensure that no containers are running
24:03 - off of that image before attempting to
24:05 - remove the image you must stop and
24:08 - delete all dependent containers to be
24:10 - able to delete an image when we ran the
24:13 - docker run command earlier it downloaded
24:16 - the Ubuntu image as it couldn't find one
24:19 - locally what if we simply want to
24:22 - download the image and keep so when we
24:25 - run the run docker run command we don't
24:28 - want to wait for it to download use the
24:30 - docker pull command to only pull the
24:33 - image and not run the container so in
24:35 - this case the docker pull a bunt oakum
24:37 - and pulls the Ubuntu image and stores it
24:41 - on our host let's look at another
24:43 - example say you were to run a docker
24:46 - container from an Ubuntu image when you
24:48 - run the docker run Ubuntu command it
24:51 - runs an instance of Ubuntu image and
24:53 - exits immediately if you were to list
24:56 - the Irani containers you wouldn't see
24:58 - the container running if you list all
25:00 - containers including those that are
25:01 - stopped you will see that the new
25:03 - container Iran is in an exited state now
25:07 - why is that
25:08 - unlike virtual machines containers are
25:11 - not meant to host an operating system
25:14 - containers are meant to run a specific
25:17 - task or process such as to host an
25:20 - instance of a web server or application
25:22 - server or a database or simply to carry
25:25 - some kind of computation or analysis
25:27 - tasks once the task is complete the
25:31 - container exits a container only lives
25:34 - as long as the process inside it is
25:37 - alive if the web service inside the
25:39 - container is stopped or crash then the
25:42 - container exits this is why when you run
25:45 - a container from an Ubuntu image it
25:48 - stops immediately because a bundle is
25:50 - just an image of an operating system
25:52 - that is used as the base image for other
25:55 - applications there is no process or
25:58 - application running in it by default if
26:01 - the image isn't running any service as
26:04 - is the case with Ubuntu you could
26:06 - instruct docker to run a process with
26:09 - the docker run command for example a
26:11 - sleep command with a duration of 5
26:14 - seconds when the container starts it
26:17 - runs to sleep command and goes into
26:19 - sleep for 5 seconds post which the sleep
26:22 - command exits and the container stops
26:25 - what we just saw was executing a command
26:28 - when we run the container but what if we
26:31 - would like to execute a command on a
26:32 - running container for example when
26:35 - around the docker PS command I can see
26:37 - that there is a running container which
26:39 - uses the bunch of image and sleeps 400
26:42 - seconds let's say I would like to see
26:45 - the contents of a file inside this
26:47 - particular container I could use the
26:50 - docker exec command to execute a command
26:53 - on my docker container in this case to
26:55 - print the contents of the Etsy hosts
26:59 - file finally let's look at one more
27:02 - option before we head over to the
27:04 - practice exercises I'm now going to run
27:07 - a docker image I developed for a simple
27:09 - web application the repository name is
27:12 - code cloud slash simple web app it runs
27:16 - a simple web server that listens on port
27:18 - 8080
27:20 - you run a docker run command like this
27:22 - it runs in the foreground or in an
27:25 - attached mode meaning you will be
27:28 - attached to the console or the standard
27:30 - out of the docker container and you will
27:33 - see the output of the web service on
27:35 - your screen
27:36 - you won't be able to do anything else on
27:38 - this console other than view the output
27:40 - until this docker container stops it
27:43 - won't respond to your inputs press the
27:46 - ctrl + C combination to stop the
27:49 - container and the application hosted on
27:52 - the container exits and you get back to
27:54 - your prompt another option is to run the
27:58 - docker container in the detached mode by
28:01 - providing the dash D option this will
28:05 - run the docker container in the
28:06 - background mode and you will be back to
28:09 - your prompt immediately the container
28:11 - will continue to run in the backend run
28:14 - the docker PS command to view the
28:16 - running container now if you would like
28:19 - to attach back to the running container
28:21 - later run the docker attach command and
28:25 - specify the name or ID of the docker
28:27 - container now remember if you are
28:30 - specifying the ID of a container in any
28:33 - docker command you can simply provide
28:35 - the first few characters alone just so
28:38 - it is different from the other container
28:40 - IDs on the host in this case I specify a
28:44 - 0 for 3d now don't worry about accessing
28:49 - the UI of the webserver for now we will
28:52 - look more into that in the upcoming
28:54 - lectures for now let's just understand
28:56 - the basic commands will now get our
28:58 - hands dirty with the docker CLI so let's
29:01 - take a look at how to access the
29:03 - practice lab environments next
29:08 - let me now walk you through the hands-on
29:12 - lab practice environment the links to
29:15 - access the labs associated with this
29:17 - course are available at cold cloud at
29:20 - code cloud comm slash P slash docker
29:24 - dash labs this link is also given in the
29:27 - description of this video
29:28 - once you're on this page use the links
29:30 - given there to access the labs
29:32 - associated to your lecture each lecture
29:36 - has its own lab so remember to choose
29:39 - the right lab for your lecture
29:41 - the labs open up right in your browser I
29:44 - would recommend to use google chrome
29:47 - while working with the labs the
29:50 - interface consists of two parts a
29:52 - terminal on the left and a quiz portal
29:54 - on the right the cooze portal on the
29:56 - right gives you challenges to solve
29:58 - follow the quiz and try and answer the
30:00 - questions asked and complete the tasks
30:03 - given to you each scenario consists of
30:05 - anywhere from 10 to 20 questions that
30:08 - needs to be answered within 30 minutes
30:10 - to an hour at the top you have the
30:13 - question numbers below that is the
30:15 - remaining time for your lab below that
30:17 - is the question if you are not able to
30:19 - solve the challenge look for hints in
30:21 - the hints section you may skip a
30:24 - question by hitting the skip button in
30:25 - the top right corner but remember that
30:28 - you will not be able to go back to a
30:30 - previous question once you have skipped
30:32 - if the quiz portal gets stuck for some
30:35 - reason click on the quiz portal tab at
30:38 - the top to open the quiz portal in a
30:41 - separate window the terminal gives you
30:48 - access to a real system running Tucker
30:50 - you can run any docker command here and
30:53 - run your own containers or applications
30:55 - you would typically be running commands
30:57 - to solve the tasks assigned in the quiz
30:59 - portal you may play around and
31:02 - experiment with this environment but
31:03 - make sure you do that after you've gone
31:06 - through the quiz so that your work does
31:08 - not interfere with the tasks provided by
31:10 - the quiz so let me walk you through a
31:13 - few questions there are two types of
31:16 - questions each lab scenario starts with
31:19 - a set of export
31:20 - retrieve multiple-choice questions where
31:22 - you're asked to explore and find
31:24 - information in the given environment and
31:26 - select the right answer this is to get
31:29 - you familiarized with a setup you're
31:31 - then asked to perform tasks like run a
31:33 - container stop them delete them build
31:36 - your own image etc here the first
31:40 - question asks us to find the version of
31:42 - docker server engine running on the host
31:44 - run the docker reversion command in the
31:47 - terminal and identify the right version
31:49 - then select the appropriate option from
31:52 - the given choices another example is the
31:56 - fourth question where it asks you to run
31:59 - a container using the Redis image if
32:03 - you're not sure of the command click on
32:05 - hints and it will show you a hint we now
32:09 - run a Redis container using the docker
32:11 - run readies command wait for the
32:13 - container to run once done click on
32:15 - check to check your work we have now
32:18 - successfully completed the task
32:20 - similarly follow along and complete all
32:23 - tasks once the lab exercise is completed
32:25 - remember to leave a feedback and let us
32:28 - know how it went a few things to note
32:30 - these are publicly accessible labs that
32:33 - anyone can access so if you catch
32:35 - yourself logged out during a peak hour
32:38 - please wait for some time and try again
32:41 - also remember to not store any private
32:45 - or confidential data on these systems
32:47 - remember that this environment is for
32:50 - learning purposes only and is only alive
32:53 - for an hour after which the lab is
32:55 - destroyed so does all your work but you
32:59 - may start over and access these labs as
33:01 - many times as you want until you feel
33:04 - confident I will also post solutions to
33:07 - these lab quizzes so if you run into
33:09 - issues you may refer to those that's it
33:12 - for now head over to the first challenge
33:14 - and I will see you on the other side
33:18 - [Music]
33:22 - we will now look at some of the other
33:24 - docker run commands at the end of this
33:27 - lecture you will go through a hands-on
33:29 - quiz where you will practice working
33:30 - with these commands we learned that we
33:33 - could use the docker run Redis command
33:36 - to run the container running a Redis
33:38 - service in this case the latest version
33:41 - of Redis which happens to be 5.0 to 5 as
33:44 - of today but what if we want to run
33:47 - another version of Redis like for
33:49 - example and older versions say 4.0 then
33:53 - you specify the version separated by a
33:55 - colon this is called a tag in that case
34:00 - docker pulls an image of the 4.0 version
34:03 - of Redis and runs that also notice that
34:08 - if you don't specify any tag as in the
34:11 - first command docker will consider the
34:13 - default tag to be latest latest is a tag
34:17 - associated to the latest version of that
34:20 - software which is governed by the
34:22 - authors of that software so as a user
34:26 - how do you find information about these
34:28 - versions and what is the latest at
34:31 - docker hub com look up an image and you
34:36 - will find all the supported tags in its
34:38 - description each version of the software
34:41 - can have multiple short and long tags
34:43 - associated with it as seen here in this
34:48 - case the version fight of 0.5 also has
34:51 - the latest tag on it
34:54 - let's now look at inputs I have a simple
34:58 - prompt application that when run asks
35:01 - for my name and on entering my name
35:03 - prints a welcome message if I were to
35:07 - docker eyes this application and run it
35:09 - as a docker container like this it
35:11 - wouldn't wait for the prompt it just
35:14 - prints whatever the application is
35:16 - supposed to print on standard out that
35:19 - is because by default the docker
35:22 - container does not listen to a standard
35:24 - input even though you are attached to
35:27 - its console it is not able to read any
35:30 - input from you it doesn't have a
35:32 - terminal to read inputs
35:34 - it runs in a non interactive mode if you
35:38 - would like to provide your input
35:40 - you must map the standard input of your
35:43 - host to the docker container using the -
35:45 - I parameter the - I parameter is for
35:48 - interactive mode and when I input my
35:51 - name it prints the expected output but
35:55 - there is something still missing from
35:56 - this the prompt when we run the app at
36:01 - first it asked us for our name but when
36:05 - docker iced that prompt is missing even
36:08 - though it seems to have accepted my
36:10 - input that is because the application
36:13 - prompt on the terminal and we have not
36:16 - attached to the containers terminal for
36:20 - this use the - T option as well the - T
36:23 - stands for a sudo terminal so with the
36:28 - combination of - int we are now attached
36:31 - to the terminal as well as in an
36:33 - interactive mode on the container we
36:36 - will now look at port mapping or port
36:39 - publishing on containers let's go back
36:41 - to the example where we run a simple web
36:44 - application in a docker container on my
36:46 - docker host remember the underlying host
36:49 - where docker is installed is called
36:51 - docker host or docker engine when we run
36:54 - a containerized web application it runs
36:57 - and we are able to see that the server
36:59 - is running but how does a user access my
37:02 - application as you can see my
37:05 - application is listening on port 5,000
37:08 - so I could access my application by
37:10 - using port 5000 but what IP do I use to
37:14 - access it from a web browser there are
37:17 - two options available one is to use the
37:20 - IP of the docker container every docker
37:23 - container gets an IP assigned by default
37:25 - in this case it is 172 dot 17.0 - but
37:30 - remember that this is an internal IP and
37:33 - is only accessible within the docker
37:35 - host so if you open a browser from
37:38 - within the docker host you can go to
37:40 - http colon forward slash forward slash
37:44 - 172 dot 17 dot 0 dot 1
37:48 - : 5,000 to access the IP address but
37:52 - since this is an internal IP users
37:55 - outside of the docker host cannot access
37:57 - it using this IP for this we could use
38:01 - the IP of the docker host which is one
38:03 - ninety two dot one sixty eight dot 1.5
38:06 - but for that to work you must have
38:09 - mapped the port inside the docker
38:11 - container to a free port on the docker
38:14 - host for example if I want the users to
38:17 - access my application through port 80 on
38:19 - my docker host I could map port 80 of
38:22 - local host to port 5000 on the docker
38:26 - container using the dash P parameter in
38:29 - my run command like this and so the user
38:34 - can access my application by going to
38:36 - the URL HTTP colon slash slash one
38:40 - ninety two dot one sixty eight dot one
38:42 - dot five colon 80 and all traffic on
38:46 - port 80 on my daugher host will get
38:49 - routed to port 5,000 inside the docker
38:52 - container this way you can run multiple
38:55 - instances of your application and map
38:58 - them to different ports on the docker
39:00 - host or run instances of different
39:03 - applications on different ports for
39:05 - example in this case and running an
39:07 - instance of MySQL that runs a database
39:10 - on my host and listens on the default
39:12 - MySQL port which happens to be three
39:15 - three zero six or
39:17 - another instance of MySQL on another
39:19 - port eight three zero six so you can run
39:22 - as many applications like this and map
39:25 - them to as many ports as you want and of
39:28 - course you cannot map to the same port
39:30 - on the docker host more than once we
39:34 - will discuss more about port mapping and
39:36 - networking of containers in the network
39:38 - lecture later on let's now look at how
39:41 - data is persisted in a docker container
39:44 - for example let's say you were to run a
39:47 - MySQL container when databases and
39:50 - tables are created the data files are
39:53 - stored in location /wor Labe MySQL
39:57 - inside the docker container remember the
40:00 - docker container has its own
40:02 - isolated filesystem and any changes to
40:04 - any files happen within the container
40:07 - let's assume you dump a lot of data into
40:10 - the database what happens if you were to
40:13 - delete the MySQL container and remove it
40:16 - as soon as you do that the container
40:19 - along with all the data inside it gets
40:22 - blown away
40:22 - meaning all your data is gone if you
40:26 - would like to persist data you would
40:28 - want to map a directory outside the
40:31 - container on the docker host to a
40:33 - directory inside the container in this
40:36 - case I create a directory called /opt
40:39 - slash data dir and map that to var Lib
40:44 - MySQL inside the docker container using
40:47 - the - V option and specifying the
40:50 - directory on the door host followed by a
40:52 - colon and the directory inside the clock
40:55 - or container
40:56 - this way when docker container runs it
40:59 - will implicitly mount the external
41:02 - directory to a folder inside the docker
41:04 - container this way all your data will
41:07 - now be stored in the external volume at
41:10 - /opt slash data directory and thus will
41:14 - remain even if you delete the docker
41:16 - container the docker PS command
41:20 - is good enough to get basic details
41:22 - about containers like their names and
41:24 - ID's but if you would like to see
41:26 - additional details about a specific
41:28 - container use the docker inspect command
41:31 - and provide the container name or ID it
41:34 - returns all details of a container in a
41:37 - JSON format such as the state Mounds
41:40 - configuration data network settings etc
41:43 - remember to use it when you're required
41:45 - to find details on a container and
41:48 - finally how do we see the logs of a
41:51 - container we ran in the background for
41:53 - example I ran my simple web application
41:55 - using the - D parameter and it ran the
41:58 - container in a detached mode how do I
42:01 - view the logs which happens to be the
42:03 - contents written to the standard out of
42:05 - that container use the docker logs
42:08 - command and specify the container ID or
42:11 - name like this well
42:14 - sit for this lecture head over to the
42:16 - challenges and practice working with
42:18 - docker commands so to start with a
42:26 - simple web application written in Python
42:28 - this piece of code is used to create a
42:31 - web application that displays a web page
42:33 - with a background color if you look
42:36 - closely into the application code you
42:38 - will see a line that sets the background
42:40 - color to red now that works just fine
42:43 - however if you decide to change the
42:46 - color in the future you will have to
42:48 - change the application code it is a best
42:51 - practice to move such information out of
42:53 - the application code and into say an
42:56 - environment variable called app color
42:59 - the next time you run the application
43:01 - set an environment variable called app
43:04 - color to a desired value and the
43:06 - application now has a new color once
43:09 - your application gets packaged into a
43:11 - docker image he would then run it with
43:14 - the docker run command followed by the
43:16 - name of the image however if you wish to
43:19 - pass the environment variable as we did
43:21 - before he would now use the docker run
43:24 - commands - II option to set an
43:27 - environment variable within the
43:29 - container to deploy multiple containers
43:32 - with different colors he would run the
43:34 - docker command multiple times and set a
43:37 - different value for the environment
43:39 - variable each time so how do you find
43:42 - the environment variable set on a
43:44 - container that's already running use the
43:48 - docker inspect command to inspect the
43:51 - properties of a running container under
43:53 - the config section you will find the
43:55 - list of environment variables set on the
43:58 - container well that's it for this
44:00 - lecture on configuring environment
44:02 - variables in docker
44:14 - [Music]
44:17 - hello and welcome to this lecture on
44:20 - docker images in this lecture we're
44:23 - going to see how to create your own
44:26 - image now before that why would you need
44:30 - to create your own image it could either
44:32 - be because you cannot find a component
44:35 - or a service that you want to use as
44:37 - part of your application on docker hub
44:39 - already or you and your team decided
44:42 - that the application you're developing
44:44 - will be derived for ease of shipping and
44:47 - deployment in this case I'm going to
44:50 - containerize an application a simple web
44:54 - application that I have built using the
44:57 - Python flask framework first we need to
45:01 - understand what we our container izing
45:03 - or what application we are creating an
45:05 - image for and how the application is
45:08 - built so start by thinking what you
45:10 - might do if you want to deploy the
45:12 - application manually we write down the
45:15 - steps required in the right order and
45:17 - creating an image for a simple web
45:19 - application if I were to set it up
45:22 - manually I would start with an operating
45:25 - system like Ubuntu then update the
45:28 - source repositories using the apt
45:30 - command then install dependencies using
45:33 - the apt command then install Python
45:35 - dependencies using the PIP command then
45:38 - copy over the source code of my
45:40 - application to a location like opt and
45:43 - then finally run the web server using
45:46 - the flask command now that I have the
45:49 - instructions create a docker file using
45:52 - these here's a quick overview of the
45:54 - process of creating your own image first
45:57 - create a docker file named docker file
46:00 - and write down the instructions for
46:02 - setting up your application in it such
46:05 - as installing dependencies where to copy
46:08 - the source code from and to and what the
46:11 - entry point of the application is etc
46:13 - once done build your image using the
46:17 - docker build command and specify the
46:19 - docker file as input as well as a tag
46:22 - for the image this will create an image
46:25 - locally on your system to make it
46:28 - available on the public docker hub
46:30 - registry run the docker push command and
46:34 - specify the name of the image you just
46:36 - created in this case the name of the
46:40 - image is my account name which is M
46:43 - Amjad followed by the image name which
46:47 - is my custom app now let's take a closer
46:52 - look at that docker file docker file is
46:55 - a text file written in a specific format
46:57 - that docker can understand it's in an
47:00 - instruction and arguments format for
47:03 - example in this docker file everything
47:06 - on the left in caps is an instruction in
47:09 - this case from run copy and entry point
47:13 - are all instructions each of these
47:16 - instruct docker to perform a specific
47:19 - action while creating the image
47:21 - everything on the right is an argument
47:23 - to those instructions the first line
47:27 - from Ubuntu defines what the base OS
47:30 - should be for this container every
47:33 - docker image must be based off of
47:35 - another image either an OS or another
47:39 - image that was created before based on
47:41 - an OS you can find official releases of
47:44 - all operating systems on docker hub it's
47:47 - important to note that all docker files
47:49 - must start with a from instruction the
47:53 - run instruction instructs docker to run
47:56 - a particular command on those base
47:58 - images so at this point docker runs the
48:01 - apt-get update commands to fetch the
48:04 - updated packages and installs required
48:07 - dependencies on the image then the copy
48:10 - instruction copies files from the local
48:12 - system onto the docker image in this
48:15 - case the source code of our application
48:17 - is in the current folder and I will be
48:19 - copying it over to the location opt
48:22 - source code inside the docker image and
48:25 - finally entry point allows us to specify
48:28 - a command that will be run when the
48:31 - image is run as a container
48:35 - when docker builds the images it builds
48:38 - this in a layered architecture each line
48:40 - of instruction creates a new layer in
48:43 - the docker image with just the changes
48:45 - from the previous layer for example the
48:48 - first layer is a base Ubuntu OS followed
48:51 - by the second instruction that creates a
48:55 - second layer which installs all the apt
48:57 - packages and then the third instruction
49:00 - creates a third layer with the Python
49:01 - packages followed by the fourth layer
49:04 - that copies the source code over and the
49:06 - final layer that updates the entry point
49:09 - of the image since each layer only
49:11 - stores the changes from the previous
49:13 - layer it is reflected in the size as
49:16 - well if you look at the base open to
49:19 - image it is around 120 MB in size the
49:23 - apt packages that I install is around
49:25 - 300 MB and the remaining layers are
49:28 - small you could see this information if
49:31 - you run the docker history command
49:32 - followed by the image name when you run
49:40 - the
49:41 - Bilka man you could see the various
49:43 - steps involved and the result of each
49:45 - task all the layers built are cast
49:48 - so the layered architecture helps you
49:50 - restart docker built from that
49:52 - particular step in case it fails or if
49:55 - you were to add new steps in the build
49:57 - process you wouldn't have to start all
49:59 - over again all the layers built are
50:06 - cached by docker so in case a particular
50:09 - step was to fail for example in this
50:12 - case step three failed and you were to
50:15 - fix the issue and rerun docker build it
50:19 - will reuse the previous layers from
50:21 - cache and continue to build the
50:23 - remaining layers the same is true if you
50:26 - were to add additional steps in the
50:29 - docker file this way rebuilding your
50:32 - image is faster and you don't have to
50:35 - wait for docker to rebuild the entire
50:37 - image each time this is helpful
50:40 - especially when you update source code
50:42 - of your application as it may change
50:44 - more frequently only the layers above
50:47 - the updated layers needs to be rebuilt
50:54 - we just saw a number of products
50:57 - containerized such as databases
51:00 - development tools operating systems etc
51:03 - but that's just not it you can
51:06 - containerized almost all of the
51:08 - application even simple ones like
51:10 - browsers or utilities like curl
51:13 - applications like Spotify Skype etc
51:17 - basically you can containerize
51:19 - everything and going forward and see
51:22 - that's how everyone is going to run
51:24 - applications nobody is going to install
51:27 - anything anymore going forward instead
51:30 - they're just going to run it using
51:33 - docker and when they don't need it
51:35 - anymore
51:35 - get rid of it easily without having to
51:38 - clean up too much
51:40 - [Music]
51:44 - in this lecture we will look at commands
51:48 - arguments and entry points in docker
51:50 - let's start with a simple scenario say
51:54 - you were to run a docker container from
51:56 - an Ubuntu image when you run the docker
51:58 - run Ubuntu command it runs an instance
52:01 - of Ubuntu image and exits immediately if
52:04 - you were to list the running containers
52:06 - you wouldn't see the container running
52:08 - if you list all containers including
52:11 - those that are stopped you will see that
52:13 - the new container
52:14 - you ran is in an exited state now why is
52:18 - that unlike virtual machines containers
52:22 - are not meant to host an operating
52:24 - system containers are meant to run a
52:27 - specific task or process such as to host
52:30 - an instance of a web server or
52:32 - application server or a database or
52:35 - simply to carry out some kind of
52:37 - computation or analysis once the task is
52:40 - complete the container exits a container
52:43 - only lives as long as the process inside
52:47 - it is alive if the web service inside
52:50 - the container is dot or crashes the
52:53 - container exits so who defines what
52:56 - process is run within the container if
52:59 - you look at the docker file for popular
53:01 - docker images like ng INX you will see
53:04 - an instruction called CMD which stands
53:06 - for command that defines the program
53:09 - that will be run within the container
53:11 - when it starts for the ng INX image it
53:14 - is the ng INX command for the MySQL
53:17 - image it is the MySQL d command what we
53:21 - tried to do earlier was to run a
53:23 - container with a plain Ubuntu operating
53:25 - system let us look at the docker file
53:28 - for this image you will see that it uses
53:32 - bash as the default command now bash is
53:36 - not really a process like a web server
53:38 - or database server it is a shell that
53:42 - listens for inputs from a terminal if it
53:45 - cannot find the terminal it exits when
53:48 - we ran the ubuntu container earlier
53:50 - docker created a container from the
53:52 - ubuntu image
53:54 - and launched the bash program by default
53:56 - docker does not attach a terminal to a
54:00 - container when it is run and so the bash
54:03 - program does not find a terminal and so
54:06 - it exits since the process that was
54:09 - started when the container was created
54:11 - finished the container exits as well so
54:15 - how do you specify a different command
54:18 - to start the container one option is to
54:21 - append a command to the docker run
54:23 - command and that way it overrides the
54:26 - default command specified within the
54:28 - image in this case I run the docker run
54:32 - Ubuntu command with the sleep 5 command
54:34 - as the added option this way when the
54:38 - container starts it runs the sleep
54:40 - program waits for 5 seconds and then
54:43 - exits but how do you make that change
54:45 - permanent say you want the image to
54:48 - always run the sleep command when it
54:50 - starts you would then create your own
54:53 - image from the base ubuntu image and
54:55 - specify a new command there are
54:59 - different ways of specifying the command
55:01 - either the command simply as is in a
55:04 - shell form or in a JSON array format
55:07 - like this but remember when you specify
55:10 - in a JSON array format the first element
55:13 - in the array should be the executable in
55:16 - this case the sleep program do not
55:19 - specify the command and parameters
55:21 - together like this the command and its
55:25 - parameters should be separate elements
55:27 - in the list so I now build my new image
55:31 - using the docker build command and name
55:34 - it as Ubuntu sleeper I could now simply
55:37 - run the docker ubuntu sleeper command
55:40 - and get the same results it always
55:43 - sleeps for 5 seconds and exits but what
55:47 - if I wish to change the number of
55:49 - seconds it sleeps currently it is
55:52 - hard-coded to 5 seconds as we learned
55:55 - before one option is to run the docker
55:58 - run command with the new command
56:00 - appended to it in this case sleep 10 and
56:03 - so the command that will be run at
56:05 - startup will be sleep 10
56:07 - but it doesn't look very good the name
56:10 - of the image
56:11 - ubuntu sleeper in itself implies that
56:14 - the container will sleep so we shouldn't
56:16 - have to specify the sleep command again
56:18 - instead we would like it to be something
56:21 - like this docker run ubuntu sleeper 10
56:25 - we only want to pass in the number of
56:27 - seconds the container should sleep and
56:29 - sleep command should be invoked
56:31 - automatically and that is where the
56:34 - entry point instructions comes into play
56:37 - the entry point instruction is like the
56:40 - command instruction as in you can
56:42 - specify the program that will be run
56:44 - when the container starts and whatever
56:47 - you specify on the command line in this
56:49 - case 10 will get appended to the entry
56:52 - point so the command that will be run
56:55 - when the container starts is sleep 10 so
56:59 - that's the difference between the two in
57:01 - case of the CMD instruction the command
57:04 - line parameters passed will get replaced
57:07 - entirely whereas in case of entry point
57:10 - the command line parameters will get
57:12 - appended now in the second case what if
57:16 - I run the ubuntu sleeper image command
57:19 - without appending the number of seconds
57:21 - then the command at startup will be just
57:25 - sleep and you get the error that the
57:28 - operand is missing so how do you
57:30 - configure a default value for the
57:33 - command if one was not specified in the
57:36 - command line that's where you would use
57:38 - both entry point as well as the command
57:41 - instruction in this case the command
57:43 - instruction will be appended to the
57:46 - entry point instruction so at startup
57:49 - the command would be sleep 5 if you
57:51 - didn't specify any parameters in the
57:53 - command line if you did then that will
57:56 - overwrite the command instruction and
57:58 - remember for this to happen you should
58:01 - always specify the entry point and
58:03 - command instructions in a JSON format
58:07 - finally what if you freely really want
58:10 - to modify the entry point during runtime
58:13 - say from sleep to an imaginary sleep 2.0
58:17 - command
58:18 - well in that case you can override it by
58:21 - using the entry point option in the
58:24 - docker run command the final command at
58:27 - startup would then be sleep 2.0 10 well
58:32 - that's it for this lecture and I will
58:34 - see you in the next we now look at
58:43 - networking in docker when you install
58:47 - docker it creates three networks
58:49 - automatically bridge no and host bridge
58:54 - is the default network a container gets
58:57 - attached to if you would like to
58:59 - associate the container with any other
59:02 - network you specify the network
59:04 - information using the network command
59:06 - line parameter like this we will now
59:10 - look at each of these networks the BRIT
59:14 - network is a private internal network
59:17 - created by docker on the host all
59:20 - containers attached to this network by
59:22 - default and they get an internal IP
59:25 - address usually in the range 170 2.17
59:28 - series the containers can access each
59:32 - other using this internal IP if required
59:35 - to access any of these containers from
59:38 - the outside world map the ports of these
59:41 - containers to ports on the docker host
59:44 - as we have seen before another way to
59:48 - access the containers externally is to
59:50 - associate the container to the hosts
59:52 - network this takes out any network
59:55 - isolation between the docker host and
59:57 - the docker container meaning if you were
60:00 - to run a web server on port 5,000 in a
60:03 - web app container it is automatically as
60:05 - accessible on the same port externally
60:08 - without requiring any port mapping as
60:11 - the web container uses the hosts network
60:14 - this would also mean that unlike before
60:17 - you will now not be able to run multiple
60:20 - web containers on the same host on the
60:23 - same port as the ports are now common to
60:26 - all containers in the host network with
60:31 - non-network the containers are not
60:34 - attached to any network and doesn't have
60:37 - any access to the external network or
60:40 - other containers they run in an isolated
60:44 - Network so we just saw the default burst
60:49 - Network with the network ID 170 2.72 0.1
60:53 - so all containers associated to this
60:56 - default network will be able to
60:58 - communicate to each other but what if we
61:01 - wish to isolate the containers within
61:03 - the docker host for example the first
61:06 - two web containers on internal network
61:08 - 172 and the second two containers on a
61:11 - different internal network like 182 by
61:16 - default docker only creates one internal
61:19 - bridge Network we could create our own
61:22 - internal network using the command
61:24 - docker network create and specified the
61:27 - driver which is bridge in this case and
61:29 - the subnet for that network followed by
61:31 - the custom isolated network name run the
61:35 - docker network LS command to list all
61:37 - networks so how do we see the network
61:41 - settings and the IP address assigned to
61:43 - an existing container run the docker
61:46 - inspect command with the ID or name of
61:49 - the container and you will find a
61:51 - section on network settings there you
61:54 - can see the type of network the
61:56 - container is attached to is internal IP
61:58 - address MAC address and other settings
62:04 - containers can reach each other using
62:06 - their names for example in this case I
62:09 - have a webserver and a MySQL database
62:12 - container running on the same node how
62:15 - can I get my web server to access the
62:17 - database on the database container one
62:20 - thing I could do is to use the internal
62:23 - IP address signed to the MySQL container
62:25 - which in this case is 170 2.72 0.3 but
62:30 - that is not very ideal because it is not
62:32 - guaranteed that the container will get
62:35 - the same IP when the system reboots the
62:39 - right way to do it is to use the
62:41 - container name all containers in a
62:44 - docker host
62:45 - can resolve each other with the name of
62:48 - the container docker has a built-in DNS
62:51 - server that helps the containers to
62:53 - resolve each other using the container
62:56 - name note that the built in DNS server
62:59 - always runs at address 127 dot 0 dot 0
63:03 - dot 11 so how does docker implement
63:07 - networking what's the technology behind
63:10 - it like how were the containers isolated
63:13 - within the host docker uses network
63:17 - namespaces that creates a separate name
63:20 - space for each container it then uses
63:23 - virtual Ethernet pairs to connect
63:26 - containers together well that's all we
63:30 - can talk about it for now more about
63:32 - these or advanced concepts that we
63:35 - discussed in the advanced course on
63:37 - docker on code cloud that's all for now
63:41 - from this lecture on networking head
63:44 - over to the practice tests and practice
63:45 - working with networking in docker I will
63:49 - see you in the next lecture hello and
63:57 - welcome to this lecture and we are
63:59 - learning advanced docker concepts in
64:02 - this lecture we're going to talk about
64:03 - docker storage drivers and file systems
64:06 - we're going to see where and how docker
64:09 - stores data and how it manages file
64:12 - systems of the containers let us start
64:15 - with how docker stores data on the local
64:18 - file system
64:19 - when you install docker on a system it
64:23 - creates this folder structure at where
64:26 - lib docker you have multiple folders
64:29 - under it called a ufs containers image
64:33 - volumes etc this is where docker stores
64:36 - all its data by default when I say data
64:40 - I mean files related to images and
64:42 - containers running on the docker host
64:44 - for example all files related to
64:48 - containers are stored under the
64:49 - containers folder and the files related
64:52 - to images are stored under the image
64:54 - folder any volumes created by the docker
64:57 - containers are created under the
64:59 - volumes folder well don't worry about
65:01 - that for now we will come back to that
65:03 - in a bit
65:04 - for now let's just understand where
65:07 - docker stores its files and in what
65:10 - format so how exactly does Dockers store
65:14 - the files of an image and a container to
65:17 - understand that we need to understand
65:19 - Dockers layered architecture let's
65:22 - quickly recap something we learned when
65:24 - docker builds images it builds these in
65:27 - a layered architecture each line of
65:30 - instruction in the docker file creates a
65:33 - new layer in the docker image with just
65:36 - the changes from the previous layer for
65:38 - example the first layer is a base Ubuntu
65:41 - operating system followed by the second
65:44 - instruction that creates a second layer
65:46 - which installs all the apt packages and
65:49 - then the third instruction creates a
65:52 - third layer which with the Python
65:54 - packages followed by the fourth layer
65:57 - that copies the source code over and
65:59 - then finally the fifth layer that
66:01 - updates the entry point of the image
66:05 - since each layer only stores the changes
66:09 - from the previous layer it is reflected
66:12 - in the size as well if you look at the
66:15 - base open to image it is around 120
66:18 - megabytes in size the apt packages that
66:21 - are installed is around 300 MB and then
66:24 - the remaining layers are small to
66:27 - understand the advantages of this
66:29 - layered architecture let's consider a
66:32 - second application this application has
66:35 - a different talker file but it's very
66:38 - similar to our first application as in
66:41 - it uses the same base image as Ubuntu
66:44 - uses the same Python and flask
66:46 - dependencies but uses a different source
66:49 - code to create a different application
66:51 - and so a different entry point as well
66:55 - when I run the docker build command to
66:58 - build a new image for this application
67:00 - since the first three layers of both the
67:03 - applications are the same docker is not
67:06 - going to build the first three layers
67:09 - instead it reuses the same tree
67:12 - layer's it built for the first
67:14 - application from the cache and only
67:17 - creates the last two layers with the new
67:19 - sources and the new entry point this way
67:23 - docker builds images faster and
67:26 - efficiently saves disk space this is
67:29 - also applicable if you were to update
67:31 - your application code whenever you
67:34 - update your application code such as the
67:37 - app dot py in this case
67:39 - docker simply reuses all the previous
67:42 - layers from cache and quickly rebuilds
67:45 - the application image by updating the
67:47 - latest source code thus saving us a lot
67:50 - of time hearing rebuilds and updates
67:55 - let's rearrange the layers bottom up so
67:59 - we can understand it better at the
68:01 - bottom we have the base ubuntu layer
68:03 - than the packages then the dependencies
68:07 - and then the source code of the
68:09 - application and then the entry point all
68:13 - of these layers are created when we run
68:15 - the docker build command to form the
68:18 - final docker image so all of these are
68:21 - the docker image layers once the build
68:25 - is complete you cannot modify the
68:27 - contents of these layers and so they are
68:29 - read-only and you can only modify them
68:31 - by initiating a new build when you run a
68:35 - container based off of this image using
68:38 - the docker run command docker creates a
68:41 - container based off of these layers and
68:43 - creates a new writable layer on top of
68:46 - the image layer the writable layer is
68:49 - used to store data created by the
68:52 - container such as log files written by
68:54 - the applications any temporary files
68:56 - generated by the container or just any
68:59 - file modified by the user on that
69:02 - container the life of this layer though
69:04 - is only as long as the container is
69:07 - alive when the container is destroyed
69:09 - this layer and all of the changes stored
69:12 - in it are also destroyed remember that
69:16 - the same image layer is shared by all
69:18 - containers created using this image if I
69:23 - were to log in to the new
69:25 - created container and say create a new
69:27 - file called temp dot txt it will create
69:31 - that file in the container layer which
69:34 - is read and write we just said that the
69:37 - files in the image layer are read-only
69:39 - meaning you cannot edit anything in
69:42 - those layers let's take an example of
69:45 - our application code since we bake our
69:47 - code into the image the code is part of
69:50 - the image layer and as such is read-only
69:53 - after running a container what if I wish
69:56 - to modify the source code to say test a
69:59 - change remember the same image layer may
70:03 - be shared between multiple containers
70:05 - created from this image so does it mean
70:08 - that I cannot modify this file inside
70:11 - the container no I can still modify this
70:14 - file but before I save the modified file
70:18 - docker automatically creates a copy of
70:20 - the file in the readwrite layer and I
70:23 - will then be modifying a different
70:25 - version of the file in the readwrite
70:27 - layer all future modifications will be
70:30 - done on this copy of the file in the
70:33 - readwrite layer this is called
70:35 - copy-on-write mechanism the image layer
70:38 - being read-only just means that the
70:40 - files in these layers will not be
70:42 - modified in the image itself so the
70:45 - image will remain the same all the time
70:47 - until you rebuild the image using the
70:50 - docker build command what happens when
70:54 - we get rid of the container all of the
70:57 - data that was stored in the container
70:59 - layer also gets deleted the change we
71:03 - made to the app dot py and the new temp
71:06 - file we created we'll also get removed
71:09 - so what if we wish to persist this data
71:12 - for example if we were working with our
71:14 - database and we would like to preserve
71:16 - the data created by the container we
71:19 - could add a persistent volume to the
71:22 - container to do this first create a
71:25 - volume using the docker volume create
71:27 - command so when I run the docker volume
71:30 - create data underscore volume command it
71:33 - creates a folder called data underscore
71:36 - volume under the var Lib docker volumes
71:41 - directory then when I run the docker
71:44 - container using the docker run command I
71:46 - could mount this volume inside the
71:49 - docker containers read/write layer using
71:51 - the dash V option like this so I would
71:55 - do a docker run - V then specify my
71:58 - newly created volume name followed by a
72:00 - colon and the location inside my
72:03 - container which is the default location
72:05 - where MySQL stores data and that is
72:08 - where Lib MySQL and then the image name
72:12 - MySQL this will create a new container
72:16 - and mount the data volume we created
72:18 - into ver Lib MySQL folder inside the
72:22 - container so all data are written by the
72:25 - database is in fact stored on the volume
72:28 - created on the docker host even if the
72:31 - container is destroyed the data is still
72:34 - active now what if you didn't run the
72:37 - docker volume create command to create
72:39 - the volume before the docker run command
72:41 - for example if I run the docker run
72:44 - command to create a new instance of
72:47 - MySQL container with the volume data
72:50 - underscore volume to which I have not
72:52 - created yet docker will automatically
72:55 - create a volume named data underscore
72:58 - volume to and mounted to the container
73:01 - you should be able to see all these
73:04 - volumes if you list the contents of the
73:07 - where Lib docker volumes folder this is
73:11 - called volume mounting as we are
73:14 - mounting a volume created by docker
73:16 - under the VAR Lib docker volumes folder
73:20 - but what if we had our data already at
73:23 - another location for example let's say
73:25 - we have some external storage on the
73:28 - docker host at four slash data and we
73:31 - would like to store database data on
73:33 - that volume and not in the default ver
73:37 - Lib docker volumes folder in that case
73:40 - we would run a container using the
73:42 - command docker run - V but in this case
73:45 - we will provide the complete path to the
73:48 - folder we would like to mount that is
73:50 - for slash data for / - QL and so it will
73:55 - create a container and mount the folder
73:57 - to the container this is called bind
74:01 - mounting so there are two types of
74:03 - mounts a volume mounting and a bind
74:05 - mount volume mount mounts a volume from
74:09 - the volumes directory and bind mount
74:11 - mounts a directory from any location on
74:14 - the docker host one final point note
74:18 - before I let you go
74:20 - using the - V is an old style the new
74:24 - way is to use - mount option the - -
74:28 - mount is the preferred way as it is more
74:30 - verbose so you have to specify each
74:33 - parameter in a key equals value format
74:36 - for example the previous command can be
74:39 - written with the - mount option as this
74:41 - using the type source and target options
74:45 - the type in this case is bind the source
74:48 - is the location on my host and target is
74:52 - the location on my container so who is
74:58 - respond
74:58 - before doing all of these operations
75:01 - maintaining the layered architecture
75:03 - creating a writable layer moving files
75:06 - across layers to enable copy and write
75:08 - etc it's the storage drivers so Dockery
75:12 - uses storage drivers to enable layered
75:14 - architecture some of the common storage
75:17 - drivers are au FS btrfs ZFS device
75:22 - mapper overlay and overlay to the
75:26 - selection of the storage driver depends
75:28 - on the underlying OS being used for
75:30 - example we to bond to the default
75:33 - storage driver is a u FS whereas this
75:36 - storage driver is not available on other
75:38 - operating systems like fedora or cent OS
75:40 - in that case device mapper may be a
75:44 - better option docker will choose the
75:47 - best storage driver available
75:49 - automatically based on the operating
75:51 - system the different storage drivers
75:54 - also provide different performance and
75:56 - stability characteristics so you may
75:59 - want to choose one that fits the needs
76:01 - of your application and your
76:03 - organization if you would like to read
76:06 - more on any of these storage drivers
76:08 - please refer to the links in the
76:10 - attached documentation for now that is
76:13 - all from the docker architecture
76:16 - concepts see you in the next lecture
76:21 - [Music]
76:26 - [Music]
76:30 - hello and welcome to this lecture on
76:32 - docker compose going forward we will be
76:36 - working with configurations in yamo file
76:39 - so it is important that you are
76:41 - comfortable with llam let's recap a few
76:45 - things real quick course we first
76:48 - learned how to run a docker container
76:50 - using the docker run command if we
76:53 - needed to setup a complex application
76:55 - running multiple services a better way
76:59 - to do it is to use docker compose with
77:02 - docker compose we could create a
77:04 - configuration file in yamo format called
77:07 - docker compose dot yellow and put
77:10 - together the
77:12 - different services and the options
77:14 - specific to this to running them in this
77:17 - file then we could simply run a docker
77:21 - compose up command to bring up the
77:23 - entire application stack this is easier
77:26 - to implement run and maintain as all
77:29 - changes are always stored in the docker
77:31 - compose configuration file however this
77:34 - is all only applicable to running
77:36 - containers on a single docker host and
77:39 - for now don't worry about the yamo file
77:42 - we will take a closer look at the yamo
77:44 - file in a bit and see how to put it
77:47 - together that was a really simple
77:50 - application that I put together let us
77:52 - look at a better example I'm going to
77:55 - use the same sample application that
77:57 - everyone uses to demonstrate docker it's
78:00 - a simple yet comprehensive application
78:03 - developed by docker to demonstrate the
78:06 - various features available in running an
78:08 - application stack on docker
78:11 - so let's first get familiarized with the
78:14 - application because we will be working
78:17 - with the same application in different
78:19 - sections through the rest of this course
78:22 - this is a sample voting application
78:24 - which provides an interface for a user
78:28 - to vote and another interface to show
78:31 - the results the application consists of
78:34 - various components such as the voting
78:37 - app which is a web application developed
78:39 - in Python to provide the user with an
78:42 - interface to choose between two options
78:44 - a cat and a dog when you make a
78:48 - selection the vote is stored in Redis
78:52 - for those of you who are new to Redis
78:54 - Redis in this case serves as a database
78:57 - in memory this load is then processed by
79:00 - the worker which is an application
79:02 - written in dotnet the worker application
79:05 - takes the new vote and updates the
79:07 - persistent database which is a Postgres
79:10 - SQL in our case the Postgres SQL simply
79:13 - has a table with a number of votes for
79:16 - each category cats and dogs in this case
79:19 - it increments the number of votes for
79:22 - cats as our ward was for cats finally
79:25 - there is
79:26 - of the vote is displayed in a web
79:28 - interface which is another web
79:29 - application developed in node.js this
79:33 - resulting application reads the count of
79:35 - votes from the Postgres sequel database
79:38 - and displays it to the user so that is
79:42 - the architecture and data flow of this
79:44 - simple voting application stack as you
79:49 - can see this sample application is built
79:51 - with a combination of different services
79:54 - different development tools and multiple
79:57 - different development platforms such as
80:00 - Python node.js net etc this sample
80:05 - application will be used to showcase how
80:08 - easy it is to set up an entire
80:10 - application stack consisting of diverse
80:13 - components in docker let us keep aside
80:17 - docker swarm services and stacks for a
80:20 - minute and see how we can put together
80:22 - this application stack on a single
80:26 - docker engine using first docker run
80:30 - commands and then docker compose let us
80:33 - assume that all images of applications
80:36 - are already built and are available on
80:39 - docker repository let us start with the
80:42 - data layer first we run the docker run
80:45 - command to start an instance of Redis by
80:48 - running the docker run Redis command we
80:51 - will add the - D parameter to run this
80:53 - container in the background and we will
80:56 - also name the container Redis now naming
81:00 - the containers is important why is that
81:03 - important hold that thought we will come
81:05 - to that in a bit
81:06 - next we will deploy the Postgres sequel
81:10 - database by running the docker run
81:12 - Postgres command this time - we will add
81:16 - the - d option to run this in the
81:19 - background and name this container DB
81:21 - for database next we will start with the
81:25 - application services we will deploy a
81:28 - front-end app for voting interface by
81:30 - running an instance of voting app image
81:32 - run the docker run command and name the
81:35 - instance boat since this is a web server
81:38 - it has a web
81:39 - you eye instance running on port 80 we
81:42 - will publish that port to 5000 on the
81:44 - host system so we can access it from a
81:47 - browser next we will deploy the result
81:50 - web application that shows the results
81:52 - to the user for this we deploy a
81:55 - container using the results - app image
81:58 - and publish port 80 - port 5001 on the
82:02 - host this way we can access the web UI
82:04 - of the resulting app on a browser
82:07 - finally we deploy the worker by running
82:10 - an instance of the worker image okay now
82:15 - this is all good and we can see that all
82:18 - the instances are running on the host
82:20 - but there is some problem it just does
82:24 - not seem to work the problem is that we
82:27 - have successfully run all the different
82:29 - containers but we haven't actually
82:31 - linked them together as in we haven't
82:34 - told the voting web application to use
82:37 - this particular Redis instance there
82:39 - could be multiple Redis instances
82:41 - running we haven't told the worker and
82:44 - the resulting app to use this particular
82:47 - Postgres sequel database that we ran so
82:50 - how do we do that
82:52 - that is where we use links link is a
82:56 - command line option which can be used to
82:58 - link two containers together for example
83:02 - the voting app web service is dependent
83:05 - on the Redis service when the web server
83:08 - starts as you can see in this piece of
83:11 - code on the web server it looks for a
83:14 - Redis service running on host Redis but
83:17 - the voting app container cannot resolve
83:20 - a host by the name Redis to make the
83:23 - voting app aware of the Redis service we
83:26 - add a link option while running the
83:28 - voting app container to link it to the
83:30 - Redis container adding a - - link option
83:34 - to the docker run command and specifying
83:36 - the name of the Redis container which is
83:39 - which in this case is Redis followed by
83:42 - a colon and the name of the host that
83:44 - the voting app is looking for which is
83:46 - also Redis in this case remember that
83:50 - this is why we named the container when
83:53 - we ran
83:53 - the first time so we could use its name
83:56 - while creating a link what this is in
84:00 - fact doing is it creates an entry into
84:02 - the e.t.c host file on the voting app
84:05 - container adding an entry with a host
84:07 - named rebus with the internal IP of the
84:10 - Redis container
84:11 - similarly we add a link for the result
84:14 - app to communicate with the database by
84:17 - adding a link option to refer the
84:19 - database by the name DB as you can see
84:23 - in this source code of the application
84:24 - it makes an attempt to connect to a
84:27 - Postgres database on host DB finally the
84:32 - worker application requires access to
84:34 - both the Redis as well as the Postgres
84:37 - database so we add two links to the
84:40 - worker application one link to link the
84:43 - Redis and the other link to link
84:44 - Postgres database note that using links
84:49 - this way is deprecated and the support
84:52 - may be removed in future in docker this
84:56 - is because as we will see in some time
84:59 - advanced and newer concepts in docker
85:02 - swarm and networking supports better
85:04 - ways of achieving what we just did here
85:07 - with links but I wanted to mention that
85:10 - anyway so you learned the concept from
85:13 - the very basics once we have the docker
85:16 - run commands tested and ready it is easy
85:19 - to generate a docker compose file from
85:21 - it we start by creating a dictionary of
85:24 - container names we will use the same
85:26 - name we used in the docker run commands
85:28 - so we take all the names and create a
85:30 - key with each of them then under each
85:33 - item we specify which image to use the
85:37 - key is the image and the value is the
85:40 - name of the image to use next inspect
85:44 - the commands and see what are the other
85:45 - options used we published ports so let's
85:49 - move those ports under the respective
85:51 - containers so we create a property
85:54 - called ports and list all the ports that
85:56 - you would like to publish under that
85:58 - finally we are left with links so
86:02 - whichever container requires a link
86:04 - created properly under it called links
86:07 - and provide an array of links such as
86:09 - Redis or TB note that you could also
86:14 - specify the name of the link this way
86:17 - without the semicolon and and the target
86:19 - target name and it will create a link
86:22 - with the same name as the target name
86:25 - specifying the DB : DB is similar to
86:29 - simply specify DB we will assume the
86:33 - same value to create a link now that we
86:37 - are all done with our docker compose
86:38 - file bringing up the stack is really
86:40 - simple from the docker compose up
86:43 - command to bring up the entire
86:44 - application stack when we looked at the
86:52 - example of the sample voting application
86:54 - we assumed that all images are already
86:58 - built out of the 5 different components
87:02 - two of them Redis and Postgres images we
87:05 - know are already available on docker hub
87:07 - there are official images from Redis and
87:11 - Postgres but the remaining three are our
87:14 - own application it is not necessary that
87:17 - they are already built and available in
87:19 - the docker registry if we would like to
87:22 - instruct docker compose to run a docker
87:25 - bill instead of trying to pull an image
87:27 - we can replace the image line with a
87:30 - build line and specify the location of a
87:33 - directory which contains the application
87:35 - code and a docker file with instructions
87:38 - to build the docker image in this
87:41 - example for the voting app I have all
87:44 - the application code in a folder named
87:46 - vote which contains all application code
87:49 - and a docker file this time when you run
87:53 - the docker compose up command it will
87:55 - first build the images give a temporary
87:58 - name for it and then use those images to
88:01 - run containers using the options you
88:04 - specified before similarly use build
88:08 - option to build the two other services
88:11 - from the respective folders we will now
88:16 - look
88:17 - at different versions of docker compose
88:19 - file this is important because you might
88:22 - see docker compose files in different
88:25 - formats at different places and wonder
88:27 - white-sand look different docker compose
88:31 - evolved over time and now supports a lot
88:34 - more options than it did in the
88:36 - beginning for example this is the trim
88:39 - down version of the docker compose file
88:42 - we used earlier this is in fact the
88:45 - original version of docker compose file
88:47 - known as version 1 has had a number of
88:51 - limitations for example if you wanted to
88:54 - deploy containers on a different network
88:56 - other than the default bridge network
88:59 - there was no way of specifying that in
89:02 - this version of the file also say you
89:05 - have a dependency or startup order of
89:07 - some kind for example your database
89:10 - container must come up first and only
89:12 - then and should the voting application
89:14 - be started there was no way you could
89:16 - specify that in the version 1 of the
89:20 - docker compose file support for these
89:23 - came in version 2 with version 2 and up
89:27 - the format of the file also changed a
89:30 - little bit you no longer specify your
89:33 - stack information directly as you did
89:36 - before it is all encapsulated in a
89:39 - Services section so create a property
89:42 - called services in the root of the file
89:44 - and then move all the services
89:46 - underneath that you will still use the
89:50 - same docker compose up command to bring
89:52 - up your application stack but how does
89:55 - docker compose know what version of the
89:58 - file you're using you're free to use
90:00 - version 1 or version 2 depending on your
90:03 - needs so how does the docker compose
90:06 - know what format you are using for
90:10 - version 2 and up you must specify the
90:13 - version of docker compose file you're
90:15 - intending to use by specifying the
90:18 - version at the top of the file in this
90:20 - case version : 2 another difference is
90:26 - with networking in version 1 the
90:29 - Campos attaches all the containers it
90:32 - runs to the default birched
90:35 - Network and then used links to enable
90:38 - communication between the containers as
90:40 - we did before with version 2 docker
90:43 - compose automatically creates a
90:46 - dedicated bridged Network for this
90:48 - application and then attaches all
90:51 - containers to that new network all
90:54 - containers are then able to communicate
90:57 - to each other using each other's service
90:59 - name
91:00 - so you basically don't need to use links
91:03 - in version 2 of docker compose you can
91:06 - simply get rid of all the links you
91:08 - mentioned in version 1 when you convert
91:11 - a file from Wersching 1 to version 2 and
91:14 - finally version 2 also introduces it
91:18 - depends on feature if you wish to
91:21 - specify a startup order for instance say
91:23 - the watering web application is
91:25 - dependent on the Redis service so you
91:28 - need to ensure that Redis container is
91:31 - started first and only then the voting
91:33 - web application must be started we could
91:36 - add a depends on property to the voting
91:38 - application and indicate that it is
91:41 - dependent on Redis then comes version 3
91:47 - which is the latest as of today version
91:50 - 3 is similar to version 2 in the
91:53 - structure meaning it has a version
91:55 - specification at the top and a Services
91:57 - section under which you put all your
91:59 - services just like in version 2 make
92:03 - sure to specify the version number as 3
92:05 - at the top version 3 comes with support
92:09 - for docker swamp which we will see later
92:11 - on there are some options that were
92:14 - removed and added to see details on
92:17 - those you can refer to the documentation
92:19 - section using the link in the reference
92:21 - page following this lecture we will see
92:24 - version 3 in much detail later when we
92:27 - discuss about docker stacks let us talk
92:31 - about networks in docker compose getting
92:35 - back to our application so far we have
92:38 - been just deploying all containers on
92:40 - the default bridged Network
92:43 - let us say we modify the architecture a
92:46 - little bit to contain the traffic from
92:48 - the different sources for example we
92:51 - would like to separate the user
92:52 - generated traffic from the applications
92:54 - internal traffic so we create a
92:57 - front-end network dedicated for traffic
92:59 - from users and a back-end network
93:02 - dedicated for traffic within the
93:04 - application we then connect the user
93:07 - facing applications which are the voting
93:10 - app and the result app to the front-end
93:13 - network and all the components to an
93:16 - internal back-end network so back in our
93:21 - docker compose file note that I have
93:24 - actually stripped out the port section
93:27 - for simplicity sake they're still there
93:29 - but they're just not shown here the
93:32 - first thing we need to do if we were to
93:34 - use networks is to define the networks
93:37 - we are going to use in our case we have
93:40 - two networks front end and back end so
93:43 - create a new property called
93:45 - networks at the root level adjacent to
93:48 - the services in the docker compose file
93:50 - and add a map of networks we are
93:53 - planning to use then under each service
93:57 - create a networks property and provide a
93:59 - list of networks that service must be
94:02 - attached to in case of Redis and DB it's
94:06 - only the back-end network in case of the
94:09 - front-end applications such as at the
94:12 - voting app and the result app they
94:14 - require to be a test to both a front-end
94:17 - and back-end network
94:19 - you must also add a section for worker
94:21 - container to be added to the back-end
94:23 - network I have just omitted that in this
94:26 - slide due to space constraints now that
94:30 - you have seen docker compose files head
94:33 - over to the coding exercises and
94:35 - practice developing some docker compose
94:37 - files that's it for this lecture and I
94:41 - will see you in the next lecture
94:45 - [Music]
94:49 - we will now look at docker registry so
94:52 - what is a registry if the containers
94:56 - were the rain then they would rain from
94:59 - the docker registry which are the clouds
95:02 - that's where docker images are stored if
95:04 - the central repository of all docker
95:08 - images let's look at a simple nginx
95:11 - container we run the docker run engine X
95:14 - command to run an instance of the nginx
95:17 - image let's take a closer look at that
95:19 - image name now the name is nginx but
95:24 - what is this image and where is this
95:26 - image pulled from this name follows
95:29 - Dockers image naming convention nginx
95:32 - here is the image or the repository name
95:35 - when you say nginx it's actually nginx
95:39 - slash nginx the first part stands for
95:43 - the user or account name
95:44 - so if you don't provide an account or a
95:47 - repository name it assumes that it is
95:50 - the same as the given name which in this
95:52 - case is nginx
95:53 - the user names is usually your docker
95:56 - hub account name or if it is an
95:58 - organization then it's the name of the
96:00 - organization if you were to create your
96:03 - own account and create your own
96:05 - repositories or images under it then you
96:08 - would use a similar pattern now where
96:11 - are these images stored and pulled from
96:14 - since we have not specified the location
96:17 - where these images are to be pulled from
96:19 - it is assumed to be on Dockers default
96:22 - registry docker hub the dns name for
96:25 - which is docker dial the registry is
96:29 - where all the images are stored whenever
96:31 - you create a new image or update an
96:33 - existing image you push it to the
96:36 - registry and every time anyone deploys
96:38 - this application it is pulled from that
96:41 - registry there are many other popular
96:43 - registries as well for example Google's
96:45 - registry is at GC r dot IO where a lot
96:48 - of kubernetes related images are stored
96:50 - like the ones used for performing
96:52 - end-to-end tests on the cluster these
96:55 - are all publicly accessible images that
96:57 - anyone can download
96:59 - and access when you have applications
97:02 - built in-house that shouldn't be made
97:04 - available to the public hosting an
97:06 - internal private registry may be a good
97:09 - solution many cloud service providers
97:11 - such as AWS as your GCP provide a
97:15 - private registry by default when you
97:17 - open an account with them on any of
97:21 - these solutions be a docker hub or
97:23 - Google registry or your internal private
97:25 - registry you may choose to make a
97:27 - repository private so that it can only
97:30 - be accessed using a set of credentials
97:32 - from Dockers perspective to run a
97:35 - container using an image from a private
97:38 - registry you first log into your private
97:40 - registry using the docker login command
97:43 - input your credentials once successful
97:46 - run the application using private
97:47 - registry as part of the image name like
97:50 - this now if you did not log into the
97:53 - private registry he will come back
97:55 - saying that the image cannot be found so
97:58 - remember to always log in before pulling
98:00 - or pushing to a private registry we said
98:03 - that cloud providers like AWS or GCP
98:07 - provide a private registry when you
98:09 - create an account with them but what if
98:11 - you are running your application
98:12 - on-premise and don't have a private
98:15 - registry how do you deploy your own
98:17 - private registry within your
98:19 - organization the docker registry is
98:22 - itself another application and of course
98:24 - is available as a docker image the name
98:27 - of the image is registry and it exposes
98:30 - the API on port 5,000 now that you have
98:34 - your custom registry running at port
98:36 - 5,000 on this docker host how do you
98:40 - push your own image to it use the docker
98:44 - image tag command to tag the image with
98:47 - a private registry URL in it in this
98:49 - case since it's running on the same door
98:52 - host I can use localhost semi colon 5000
98:56 - followed by the image name I can then
98:59 - push my image to my local private
99:01 - registry using the command docker push
99:03 - and the new image name with the docker
99:06 - registry information in it from there on
99:08 - I can pull my image from anywhere within
99:10 - this network using either localhost
99:13 - you're on the same host or the IP or
99:16 - domain name of my docker host if I'm
99:18 - accessing from another host in my
99:21 - environment well let's sit for this
99:23 - lecture
99:24 - hello words of the practice test and
99:25 - practice working with private docker
99:28 - registries welcome to this lecture on
99:34 - docker engine in this lecture we will
99:37 - take a deeper look at Dockers
99:39 - architecture how it actually runs
99:41 - applications in isolated containers and
99:44 - how it works under the hood
99:47 - docker engine as we have learned before
99:50 - is simply referred to a host with docker
99:52 - installed on it when you install docker
99:55 - on a Linux host you're actually
99:57 - installing three different components
99:59 - the docker daemon the rest API server
100:03 - and the docker CLI the docker daemon is
100:06 - a background process that manages docker
100:09 - objects such as the images containers
100:11 - volumes and networks the docker REST API
100:14 - server is the API interface that
100:17 - programs can use to talk to the daemon
100:19 - and provide instructions you could
100:22 - create your own tools using this REST
100:24 - API and the docker CLI is nothing but
100:27 - the command-line interface that we've
100:29 - been using until now to perform actions
100:31 - such as running a container stopping
100:34 - containers destroying images etc it uses
100:38 - the REST API to interact with the docker
100:40 - demon something to note here is that the
100:44 - docker CLI need not necessarily be on
100:47 - the same host it could be on another
100:50 - system like a laptop and can still work
100:53 - with a remote docker engine simply use
100:57 - the dash H option on the docker command
101:00 - and specify the remote docker engine
101:03 - address and a port as shown here for
101:06 - example to run a container based on ng I
101:09 - and X on a remote docker host run the
101:12 - command docker - H equals 10.1 23 2000
101:18 - call in' - 375
101:21 - run ngan X
101:27 - now let's try and understand how exactly
101:31 - our applications containerized in docker
101:33 - how does it work under the hood
101:36 - docker uses namespaces to isolate
101:39 - workspace process IDs network
101:42 - inter-process communication mounds and
101:45 - unix time sharing systems are created in
101:48 - their own namespace thereby providing
101:51 - isolation between containers let's take
101:56 - a look at one of the namespace isolation
101:59 - technique process ID namespaces whenever
102:02 - a Linux system boots up it starts with
102:05 - just one process with a process ID of
102:07 - one this is the root process and kicks
102:10 - off all the other processes in the
102:12 - system by the time the system boots up
102:15 - completely we have a handful of
102:17 - processes running this can be seen by
102:20 - running the PS command to list all the
102:22 - running processes the process IDs are
102:26 - unique and two processes cannot have the
102:29 - same process ID now if we were to create
102:33 - a container which is basically like a
102:35 - child system within the current system
102:37 - the child system needs to think that it
102:41 - is an independent system on its own and
102:43 - it has its own set of processes
102:45 - originating from a root process with a
102:48 - process ID of one but we note that there
102:52 - is no hard isolation between the
102:54 - containers and the underlying host so
102:57 - the processes running inside the
102:58 - container or in fact processes running
103:01 - on the underlying host and so two
103:03 - processes cannot have the same process
103:05 - ID of one this is where namespaces come
103:09 - into play with process ID namespaces
103:11 - each process can have multiple process
103:14 - IDs associated with it for example when
103:17 - the processes start in the container
103:18 - it's actually just another set of
103:20 - processes on the base Linux system and
103:23 - it gets the next available process ID in
103:25 - this case five and six however they also
103:29 - get another process ID starting with PID
103:31 - one in the container namespace which is
103:34 - only visible inside the container so the
103:37 - container thinks that it has its own
103:39 - root process tree and so it is an
103:41 - independence
103:43 - so how does that relate to an actual
103:46 - system how do you see this on a host
103:48 - let's say I were to run an ng I in X
103:51 - Server as a container we know that the
103:54 - nginx container runs an NGO next service
103:57 - if we were to list all the services
104:00 - inside the docker container we see that
104:02 - the ng INX service running with a
104:05 - process ID of 1 this is the process ID
104:08 - of the service inside of the container
104:11 - namespace if we list the services on the
104:14 - docker host we will see the same service
104:17 - but with a different process ID that
104:20 - indicates that all processes are in fact
104:23 - running on the same host but separated
104:26 - into their own containers using
104:29 - namespaces so we learned that the
104:33 - underlying docker host as well as the
104:35 - containers share the same system
104:38 - resources such as CPU and memory how
104:42 - much of the resources are dedicated to
104:44 - the host and the containers and how does
104:46 - docker manage and share the resources
104:49 - between the containers by default there
104:52 - is no restriction as to how much of a
104:54 - resource a container can use and hence a
104:57 - container may end up utilizing all of
105:00 - the resources on the underlying host but
105:03 - there is a way to restrict the amount of
105:05 - CPU or memory a container can use docker
105:09 - uses three groups or control groups to
105:12 - restrict the amount of hardware
105:14 - resources allocated to each container
105:17 - this can be done by providing the - -
105:21 - CPUs option to the docker run command
105:23 - providing a value of 0.5 will ensure
105:27 - that the container does not take up more
105:29 - than 50% of the host CPU at any given
105:33 - time the same goes with memory setting a
105:36 - value of 100 m to the - - memory option
105:39 - limits the amount of memory the
105:41 - container can use to a hundred megabytes
105:44 - if you are interested in reading more on
105:47 - this topic
105:48 - refer to the links I posted in the
105:51 - reference page that's it for now on
105:53 - docker engine
105:55 - [Music]
105:59 - earlier in this course we learned that
106:02 - containers share the underlying OS
106:04 - kernel and as a result we cannot have a
106:07 - Windows container running on Linux host
106:09 - or vice versa
106:10 - we need to keep this in mind while going
106:13 - through this lecture as it's very
106:14 - important concept and most beginners
106:16 - tend to have an issue with it so what
106:19 - are the options available for docker on
106:22 - Windows there are two options available
106:24 - the first one is darker on Windows using
106:28 - docker toolbox and the second one is the
106:31 - docker desktop for Windows option we
106:33 - will look at each of these now let's
106:36 - take a look at the first option docker
106:38 - toolbox this was the original support
106:41 - for docker on Windows imagine that you
106:44 - have a Windows laptop and no access to
106:47 - any Linux system whatsoever but you
106:49 - would like to try docker you don't have
106:52 - access to a Linux system in the lab or
106:54 - in the cloud what would you do what I
106:57 - did was to install a virtualization
106:59 - software on my windows system like
107:01 - Oracle VirtualBox or VMware Workstation
107:03 - and deploy a Linux VM on it such as
107:06 - Ubuntu or Debian then install docker on
107:09 - the Linux VM and then play around with
107:11 - it this is what the first option really
107:15 - does it doesn't really have anything
107:18 - much to do with Windows you cannot
107:20 - create Windows based docker images or
107:23 - run Windows based docker containers you
107:25 - obviously cannot run Linux container
107:27 - directly on Windows either you're just
107:30 - working with docker on a Linux virtual
107:32 - machine on a Windows host docker however
107:36 - provides us with a set of tools to make
107:38 - this easy which is called as the docker
107:41 - toolbox the docker toolbox contains a
107:44 - set of tools like Oracle VirtualBox
107:46 - docker engine docker machine docker
107:49 - compose and a user interface called kite
107:51 - Matic this will help you get started by
107:54 - simply downloading and running the
107:56 - docker toolbox executable it will
107:58 - install VirtualBox deploy a lightweight
108:00 - VM called boot to docker which has
108:03 - darker running in it already so that you
108:06 - are all set to start with docker easily
108:08 - and with within a short period
108:10 - now what about requirements you must
108:13 - ensure that your operating system is
108:15 - 64-bit windows 7 or higher and that the
108:18 - virtualization is enabled on the system
108:20 - now remember docker to box is a legacy
108:24 - situation for older Windows systems that
108:26 - do not meet requirements for the newer
108:28 - docker for Windows option the second
108:32 - option is the newer an option called
108:35 - docker desktop for Windows in the
108:37 - previous option we saw that we had
108:38 - Oracle VirtualBox installed on Windows
108:41 - and then a Linux system and then docker
108:44 - on that Linux system now with docker for
108:47 - Windows we take out Oracle VirtualBox
108:48 - and use the native virtualization
108:50 - technology available with Windows called
108:52 - Microsoft hyper-v during the
108:55 - installation process for docker for
108:56 - Windows it will still automatically
108:58 - create a Linux system underneath but
109:00 - this time it is created on the Microsoft
109:03 - hyper-v instead of Oracle VirtualBox and
109:05 - have docker running on that system
109:07 - because of this dependency on hyper-v
109:10 - this option is only supported for
109:12 - Windows 10 enterprise or professional
109:14 - Edition and on Windows Server 2016
109:17 - because both these operating systems
109:19 - come with hyper-v support by default now
109:22 - here's the most important point so far
109:25 - whatever we have been discussing with
109:27 - Dockers support for Windows it is
109:30 - strictly for Linux containers Linux
109:32 - applications packaged into Linux docker
109:35 - images we're not talking about Windows
109:37 - applications or Windows images or
109:39 - windows containers both the options we
109:42 - just discussed will help you run a Linux
109:44 - container on a Windows host with Windows
109:48 - Server 2016 Microsoft announced support
109:52 - for Windows containers for the first
109:53 - time you can now packaged applications
109:56 - Windows applications into Windows docker
109:58 - containers and run them on Windows
110:01 - docker host using docker desktop for
110:03 - Windows when you install docker desktop
110:06 - for Windows the default option is to
110:09 - work with Linux containers but if you
110:12 - would like to run Windows containers
110:13 - then you must explicitly configure
110:16 - docker for Windows to switch to using
110:19 - Windows containers in early
110:22 - 20:16 Microsoft announced Windows
110:25 - containers now you could create Windows
110:27 - based images and run Windows containers
110:29 - on a Windows server just like how you
110:31 - would run Linux containers on a Linux
110:34 - system now you can create Windows images
110:37 - container as your applications and share
110:39 - them through the docker store as well
110:42 - unlike in Linux there are two types of
110:45 - containers in Windows the first one is a
110:48 - Windows Server container which works
110:50 - exactly like Linux containers where the
110:52 - OS kernel is shared with the underlying
110:54 - operating system to allow better
110:57 - security boundary between containers and
110:59 - to a lot of kernels with different
111:00 - versions and configurations to coexist
111:03 - the second option was introduced known
111:05 - as the hyper-v isolation with hyper-v
111:08 - isolation each container is run within a
111:12 - highly optimized virtual machine
111:14 - guaranteeing complete kernel isolation
111:16 - between the containers and the
111:18 - underlying host now while in the linux
111:20 - world you had a number of base images
111:23 - for linux systems such as Ubuntu debian
111:25 - fedora Alpine etc if you remember that
111:28 - that is what you specify at the
111:30 - beginning of the docker file in the
111:32 - windows world we have two options the
111:35 - windows server core and nano server a
111:38 - nano server is a headless deployment
111:40 - option for Windows Server which runs at
111:43 - a fraction of size of the full operating
111:45 - system you can think of this like the
111:47 - Alpine image in Linux the windows server
111:50 - core though is not a slight weight as
111:53 - you might expect it to be finally
111:56 - windows containers are supported on
111:58 - Windows Server 2016
112:00 - nano server and windows 10 professional
112:03 - and Enterprise Edition remember on
112:05 - Windows 10 professional and Enterprise
112:07 - Edition only supports hyper-v isolated
112:11 - containers meaning as we just discussed
112:13 - every container deployed is deployed on
112:16 - a highly optimized virtual machine well
112:19 - that's it about docker on windows now
112:23 - before I finish I want to point out one
112:25 - important fact we saw two ways of
112:28 - running a docker container using
112:29 - VirtualBox or hyper week but remember
112:32 - VirtualBox and hyper-v cannot coexist on
112:34 - the same
112:35 - windows host so if you started off with
112:37 - duck or toolbox with VirtualBox and if
112:39 - you plan to migrate to hyper-v remember
112:42 - you cannot have both solutions at the
112:44 - same time
112:44 - there is a migration guide available on
112:47 - docker documentation page on how to
112:49 - migrate from Rochelle box to hyper wait
112:51 - that's it for now thank you and I will
112:54 - see you the next lecture we now look at
113:00 - docker on Mac Locker on Mac is similar
113:04 - to docker on Windows there are two
113:06 - options to get started docker on Mac
113:08 - using docker toolbox or docker Desktop
113:11 - for Mac option let's look at the first
113:14 - option docker toolbox this was the
113:17 - original support for docker on Mac it is
113:20 - darker on a Linux VM created using
113:22 - VirtualBox on Mac as with Windows it has
113:26 - nothing to do with Mac applications or
113:28 - Mac based images or Mac containers it
113:30 - purely runs Linux containers on a Mac OS
113:33 - docker toolbox contains a set of tools
113:36 - like Oracle VirtualBox docker and Jain
113:38 - docker machine docker compose and a user
113:41 - interface called CAD Matic when you
113:43 - download and install the docker toolbox
113:45 - executable it installs VirtualBox
113:48 - deploys lightweight VM called boot a
113:50 - docker which has dr running in it
113:52 - already
113:52 - this requires mac OS 10.8 or newer the
113:58 - second option is the newer option called
114:00 - docker Desktop for Mac with docker
114:03 - Desktop for Mac we take out or
114:05 - commercial box and use hyper cat
114:08 - virtualization technology during the
114:10 - installation process for docker for Mac
114:13 - it will still automatically create a
114:15 - Linux system underneath but this time it
114:18 - is created on hyper kit instead of
114:20 - Oracle VirtualBox and have dr. running
114:22 - on that system
114:24 - this requires Mac OS Sierra 10 or 12 or
114:28 - newer and Martin and the Mac hardware
114:31 - must be 2010 or newer model finally
114:34 - remember that all of this is to be able
114:37 - to run the Linux container on Mac as of
114:40 - this recording there are no Mac based
114:42 - images or containers well that's it with
114:45 - docker
114:45 - on Mac for now
114:48 - [Music]
114:52 - we will now try to understand what
114:54 - container orchestration is so far in
114:58 - this course we've seen that with docker
115:00 - you can run a single instance of the
115:02 - application with a simple docker run
115:05 - command in this case to run a node.js
115:07 - based application you're on the docker
115:10 - run nodejs command but that's just one
115:13 - instance of your application on one
115:15 - docker host what happens when the number
115:17 - of users increase and that instance is
115:20 - no longer able to handle the load you
115:23 - deploy additional instance of your
115:25 - application by running the docker run
115:27 - command multiple times so that's
115:29 - something you have to do yourself you
115:31 - have to keep a close watch on the load
115:33 - and performance of your application and
115:35 - deploy additional instances yourself and
115:38 - not just that you have to keep a close
115:39 - watch on the health of these
115:41 - applications and if a container was to
115:44 - fail you should be able to detect that
115:46 - and run the docker run commander game to
115:48 - deploy another instance of that
115:50 - application what about the health of the
115:53 - docker host itself what if the host
115:55 - crashes and is inaccessible the
115:58 - containers hosted on that host become
116:01 - inaccessible too so what do you do in
116:04 - order to solve these issues you will
116:06 - need a dedicated engineer who can sit
116:09 - and monitor the state performance and
116:12 - health of the containers and take
116:13 - necessary actions to remediate the
116:15 - situation but when you have large
116:17 - applications deployed with tens of
116:20 - thousands of containers that's that's
116:22 - not a practical approach so you can
116:24 - build your own scripts and and that will
116:27 - help you tackle these issues to some
116:30 - extent container orchestration is just a
116:34 - solution for that it is a solution that
116:37 - consists of a set of tools and scripts
116:39 - that can help host containers in a
116:42 - production environment typically a
116:44 - container orchestration solution
116:46 - consists of multiple docker hosts that
116:49 - can host containers that way even if one
116:52 - fails the application is still
116:54 - accessible through the others the
116:56 - container orchestration solution easily
116:59 - allows you to deploy hundreds or
117:01 - thousands of instances of your
117:03 - application with a single command this
117:06 - is a command used for docker swarm we
117:09 - will look at the command itself in a bit
117:11 - some orchestration solutions can help
117:14 - you automatically scale up the number of
117:16 - instances when users increase and scale
117:19 - down the number of instances when the
117:21 - demand decreases some solutions can even
117:24 - help you in automatically adding
117:26 - additional hosts to support the user
117:28 - load and not just clustering and scaling
117:31 - the container orchestration solutions
117:33 - also provide support for advanced
117:35 - networking between these containers
117:37 - across different hosts as well as load
117:40 - balancing user requests across different
117:42 - house they also provide support for
117:44 - sharing storage between the house as
117:46 - well as support for configuration
117:47 - management and security within the
117:49 - cluster there are multiple container
117:52 - orchestration solutions available today
117:54 - docker has to her swamp kubernetes from
117:58 - Google and mezzo mezz from Paget
118:01 - well docker swamp is really easy to set
118:04 - up and get started
118:04 - it lacks some of the advanced auto
118:07 - scaling features required for complex
118:09 - production grade applications mezzos on
118:12 - the other hand is quite difficult to set
118:14 - up and get started
118:15 - but supports many advanced features
118:18 - kubernetes arguably the most popular of
118:21 - it all is a bit difficult to set up and
118:23 - get started but provides a lot of
118:25 - options to customize deployments and has
118:28 - support for many different vendors
118:31 - kubernetes is now supported on all
118:33 - public cloud service providers like GCP
118:36 - azure and AWS and the kubernetes project
118:39 - is one of the top-ranked projects on
118:42 - github in the upcoming lectures we will
118:45 - take a quick look at docker swamp and
118:47 - kubernetes
118:49 - [Music]
118:53 - we will now get a quick introduction to
118:56 - docker swarm dr. Swan has a lot of
118:59 - concepts to cover and requires its own
119:02 - course but we will try to take a quick
119:05 - look at some of the basic details so you
119:06 - can get a brief idea on what it is with
119:10 - docker swamp you could now combine
119:12 - multiple docker machines together into a
119:14 - single cluster docker swarm will take
119:17 - care of distributing your services or
119:19 - your application instances into separate
119:22 - hosts for high availability and for load
119:25 - balancing across different systems and
119:27 - hardware to set up a docker swamp you
119:30 - must first have hosts or multiple hosts
119:32 - with docker installed on them then you
119:35 - must designate one host to be the
119:37 - manager or the master or the swamp
119:40 - manager as it is called and others as
119:42 - slaves or workers once you're done with
119:45 - that run the docker swarm init command
119:47 - on the swarm manager and that will
119:49 - initialize the swamp manager the output
119:52 - will also provide the command to be run
119:54 - on the workers so copy the command and
119:57 - run it on the worker nodes to join the
119:59 - manager after joining the swamp the
120:01 - workers are also referred to as nodes
120:04 - and you're now ready to create services
120:06 - and deploy them on the swamp cluster so
120:10 - let's get into some more details as you
120:13 - already know to run an instance of my
120:16 - web server
120:17 - I run the docker run command and specify
120:20 - the name of the image I wish to run this
120:22 - creates a new container instance of my
120:25 - application and serves my web server now
120:28 - that we have learned how to create a
120:29 - swamp cluster how do I utilize my
120:31 - cluster to run multiple instances of my
120:34 - web server now one way to do this would
120:36 - be to run the docker run command on each
120:39 - worker node but that's not ideal as I
120:41 - might have to login to each node and run
120:43 - this command and there there could be
120:45 - hundreds of nodes I will have to setup
120:47 - load balancing myself a large monitor
120:50 - the state of each instance myself and if
120:52 - instances were to fail I'll have to
120:54 - restart them myself so it's going to be
120:56 - an impossible task that is where docker
120:59 - swarm orchestration consent dr. Swan
121:02 - Orchestrator does
121:03 - all of this for us so far we've only set
121:07 - up this one cluster but we haven't seen
121:08 - orchestration in action the key
121:11 - component of swarm orchestration is the
121:13 - Ducker a service dog or services are one
121:17 - or more instances of a single
121:19 - application or service that runs across
121:21 - to saw the nodes in the strong cluster
121:23 - for example in this case I could create
121:26 - a docker service to run multiple
121:28 - instances of my web server application
121:31 - across worker nodes in my swamp cluster
121:33 - for this around the docker service
121:36 - create command on the manager node and
121:38 - specify my image name there which is my
121:41 - web server in this case and use the
121:43 - option replicas to specify the number of
121:46 - instances of my web server I would like
121:48 - to run across the cluster since I
121:51 - specified three replicas and I get three
121:54 - instances of my web server distributed
121:57 - across the different worker nodes
121:59 - remember the docker service command must
122:02 - be run on the manager node and not on
122:04 - the worker node the docker service
122:06 - create command is similar to the docker
122:09 - run command in terms of the options
122:10 - passed such as the - II environment
122:13 - variable the - P for publishing ports at
122:16 - the network option to attach container
122:18 - to a network etc well that's a
122:22 - high-level introduction to docker Swan
122:23 - there's a lot more to know such as
122:26 - configuring multiple managers overlay
122:28 - networks etc as I mentioned it requires
122:31 - its own separate course well that's it
122:34 - for now
122:35 - in the next lecture we will look at
122:37 - kubernetes at a higher level we will now
122:44 - get a brief introduction to basic
122:46 - kubernetes concepts again kubernetes
122:49 - requires its own course well a few
122:52 - courses at least five but we will try to
122:55 - get a brief introduction to it here with
122:58 - docker you were able to run a single
123:00 - instance of an application using the
123:02 - docker CLI by running the docker run
123:05 - command which is grid running an
123:08 - application has never been so easy
123:10 - before with kubernetes using the
123:13 - kubernetes CLI known as cube control you
123:17 - run a thousand instance of the same
123:19 - application with a single command
123:21 - kubernetes can scale it up to two
123:24 - thousand with another command kubernetes
123:27 - can be even configured to do this
123:29 - automatically so that instances and the
123:31 - infrastructure itself can scale up and
123:34 - down based on user load kubernetes can
123:38 - upgrade these 2,000 instances of the
123:41 - application in a rolling upgrade fashion
123:44 - one at a time with a single command if
123:47 - something goes wrong it can help you
123:49 - roll back these images with a single
123:51 - command kubernetes can help you test new
123:53 - features of your application by only
123:56 - upgrading a percentage of these
123:58 - instances through a be testing methods
124:01 - the kubernetes open architecture
124:03 - provides support for many many different
124:06 - network and storage vendors any network
124:09 - or storage brand that you can think of
124:11 - has a plugin for kubernetes kubernetes
124:15 - supports a variety of authentication and
124:17 - authorization mechanisms all major cloud
124:20 - service providers have native support
124:22 - for kubernetes so what's the relation
124:26 - between docker and kubernetes well
124:28 - kubernetes uses docker host to host
124:31 - applications in the form of docker
124:33 - containers well it need not be docker
124:37 - all the time
124:38 - kubernetes supports alternatives to
124:40 - Dockers as well such as rocket or a cryo
124:43 - but let's take a quick look at the
124:45 - kubernetes architecture a kubernetes
124:48 - cluster consists of a set of nodes let
124:51 - us start with nodes a node is a machine
124:54 - physical or virtual on which a cobranet
124:56 - is the kubernetes software a set of
124:59 - tools are installed a node is a worker
125:02 - machine and that is where containers
125:04 - will be launched by kubernetes but what
125:07 - if the node on which the application is
125:09 - running fails well obviously our
125:10 - application goes down so you need to
125:13 - have more than one nodes a cluster is a
125:15 - set of nodes grouped together this way
125:18 - even if one node fails you have your
125:20 - application still accessible from the
125:23 - other nodes now we have a cluster but
125:26 - who is responsible for managing this
125:28 - class
125:29 - where is the information about the
125:31 - members of the cluster stored how are
125:33 - the nodes monitored when a node fails
125:36 - how do you move the workload of the
125:37 - failed nodes to another worker node
125:39 - that's where the master comes in the
125:43 - master is a note with the kubernetes
125:45 - control plane components installed the
125:48 - master watches over the notes are in the
125:51 - cluster and is responsible for the
125:53 - actual orchestration of containers on
125:55 - the worker notes when you install
125:58 - kubernetes on a system you are actually
126:01 - installing the following components an
126:03 - API server and EDD server a cubelet
126:07 - service contain a runtime engine like
126:10 - docker and a bunch of controllers and
126:13 - the scheduler the API server acts as the
126:16 - front end for kubernetes the users
126:18 - management devices command line
126:21 - interfaces all talk to the API server to
126:23 - interact with the kubernetes cluster
126:25 - next is the at CD a key value store the
126:28 - ED CD is a distributed reliable key
126:31 - value store used by kubernetes to store
126:33 - all data used to manage the cluster
126:35 - think of it this way when you have
126:37 - multiple nodes and multiple masters in
126:40 - your cluster let CD stores all that
126:42 - information on all the nodes in the
126:44 - cluster in a distributed manner a CD is
126:47 - responsible for implementing logs within
126:50 - the cluster to ensure there are no
126:52 - conflicts between the masters the
126:54 - scheduler is responsible for
126:56 - distributing work or containers across
126:58 - multiple nodes it looks for newly
127:00 - created containers and assigns them to
127:03 - notes the controllers are the brain
127:06 - behind orchestration they're responsible
127:08 - for noticing and responding when nodes
127:11 - containers or endpoints goes down the
127:14 - controllers makes decisions to bring up
127:16 - new containers in such cases the
127:19 - container runtime is the underlying
127:20 - software that is used to run containers
127:23 - in our case it happens to be docker and
127:26 - finally cubelet is the agent that runs
127:29 - on each node in the cluster the agent is
127:31 - responsible for making sure that the
127:33 - containers are running on the nodes as
127:36 - expected and finally we also need to
127:39 - learn a little bit about one of the
127:41 - command-line utilities known
127:42 - the cube command-line tool or the cube
127:45 - control tool or your cuddle as it is
127:47 - also called the cube control tool is the
127:51 - kubernetes CLI which is used to deploy
127:53 - and manage applications on a kubernetes
127:55 - cluster to get cluster related
127:57 - information to get the status with the
127:59 - nodes in the cluster and many other
128:01 - things the cube control run command is
128:04 - used to deploy an application on the
128:06 - cluster the keep control cluster info
128:08 - command is used to view information
128:10 - about the cluster and the cube control
128:12 - get nodes command is used to list all
128:15 - the notes part of the cluster so to run
128:18 - hundreds of instances of your
128:19 - application across hundreds of nodes all
128:22 - I need is a single kubernetes command
128:24 - like this well that's all we have for
128:27 - now a quick introduction to Panetta's
128:30 - and its architecture
128:31 - we currently have three courses on code
128:34 - cloud on kubernetes that will take you
128:37 - from the absolute beginner to a
128:39 - certified expert so have a look at it
128:43 - when you get a chance so we're at the
128:49 - end of this beginners course to docker I
128:52 - hope you had a great learning experience
128:53 - if so please leave a comment below if
128:57 - you like my way of teaching you will
128:59 - love my other courses hosted on my site
129:02 - at code cloud we have courses for docker
129:04 - swarm kubernetes advanced courses on
129:07 - kubernetes certifications as well as
129:10 - openshift we have courses for automation
129:12 - tools like ansible chef and puppet and
129:15 - many more on the way with it code cloud
129:18 - at www.calculated.com/support
129:28 - you
129:30 - [Music]

Cleaned transcript:

hello this is moonshot mohamed and welcome to the docker for beginners course this is an introduction to application containerization with docker for the absolute beginners made by code cloud for the programming knowledge audience thank you for joining and I hope you enjoyed the course hello and welcome to the docker for beginners course my name is moonshot monolith and I will be your instructor for this course I'm a DevOps and cloud trainer at code cloud comm which is an interactive handson online learning platform I've been working in the industry as a consultant for over thirteen years and have helped hundreds of thousands of students learn technology in a fun and interactive way in this course you will learn docker through a series of lectures that use animation illustration and some fun analogies that simplify complex concepts with demos that will show you how to install and get started with docker and most importantly we have handson labs that you can access right in your browser I will explain more about it in a bit but first let's look at the objectives of this course in this course we first try to understand what containers are what docker is and why you might need it and what it can do for you we will see how to run a docker container how to build your own docker image we will see networking in docker and how to use docker compose what docker registry is how to deploy your own private registry and we then look at some of these concepts in debt and we try to understand how docker really works under the hood you look at docker for Windows and Mac before finally getting a basic introduction to container orchestration tools like dr. Swann and kubernetes here's a quick note about handson labs first of all to complete this course you don't have to set up your own labs well you may set it up if you wish to if you wish to have your own environment and we have a demo as well but as part of this course we provide real labs that you can access right in your browser anywhere any and as many times as you want the labs give you instant access to a terminal to a docker host and an accompanying quiz portal the quiz portal asks a set of questions such as exploring the environment and gathering information or you might be asked to perform an action such as run docker container the quiz portal then validates your work and gives you feedback instantly every lecture in this course is accompanied by such challenging interactive quizzes that makes learning docker a fun activity so I hope you're as thrilled as I am to get started so let us begin we're going to start by looking at a highlevel overview on why you need docker and what it can do for you let me start by sharing how I got introduced to Locker in one of my previous projects I had this requirement to set up an endtoend application stack including various different technologies like a web server using node.js and a database such as MongoDB and a messaging system like Redis and an orchestration tool like ansible we had a lot of issues developing this application stack with all these different components first of all their compatibility with the underlying OS was an issue we had to ensure that all these different services were compatible with the version of OS we were planning to use there have been times when certain version of these services were not compatible with the OS and we've had to go back and look at different OS that was compatible with all of these different services secondly we had to check the compatibility between these services and the libraries and dependencies on the OS we've had issues where one service requires one version of a dependent library whereas another service requires another version the architecture of our application changed over time we've had to upgrade to newer versions of these components or change the database etc and every time something changed we had to go through the same process of checking compatibility between these various components and the underlying infrastructure this compatibility matrix issue is usually referred to as the matrix from hell next every time we had a new developer on board we found it really difficult to set up a new environment the new developers had to follow a large set of instructions and run hundreds of commands to finally set up their environment we had to make sure they were using the right operating system the right versions of each of these components and each developer had to set all that up by himself each time we also had different development tests and production environments one developer may be comfortable using one OS and the others may be comfortable using another one and so we couldn't guarantee that the application that we were building would run the same way in different environments and so all of this made our life in developing building and shipping the application really difficult so I needed something that could help us with the compatibility issue and something that will allow us to modify or change these components without affecting the other components and even modify the underlying operating systems as required and that search landed me on docker with docker I was able to run each component in a separate container with its own dependencies and its own libraries all on the same VM and the OS but within separate environments or containers we just had to build the docker configuration once and all our developers could now get started with a simple docker run command a respective of what the underlying operating system layer on all they needed to do was to make sure they had doc are installed on their systems so what are containers containers are completely isolated environments as in they can have their own processes for services their own network interfaces their own mounts just like washing machines except they all share the same OS kernel we will look at what that means in a bit but it's also important to note that containers are not new with docker containers have existed for about 10 years now and some of the different types of containers are Alexei Alexei Alexei FS etc docker utilizes Alex containers setting up these container environments is hard as they're very lowlevel and that is where docker offers a high level tool with several powerful functionalities making it really easy for endusers like us to understand how docker works let us revisit some basic concepts of operating systems first if you look at operating systems like Ubuntu Fedora Susi air scent OS they all consist of two things an OS kernel and a set of software the OS kernel is responsible for interacting with the underlying hardware while the OS kernel remains the same which is Linux in this case it's the software above it that makes these operating systems different this software may consist of a different user interface drivers compilers file managers developer tools etc so you have a common Linux kernel shared across all races and some custom software that differentiate operating systems from each other we said earlier that docker containers share the underlying kernel so what does that actually mean sharing the kernel let's say we have a system with an Ubuntu OS with docker installed on it docker can run any flavor of OS on top of it as long as they are all based on the same kernel in this case Linux if the underlying OS is Ubuntu docker can run a container based on another distribution like debian fedora SUSE or Sint OS each docker container only has the additional software that we just talked about in the previous slide that makes these operating systems different and docker utilizes the underlying kernel of the docker host which works with all OSS above so what is an OS that do not share the same kernel as this Windows and so you won't be able to run a Windows based container on a docker host with Linux on it for that you will require a docker on a Windows server now it is when I say this that most of my students go hey hold on there that's not true and they installed are on Windows run a container based on Linux and go see it's possible well when you install docker on Windows and run a Linux container on Windows you're not really running a Linux container on Windows Windows runs a Linux container on a Linux virtual machine under the hoods so it's really Linux container on Linux virtual machine on Windows we discuss more about this on the docker on Windows or Mac later during this course now you might ask isn't that a disadvantage then not being able to run another kernel on the OS the answer is no because unlike hypervisors docker is not meant to virtualize and run different operating systems and kernels on the same hardware the main purpose of docker is to package and container as applications and to ship them and to run them anywhere any times as many times as you want so that brings us to the differences between virtual machines and containers something that we tend to do is specially those from a virtualization background as you can see on the right in case of docker we have the underlying hardware infrastructure and then the OS and then docker installed on the OS docker then manages the containers that run with libraries and dependencies alone in case of virtual machines we have the hypervisor like ESX on the hardware and then the virtual machines on them as you can see each virtual machine has its own OS inside it then the dependencies and then the application the overhead causes higher utilization of underlying resources as there are multiple virtual operating systems and kernels running the virtual machines also consume higher disk space as each VM is heavy and is usually in gigabytes in size whereas docker containers are lightweight and are usually in megabytes in size this allows docker containers to boot up faster usually in a matter of seconds whereas VMs as we know takes minutes to boot up as it needs to boot up the entire operating system it is also important to note that docker has less isolation as more resources are shared between the containers like the kernel whereas VMs have complete isolation from each other since VMs don't rely on the underlying OS or kernel you can run different types of application occasions built on different OSS such as Linux based or windowsbased apps on the same paralyzer so those are some differences between the two now having said that it's not an either container or virtual machine situation its containers and virtual machines now when you have large environments with thousands of application containers running on thousands of dog or hosts you will often see containers provisioned on virtual docker hosts that way we can utilize the advantages of both technologies we can use the benefits of virtualization to easily provision or decommission docker House serves as required at the same time make use of the benefits of docker to easily provision applications and quickly scale them as required but remember that in this case we will not be provisioning that many virtual machines as we used to before because earlier we provisioned a virtual machine for each application now you might provision a virtual machine for hundreds or thousands of containers so how is it done there are lots of containerized versions of applications readily available as of today so most organizations have their products containerized and available in a public dock or repository called docker hub or docker store for example you can find images of most common operating systems databases and other services and tools once you identify the images you need and you install docker on your hosts bringing up an application is as easy as running a docker run command with the name of the image in this case running a docker run ansible command will run an instance of ansible on the docker host similarly run an instance of MongoDB Redis and nodejs using the docker run command if we need to run multiple instances of the web service simply add as many instances as you need and configure a load balancer of some kind in the front in case one of the instances were to fail simply destroy that instance and launch anyone there are other solutions available for handling such cases that we will look at later during this course and for now don't focus too much on the command we'll get to that in a bit we've been talking about images and containers let's understand the difference between the two an image is a package or a template just like a VM template that you might have worked with in the virtualization world it is used to create one or more containers containers are running instances of images that are isolated and have their own environments and set of processors as we have seen before a lot of products have been derived already in case you cannot find what you're looking for you could create your own image and push it to docker hub repository making it available for public so if you look at it traditionally developers developed applications then they hand it over to ops team to deploy and manage it in production environments they do that by providing a set of instructions such as information about how the hosts must be set up what prerequisites are to be installed on the host and how the dependencies are to be configured etc since the ops team did not really develop the application on their own then struggle with setting it up when the hidden issue they worked with the developers to resolve it with docker the developers and operations teams work handinhand to transform the guide into a docker file with both of their requirements this docker file is then used to create an image for their applications this image can now run on any host with docker installed on it and is guaranteed to run the same way everywhere so the ops team can now simply use the image to deploy the application since the image was already working when the developer built it and operations are have not modified it it continues to work the same way when deployed in production and that's one example of how a tool like docker contributes to the DevOps culture well that's it for now and in the upcoming lecture we will look at how to get started with docker we will now see how to get started with docker now docker has two editions the Community Edition and the Enterprise Edition the Community Edition is the set of free docker products the Enterprise Edition is the certified and supported container platform that comes with enterprise addons like the image management image security Universal control plane for managing and orchestrating container runtimes but of course these come with a price we will discuss more about container orchestration later in this course and along with some alternatives for now we will go ahead with the Community Edition the Community Edition is available on Linux Mac Windows or on cloud platforms like anti Bleus or either in the upcoming demo we will take a look at how to install and get started with docker on a Linux system now if you are on Mac or Windows you have two options either install a Linux VM using VirtualBox or some kind of virtualization platform and then follow along with the upcoming demo which is really the most easiest way to get started with docker the second option is to install docker Desktop for Mac or the docker desktop for Windows which are native applications so if that is really what you want like check out the docker for Mac and the windows sections towards the end of this course and then head back here once you are all set up we will now head over to a demo and we will take a look at how to install docker on a Linux machine in this demo we look at how to install and get started with docker first of all identify a system physical or virtual machine or laptop that has a supported operating system in my case I have an Ubuntu VM go to doctor comm and click on get docker you will be taken to the docker engine Community Edition page that is the free version that we are after from the lefthand menu select your system type I choose Linux in my case and then select your OS flavor I choose Ubuntu read through the prerequisites and requirements your abun to system must be 64bit and one of these supported versions like disco cosmic Bionic or decennial in my case I have a bionic version to confirm view the Etsy release file next uninstall any older version if one exists so let's just make sure that there is none on my host so I'll just copy and paste that command and I confirm that there are no older version that exists on my system the next step is to set up repository and install the software now there are two ways to go about this the first is using the package manager by first updating the repository using the aptget update command then installing the prerequisite packages and then adding Dockers of facial GPG keys and then installing docker but I'm not going to go that route there is an easier way if you scroll all the way to the bottom you will find the instructions to install docker using the convenience script it's a script that automates the entire installation process and works on most operating systems run the first command to download a copy of the script and then run the second command to execute the script to install docker automatically give it a few minutes to complete the installation the installation is now successful that has now checked the version of docker using the darker version command we've installed version 19.0 3.1 we will now run a simple container to ensure everything is working as expected for this head over to docker hub at hub docker calm here you will find a list of the most popular docker images like engine eggs MongoDB Alpine nodejs Redis etc let's search for a fun image called we'll say we'll say is Dockers version of cows a which is basically a simple application that trends a cow saying something in this case it happens to be a whale copy the docker run command given here remember to add a sudo and we will change the message to hello world I'm running this command docker pulls the image of the whales II application from docker hub and runs it and we have our avail saying hello great we're all set remember for the purpose of this course you don't really need to set up a docker system on your own we provide handson labs that you will get access to but if you wish to experiment on your own and follow along feel free to do so we now look at some of the docker commands at the end of this lecture you will go through a handson quiz where you will practice working with these commands let's start by looking at docker run command the docker run command is used to run a container from an image running the docker run nginx command will run an instance of the nginx application from the docker host if it already exists if the image is not present on the host it will go out to docker hub and pull the image down but this is only done the first time for the subsequent executions the same image will be reused the docker PS command lists all running containers and some basic information about them such as the container ID the name of the image we used to run the containers the current status and the name of the container each container automatically gets a random ID and name created for it by docker which in this case is silly Samet to see all containers running or not use the a option this outputs all running as well as previously stopped or exited containers will talk about the command and port fields shown in this output later in this course for now let's just focus on the basic command to stop a running container use the Tucker stop command but you must provide either the container ID or the continue name in the stop command if you're not sure of the name run the docker PS command to get it on success you will see the name printed out and running docker PS again will show no running containers running docker PS a however shows the container silly summit and that it is in an accident states a few seconds ago now what if we don't want this container lying around consuming space what if we want to get rid of it for good use the docker RM command to remove a stopped or exited container permanently if it prints the name back we're good run the docker PS command again to verify that it's no longer present good but what about the nginx image that was downloaded at first we're not using that anymore so how do we get rid of that image but first how do we see a list of images present on our hosts run the docker images command to see a list of available images and their sizes on our hosts we have four images the nginx Redis Ubuntu and Alpine we will talk about tags later in this course when we discuss about images to remove an image that you no longer plan to use run the docker RM I command remember you must ensure that no containers are running off of that image before attempting to remove the image you must stop and delete all dependent containers to be able to delete an image when we ran the docker run command earlier it downloaded the Ubuntu image as it couldn't find one locally what if we simply want to download the image and keep so when we run the run docker run command we don't want to wait for it to download use the docker pull command to only pull the image and not run the container so in this case the docker pull a bunt oakum and pulls the Ubuntu image and stores it on our host let's look at another example say you were to run a docker container from an Ubuntu image when you run the docker run Ubuntu command it runs an instance of Ubuntu image and exits immediately if you were to list the Irani containers you wouldn't see the container running if you list all containers including those that are stopped you will see that the new container Iran is in an exited state now why is that unlike virtual machines containers are not meant to host an operating system containers are meant to run a specific task or process such as to host an instance of a web server or application server or a database or simply to carry some kind of computation or analysis tasks once the task is complete the container exits a container only lives as long as the process inside it is alive if the web service inside the container is stopped or crash then the container exits this is why when you run a container from an Ubuntu image it stops immediately because a bundle is just an image of an operating system that is used as the base image for other applications there is no process or application running in it by default if the image isn't running any service as is the case with Ubuntu you could instruct docker to run a process with the docker run command for example a sleep command with a duration of 5 seconds when the container starts it runs to sleep command and goes into sleep for 5 seconds post which the sleep command exits and the container stops what we just saw was executing a command when we run the container but what if we would like to execute a command on a running container for example when around the docker PS command I can see that there is a running container which uses the bunch of image and sleeps 400 seconds let's say I would like to see the contents of a file inside this particular container I could use the docker exec command to execute a command on my docker container in this case to print the contents of the Etsy hosts file finally let's look at one more option before we head over to the practice exercises I'm now going to run a docker image I developed for a simple web application the repository name is code cloud slash simple web app it runs a simple web server that listens on port 8080 you run a docker run command like this it runs in the foreground or in an attached mode meaning you will be attached to the console or the standard out of the docker container and you will see the output of the web service on your screen you won't be able to do anything else on this console other than view the output until this docker container stops it won't respond to your inputs press the ctrl + C combination to stop the container and the application hosted on the container exits and you get back to your prompt another option is to run the docker container in the detached mode by providing the dash D option this will run the docker container in the background mode and you will be back to your prompt immediately the container will continue to run in the backend run the docker PS command to view the running container now if you would like to attach back to the running container later run the docker attach command and specify the name or ID of the docker container now remember if you are specifying the ID of a container in any docker command you can simply provide the first few characters alone just so it is different from the other container IDs on the host in this case I specify a 0 for 3d now don't worry about accessing the UI of the webserver for now we will look more into that in the upcoming lectures for now let's just understand the basic commands will now get our hands dirty with the docker CLI so let's take a look at how to access the practice lab environments next let me now walk you through the handson lab practice environment the links to access the labs associated with this course are available at cold cloud at code cloud comm slash P slash docker dash labs this link is also given in the description of this video once you're on this page use the links given there to access the labs associated to your lecture each lecture has its own lab so remember to choose the right lab for your lecture the labs open up right in your browser I would recommend to use google chrome while working with the labs the interface consists of two parts a terminal on the left and a quiz portal on the right the cooze portal on the right gives you challenges to solve follow the quiz and try and answer the questions asked and complete the tasks given to you each scenario consists of anywhere from 10 to 20 questions that needs to be answered within 30 minutes to an hour at the top you have the question numbers below that is the remaining time for your lab below that is the question if you are not able to solve the challenge look for hints in the hints section you may skip a question by hitting the skip button in the top right corner but remember that you will not be able to go back to a previous question once you have skipped if the quiz portal gets stuck for some reason click on the quiz portal tab at the top to open the quiz portal in a separate window the terminal gives you access to a real system running Tucker you can run any docker command here and run your own containers or applications you would typically be running commands to solve the tasks assigned in the quiz portal you may play around and experiment with this environment but make sure you do that after you've gone through the quiz so that your work does not interfere with the tasks provided by the quiz so let me walk you through a few questions there are two types of questions each lab scenario starts with a set of export retrieve multiplechoice questions where you're asked to explore and find information in the given environment and select the right answer this is to get you familiarized with a setup you're then asked to perform tasks like run a container stop them delete them build your own image etc here the first question asks us to find the version of docker server engine running on the host run the docker reversion command in the terminal and identify the right version then select the appropriate option from the given choices another example is the fourth question where it asks you to run a container using the Redis image if you're not sure of the command click on hints and it will show you a hint we now run a Redis container using the docker run readies command wait for the container to run once done click on check to check your work we have now successfully completed the task similarly follow along and complete all tasks once the lab exercise is completed remember to leave a feedback and let us know how it went a few things to note these are publicly accessible labs that anyone can access so if you catch yourself logged out during a peak hour please wait for some time and try again also remember to not store any private or confidential data on these systems remember that this environment is for learning purposes only and is only alive for an hour after which the lab is destroyed so does all your work but you may start over and access these labs as many times as you want until you feel confident I will also post solutions to these lab quizzes so if you run into issues you may refer to those that's it for now head over to the first challenge and I will see you on the other side we will now look at some of the other docker run commands at the end of this lecture you will go through a handson quiz where you will practice working with these commands we learned that we could use the docker run Redis command to run the container running a Redis service in this case the latest version of Redis which happens to be 5.0 to 5 as of today but what if we want to run another version of Redis like for example and older versions say 4.0 then you specify the version separated by a colon this is called a tag in that case docker pulls an image of the 4.0 version of Redis and runs that also notice that if you don't specify any tag as in the first command docker will consider the default tag to be latest latest is a tag associated to the latest version of that software which is governed by the authors of that software so as a user how do you find information about these versions and what is the latest at docker hub com look up an image and you will find all the supported tags in its description each version of the software can have multiple short and long tags associated with it as seen here in this case the version fight of 0.5 also has the latest tag on it let's now look at inputs I have a simple prompt application that when run asks for my name and on entering my name prints a welcome message if I were to docker eyes this application and run it as a docker container like this it wouldn't wait for the prompt it just prints whatever the application is supposed to print on standard out that is because by default the docker container does not listen to a standard input even though you are attached to its console it is not able to read any input from you it doesn't have a terminal to read inputs it runs in a non interactive mode if you would like to provide your input you must map the standard input of your host to the docker container using the I parameter the I parameter is for interactive mode and when I input my name it prints the expected output but there is something still missing from this the prompt when we run the app at first it asked us for our name but when docker iced that prompt is missing even though it seems to have accepted my input that is because the application prompt on the terminal and we have not attached to the containers terminal for this use the T option as well the T stands for a sudo terminal so with the combination of int we are now attached to the terminal as well as in an interactive mode on the container we will now look at port mapping or port publishing on containers let's go back to the example where we run a simple web application in a docker container on my docker host remember the underlying host where docker is installed is called docker host or docker engine when we run a containerized web application it runs and we are able to see that the server is running but how does a user access my application as you can see my application is listening on port 5,000 so I could access my application by using port 5000 but what IP do I use to access it from a web browser there are two options available one is to use the IP of the docker container every docker container gets an IP assigned by default in this case it is 172 dot 17.0 but remember that this is an internal IP and is only accessible within the docker host so if you open a browser from within the docker host you can go to http colon forward slash forward slash 172 dot 17 dot 0 dot 1 5,000 to access the IP address but since this is an internal IP users outside of the docker host cannot access it using this IP for this we could use the IP of the docker host which is one ninety two dot one sixty eight dot 1.5 but for that to work you must have mapped the port inside the docker container to a free port on the docker host for example if I want the users to access my application through port 80 on my docker host I could map port 80 of local host to port 5000 on the docker container using the dash P parameter in my run command like this and so the user can access my application by going to the URL HTTP colon slash slash one ninety two dot one sixty eight dot one dot five colon 80 and all traffic on port 80 on my daugher host will get routed to port 5,000 inside the docker container this way you can run multiple instances of your application and map them to different ports on the docker host or run instances of different applications on different ports for example in this case and running an instance of MySQL that runs a database on my host and listens on the default MySQL port which happens to be three three zero six or another instance of MySQL on another port eight three zero six so you can run as many applications like this and map them to as many ports as you want and of course you cannot map to the same port on the docker host more than once we will discuss more about port mapping and networking of containers in the network lecture later on let's now look at how data is persisted in a docker container for example let's say you were to run a MySQL container when databases and tables are created the data files are stored in location /wor Labe MySQL inside the docker container remember the docker container has its own isolated filesystem and any changes to any files happen within the container let's assume you dump a lot of data into the database what happens if you were to delete the MySQL container and remove it as soon as you do that the container along with all the data inside it gets blown away meaning all your data is gone if you would like to persist data you would want to map a directory outside the container on the docker host to a directory inside the container in this case I create a directory called /opt slash data dir and map that to var Lib MySQL inside the docker container using the V option and specifying the directory on the door host followed by a colon and the directory inside the clock or container this way when docker container runs it will implicitly mount the external directory to a folder inside the docker container this way all your data will now be stored in the external volume at /opt slash data directory and thus will remain even if you delete the docker container the docker PS command is good enough to get basic details about containers like their names and ID's but if you would like to see additional details about a specific container use the docker inspect command and provide the container name or ID it returns all details of a container in a JSON format such as the state Mounds configuration data network settings etc remember to use it when you're required to find details on a container and finally how do we see the logs of a container we ran in the background for example I ran my simple web application using the D parameter and it ran the container in a detached mode how do I view the logs which happens to be the contents written to the standard out of that container use the docker logs command and specify the container ID or name like this well sit for this lecture head over to the challenges and practice working with docker commands so to start with a simple web application written in Python this piece of code is used to create a web application that displays a web page with a background color if you look closely into the application code you will see a line that sets the background color to red now that works just fine however if you decide to change the color in the future you will have to change the application code it is a best practice to move such information out of the application code and into say an environment variable called app color the next time you run the application set an environment variable called app color to a desired value and the application now has a new color once your application gets packaged into a docker image he would then run it with the docker run command followed by the name of the image however if you wish to pass the environment variable as we did before he would now use the docker run commands II option to set an environment variable within the container to deploy multiple containers with different colors he would run the docker command multiple times and set a different value for the environment variable each time so how do you find the environment variable set on a container that's already running use the docker inspect command to inspect the properties of a running container under the config section you will find the list of environment variables set on the container well that's it for this lecture on configuring environment variables in docker hello and welcome to this lecture on docker images in this lecture we're going to see how to create your own image now before that why would you need to create your own image it could either be because you cannot find a component or a service that you want to use as part of your application on docker hub already or you and your team decided that the application you're developing will be derived for ease of shipping and deployment in this case I'm going to containerize an application a simple web application that I have built using the Python flask framework first we need to understand what we our container izing or what application we are creating an image for and how the application is built so start by thinking what you might do if you want to deploy the application manually we write down the steps required in the right order and creating an image for a simple web application if I were to set it up manually I would start with an operating system like Ubuntu then update the source repositories using the apt command then install dependencies using the apt command then install Python dependencies using the PIP command then copy over the source code of my application to a location like opt and then finally run the web server using the flask command now that I have the instructions create a docker file using these here's a quick overview of the process of creating your own image first create a docker file named docker file and write down the instructions for setting up your application in it such as installing dependencies where to copy the source code from and to and what the entry point of the application is etc once done build your image using the docker build command and specify the docker file as input as well as a tag for the image this will create an image locally on your system to make it available on the public docker hub registry run the docker push command and specify the name of the image you just created in this case the name of the image is my account name which is M Amjad followed by the image name which is my custom app now let's take a closer look at that docker file docker file is a text file written in a specific format that docker can understand it's in an instruction and arguments format for example in this docker file everything on the left in caps is an instruction in this case from run copy and entry point are all instructions each of these instruct docker to perform a specific action while creating the image everything on the right is an argument to those instructions the first line from Ubuntu defines what the base OS should be for this container every docker image must be based off of another image either an OS or another image that was created before based on an OS you can find official releases of all operating systems on docker hub it's important to note that all docker files must start with a from instruction the run instruction instructs docker to run a particular command on those base images so at this point docker runs the aptget update commands to fetch the updated packages and installs required dependencies on the image then the copy instruction copies files from the local system onto the docker image in this case the source code of our application is in the current folder and I will be copying it over to the location opt source code inside the docker image and finally entry point allows us to specify a command that will be run when the image is run as a container when docker builds the images it builds this in a layered architecture each line of instruction creates a new layer in the docker image with just the changes from the previous layer for example the first layer is a base Ubuntu OS followed by the second instruction that creates a second layer which installs all the apt packages and then the third instruction creates a third layer with the Python packages followed by the fourth layer that copies the source code over and the final layer that updates the entry point of the image since each layer only stores the changes from the previous layer it is reflected in the size as well if you look at the base open to image it is around 120 MB in size the apt packages that I install is around 300 MB and the remaining layers are small you could see this information if you run the docker history command followed by the image name when you run the Bilka man you could see the various steps involved and the result of each task all the layers built are cast so the layered architecture helps you restart docker built from that particular step in case it fails or if you were to add new steps in the build process you wouldn't have to start all over again all the layers built are cached by docker so in case a particular step was to fail for example in this case step three failed and you were to fix the issue and rerun docker build it will reuse the previous layers from cache and continue to build the remaining layers the same is true if you were to add additional steps in the docker file this way rebuilding your image is faster and you don't have to wait for docker to rebuild the entire image each time this is helpful especially when you update source code of your application as it may change more frequently only the layers above the updated layers needs to be rebuilt we just saw a number of products containerized such as databases development tools operating systems etc but that's just not it you can containerized almost all of the application even simple ones like browsers or utilities like curl applications like Spotify Skype etc basically you can containerize everything and going forward and see that's how everyone is going to run applications nobody is going to install anything anymore going forward instead they're just going to run it using docker and when they don't need it anymore get rid of it easily without having to clean up too much in this lecture we will look at commands arguments and entry points in docker let's start with a simple scenario say you were to run a docker container from an Ubuntu image when you run the docker run Ubuntu command it runs an instance of Ubuntu image and exits immediately if you were to list the running containers you wouldn't see the container running if you list all containers including those that are stopped you will see that the new container you ran is in an exited state now why is that unlike virtual machines containers are not meant to host an operating system containers are meant to run a specific task or process such as to host an instance of a web server or application server or a database or simply to carry out some kind of computation or analysis once the task is complete the container exits a container only lives as long as the process inside it is alive if the web service inside the container is dot or crashes the container exits so who defines what process is run within the container if you look at the docker file for popular docker images like ng INX you will see an instruction called CMD which stands for command that defines the program that will be run within the container when it starts for the ng INX image it is the ng INX command for the MySQL image it is the MySQL d command what we tried to do earlier was to run a container with a plain Ubuntu operating system let us look at the docker file for this image you will see that it uses bash as the default command now bash is not really a process like a web server or database server it is a shell that listens for inputs from a terminal if it cannot find the terminal it exits when we ran the ubuntu container earlier docker created a container from the ubuntu image and launched the bash program by default docker does not attach a terminal to a container when it is run and so the bash program does not find a terminal and so it exits since the process that was started when the container was created finished the container exits as well so how do you specify a different command to start the container one option is to append a command to the docker run command and that way it overrides the default command specified within the image in this case I run the docker run Ubuntu command with the sleep 5 command as the added option this way when the container starts it runs the sleep program waits for 5 seconds and then exits but how do you make that change permanent say you want the image to always run the sleep command when it starts you would then create your own image from the base ubuntu image and specify a new command there are different ways of specifying the command either the command simply as is in a shell form or in a JSON array format like this but remember when you specify in a JSON array format the first element in the array should be the executable in this case the sleep program do not specify the command and parameters together like this the command and its parameters should be separate elements in the list so I now build my new image using the docker build command and name it as Ubuntu sleeper I could now simply run the docker ubuntu sleeper command and get the same results it always sleeps for 5 seconds and exits but what if I wish to change the number of seconds it sleeps currently it is hardcoded to 5 seconds as we learned before one option is to run the docker run command with the new command appended to it in this case sleep 10 and so the command that will be run at startup will be sleep 10 but it doesn't look very good the name of the image ubuntu sleeper in itself implies that the container will sleep so we shouldn't have to specify the sleep command again instead we would like it to be something like this docker run ubuntu sleeper 10 we only want to pass in the number of seconds the container should sleep and sleep command should be invoked automatically and that is where the entry point instructions comes into play the entry point instruction is like the command instruction as in you can specify the program that will be run when the container starts and whatever you specify on the command line in this case 10 will get appended to the entry point so the command that will be run when the container starts is sleep 10 so that's the difference between the two in case of the CMD instruction the command line parameters passed will get replaced entirely whereas in case of entry point the command line parameters will get appended now in the second case what if I run the ubuntu sleeper image command without appending the number of seconds then the command at startup will be just sleep and you get the error that the operand is missing so how do you configure a default value for the command if one was not specified in the command line that's where you would use both entry point as well as the command instruction in this case the command instruction will be appended to the entry point instruction so at startup the command would be sleep 5 if you didn't specify any parameters in the command line if you did then that will overwrite the command instruction and remember for this to happen you should always specify the entry point and command instructions in a JSON format finally what if you freely really want to modify the entry point during runtime say from sleep to an imaginary sleep 2.0 command well in that case you can override it by using the entry point option in the docker run command the final command at startup would then be sleep 2.0 10 well that's it for this lecture and I will see you in the next we now look at networking in docker when you install docker it creates three networks automatically bridge no and host bridge is the default network a container gets attached to if you would like to associate the container with any other network you specify the network information using the network command line parameter like this we will now look at each of these networks the BRIT network is a private internal network created by docker on the host all containers attached to this network by default and they get an internal IP address usually in the range 170 2.17 series the containers can access each other using this internal IP if required to access any of these containers from the outside world map the ports of these containers to ports on the docker host as we have seen before another way to access the containers externally is to associate the container to the hosts network this takes out any network isolation between the docker host and the docker container meaning if you were to run a web server on port 5,000 in a web app container it is automatically as accessible on the same port externally without requiring any port mapping as the web container uses the hosts network this would also mean that unlike before you will now not be able to run multiple web containers on the same host on the same port as the ports are now common to all containers in the host network with nonnetwork the containers are not attached to any network and doesn't have any access to the external network or other containers they run in an isolated Network so we just saw the default burst Network with the network ID 170 2.72 0.1 so all containers associated to this default network will be able to communicate to each other but what if we wish to isolate the containers within the docker host for example the first two web containers on internal network 172 and the second two containers on a different internal network like 182 by default docker only creates one internal bridge Network we could create our own internal network using the command docker network create and specified the driver which is bridge in this case and the subnet for that network followed by the custom isolated network name run the docker network LS command to list all networks so how do we see the network settings and the IP address assigned to an existing container run the docker inspect command with the ID or name of the container and you will find a section on network settings there you can see the type of network the container is attached to is internal IP address MAC address and other settings containers can reach each other using their names for example in this case I have a webserver and a MySQL database container running on the same node how can I get my web server to access the database on the database container one thing I could do is to use the internal IP address signed to the MySQL container which in this case is 170 2.72 0.3 but that is not very ideal because it is not guaranteed that the container will get the same IP when the system reboots the right way to do it is to use the container name all containers in a docker host can resolve each other with the name of the container docker has a builtin DNS server that helps the containers to resolve each other using the container name note that the built in DNS server always runs at address 127 dot 0 dot 0 dot 11 so how does docker implement networking what's the technology behind it like how were the containers isolated within the host docker uses network namespaces that creates a separate name space for each container it then uses virtual Ethernet pairs to connect containers together well that's all we can talk about it for now more about these or advanced concepts that we discussed in the advanced course on docker on code cloud that's all for now from this lecture on networking head over to the practice tests and practice working with networking in docker I will see you in the next lecture hello and welcome to this lecture and we are learning advanced docker concepts in this lecture we're going to talk about docker storage drivers and file systems we're going to see where and how docker stores data and how it manages file systems of the containers let us start with how docker stores data on the local file system when you install docker on a system it creates this folder structure at where lib docker you have multiple folders under it called a ufs containers image volumes etc this is where docker stores all its data by default when I say data I mean files related to images and containers running on the docker host for example all files related to containers are stored under the containers folder and the files related to images are stored under the image folder any volumes created by the docker containers are created under the volumes folder well don't worry about that for now we will come back to that in a bit for now let's just understand where docker stores its files and in what format so how exactly does Dockers store the files of an image and a container to understand that we need to understand Dockers layered architecture let's quickly recap something we learned when docker builds images it builds these in a layered architecture each line of instruction in the docker file creates a new layer in the docker image with just the changes from the previous layer for example the first layer is a base Ubuntu operating system followed by the second instruction that creates a second layer which installs all the apt packages and then the third instruction creates a third layer which with the Python packages followed by the fourth layer that copies the source code over and then finally the fifth layer that updates the entry point of the image since each layer only stores the changes from the previous layer it is reflected in the size as well if you look at the base open to image it is around 120 megabytes in size the apt packages that are installed is around 300 MB and then the remaining layers are small to understand the advantages of this layered architecture let's consider a second application this application has a different talker file but it's very similar to our first application as in it uses the same base image as Ubuntu uses the same Python and flask dependencies but uses a different source code to create a different application and so a different entry point as well when I run the docker build command to build a new image for this application since the first three layers of both the applications are the same docker is not going to build the first three layers instead it reuses the same tree layer's it built for the first application from the cache and only creates the last two layers with the new sources and the new entry point this way docker builds images faster and efficiently saves disk space this is also applicable if you were to update your application code whenever you update your application code such as the app dot py in this case docker simply reuses all the previous layers from cache and quickly rebuilds the application image by updating the latest source code thus saving us a lot of time hearing rebuilds and updates let's rearrange the layers bottom up so we can understand it better at the bottom we have the base ubuntu layer than the packages then the dependencies and then the source code of the application and then the entry point all of these layers are created when we run the docker build command to form the final docker image so all of these are the docker image layers once the build is complete you cannot modify the contents of these layers and so they are readonly and you can only modify them by initiating a new build when you run a container based off of this image using the docker run command docker creates a container based off of these layers and creates a new writable layer on top of the image layer the writable layer is used to store data created by the container such as log files written by the applications any temporary files generated by the container or just any file modified by the user on that container the life of this layer though is only as long as the container is alive when the container is destroyed this layer and all of the changes stored in it are also destroyed remember that the same image layer is shared by all containers created using this image if I were to log in to the new created container and say create a new file called temp dot txt it will create that file in the container layer which is read and write we just said that the files in the image layer are readonly meaning you cannot edit anything in those layers let's take an example of our application code since we bake our code into the image the code is part of the image layer and as such is readonly after running a container what if I wish to modify the source code to say test a change remember the same image layer may be shared between multiple containers created from this image so does it mean that I cannot modify this file inside the container no I can still modify this file but before I save the modified file docker automatically creates a copy of the file in the readwrite layer and I will then be modifying a different version of the file in the readwrite layer all future modifications will be done on this copy of the file in the readwrite layer this is called copyonwrite mechanism the image layer being readonly just means that the files in these layers will not be modified in the image itself so the image will remain the same all the time until you rebuild the image using the docker build command what happens when we get rid of the container all of the data that was stored in the container layer also gets deleted the change we made to the app dot py and the new temp file we created we'll also get removed so what if we wish to persist this data for example if we were working with our database and we would like to preserve the data created by the container we could add a persistent volume to the container to do this first create a volume using the docker volume create command so when I run the docker volume create data underscore volume command it creates a folder called data underscore volume under the var Lib docker volumes directory then when I run the docker container using the docker run command I could mount this volume inside the docker containers read/write layer using the dash V option like this so I would do a docker run V then specify my newly created volume name followed by a colon and the location inside my container which is the default location where MySQL stores data and that is where Lib MySQL and then the image name MySQL this will create a new container and mount the data volume we created into ver Lib MySQL folder inside the container so all data are written by the database is in fact stored on the volume created on the docker host even if the container is destroyed the data is still active now what if you didn't run the docker volume create command to create the volume before the docker run command for example if I run the docker run command to create a new instance of MySQL container with the volume data underscore volume to which I have not created yet docker will automatically create a volume named data underscore volume to and mounted to the container you should be able to see all these volumes if you list the contents of the where Lib docker volumes folder this is called volume mounting as we are mounting a volume created by docker under the VAR Lib docker volumes folder but what if we had our data already at another location for example let's say we have some external storage on the docker host at four slash data and we would like to store database data on that volume and not in the default ver Lib docker volumes folder in that case we would run a container using the command docker run V but in this case we will provide the complete path to the folder we would like to mount that is for slash data for / QL and so it will create a container and mount the folder to the container this is called bind mounting so there are two types of mounts a volume mounting and a bind mount volume mount mounts a volume from the volumes directory and bind mount mounts a directory from any location on the docker host one final point note before I let you go using the V is an old style the new way is to use mount option the mount is the preferred way as it is more verbose so you have to specify each parameter in a key equals value format for example the previous command can be written with the mount option as this using the type source and target options the type in this case is bind the source is the location on my host and target is the location on my container so who is respond before doing all of these operations maintaining the layered architecture creating a writable layer moving files across layers to enable copy and write etc it's the storage drivers so Dockery uses storage drivers to enable layered architecture some of the common storage drivers are au FS btrfs ZFS device mapper overlay and overlay to the selection of the storage driver depends on the underlying OS being used for example we to bond to the default storage driver is a u FS whereas this storage driver is not available on other operating systems like fedora or cent OS in that case device mapper may be a better option docker will choose the best storage driver available automatically based on the operating system the different storage drivers also provide different performance and stability characteristics so you may want to choose one that fits the needs of your application and your organization if you would like to read more on any of these storage drivers please refer to the links in the attached documentation for now that is all from the docker architecture concepts see you in the next lecture hello and welcome to this lecture on docker compose going forward we will be working with configurations in yamo file so it is important that you are comfortable with llam let's recap a few things real quick course we first learned how to run a docker container using the docker run command if we needed to setup a complex application running multiple services a better way to do it is to use docker compose with docker compose we could create a configuration file in yamo format called docker compose dot yellow and put together the different services and the options specific to this to running them in this file then we could simply run a docker compose up command to bring up the entire application stack this is easier to implement run and maintain as all changes are always stored in the docker compose configuration file however this is all only applicable to running containers on a single docker host and for now don't worry about the yamo file we will take a closer look at the yamo file in a bit and see how to put it together that was a really simple application that I put together let us look at a better example I'm going to use the same sample application that everyone uses to demonstrate docker it's a simple yet comprehensive application developed by docker to demonstrate the various features available in running an application stack on docker so let's first get familiarized with the application because we will be working with the same application in different sections through the rest of this course this is a sample voting application which provides an interface for a user to vote and another interface to show the results the application consists of various components such as the voting app which is a web application developed in Python to provide the user with an interface to choose between two options a cat and a dog when you make a selection the vote is stored in Redis for those of you who are new to Redis Redis in this case serves as a database in memory this load is then processed by the worker which is an application written in dotnet the worker application takes the new vote and updates the persistent database which is a Postgres SQL in our case the Postgres SQL simply has a table with a number of votes for each category cats and dogs in this case it increments the number of votes for cats as our ward was for cats finally there is of the vote is displayed in a web interface which is another web application developed in node.js this resulting application reads the count of votes from the Postgres sequel database and displays it to the user so that is the architecture and data flow of this simple voting application stack as you can see this sample application is built with a combination of different services different development tools and multiple different development platforms such as Python node.js net etc this sample application will be used to showcase how easy it is to set up an entire application stack consisting of diverse components in docker let us keep aside docker swarm services and stacks for a minute and see how we can put together this application stack on a single docker engine using first docker run commands and then docker compose let us assume that all images of applications are already built and are available on docker repository let us start with the data layer first we run the docker run command to start an instance of Redis by running the docker run Redis command we will add the D parameter to run this container in the background and we will also name the container Redis now naming the containers is important why is that important hold that thought we will come to that in a bit next we will deploy the Postgres sequel database by running the docker run Postgres command this time we will add the d option to run this in the background and name this container DB for database next we will start with the application services we will deploy a frontend app for voting interface by running an instance of voting app image run the docker run command and name the instance boat since this is a web server it has a web you eye instance running on port 80 we will publish that port to 5000 on the host system so we can access it from a browser next we will deploy the result web application that shows the results to the user for this we deploy a container using the results app image and publish port 80 port 5001 on the host this way we can access the web UI of the resulting app on a browser finally we deploy the worker by running an instance of the worker image okay now this is all good and we can see that all the instances are running on the host but there is some problem it just does not seem to work the problem is that we have successfully run all the different containers but we haven't actually linked them together as in we haven't told the voting web application to use this particular Redis instance there could be multiple Redis instances running we haven't told the worker and the resulting app to use this particular Postgres sequel database that we ran so how do we do that that is where we use links link is a command line option which can be used to link two containers together for example the voting app web service is dependent on the Redis service when the web server starts as you can see in this piece of code on the web server it looks for a Redis service running on host Redis but the voting app container cannot resolve a host by the name Redis to make the voting app aware of the Redis service we add a link option while running the voting app container to link it to the Redis container adding a link option to the docker run command and specifying the name of the Redis container which is which in this case is Redis followed by a colon and the name of the host that the voting app is looking for which is also Redis in this case remember that this is why we named the container when we ran the first time so we could use its name while creating a link what this is in fact doing is it creates an entry into the e.t.c host file on the voting app container adding an entry with a host named rebus with the internal IP of the Redis container similarly we add a link for the result app to communicate with the database by adding a link option to refer the database by the name DB as you can see in this source code of the application it makes an attempt to connect to a Postgres database on host DB finally the worker application requires access to both the Redis as well as the Postgres database so we add two links to the worker application one link to link the Redis and the other link to link Postgres database note that using links this way is deprecated and the support may be removed in future in docker this is because as we will see in some time advanced and newer concepts in docker swarm and networking supports better ways of achieving what we just did here with links but I wanted to mention that anyway so you learned the concept from the very basics once we have the docker run commands tested and ready it is easy to generate a docker compose file from it we start by creating a dictionary of container names we will use the same name we used in the docker run commands so we take all the names and create a key with each of them then under each item we specify which image to use the key is the image and the value is the name of the image to use next inspect the commands and see what are the other options used we published ports so let's move those ports under the respective containers so we create a property called ports and list all the ports that you would like to publish under that finally we are left with links so whichever container requires a link created properly under it called links and provide an array of links such as Redis or TB note that you could also specify the name of the link this way without the semicolon and and the target target name and it will create a link with the same name as the target name specifying the DB DB is similar to simply specify DB we will assume the same value to create a link now that we are all done with our docker compose file bringing up the stack is really simple from the docker compose up command to bring up the entire application stack when we looked at the example of the sample voting application we assumed that all images are already built out of the 5 different components two of them Redis and Postgres images we know are already available on docker hub there are official images from Redis and Postgres but the remaining three are our own application it is not necessary that they are already built and available in the docker registry if we would like to instruct docker compose to run a docker bill instead of trying to pull an image we can replace the image line with a build line and specify the location of a directory which contains the application code and a docker file with instructions to build the docker image in this example for the voting app I have all the application code in a folder named vote which contains all application code and a docker file this time when you run the docker compose up command it will first build the images give a temporary name for it and then use those images to run containers using the options you specified before similarly use build option to build the two other services from the respective folders we will now look at different versions of docker compose file this is important because you might see docker compose files in different formats at different places and wonder whitesand look different docker compose evolved over time and now supports a lot more options than it did in the beginning for example this is the trim down version of the docker compose file we used earlier this is in fact the original version of docker compose file known as version 1 has had a number of limitations for example if you wanted to deploy containers on a different network other than the default bridge network there was no way of specifying that in this version of the file also say you have a dependency or startup order of some kind for example your database container must come up first and only then and should the voting application be started there was no way you could specify that in the version 1 of the docker compose file support for these came in version 2 with version 2 and up the format of the file also changed a little bit you no longer specify your stack information directly as you did before it is all encapsulated in a Services section so create a property called services in the root of the file and then move all the services underneath that you will still use the same docker compose up command to bring up your application stack but how does docker compose know what version of the file you're using you're free to use version 1 or version 2 depending on your needs so how does the docker compose know what format you are using for version 2 and up you must specify the version of docker compose file you're intending to use by specifying the version at the top of the file in this case version 2 another difference is with networking in version 1 the Campos attaches all the containers it runs to the default birched Network and then used links to enable communication between the containers as we did before with version 2 docker compose automatically creates a dedicated bridged Network for this application and then attaches all containers to that new network all containers are then able to communicate to each other using each other's service name so you basically don't need to use links in version 2 of docker compose you can simply get rid of all the links you mentioned in version 1 when you convert a file from Wersching 1 to version 2 and finally version 2 also introduces it depends on feature if you wish to specify a startup order for instance say the watering web application is dependent on the Redis service so you need to ensure that Redis container is started first and only then the voting web application must be started we could add a depends on property to the voting application and indicate that it is dependent on Redis then comes version 3 which is the latest as of today version 3 is similar to version 2 in the structure meaning it has a version specification at the top and a Services section under which you put all your services just like in version 2 make sure to specify the version number as 3 at the top version 3 comes with support for docker swamp which we will see later on there are some options that were removed and added to see details on those you can refer to the documentation section using the link in the reference page following this lecture we will see version 3 in much detail later when we discuss about docker stacks let us talk about networks in docker compose getting back to our application so far we have been just deploying all containers on the default bridged Network let us say we modify the architecture a little bit to contain the traffic from the different sources for example we would like to separate the user generated traffic from the applications internal traffic so we create a frontend network dedicated for traffic from users and a backend network dedicated for traffic within the application we then connect the user facing applications which are the voting app and the result app to the frontend network and all the components to an internal backend network so back in our docker compose file note that I have actually stripped out the port section for simplicity sake they're still there but they're just not shown here the first thing we need to do if we were to use networks is to define the networks we are going to use in our case we have two networks front end and back end so create a new property called networks at the root level adjacent to the services in the docker compose file and add a map of networks we are planning to use then under each service create a networks property and provide a list of networks that service must be attached to in case of Redis and DB it's only the backend network in case of the frontend applications such as at the voting app and the result app they require to be a test to both a frontend and backend network you must also add a section for worker container to be added to the backend network I have just omitted that in this slide due to space constraints now that you have seen docker compose files head over to the coding exercises and practice developing some docker compose files that's it for this lecture and I will see you in the next lecture we will now look at docker registry so what is a registry if the containers were the rain then they would rain from the docker registry which are the clouds that's where docker images are stored if the central repository of all docker images let's look at a simple nginx container we run the docker run engine X command to run an instance of the nginx image let's take a closer look at that image name now the name is nginx but what is this image and where is this image pulled from this name follows Dockers image naming convention nginx here is the image or the repository name when you say nginx it's actually nginx slash nginx the first part stands for the user or account name so if you don't provide an account or a repository name it assumes that it is the same as the given name which in this case is nginx the user names is usually your docker hub account name or if it is an organization then it's the name of the organization if you were to create your own account and create your own repositories or images under it then you would use a similar pattern now where are these images stored and pulled from since we have not specified the location where these images are to be pulled from it is assumed to be on Dockers default registry docker hub the dns name for which is docker dial the registry is where all the images are stored whenever you create a new image or update an existing image you push it to the registry and every time anyone deploys this application it is pulled from that registry there are many other popular registries as well for example Google's registry is at GC r dot IO where a lot of kubernetes related images are stored like the ones used for performing endtoend tests on the cluster these are all publicly accessible images that anyone can download and access when you have applications built inhouse that shouldn't be made available to the public hosting an internal private registry may be a good solution many cloud service providers such as AWS as your GCP provide a private registry by default when you open an account with them on any of these solutions be a docker hub or Google registry or your internal private registry you may choose to make a repository private so that it can only be accessed using a set of credentials from Dockers perspective to run a container using an image from a private registry you first log into your private registry using the docker login command input your credentials once successful run the application using private registry as part of the image name like this now if you did not log into the private registry he will come back saying that the image cannot be found so remember to always log in before pulling or pushing to a private registry we said that cloud providers like AWS or GCP provide a private registry when you create an account with them but what if you are running your application onpremise and don't have a private registry how do you deploy your own private registry within your organization the docker registry is itself another application and of course is available as a docker image the name of the image is registry and it exposes the API on port 5,000 now that you have your custom registry running at port 5,000 on this docker host how do you push your own image to it use the docker image tag command to tag the image with a private registry URL in it in this case since it's running on the same door host I can use localhost semi colon 5000 followed by the image name I can then push my image to my local private registry using the command docker push and the new image name with the docker registry information in it from there on I can pull my image from anywhere within this network using either localhost you're on the same host or the IP or domain name of my docker host if I'm accessing from another host in my environment well let's sit for this lecture hello words of the practice test and practice working with private docker registries welcome to this lecture on docker engine in this lecture we will take a deeper look at Dockers architecture how it actually runs applications in isolated containers and how it works under the hood docker engine as we have learned before is simply referred to a host with docker installed on it when you install docker on a Linux host you're actually installing three different components the docker daemon the rest API server and the docker CLI the docker daemon is a background process that manages docker objects such as the images containers volumes and networks the docker REST API server is the API interface that programs can use to talk to the daemon and provide instructions you could create your own tools using this REST API and the docker CLI is nothing but the commandline interface that we've been using until now to perform actions such as running a container stopping containers destroying images etc it uses the REST API to interact with the docker demon something to note here is that the docker CLI need not necessarily be on the same host it could be on another system like a laptop and can still work with a remote docker engine simply use the dash H option on the docker command and specify the remote docker engine address and a port as shown here for example to run a container based on ng I and X on a remote docker host run the command docker H equals 10.1 23 2000 call in' 375 run ngan X now let's try and understand how exactly our applications containerized in docker how does it work under the hood docker uses namespaces to isolate workspace process IDs network interprocess communication mounds and unix time sharing systems are created in their own namespace thereby providing isolation between containers let's take a look at one of the namespace isolation technique process ID namespaces whenever a Linux system boots up it starts with just one process with a process ID of one this is the root process and kicks off all the other processes in the system by the time the system boots up completely we have a handful of processes running this can be seen by running the PS command to list all the running processes the process IDs are unique and two processes cannot have the same process ID now if we were to create a container which is basically like a child system within the current system the child system needs to think that it is an independent system on its own and it has its own set of processes originating from a root process with a process ID of one but we note that there is no hard isolation between the containers and the underlying host so the processes running inside the container or in fact processes running on the underlying host and so two processes cannot have the same process ID of one this is where namespaces come into play with process ID namespaces each process can have multiple process IDs associated with it for example when the processes start in the container it's actually just another set of processes on the base Linux system and it gets the next available process ID in this case five and six however they also get another process ID starting with PID one in the container namespace which is only visible inside the container so the container thinks that it has its own root process tree and so it is an independence so how does that relate to an actual system how do you see this on a host let's say I were to run an ng I in X Server as a container we know that the nginx container runs an NGO next service if we were to list all the services inside the docker container we see that the ng INX service running with a process ID of 1 this is the process ID of the service inside of the container namespace if we list the services on the docker host we will see the same service but with a different process ID that indicates that all processes are in fact running on the same host but separated into their own containers using namespaces so we learned that the underlying docker host as well as the containers share the same system resources such as CPU and memory how much of the resources are dedicated to the host and the containers and how does docker manage and share the resources between the containers by default there is no restriction as to how much of a resource a container can use and hence a container may end up utilizing all of the resources on the underlying host but there is a way to restrict the amount of CPU or memory a container can use docker uses three groups or control groups to restrict the amount of hardware resources allocated to each container this can be done by providing the CPUs option to the docker run command providing a value of 0.5 will ensure that the container does not take up more than 50% of the host CPU at any given time the same goes with memory setting a value of 100 m to the memory option limits the amount of memory the container can use to a hundred megabytes if you are interested in reading more on this topic refer to the links I posted in the reference page that's it for now on docker engine earlier in this course we learned that containers share the underlying OS kernel and as a result we cannot have a Windows container running on Linux host or vice versa we need to keep this in mind while going through this lecture as it's very important concept and most beginners tend to have an issue with it so what are the options available for docker on Windows there are two options available the first one is darker on Windows using docker toolbox and the second one is the docker desktop for Windows option we will look at each of these now let's take a look at the first option docker toolbox this was the original support for docker on Windows imagine that you have a Windows laptop and no access to any Linux system whatsoever but you would like to try docker you don't have access to a Linux system in the lab or in the cloud what would you do what I did was to install a virtualization software on my windows system like Oracle VirtualBox or VMware Workstation and deploy a Linux VM on it such as Ubuntu or Debian then install docker on the Linux VM and then play around with it this is what the first option really does it doesn't really have anything much to do with Windows you cannot create Windows based docker images or run Windows based docker containers you obviously cannot run Linux container directly on Windows either you're just working with docker on a Linux virtual machine on a Windows host docker however provides us with a set of tools to make this easy which is called as the docker toolbox the docker toolbox contains a set of tools like Oracle VirtualBox docker engine docker machine docker compose and a user interface called kite Matic this will help you get started by simply downloading and running the docker toolbox executable it will install VirtualBox deploy a lightweight VM called boot to docker which has darker running in it already so that you are all set to start with docker easily and with within a short period now what about requirements you must ensure that your operating system is 64bit windows 7 or higher and that the virtualization is enabled on the system now remember docker to box is a legacy situation for older Windows systems that do not meet requirements for the newer docker for Windows option the second option is the newer an option called docker desktop for Windows in the previous option we saw that we had Oracle VirtualBox installed on Windows and then a Linux system and then docker on that Linux system now with docker for Windows we take out Oracle VirtualBox and use the native virtualization technology available with Windows called Microsoft hyperv during the installation process for docker for Windows it will still automatically create a Linux system underneath but this time it is created on the Microsoft hyperv instead of Oracle VirtualBox and have docker running on that system because of this dependency on hyperv this option is only supported for Windows 10 enterprise or professional Edition and on Windows Server 2016 because both these operating systems come with hyperv support by default now here's the most important point so far whatever we have been discussing with Dockers support for Windows it is strictly for Linux containers Linux applications packaged into Linux docker images we're not talking about Windows applications or Windows images or windows containers both the options we just discussed will help you run a Linux container on a Windows host with Windows Server 2016 Microsoft announced support for Windows containers for the first time you can now packaged applications Windows applications into Windows docker containers and run them on Windows docker host using docker desktop for Windows when you install docker desktop for Windows the default option is to work with Linux containers but if you would like to run Windows containers then you must explicitly configure docker for Windows to switch to using Windows containers in early 2016 Microsoft announced Windows containers now you could create Windows based images and run Windows containers on a Windows server just like how you would run Linux containers on a Linux system now you can create Windows images container as your applications and share them through the docker store as well unlike in Linux there are two types of containers in Windows the first one is a Windows Server container which works exactly like Linux containers where the OS kernel is shared with the underlying operating system to allow better security boundary between containers and to a lot of kernels with different versions and configurations to coexist the second option was introduced known as the hyperv isolation with hyperv isolation each container is run within a highly optimized virtual machine guaranteeing complete kernel isolation between the containers and the underlying host now while in the linux world you had a number of base images for linux systems such as Ubuntu debian fedora Alpine etc if you remember that that is what you specify at the beginning of the docker file in the windows world we have two options the windows server core and nano server a nano server is a headless deployment option for Windows Server which runs at a fraction of size of the full operating system you can think of this like the Alpine image in Linux the windows server core though is not a slight weight as you might expect it to be finally windows containers are supported on Windows Server 2016 nano server and windows 10 professional and Enterprise Edition remember on Windows 10 professional and Enterprise Edition only supports hyperv isolated containers meaning as we just discussed every container deployed is deployed on a highly optimized virtual machine well that's it about docker on windows now before I finish I want to point out one important fact we saw two ways of running a docker container using VirtualBox or hyper week but remember VirtualBox and hyperv cannot coexist on the same windows host so if you started off with duck or toolbox with VirtualBox and if you plan to migrate to hyperv remember you cannot have both solutions at the same time there is a migration guide available on docker documentation page on how to migrate from Rochelle box to hyper wait that's it for now thank you and I will see you the next lecture we now look at docker on Mac Locker on Mac is similar to docker on Windows there are two options to get started docker on Mac using docker toolbox or docker Desktop for Mac option let's look at the first option docker toolbox this was the original support for docker on Mac it is darker on a Linux VM created using VirtualBox on Mac as with Windows it has nothing to do with Mac applications or Mac based images or Mac containers it purely runs Linux containers on a Mac OS docker toolbox contains a set of tools like Oracle VirtualBox docker and Jain docker machine docker compose and a user interface called CAD Matic when you download and install the docker toolbox executable it installs VirtualBox deploys lightweight VM called boot a docker which has dr running in it already this requires mac OS 10.8 or newer the second option is the newer option called docker Desktop for Mac with docker Desktop for Mac we take out or commercial box and use hyper cat virtualization technology during the installation process for docker for Mac it will still automatically create a Linux system underneath but this time it is created on hyper kit instead of Oracle VirtualBox and have dr. running on that system this requires Mac OS Sierra 10 or 12 or newer and Martin and the Mac hardware must be 2010 or newer model finally remember that all of this is to be able to run the Linux container on Mac as of this recording there are no Mac based images or containers well that's it with docker on Mac for now we will now try to understand what container orchestration is so far in this course we've seen that with docker you can run a single instance of the application with a simple docker run command in this case to run a node.js based application you're on the docker run nodejs command but that's just one instance of your application on one docker host what happens when the number of users increase and that instance is no longer able to handle the load you deploy additional instance of your application by running the docker run command multiple times so that's something you have to do yourself you have to keep a close watch on the load and performance of your application and deploy additional instances yourself and not just that you have to keep a close watch on the health of these applications and if a container was to fail you should be able to detect that and run the docker run commander game to deploy another instance of that application what about the health of the docker host itself what if the host crashes and is inaccessible the containers hosted on that host become inaccessible too so what do you do in order to solve these issues you will need a dedicated engineer who can sit and monitor the state performance and health of the containers and take necessary actions to remediate the situation but when you have large applications deployed with tens of thousands of containers that's that's not a practical approach so you can build your own scripts and and that will help you tackle these issues to some extent container orchestration is just a solution for that it is a solution that consists of a set of tools and scripts that can help host containers in a production environment typically a container orchestration solution consists of multiple docker hosts that can host containers that way even if one fails the application is still accessible through the others the container orchestration solution easily allows you to deploy hundreds or thousands of instances of your application with a single command this is a command used for docker swarm we will look at the command itself in a bit some orchestration solutions can help you automatically scale up the number of instances when users increase and scale down the number of instances when the demand decreases some solutions can even help you in automatically adding additional hosts to support the user load and not just clustering and scaling the container orchestration solutions also provide support for advanced networking between these containers across different hosts as well as load balancing user requests across different house they also provide support for sharing storage between the house as well as support for configuration management and security within the cluster there are multiple container orchestration solutions available today docker has to her swamp kubernetes from Google and mezzo mezz from Paget well docker swamp is really easy to set up and get started it lacks some of the advanced auto scaling features required for complex production grade applications mezzos on the other hand is quite difficult to set up and get started but supports many advanced features kubernetes arguably the most popular of it all is a bit difficult to set up and get started but provides a lot of options to customize deployments and has support for many different vendors kubernetes is now supported on all public cloud service providers like GCP azure and AWS and the kubernetes project is one of the topranked projects on github in the upcoming lectures we will take a quick look at docker swamp and kubernetes we will now get a quick introduction to docker swarm dr. Swan has a lot of concepts to cover and requires its own course but we will try to take a quick look at some of the basic details so you can get a brief idea on what it is with docker swamp you could now combine multiple docker machines together into a single cluster docker swarm will take care of distributing your services or your application instances into separate hosts for high availability and for load balancing across different systems and hardware to set up a docker swamp you must first have hosts or multiple hosts with docker installed on them then you must designate one host to be the manager or the master or the swamp manager as it is called and others as slaves or workers once you're done with that run the docker swarm init command on the swarm manager and that will initialize the swamp manager the output will also provide the command to be run on the workers so copy the command and run it on the worker nodes to join the manager after joining the swamp the workers are also referred to as nodes and you're now ready to create services and deploy them on the swamp cluster so let's get into some more details as you already know to run an instance of my web server I run the docker run command and specify the name of the image I wish to run this creates a new container instance of my application and serves my web server now that we have learned how to create a swamp cluster how do I utilize my cluster to run multiple instances of my web server now one way to do this would be to run the docker run command on each worker node but that's not ideal as I might have to login to each node and run this command and there there could be hundreds of nodes I will have to setup load balancing myself a large monitor the state of each instance myself and if instances were to fail I'll have to restart them myself so it's going to be an impossible task that is where docker swarm orchestration consent dr. Swan Orchestrator does all of this for us so far we've only set up this one cluster but we haven't seen orchestration in action the key component of swarm orchestration is the Ducker a service dog or services are one or more instances of a single application or service that runs across to saw the nodes in the strong cluster for example in this case I could create a docker service to run multiple instances of my web server application across worker nodes in my swamp cluster for this around the docker service create command on the manager node and specify my image name there which is my web server in this case and use the option replicas to specify the number of instances of my web server I would like to run across the cluster since I specified three replicas and I get three instances of my web server distributed across the different worker nodes remember the docker service command must be run on the manager node and not on the worker node the docker service create command is similar to the docker run command in terms of the options passed such as the II environment variable the P for publishing ports at the network option to attach container to a network etc well that's a highlevel introduction to docker Swan there's a lot more to know such as configuring multiple managers overlay networks etc as I mentioned it requires its own separate course well that's it for now in the next lecture we will look at kubernetes at a higher level we will now get a brief introduction to basic kubernetes concepts again kubernetes requires its own course well a few courses at least five but we will try to get a brief introduction to it here with docker you were able to run a single instance of an application using the docker CLI by running the docker run command which is grid running an application has never been so easy before with kubernetes using the kubernetes CLI known as cube control you run a thousand instance of the same application with a single command kubernetes can scale it up to two thousand with another command kubernetes can be even configured to do this automatically so that instances and the infrastructure itself can scale up and down based on user load kubernetes can upgrade these 2,000 instances of the application in a rolling upgrade fashion one at a time with a single command if something goes wrong it can help you roll back these images with a single command kubernetes can help you test new features of your application by only upgrading a percentage of these instances through a be testing methods the kubernetes open architecture provides support for many many different network and storage vendors any network or storage brand that you can think of has a plugin for kubernetes kubernetes supports a variety of authentication and authorization mechanisms all major cloud service providers have native support for kubernetes so what's the relation between docker and kubernetes well kubernetes uses docker host to host applications in the form of docker containers well it need not be docker all the time kubernetes supports alternatives to Dockers as well such as rocket or a cryo but let's take a quick look at the kubernetes architecture a kubernetes cluster consists of a set of nodes let us start with nodes a node is a machine physical or virtual on which a cobranet is the kubernetes software a set of tools are installed a node is a worker machine and that is where containers will be launched by kubernetes but what if the node on which the application is running fails well obviously our application goes down so you need to have more than one nodes a cluster is a set of nodes grouped together this way even if one node fails you have your application still accessible from the other nodes now we have a cluster but who is responsible for managing this class where is the information about the members of the cluster stored how are the nodes monitored when a node fails how do you move the workload of the failed nodes to another worker node that's where the master comes in the master is a note with the kubernetes control plane components installed the master watches over the notes are in the cluster and is responsible for the actual orchestration of containers on the worker notes when you install kubernetes on a system you are actually installing the following components an API server and EDD server a cubelet service contain a runtime engine like docker and a bunch of controllers and the scheduler the API server acts as the front end for kubernetes the users management devices command line interfaces all talk to the API server to interact with the kubernetes cluster next is the at CD a key value store the ED CD is a distributed reliable key value store used by kubernetes to store all data used to manage the cluster think of it this way when you have multiple nodes and multiple masters in your cluster let CD stores all that information on all the nodes in the cluster in a distributed manner a CD is responsible for implementing logs within the cluster to ensure there are no conflicts between the masters the scheduler is responsible for distributing work or containers across multiple nodes it looks for newly created containers and assigns them to notes the controllers are the brain behind orchestration they're responsible for noticing and responding when nodes containers or endpoints goes down the controllers makes decisions to bring up new containers in such cases the container runtime is the underlying software that is used to run containers in our case it happens to be docker and finally cubelet is the agent that runs on each node in the cluster the agent is responsible for making sure that the containers are running on the nodes as expected and finally we also need to learn a little bit about one of the commandline utilities known the cube commandline tool or the cube control tool or your cuddle as it is also called the cube control tool is the kubernetes CLI which is used to deploy and manage applications on a kubernetes cluster to get cluster related information to get the status with the nodes in the cluster and many other things the cube control run command is used to deploy an application on the cluster the keep control cluster info command is used to view information about the cluster and the cube control get nodes command is used to list all the notes part of the cluster so to run hundreds of instances of your application across hundreds of nodes all I need is a single kubernetes command like this well that's all we have for now a quick introduction to Panetta's and its architecture we currently have three courses on code cloud on kubernetes that will take you from the absolute beginner to a certified expert so have a look at it when you get a chance so we're at the end of this beginners course to docker I hope you had a great learning experience if so please leave a comment below if you like my way of teaching you will love my other courses hosted on my site at code cloud we have courses for docker swarm kubernetes advanced courses on kubernetes certifications as well as openshift we have courses for automation tools like ansible chef and puppet and many more on the way with it code cloud at www.calculated.com/support you
