With timestamps:

00:00 - first of all let's see what is computer
00:02 - vision because opencv is an open source
00:06 - computer vision library
00:08 - so computer vision is the way of
00:11 - teaching intelligence to machines and
00:13 - making them see things just like humans
00:17 - so what happens when a human see an
00:20 - image
00:21 - he will be able to recognize the faces
00:24 - which are there inside the images so in
00:27 - its simplest form computer vision is
00:30 - what allows computers to see and process
00:33 - visual data just like humans computer
00:36 - vision involves analyzing images to
00:39 - produce useful information
00:41 - so to give you some examples a
00:44 - self-driving car
00:45 - it can detect the lanes using computer
00:48 - visions or you might have wondered how
00:51 - facebook detects images when you upload
00:55 - the images of you with your friends it
00:57 - becomes possible by facebook's face and
01:00 - image recognition technology so now
01:03 - let's see what is opencv so opencv which
01:07 - stands for open source computer vision
01:10 - is a library of programming functions
01:13 - mainly aimed at real-time computer
01:16 - vision
01:17 - it is originally developed by intel and
01:20 - then it was later supported by a
01:23 - developer called willow garage and now
01:25 - it is supported and maintained by itc's
01:29 - now opencv is available on mac windows
01:32 - and various linux operating systems so
01:35 - we can say that opencv is a
01:37 - cross-platform library now you can work
01:39 - on opencv using c c plus plus or python
01:43 - and we will be using python to learn
01:45 - opencv
01:47 - now opencv is a open source and free
01:50 - library which is licensed under bsd
01:54 - license and it said that it is very easy
01:58 - to use and install that we will see when
02:00 - we will install opencv on various
02:04 - operating systems now because opencv
02:07 - primarily deals with computer vision
02:10 - that means dealing with mainly images or
02:13 - videos so i wanted to show you how a
02:17 - digital image is seen by a computer so
02:21 - digital images are typically stored in
02:24 - the form of matrix now if you have heard
02:28 - about ppi or pixel per inch which refers
02:32 - display resolution that means how many
02:35 - individual pixels are displayed
02:38 - in one inch of digital image so when a
02:42 - computer sees a picture
02:44 - it sees it in the form of pixel matrix
02:48 - now there are two type of digital images
02:51 - one are called grayscale images and
02:54 - other are called colored images so in
02:57 - grayscale images
02:59 - each pixel represents the intensity of
03:02 - only one shade that means how bright or
03:06 - dark the pixel is
03:08 - in other word it is said that it has
03:11 - only one channel so on the right hand
03:14 - side you can see a grayscale image and
03:17 - on the left hand side you can see a
03:20 - colored image so in colored images we
03:23 - have three channels
03:25 - that is r g b which stands for red green
03:30 - blue so grayscale images have one
03:33 - channel and colored images have three
03:37 - channels your standard digital camera
03:40 - have three channels that means red green
03:43 - blue channels so we will learn more
03:45 - about images and how we can process
03:47 - images using opencv in the later videos
03:51 - now there is one more thing which i want
03:53 - to show you is
03:55 - numpy
03:56 - so we are going to learn opencv using
03:59 - python so when you will install opencv
04:02 - library for python on your operating
04:05 - system
04:06 - numpy will be automatically installed
04:08 - with this library so first of all what
04:11 - is numpy so numpy is a highly optimized
04:15 - library for numerical operations now as
04:18 - i told you digital images are 2d arrays
04:21 - of pixels and numpy library is a general
04:24 - purpose array processing package library
04:28 - so it provides a high performance
04:31 - multi-dimensional array object and tools
04:34 - for working with these arrays which
04:37 - makes the processing of images easier
04:41 - now all the opencv array structures are
04:43 - converted to and converted from numpy
04:47 - arrays and in addition you can use more
04:50 - convenient indexing system rather than
04:54 - using
04:55 - for loops so when you want to learn
04:57 - opencv using python you need to have
05:00 - some knowledge about numpy also so if
05:04 - you have some knowledge of numpy library
05:06 - it's good but don't worry i will teach
05:08 - you step by step so you will not miss
05:11 - anything so that was a brief
05:13 - introduction about computer vision and
05:15 - opencv
05:17 - in this video i'm going to show you how
05:18 - you can install opencv for python on
05:22 - your windows operating system
05:25 - so obviously you need to install python
05:27 - on your windows operating system in
05:29 - order to install opencv for python so
05:32 - first of all i'm going to show you how
05:34 - you can install python on your windows
05:36 - operating system and then we will see
05:39 - how to install opencv using python now
05:43 - if you have already installed python on
05:45 - your windows operating system you can
05:47 - skip about five minutes of this video
05:51 - and go directly to the point where
05:54 - i am going to show you how you can
05:56 - install opencv
05:58 - for python so let's get started so first
06:01 - of all open your favorite browser on
06:03 - your windows 10 operating system and
06:05 - then search for python and the first
06:07 - link which will appear here will be from
06:09 - python.org so we are going to click on
06:11 - that link and once this python.org
06:13 - website is open you just need to scroll
06:15 - down a little until you see this
06:17 - downloads section
06:19 - and you can see at the time of making
06:21 - this video python 3.7.0 is the latest
06:24 - version of python available so we are
06:27 - going to click on this link which says
06:28 - python 3.7.0 and you will be redirected
06:32 - to this page which says python
06:34 - 3.7.0 and now i'm going to scroll down
06:36 - until i see
06:38 - the files here and you will see there
06:40 - are various kinds of installer available
06:43 - here we are going to install the python
06:45 - using the executable installer so we are
06:47 - going to choose this option which says
06:49 - windows x86 hyphen 64 executable
06:53 - installer and now i will wait for this
06:55 - executable to be downloaded and once
06:57 - this executable is downloaded you just
06:59 - need to click on this exe file and i'm
07:02 - going to minimize the browser here so
07:04 - you can see python's 3.7.0 setup window
07:07 - has been started and on the first window
07:09 - you will see two options here one is
07:12 - install now and other is customize
07:15 - installation
07:16 - so what we are going to choose is this
07:18 - option which says customize installation
07:21 - because when you choose this install now
07:23 - option python will be installed at this
07:26 - part which i don't want to use you can
07:30 - see it's a long path which i don't want
07:32 - to remember so i will use this option
07:35 - which says customize installation and i
07:38 - will also check this option which says
07:39 - add python 3.7 to path so now let's
07:42 - click on customize installation and next
07:45 - you will see this optional feature
07:47 - window and you can see there are some
07:50 - optional feature which this python
07:52 - installer will install for example
07:54 - documentation pip it will install which
07:56 - is a python package manager idle ide
08:00 - python test suit and other feature it's
08:03 - going to install so i'm going to leave
08:04 - everything as default and then i'm going
08:06 - to click next and now this next window
08:08 - will open which says advanced option
08:11 - here i'm going to check this option
08:13 - which says install for all users and i'm
08:16 - going to leave other check boxes as
08:19 - checked and then you will see this
08:21 - section here which says customize
08:22 - install location so i want to install
08:24 - python on my c directory so what i'm
08:26 - going to do is i'm going to open the
08:28 - windows explorer and i'm going to go to
08:31 - the c directory here
08:33 - and once the c directory is open i'm
08:35 - going to right click here and i'm going
08:36 - to create a new directory and i'm going
08:38 - to name this directory as python and
08:41 - then i'm going to press enter
08:43 - and this path i'm going to give here in
08:46 - the customize install location so i'm
08:49 - going to just give this part which says
08:51 - see colon slash
08:53 - python and then backslash python37
08:57 - 37 here means that we are going to
08:59 - install 3.7 version of python so now
09:02 - python will be installed at this
09:04 - location on my computer and then i'm
09:06 - going to click on the install button
09:08 - here and then you will see the
09:09 - installation will start and it will be
09:11 - finished in a few seconds so just wait
09:13 - for the installation to complete and
09:15 - after some time i can see this message
09:16 - which says setup was successful so i'm
09:18 - going to click on this close button
09:20 - which is going to close this installer
09:22 - so now in order to check whether python
09:24 - is installed on our windows operating
09:26 - system or not we are going to search for
09:28 - python here and you will see few options
09:31 - here one is this python 3.7 terminal
09:35 - other is
09:37 - idle ide so first of all we are going to
09:39 - click on this option which says python
09:41 - 3.7 64-bit which is going to open this
09:44 - kind of terminal so this is a python
09:47 - terminal and here we can for example
09:50 - print something so i am going to just
09:51 - write print and in the parenthesis and
09:54 - in between the double quotes i can just
09:56 - write hello
09:58 - world and then press enter which is
10:00 - going to in return print hello world
10:03 - that means python 3.7 terminal is
10:06 - working so i'm going to close this
10:08 - terminal now and once again i'm going to
10:10 - search for python here and this time i'm
10:12 - going to select this option which says
10:14 - idle okay so just select this option
10:16 - which says idle and in the parenthesis
10:18 - python 3.7 64-bit so this idle is an ide
10:23 - which comes with python installation at
10:26 - the time of installation we have chosen
10:28 - this option to install idle that's why
10:30 - we can see this option here and also
10:32 - this is an interactive shell so you can
10:34 - once again write a print and inside the
10:37 - parenthesis you can just write for
10:39 - example once again hello
10:42 - world and then press enter and it's
10:44 - going to give you this kind of output
10:46 - here so now python interactive shell is
10:48 - working and idle ide is also working so
10:51 - i'm going to close this idle ide and now
10:54 - i want to check whether python is
10:56 - working using my command prompt or not
10:58 - so i'm going to right click on this
11:00 - windows button and then i'm going to
11:03 - click on command prompt
11:05 - and here i'm going to first of all write
11:07 - python and then press enter
11:10 - and you can see this python option is
11:12 - working now even on your command prompt
11:16 - right so here also you can just write
11:18 - print and inside the parenthesis you can
11:21 - just print hello
11:23 - world and then press enter and it prints
11:26 - hello world in return
11:31 - now once python is installed on your
11:34 - windows 10 operating system we are going
11:36 - to install opencv using pip
11:39 - now pip is automatically installed on
11:42 - your windows operating system with the
11:45 - python installation so you don't need to
11:48 - separately install pip on your windows
11:51 - operating system it comes automatically
11:53 - with your python installation so to
11:57 - verify this first of all i'm going to
11:59 - check the python version so you can just
12:02 - give this command python hyphen hyphen
12:06 - version
12:07 - and then you can check the pip version
12:09 - so you can just give this command pip
12:12 - hyphen hyphen
12:13 - version
12:15 - so just give this command and it's going
12:16 - to give you the version of pip which is
12:19 - installed on your windows operating
12:21 - system so to install opencv using pip
12:24 - you just need to give this command pip
12:26 - install
12:28 - open
12:28 - cv
12:30 - hyphen python and i'm going to press
12:32 - enter so you can see opencv related
12:35 - packages are downloading now so now
12:38 - opencv python package is installed using
12:41 - pip on my windows operating system now
12:44 - you will observe one more thing here and
12:46 - that is numpy package will be
12:49 - automatically installed with your opencv
12:53 - python package so now once opencv python
12:56 - package is installed we can verify it by
12:59 - just opening our python shell so i'm
13:02 - going to just give a python command to
13:05 - open the python shell and then here i'm
13:08 - going to just write
13:09 - import
13:11 - cv2 okay so once you give this command
13:15 - it should not give you any error and if
13:18 - this import gives you error that means
13:20 - opencv is not correctly installed on
13:23 - your operating system now after
13:26 - importing you can just check the version
13:28 - of opencv which you have installed
13:31 - using cv2 dot
13:33 - underscore underscore
13:36 - version underscore underscore and then
13:38 - press enter and it's going to give you
13:41 - the version of opencv which is installed
13:44 - on your operating system and in our case
13:46 - this is 4.0.0 at the time of making this
13:49 - video now you can check the same by
13:52 - writing your code inside
13:55 - a python file also so here i have opened
14:00 - my visual studio code editor and i have
14:02 - already created sample.py file and here
14:06 - also i'm going to import the cv2 package
14:10 - first of all and then i'm going to print
14:13 - the version of cv2 using this print
14:16 - statement so i'm going to just write cv2
14:19 - dot underscore underscore
14:22 - version
14:23 - underscore underscore and then save this
14:26 - script and to open the terminal inside
14:29 - visual studio code you can just press
14:31 - ctrl shift p
14:33 - and then type
14:35 - toggle integrated terminal so just type
14:37 - toggle integrated terminal and then
14:39 - click on this first option which says
14:41 - toggle integrated terminal this is going
14:44 - to open the terminal inside your visual
14:47 - studio code editor so here you can run
14:50 - your python script using the python
14:52 - command so python and then name of the
14:55 - script which is sample.py in my case and
14:58 - then press enter and it's also going to
15:00 - give you the version of opencv which is
15:03 - installed on your operating system
15:05 - so this is how you can install opencv
15:08 - for python on your windows operating
15:10 - system in the last video we have seen
15:12 - how we can install
15:14 - opencv for python now from this video we
15:16 - will actually start writing some code
15:19 - now moving forward i will be using
15:21 - pycharm ide to demonstrate how opencv
15:24 - works but you are free to use any ide or
15:28 - any other editor in order to use opencv
15:33 - now on a pycharm ide you need to install
15:37 - opencv little bit differently
15:41 - so if you are using opencv you just need
15:44 - to
15:44 - create a project inside opencv
15:47 - and then you just need to click on file
15:50 - and then go to the settings
15:53 - now once the settings window opens
15:56 - you just need to go to the project and
15:59 - then it'll say after colon your project
16:03 - name so my project name is opencv
16:05 - examples that's why it's written here
16:08 - opencv examples so project colon your
16:11 - project name so just click on this
16:14 - section and then click on project
16:16 - interpreter and on the right hand side
16:18 - you will see all the packages which
16:20 - comes pre-installed when you create a
16:23 - project inside pycharm ide
16:27 - now we want
16:28 - opencv python package so to install
16:32 - opencv python package
16:34 - on pycharm you just need to click on
16:36 - this plus button here
16:38 - and then you just need to type
16:41 - open cv hyphen python now the first
16:45 - result you can see here is opencv hyphen
16:48 - python and the version which is
16:51 - available right now is 4.0.0
16:54 - 0.21 which is the latest version so to
16:58 - install this package for your pycharm
17:00 - ide you just need to click on install
17:02 - package
17:03 - button and then after some time you will
17:05 - see this message which says package
17:07 - opencv hyphen python installed
17:09 - successfully in the green bar that means
17:12 - opencv package is installed successfully
17:15 - so you can close this window and now you
17:18 - will be able to see opencv hyphen python
17:21 - is added to your packages and also numpy
17:25 - is added to your packages which comes
17:27 - with your opencv python package
17:30 - so i'm going to just click ok and now
17:33 - you will be able to import this cv2
17:36 - package in your python script now in
17:39 - this video i'm going to show you
17:41 - how you can read images and write images
17:45 - using
17:46 - cv2 package now let me show you where
17:49 - you can find some sample images for your
17:52 - project
17:53 - so you can open
17:55 - the browser and then go to this url
17:59 - github.com
18:01 - forward slash opencv
18:03 - so just go to this url and then under
18:06 - this opencv project in github you will
18:09 - be able to see these repositories you
18:13 - just need to choose this repository
18:15 - which says opencv
18:17 - and then
18:18 - you can scroll down
18:21 - and all the images you will find inside
18:24 - the samples folder so i'm going to go
18:26 - inside the sample folder and then
18:29 - inside the sample folder you just need
18:31 - to go inside the data folder so here you
18:34 - will find many sample images and videos
18:38 - and other files which you can use in
18:41 - your project for the learning purpose
18:45 - so you can use these images in order to
18:49 - develop your example
18:51 - so what i generally do is i just go to
18:54 - this repository
18:56 - which is under the url github.com
18:59 - forward slash opencv forward slash
19:01 - opencv and then i either download the
19:06 - zip file of this project or clone this
19:10 - github repository on my operating system
19:15 - and once you
19:16 - clone or download this repository it
19:19 - will look like this so it will be
19:22 - downloaded as this folder which is
19:25 - opencv hyphen master
19:27 - and once again you can go to the samples
19:30 - folder here and inside the samples
19:33 - folder you can go to the data folder and
19:36 - you will find all those images which i
19:39 - have shown you on the github repository
19:42 - now to start with we will be using
19:45 - this image which is lena dot jpg so i'm
19:49 - going to just copy
19:50 - this image for now and then i'm going to
19:53 - go to my pycharm ide and then i'm going
19:57 - to just paste this image inside my
20:00 - project so this jpg image will be
20:04 - directly available inside my project
20:07 - folder
20:08 - now let's see how we can read images
20:11 - using the cv2 module
20:14 - so you just need to
20:16 - use cv2 and there is a method called i
20:20 - am read
20:22 - which enables you to read the images so
20:25 - the first argument which you need to
20:27 - give here is the image name so i'm going
20:29 - to give the image name which is lenna
20:33 - dot jpg
20:36 - and the second argument here is a flag
20:40 - so there are three flags you can give
20:43 - here you can either give 0
20:46 - or
20:47 - one
20:48 - or
20:49 - minus one flag here so this second
20:52 - argument is a flag which specifies the
20:55 - way images should be read so let me show
20:58 - you all the flags here so the first flag
21:01 - is cv2 dot i am read underscore color or
21:06 - you can give the integer value of it
21:08 - which is one and whenever you give this
21:11 - flag as the second argument of imread
21:14 - function it's going to load the colored
21:17 - image if you give this flag which is cb2
21:20 - dot i'm read underscore grayscale or if
21:24 - you give this
21:25 - integer value which is 0 it's going to
21:28 - load your image in grayscale mode and
21:32 - the third flag is i am read underscore
21:35 - unchanged or the value -1 which is going
21:38 - to load
21:39 - your image
21:41 - as it is including the alpha channel so
21:44 - for now we are going to just give here
21:47 - zero flag which means we want to load
21:50 - our image in grayscale so now let's run
21:53 - the code and let's see what happens
21:55 - until this point
21:57 - so you can see our code runs fine
21:59 - without giving any error
22:02 - now let me give any random
22:04 - name here as the file name and once
22:08 - again run this code and once again you
22:11 - will see that there is no exception
22:14 - which is thrown here so even if you give
22:17 - the wrong
22:19 - file path or file name here
22:22 - this function is not going to give you
22:24 - any error now in case of wrong path or
22:28 - wrong file name let's say i'm going to
22:32 - just assign
22:33 - this value to a new variable which is
22:36 - img and let's print the value of this
22:40 - img using
22:42 - the print method and then let me run the
22:45 - code once again
22:46 - and you can see whenever you will give
22:49 - the wrong file name here or wrong path
22:52 - here as a result of this method you will
22:56 - get none so you can check the value of
22:59 - image and if it is equal to none then
23:03 - you know that you have done something
23:05 - wrong or you have given some wrong file
23:08 - name or wrong path here let's give the
23:12 - correct
23:13 - file name so i'm going to give the
23:15 - correct file name and then run the code
23:17 - once again
23:19 - and now this time you will see it's
23:21 - going to give you
23:23 - a matrix which means that it has read
23:27 - all the pixels from this image and then
23:30 - assigned it to our img variable and the
23:34 - result you can see in the form of this
23:37 - matrix so until now we have just read
23:40 - the image
23:41 - now we want to display our image so in
23:44 - order to display our image what we can
23:47 - do is we can use a cv 2
23:52 - dot i am show method so just write i am
23:56 - show here which is going to show your
23:59 - image so the first argument here will be
24:02 - the name of your window in which your
24:05 - image will open so you can give any name
24:08 - here for example i'm going to give
24:11 - image name here to my window and then
24:15 - the second argument is the image
24:18 - variable which you have read using the i
24:21 - am read method so i'm going to just pass
24:24 - img variable which is this variable here
24:28 - so now will it show the image let's
24:30 - check so i'm going to run the code once
24:32 - again and you can see the image is shown
24:36 - for a millisecond and then it disappears
24:40 - so now we need to add something which
24:42 - will wait for the image to disappear so
24:46 - i'm going to add one more
24:48 - method here which is cb 2 dot weight key
24:53 - so this cv2 dot weight key is the
24:56 - keyboard binding function and the
24:58 - argument which it takes is the number of
25:00 - milliseconds for which you want to show
25:03 - your
25:04 - image window so let's give 5000 value
25:08 - here which means we want to show the
25:10 - image for 5 seconds and at last what we
25:14 - are going to do is after we have done
25:16 - seeing our image we will destroy the
25:19 - window which we have created so you can
25:22 - just give this method cv to destroy all
25:25 - windows so destroy all windows simply
25:28 - destroys all the windows which we have
25:32 - created there is one more method which
25:34 - is destroy window
25:36 - and this method you can use to destroy a
25:40 - particular window which we will see
25:43 - little bit later but for now we will
25:46 - just use this method which says destroy
25:49 - all windows
25:50 - so now let's run our code and let's see
25:52 - what happens
25:54 - so now this time you can see
25:56 - our image is loaded
25:59 - for five seconds and our image is loaded
26:02 - in grayscale mode now if you give here
26:05 - zero as the argument of weight key
26:08 - method then let's see what happens so
26:10 - i'm going to run my code and now you
26:13 - will observe that your window will not
26:16 - disappear after five second or any
26:19 - number of seconds it's going to wait for
26:22 - the closing of this window which we can
26:24 - close from this close button and you can
26:28 - see it's loaded in the gray scale mode
26:31 - i'm going to close this
26:33 - window and here as an argument of i am
26:37 - read
26:38 - image the second argument i want to give
26:40 - here right now is one which means the
26:43 - colored image and let's run the code and
26:46 - you can see
26:47 - this image is loaded in the colored mode
26:51 - now let's also check the -1 argument
26:55 - which is unchanged so it's going to just
26:58 - load the image as it is with alpha
27:02 - channels so let me just close this image
27:04 - once again
27:06 - so now we have understood how we can
27:09 - read an image
27:11 - using i am read function
27:13 - so let's see how we can write an image
27:17 - to a file
27:19 - using a function called i am right
27:22 - so we are going to just use cv
27:26 - 2 once again and then there is a method
27:29 - called im write which you can use to
27:33 - write an image in the form of a file so
27:37 - the first argument here will be the
27:40 - image name whatever you want to give
27:42 - here so here let's say we want to give
27:45 - the name to our image as lena
27:49 - underscore copy dot png so the image
27:53 - will be saved as the file name lena
27:56 - underscore copy dot png file and the
27:59 - second argument which it takes is the
28:01 - image you want to save so let's save the
28:04 - same image which we have read using the
28:07 - i am read function inside this img
28:10 - variable and pass it as the second
28:14 - variable here and let's run the code and
28:16 - let's see what happens so our image is
28:19 - loaded using this i am show function and
28:23 - now when i close this
28:26 - window here you will observe one more
28:29 - file will be created here so let me just
28:31 - close this window and now you can see as
28:34 - soon as i close this window this method
28:37 - is called and after this this i am right
28:40 - method will be called and when this
28:43 - method is called
28:44 - this image is created with the name lena
28:47 - underscore copy dot png we can also open
28:50 - this image and you can see it has the
28:53 - same image which we have seen in the
28:56 - case of
28:58 - lena dot jpg so let me close these two
29:01 - images so now we have understood how we
29:03 - can read the images and write the images
29:07 - using i am read function and i am right
29:10 - function so let's make our code little
29:13 - bit better and what we want to do here
29:16 - now is let's say if somebody presses an
29:20 - escape key then only we want to destroy
29:24 - all the windows without saving it into a
29:28 - new file otherwise if somebody presses
29:30 - the s key then we are going to save this
29:34 - file
29:35 - with the new name let's say lena copy
29:38 - dot png file so i'm going to just
29:41 - capture the output of my weight key so
29:44 - just create a new variable let's say k
29:48 - so now when we press any key this key
29:50 - will be captured in this variable now as
29:53 - you know every key has its own
29:57 - value so we are going to just use
30:00 - uh if condition and we are going to just
30:03 - check whether the value of k
30:06 - is equal to 27 which means that we have
30:10 - pressed the escape key and if somebody
30:13 - have pressed the escape key we are going
30:16 - to simply destroy all the windows
30:19 - otherwise let's give the second
30:22 - condition which is l if k is equal to
30:25 - ord and this is a built in function and
30:29 - it takes one argument which is the key
30:33 - name which we want to press so let's say
30:36 - somebody presses the s key and if
30:39 - somebody presses the s key we just want
30:41 - to save the image which we have read
30:44 - using the i am read function to a second
30:47 - image which we call
30:49 - lena underscore copy dot png and then we
30:52 - will simply destroy all the windows
30:56 - which we have created using im show
31:00 - method so let's run the code and let's
31:02 - see what happens so i am going to run
31:03 - the code
31:04 - so this image is loaded and as soon as i
31:08 - press escape button
31:11 - you can see this image disappears that
31:13 - means this condition is met and this
31:17 - method is called and all the windows
31:19 - will be destroyed without saving the
31:21 - image let's delete this image and let's
31:24 - see what will happen when we press the
31:27 - save key so let me just
31:29 - delete this
31:30 - image and now let's run the code once
31:33 - again and this time i'm going to press
31:36 - the s key and once again you can see the
31:39 - windows are destroyed but this image is
31:43 - created once again using this function
31:46 - that means
31:47 - this time this condition is fulfilled
31:51 - and this image is created and after that
31:54 - all the windows are destroyed so this
31:57 - code is working fine for me but in the
31:59 - documentation it's also written that if
32:02 - you are using a 64-bit machine it's
32:05 - better to use this notation with your
32:09 - weight key method which is weight key
32:12 - and this mask here and then once again
32:15 - when we run our code it works
32:19 - as it is but in case if it doesn't work
32:21 - you can try
32:23 - using this mask here so this is how you
32:26 - can read and write images using opencv
32:29 - in this video we will see how to read
32:32 - display and save videos using cameras
32:35 - so often we have to capture live stream
32:39 - from camera so first of all we will see
32:41 - how we can capture the live stream from
32:43 - the camera the same method you can use
32:46 - to display the video from a video file
32:50 - so let's get started and let's see how
32:52 - we can capture the live stream from your
32:54 - default camera
32:56 - so i'm going to just create a variable
32:58 - called cap and then inside your cv2
33:02 - package there is a class called
33:04 - video capture we are going to take this
33:07 - class and create an object of it and as
33:10 - an argument here you can provide either
33:13 - the input file name so for example if
33:16 - you want to just read the video from a
33:19 - particular file you can give the file
33:22 - name for example my
33:25 - file dot avi or mp4 or you need to
33:29 - provide the device index of your camera
33:32 - from which you want to read
33:34 - so by default this index will be either
33:38 - zero or in many devices it's also minus
33:41 - one so first of all we are going to try
33:44 - with zero device index which in most
33:48 - cases works so if this device index 0
33:51 - doesn't work try with -1 now if you have
33:55 - multiple cameras and if you want to
33:59 - use the other camera then you can also
34:02 - try one for the second camera or two for
34:05 - the third camera and so on so we are
34:08 - going to use the default camera which is
34:10 - at device index zero so this is the
34:14 - argument we need to provide here and
34:16 - then we are going to create a while loop
34:19 - in order to capture the frame
34:21 - continuously so let's create a while
34:26 - loop here and this loop we are going to
34:29 - run indefinitely so we are going to just
34:32 - say that while this
34:35 - loop is true we want to capture the
34:38 - frames so we are going to just define
34:42 - these variable ret
34:44 - and
34:45 - frame and then using this cap instance
34:50 - we are going to call a method called
34:52 - read
34:53 - now this read method is going to return
34:56 - true if the frame is available and this
35:00 - frame will be saved into this frame
35:03 - variable
35:04 - so here
35:06 - the true or false will be saved if the
35:09 - frame is available this ret will be true
35:12 - otherwise it will be false and this
35:14 - frame variable will actually
35:16 - capture or save the frame now in order
35:20 - to show this captured frame we can use i
35:23 - am show so i'm going to just use a cv
35:27 - 2
35:28 - dot
35:29 - i am show which is going to show this
35:32 - frame inside the window first of all you
35:35 - can give the name to your window for
35:39 - example
35:40 - frame
35:41 - and then second argument will be the
35:43 - frame which you are reading which is
35:46 - this variable
35:47 - here now in the next step we have seen
35:50 - in the last video also we are going to
35:52 - use the cv2 dot weight key
35:56 - in order to wait for the user input and
36:00 - if this input will be q
36:02 - we will quit the window and destroy all
36:06 - windows so we are going to just say
36:08 - cv2.weight key and the argument here
36:10 - will be 1
36:11 - and i have told you you need to provide
36:14 - this mask for
36:16 - 64
36:18 - bit
36:18 - machines so you can provide this mask
36:22 - and then we are going to
36:24 - just see if this
36:26 - key which is pressed is q or not so we
36:29 - are going to use the ord method for this
36:33 - and we will just check if the q key is
36:36 - pressed and if this q key is pressed we
36:39 - are going to break from the loop and we
36:42 - will come out of the loop and after we
36:45 - come out of the loop the first thing we
36:47 - need to do
36:48 - is to actually release the capture
36:52 - variable so this is important after
36:54 - reading your video you need to release
36:57 - the resources so you need to just call
37:00 - this method cap.release and then
37:03 - we will just destroy all windows so
37:06 - let's run this script and let's see what
37:08 - is the output so i'm going to run the
37:10 - script and you can see in this window
37:12 - the input from my default webcam of my
37:17 - laptop right now i'm just showing some
37:20 - book
37:20 - in front of this camera that's why you
37:23 - will see
37:24 - this book and as soon as i press q key
37:29 - our window will be destroyed and we come
37:32 - out of this script now let's say you
37:33 - want to change the frames to gray so we
37:37 - want to convert our video input from
37:41 - the colored image to the grayscale image
37:44 - for that what you can do is you can
37:46 - define a variable called grey or
37:48 - anything else and then there is a method
37:52 - called cv2 dot cvt color which is to
37:57 - convert color and the first argument
37:59 - which it takes is the source so in our
38:03 - case the source is the frame which we
38:06 - are capturing from
38:08 - the cap dot read method the second
38:11 - argument is
38:13 - actually the conversion what we want to
38:16 - do
38:16 - so we will just call cv2
38:20 - dot
38:21 - color underscore and by default the
38:24 - default colored image is
38:27 - captured as
38:29 - bgr image that means blue green
38:33 - red channel images and we want to
38:35 - convert it to a grayscale image so we
38:38 - will just write
38:39 - bgr
38:41 - to
38:42 - gray
38:43 - this means we want to capture the bgr
38:45 - image to the grayscale image and now
38:48 - this is going to give us the grayscale
38:51 - image and this input we can just
38:54 - transfer to the i am show method as the
38:58 - second argument of this i am show method
39:01 - so let's run this script once again and
39:03 - let's see what is the output of the
39:05 - script and now you can see the video
39:08 - captured is in grayscale image and as
39:12 - soon as i press q it's going to release
39:15 - all the captured resources and then
39:18 - destroy all windows now as i said if you
39:21 - want to display the image from
39:24 - a video file you just need to give the
39:26 - name of the video file for example name
39:30 - and then the extension which is let's
39:32 - say avi or mp4 or any other format now
39:36 - using this cap instance you can read few
39:39 - properties about the video which is
39:42 - captured and the first property is if
39:45 - the video is open or not so in case
39:49 - whenever you provide the file name and
39:51 - the file path is wrong then this is
39:54 - going to give you false so there is a
39:58 - method called is opened and this means
40:01 - if the file name of the video which you
40:04 - want to provide here is correct this is
40:06 - going to give us true otherwise this is
40:09 - opened is going to give us false in case
40:13 - the file path is wrong or the index
40:16 - which you give here for the device is
40:18 - wrong so let's give any random index
40:21 - here and then let's see what happens so
40:24 - i'm going to run the script and you will
40:26 - see nothing will happen because
40:30 - this is opened is going to give you
40:33 - false let's print that also and let's
40:36 - verify
40:37 - with the print statement the same thing
40:39 - so i'm going to just
40:41 - use this and then
40:43 - run the program once again and you can
40:44 - see
40:45 - it prints false that means you cannot
40:49 - capture the video using this index so my
40:52 - device is at index 0 by default so i
40:55 - need to give this index name otherwise
40:59 - for example i provide the wrong file
41:02 - name here also it's going to give us the
41:05 - false value
41:06 - there is a method called cap dot open
41:08 - also so if this cap is open gives you
41:12 - false you can try opening your capture
41:15 - video using cap.open also now there are
41:19 - other properties which you can read
41:21 - using this
41:22 - cap instance and the property you can
41:25 - read using
41:26 - a method called get so you can just
41:29 - write cap dot get and as an argument of
41:33 - this get method you can provide the prop
41:36 - id so there are different prop ids which
41:38 - you can read so let's say we want to
41:42 - read the property which is called frame
41:45 - width and frame height which is going to
41:48 - give you the height and width of your
41:51 - frame so for this you just need to write
41:54 - cv2 dot
41:56 - cap underscore prop underscore frame
41:59 - underscore width this is going to give
42:02 - you the width of your frame and if you
42:05 - want to get the height of your frame you
42:07 - can use gap underscore prop underscore
42:10 - frame underscore height and this whole
42:12 - list you can find on the official
42:15 - documentation of opencv so i will
42:18 - provide you this link where you can see
42:22 - different values of the prop id so right
42:25 - now i have used this id and this id but
42:29 - there are several number of ids
42:32 - available here which you can use to read
42:34 - the property
42:36 - of your frame so let's use print method
42:40 - to just print out what property we are
42:44 - reading and let's once again run this
42:46 - script and here you can see in the
42:49 - output you can see the value 640 and 480
42:53 - which is the width and height of your
42:55 - frame by default now let's see how we
42:58 - can save the image which we have
43:00 - captured from our webcam or the default
43:04 - camera
43:06 - so as we already know that we read frame
43:09 - by frame when we capture
43:11 - the videos from your default camera
43:14 - so for creating the capture you have
43:17 - used video capture class and for saving
43:20 - the video we are going to create the
43:23 - video writer class so i'm going to first
43:26 - of all create a variable called out for
43:29 - output and then i'm going to call
43:32 - a class called video writer so let's
43:36 - call this class which is video writer
43:39 - and now this class takes few argument
43:42 - the first argument is the name of your
43:45 - output file so for example i can just
43:49 - give the name output dot avi the
43:52 - extension of the file
43:54 - the second argument here is the 4cc code
43:57 - now 4cc is a 4 byte code which is used
44:01 - to specify the video codec and if you
44:04 - want to know more about 4cc code you can
44:08 - visit this website which is
44:10 - 4cc.org forward slash codec dot php and
44:14 - here you can find several 4cc codes so
44:18 - for now what we are going to do is we
44:20 - are going to
44:21 - just get the four cc code using a class
44:25 - called video writer underscore 4cc so
44:28 - i'm going to declare a variable 4cc and
44:30 - then i'm going to use cv2 to call a
44:33 - class called
44:35 - video writer 4cc so as an argument of
44:39 - this class you just need to provide the
44:41 - 4cc code so for example i can give
44:45 - this kind of code so you can provide
44:47 - this argument which is xtrx and then
44:49 - your four cc code which is x with in
44:53 - this case or otherwise
44:55 - what you can do here is you can also
44:58 - give this code in this format so for
45:02 - example x
45:03 - comma then second argument is v
45:07 - and then the third argument is i
45:10 - and the fourth argument is
45:13 - d so you can either give
45:16 - this type of notation or you can just
45:18 - use asterix and then in single quotes
45:21 - you can just write xvid or any other
45:25 - code here and then this 4cc code we are
45:27 - going to pass
45:28 - as the second argument the third
45:30 - argument is the number of frames per
45:32 - second so let's say we just want to use
45:35 - uh 20 frames per second and the fourth
45:39 - argument is the size so we already know
45:42 - that the size in which we are capturing
45:44 - is 640
45:46 - by
45:47 - 480 so we are going to provide this in
45:50 - the form of tuple so 640
45:53 - comma 480 so this will be the size of
45:57 - the video which will be saved in this
46:00 - file now inside our loop as we have seen
46:03 - we are just reading the frame here in
46:06 - the frame variable and this is the
46:08 - boolean variable if the frame is
46:10 - available it's true otherwise it's false
46:13 - right so first of all we are going to
46:15 - check if its value is true or false so
46:19 - we can just write if ret is equal to
46:24 - true then only we are going to just save
46:27 - this file into the output file so i'm
46:30 - going to
46:31 - just put everything inside
46:34 - this uh if condition otherwise we are
46:37 - going to break out of this loop so i'm
46:39 - going i'm going to just say else break
46:43 - now inside this if condition we can just
46:47 - write this frame into a file using a
46:50 - method called out dot write so out is
46:54 - the instance of video writer so i'm
46:56 - going to just use out dot
47:00 - write
47:01 - and then we are going to just pass the
47:03 - frame which we have captured which is
47:06 - inside the frame variable and now at
47:10 - last we are going to release all the
47:13 - resources using the out
47:16 - instance which is the instance of video
47:18 - writer so i'm going to just write out
47:20 - dot
47:21 - release
47:22 - and then let's run the script and let's
47:25 - see what happens so one thing to note
47:28 - here is our video will be saved
47:31 - as it is that is in the bgr mode that is
47:35 - in the colored mode so let's run the
47:38 - code and let's see what happens so i'm
47:40 - going to just start my script once again
47:43 - and now
47:44 - i'm going to just press q so you can see
47:48 - here our video is shown in the grayscale
47:51 - and our video will be saved in the
47:53 - original from format because we are
47:56 - saving every frame before the conversion
47:59 - so it will be saved in the original
48:01 - format so i'm going to just uh close
48:04 - this uh script and as soon as i close
48:08 - the script you can see the output.avi
48:11 - file and in order to verify this file
48:13 - i'm going to go to the project and here
48:16 - i'm going to start
48:18 - this file using let's say vlc media
48:21 - player and you can see it shows the
48:23 - output of the
48:25 - output.avi file so this is how you can
48:28 - read videos display and save videos
48:30 - using the default camera or the video
48:34 - file in this video we will learn how to
48:36 - draw different geometric shapes using
48:39 - opencv so to start with i have this code
48:43 - and i have already explained what this
48:45 - code does so this i am read is used to
48:49 - read an image and then we are just
48:52 - showing this image into a window using
48:55 - this i am show method and then using the
48:59 - wait key we will wait for the closing
49:03 - event and the destroy all window will
49:05 - destroy
49:06 - all the windows which we have created so
49:10 - this we have already seen
49:12 - now let's say we want to draw some
49:14 - geometric shapes on this image so to
49:18 - start with
49:20 - let's learn how to draw a line on our
49:24 - image which we have read from this read
49:27 - function
49:28 - so what we are going to do is we will
49:31 - overwrite this image so we have already
49:35 - uh created this image variable
49:38 - so what we are going to do is we will
49:40 - draw a line on the same image so i'm
49:43 - going to just write img is equal to
49:47 - cv2 dot lines and you can see in the
49:51 - suggestion this line method takes few
49:54 - arguments so the first argument is the
49:57 - image itself the second argument is the
49:59 - starting coordinates of point one
50:03 - the third argument is the ending
50:05 - coordinates of point two
50:07 - and then the fourth argument is the
50:10 - color and fifth argument is the
50:13 - thickness
50:14 - so let's use this line method and then
50:18 - give these arguments one by one
50:20 - so we want to write to
50:23 - the image which we have read using this
50:27 - file
50:28 - so the first argument is the image
50:30 - variable and the second argument is the
50:32 - coordinates so the coordinates should be
50:35 - given in the form of tuple so let's say
50:38 - we start with 0 comma 0 coordinate and
50:42 - the ending coordinates will be let's say
50:46 - 255
50:47 - comma 255 okay the fourth argument will
50:51 - be the color and the color you need to
50:54 - give in the bgr format
50:56 - so if you want to uh give the blue color
51:00 - then you can just write 255
51:03 - comma zero comma zero because first is
51:07 - the
51:08 - blue color
51:10 - second is the green color and third is
51:14 - the red channel color so if you specify
51:17 - here 255 in the first channel that means
51:20 - the blue channel then it's going to draw
51:23 - the blue line
51:25 - if you give
51:26 - here 255 and then you make other
51:29 - channels
51:31 - 0 then it's going to draw the green line
51:34 - and if this 255 comes at last and the
51:38 - other channels are 0 then it's going to
51:41 - draw the red line so let's say we want
51:43 - to draw the red line that's why i have
51:45 - given 255 here
51:48 - and the next argument is the thickness
51:51 - so the thickness you provide in
51:54 - the numbers so starting from one one is
51:58 - the lowest thickness you can increase
52:00 - the thickness two or three or let's say
52:04 - 5 or 10 so it's going to increase the
52:07 - thickness based upon this number so
52:09 - let's say we want to give the thickness
52:11 - to our line
52:13 - 5. so this is going to draw a red line
52:17 - on our image so let's run the code and
52:19 - let's see what happens
52:21 - so you can see our image is loaded in
52:24 - the grayscale that's why you don't see
52:27 - any color on the line but our line is
52:31 - created here so let us load this image
52:34 - in the colored format by changing this
52:37 - argument to 1
52:38 - and let's run the code once again and
52:41 - you will see the image is loaded in the
52:44 - colored format and the line color is red
52:48 - now if you want to change the thickness
52:50 - of this line you can just increase this
52:52 - number and if you want to change the
52:54 - color of this line you can change it
52:57 - using these color channels so let's
53:00 - change the line color to
53:03 - green let's say and i'm going to run the
53:06 - code and you can see the thickness of
53:08 - the line is increased and now the color
53:10 - of the line is green now if you want to
53:14 - draw the line with any other color you
53:17 - can just go to your favorite browser and
53:19 - search for rbg
53:21 - color
53:22 - picker but always remember our image
53:25 - will be loaded in the bgr format so in
53:29 - the reverse order so blue green and then
53:33 - the red channel so let's say we want
53:36 - this uh
53:38 - color here and it's the rbg
53:41 - channels are this so i'm going to just
53:43 - copy all these channels and then i'm
53:46 - going to give these channels in the
53:48 - reverse order so first of all
53:51 - 147
53:52 - then
53:53 - 96
53:55 - and then third channel is the 44 and
53:59 - then i'm going to run my code and you
54:02 - can see you get the same color which you
54:04 - have chosen here
54:07 - so this is how you can change the color
54:09 - of your lines now there is a function
54:12 - called arrowed line let's say we want to
54:17 - use this function which is called
54:20 - arrowed
54:21 - line
54:22 - and this is going to draw the arrow line
54:25 - as it says so let's say we want to just
54:29 - uh
54:30 - draw this arrowed line in blue color so
54:33 - i'm going to just give
54:35 - the color channels here and then run the
54:38 - code and this arrowed line is
54:40 - overlapping on the previous line that's
54:43 - why you don't see the previous line so
54:45 - let's change the coordinate of this line
54:49 - so let's
54:50 - draw this line
54:52 - in this coordinate which is going to
54:55 - draw the straight line in my opinion
54:57 - let's see what happens when i run the
54:59 - code and you can see it draws the
55:01 - straight line
55:03 - from
55:04 - left to right which is the arrow line
55:07 - and this was our original line now let's
55:10 - see how to draw the rectangle
55:13 - so to draw the rectangle we will do the
55:16 - same we will just overwrite on the same
55:19 - image so we will just say image is equal
55:22 - to cv2 dot
55:25 - rectangle which is a method and you can
55:28 - see what are the argument it takes so
55:31 - the first argument is the image itself
55:33 - the second argument is the point one and
55:36 - point two this point one and point two
55:39 - coordinate i am going to explain in a
55:41 - bit the third argument is the color
55:44 - which is same as line and the fourth
55:46 - argument is the thickness
55:49 - so let's use this
55:52 - rectangle function to draw the rectangle
55:54 - so first of all i'm going to just
55:57 - pass the image variable here the second
55:59 - argument is the top left vertex
56:03 - coordinates so let me just
56:06 - draw
56:07 - something here so you will be able to
56:09 - understand in a better way so when you
56:12 - want to draw a rectangle using opencv
56:16 - this here is a top left vertex
56:18 - coordinates which is x1 and y1 and this
56:22 - is here
56:23 - the lower right vertex coordinates so
56:26 - the top left vertex coordinates you give
56:29 - in the second argument so let's give
56:32 - some coordinates here so 3 84 comma 0
56:37 - and the lower right coordinates i want
56:39 - to give here is let's say
56:42 - 510 comma 128 so let's say we want to
56:46 - give the red color so i'm going to just
56:49 - write 0 comma 0 comma 255
56:53 - and the thickness i want to give here is
56:55 - 5 and i'm going to just remove this
56:57 - because it'll
56:59 - just create problems and now let's run
57:02 - the code and you can see the rectangle
57:05 - is drawn
57:06 - with the red color of thickness five you
57:09 - can change the thickness of this
57:12 - rectangle by changing
57:15 - the value of the thickness and then you
57:18 - can run the code and now the thickness
57:21 - of this rectangle line is increased now
57:25 - one more thing you can provide here is
57:28 - instead of giving the thickness value if
57:31 - you write here minus one
57:33 - then it's going to fill
57:35 - the rectangle with the color which you
57:38 - provide here so when we give minus one
57:41 - here let's see what happens
57:43 - so now we get the filled rectangle
57:47 - because we have provided minus one
57:50 - option here so if you provide minus one
57:53 - then your rectangle or whatever shape
57:56 - you are creating will be filled with the
57:59 - color which you specify here so let me
58:02 - just change the thickness to 10 once
58:04 - again and now let's see how we can draw
58:08 - the circle so to draw the circle we once
58:11 - again use cv2
58:13 - dot
58:14 - circle
58:15 - function and once again you can see what
58:18 - are the argument which it takes so the
58:20 - first argument is the image the second
58:22 - argument is the center of the circle the
58:25 - third argument is the radius of the
58:27 - circle and the fourth and fifth argument
58:31 - is the color and the thickness once
58:33 - again so once again we will provide the
58:35 - image the second argument
58:38 - is the center of the circle so let's
58:42 - give the center of the circle which is
58:44 - the coordinate on which you want to
58:47 - give the center so i'm going to provide
58:51 - let's say
58:52 - 447
58:53 - comma 63 here and the third argument is
58:58 - the radius so radius we want to provide
59:01 - here is let's say 63 and the fourth
59:03 - argument is the color so let's uh use
59:07 - 0 comma 255
59:10 - comma 0 which is going to draw the green
59:13 - color and then let's give minus 1 here
59:17 - so our circle will be filled with green
59:20 - color and let's run the code and let's
59:23 - see what happens so you can see this
59:25 - circle is drawn here
59:28 - and this circle is filled with the green
59:31 - color now let's see how we can put some
59:34 - text
59:35 - into the image so to put the text on our
59:39 - image we will once again use the image
59:41 - variable and overwrite on it and then we
59:44 - will use a method called put text so
59:49 - this is the method which we are going to
59:51 - use the first argument is the image the
59:54 - second argument here is the text which
59:57 - we want to put so let's say we want to
59:59 - just print
60:00 - opencv
60:01 - on our image so we can just write opencv
60:05 - as the second argument
60:07 - the third argument is the starting point
60:10 - of your text so you need to give the
60:13 - coordinates
60:14 - where you want to start your text from
60:17 - so the coordinates i want to give here
60:19 - is let's say 10 comma 500
60:22 - and then the next argument is the font
60:26 - face so the font face you need to give
60:29 - here
60:30 - using a variable so i'm going to create
60:33 - a variable let's say font and then
60:36 - there are many font faces available
60:39 - using cv2 so you can just write cv2 dot
60:44 - font
60:45 - in capital and you can see what are the
60:47 - options available here i'm going to
60:49 - choose the first one itself which is
60:52 - font hershey simplex
60:55 - font and then we are going to pass this
60:57 - font
60:58 - as the fourth argument the fifth
61:01 - argument here
61:02 - will be the font size so let's say i
61:05 - want to give the font size
61:07 - four here the sixth argument here is the
61:11 - color of your font so let's say i want
61:14 - to
61:15 - just draw
61:17 - 255 255 255 which is going to give us a
61:22 - whitish kind of color the next argument
61:25 - we will give here is the thickness so
61:27 - let's say i want to provide the
61:29 - thickness
61:30 - 10 here and the next argument you can
61:33 - give here is the line type so let's say
61:36 - i want to give the line type cv2 dot
61:41 - capital line
61:42 - underscore
61:44 - and now let's run our script and let's
61:46 - see what happens
61:47 - so you can see here opencv is printed
61:52 - in the white color of thickness 10 and
61:56 - if you want to change this color you can
61:58 - change it from here so i'm going to just
62:00 - put the first channel as 0 and now this
62:04 - color is changed to yellow color now one
62:08 - more thing i want to show here is
62:10 - how you can create an image using numpy
62:14 - zeros method so either you can use a
62:18 - image which you read from i am read
62:21 - method or what you can do here is i'm
62:24 - going to just comment this code and we
62:26 - can create an image using the numpy
62:30 - zeros method so i'm going to create this
62:34 - img variable and then i'm going to use
62:38 - the numpy module so just import this
62:40 - numpy as import numpy as np and then we
62:43 - are going to use this mp to call
62:46 - the zeros method now in order to create
62:50 - a black image using this zeros method
62:53 - you need to give the first argument in
62:55 - the form of list and inside this list
62:58 - the first
63:00 - element will be the height second will
63:02 - be the width and third will be
63:05 - three so let's say we want to provide
63:07 - the height uh 512 we want to provide the
63:10 - width also 512
63:12 - and the third argument will be uh 3 and
63:16 - the next argument you give here is the d
63:18 - type or data type so you can just write
63:21 - np
63:22 - dot
63:24 - u
63:25 - int 8 here so this method is going to
63:29 - give you a black image of the size 512
63:33 - by 512 so let's run our code and let's
63:36 - see what happens so you can see now you
63:39 - can see the black image and on our black
63:43 - image the line is drawn the arrowed line
63:46 - is done and the text and the circle and
63:49 - the rectangle are drawn here so this is
63:52 - how you can draw different geometric
63:54 - shapes
63:55 - on your image there are several other
63:59 - methods you can use for example cv2 dot
64:02 - polyline method or cv2 dot eclipse
64:05 - method to draw eclipse and polygon on
64:08 - your image so just try those method to
64:12 - draw different shapes on your image so
64:15 - in this video we will see how to set
64:18 - some properties to our captured images
64:23 - so in the video capture lesson we have
64:26 - seen
64:26 - that when we create
64:29 - a cap variable using the video capture
64:33 - class we can get many properties using
64:37 - the cap dot get method so we were able
64:41 - to get the width of the frame and the
64:43 - height of the frame
64:45 - similarly we can use the cap dot set
64:49 - function to set some values so you can
64:53 - just write cap dot set and then you can
64:57 - set the values of the property generally
65:00 - all the properties which you can read
65:03 - like this you can also able to set those
65:07 - property using the set method
65:10 - now this notation you can also give in
65:12 - the form of numbers so every
65:15 - property here has a defined number so
65:18 - for example instead of using cv2 dot cap
65:22 - prop underscore frame width you can just
65:26 - write
65:27 - 3 here and that will work also so every
65:30 - property has a number associated with it
65:34 - so using that number either you can just
65:38 - let's say we want to set the width and
65:40 - height either you can write
65:42 - this as the first argument and the
65:44 - second argument is the actual width you
65:47 - want in the video right or you can just
65:52 - give the number of that property and
65:55 - then give its value so let's say we want
65:58 - to change the width of this video to
66:01 - let's say one two zero eight and then
66:05 - let's uh just set the height so cap dot
66:08 - set
66:09 - and the associated number for the height
66:13 - parameter will be
66:14 - four so three for width and four
66:17 - for the height and let's say we want to
66:19 - just
66:21 - move it to 720
66:24 - and then we will once again print the
66:26 - value of
66:28 - the width and height and this time we
66:30 - are going to just give their associated
66:32 - numbers which is three and four
66:36 - so let's run this program you might
66:39 - already know this program what this
66:41 - program is doing so it's just capturing
66:44 - the video from your default device at
66:48 - index 0
66:50 - and then it's just
66:52 - showing all the frames using this i am
66:55 - show method in a window
66:58 - so now i'm going to run this script and
67:01 - let's see what happens
67:02 - so when i run the script you can see
67:05 - the size of this frame is changed so
67:09 - let's see in the terminal also you can
67:12 - see before the original size of the
67:16 - video we are capturing is 640
67:19 - and
67:20 - 480 so width was 640 and the height is
67:24 - 480
67:26 - now once we have changed the width and
67:28 - height you can see the width is changed
67:31 - to one two eight zero and the height is
67:35 - changed to 720 so even if i have given
67:38 - here
67:39 - one two zero eight the default camera
67:43 - will automatically set its value
67:45 - according to its resolution
67:49 - so let's just close uh this video and
67:52 - let's say we want to just change this
67:55 - value to some random number so let's say
67:57 - 700 by 700 will it work or not so let's
68:02 - run the script once again and let's see
68:05 - what happens
68:06 - so the script is running and you can see
68:10 - that even though we have provided the
68:12 - 700 and 700 the camera will
68:15 - automatically take the resolution which
68:18 - is available for your default
68:21 - camera so the resolution remains the
68:24 - same even though we have set the
68:26 - different value to it so you need to
68:29 - keep in mind even though you can give
68:32 - any value here but the camera will only
68:35 - set the resolution which is available
68:38 - for it so let's give
68:40 - a
68:41 - very big value here so i'm going to
68:43 - provide let's say
68:45 - 3000 here and height also 3000
68:50 - and let's run the script once again and
68:52 - let's see what happens so when we run
68:55 - the script you will see
68:57 - the resolution is changed but the
69:00 - resolution will change to the maximum
69:02 - resolution of my
69:05 - default camera which is 1280 and 720
69:09 - this is the maximum resolution which is
69:11 - available for my webcam so let me just
69:14 - close this
69:16 - window so this is how you can set some
69:19 - values so there are many values you can
69:22 - set
69:22 - using this set method you just need to
69:25 - go to the documentation and then search
69:29 - for the value you want to set
69:31 - so in the last two videos we have seen
69:33 - how to capture videos from our default
69:37 - camera device or how to
69:40 - add geometric shapes on the images
69:45 - now in this video we are going to
69:46 - combine the knowledge we have gained in
69:49 - the last two videos so if you haven't
69:52 - seen the last two videos i will
69:54 - recommend you to watch those videos and
69:57 - then come to this video
69:59 - so in this video we will see how we can
70:03 - just
70:04 - draw something on a video and more
70:07 - specifically
70:08 - the aim of this video is how to show
70:12 - the current
70:14 - date and time
70:15 - on a live video
70:18 - so now in the last video we have seen
70:21 - how to draw
70:23 - shapes on images and we have also seen
70:27 - how to
70:28 - put text on our images right so let's
70:32 - say we just want to print the value of
70:35 - width and height on the default camera
70:39 - and let me just remove this line which
70:41 - we have used to convert the bgr image to
70:44 - the grayscale image so we will just
70:48 - see the colored bgr image so now what we
70:52 - want to do is we want to print the width
70:55 - and height which we get from these
70:58 - properties
70:59 - on our video which we are capturing so
71:03 - in the last video we have already seen
71:06 - that we can use a method which is cv
71:10 - 2
71:11 - dot put text yeah so this method we have
71:15 - seen in the last video and first of all
71:18 - we will define the font which we will
71:21 - pass to our put text method so the font
71:26 - i'm using here is cv2 dot font hershey
71:29 - underscore simplex
71:31 - and now the first argument here will be
71:34 - the frame which we are capturing because
71:36 - every frame is just like an image and a
71:39 - video is the combination of multiple
71:42 - images so the first argument here will
71:45 - be the frame the second argument here
71:48 - will be your text so the text which we
71:51 - want to show here is let's say width and
71:55 - height so let's define a variable which
71:59 - we want to show on our video so let's
72:01 - say the variable name is text and first
72:05 - of all i'm going to define the width so
72:08 - just say
72:09 - width
72:10 - and then we are going to provide the
72:13 - value of the width using the
72:15 - concatenation operator
72:18 - now because
72:19 - this value will be in integer and we
72:22 - want to convert it to the string so we
72:26 - will use the str method to convert the
72:29 - integer to the string and then we can
72:32 - pass the width
72:34 - here
72:35 - inside our string variable once again we
72:39 - will use the concatenation operator and
72:41 - then let's provide some space here and
72:44 - then we will just write the height
72:48 - colon and then once again the
72:50 - concatenation operator and once again we
72:53 - will use this
72:55 - string method and inside this string
72:56 - method we will now
72:58 - take the hide okay
73:01 - and now we will pass this text to our
73:04 - put text argument now the third argument
73:08 - is the coordinates so let's say i want
73:10 - to just put this text at the coordinate
73:13 - 10 comma 50
73:15 - the fourth argument is the font which we
73:18 - have already declared
73:20 - the fifth argument is
73:22 - the thickness so let's say the thickness
73:24 - we want is one
73:26 - and then the color so let's say the
73:29 - color we want is
73:31 - 0 comma 255
73:34 - comma 255 and then the thickness so i
73:38 - think the thickness comes after the
73:39 - color and the value 1 we have set for
73:43 - the font scale so you can change the
73:45 - font scale one two three four any font
73:48 - you can change it from here so this
73:50 - value one is for the font scale and the
73:53 - value we are providing right now is for
73:56 - the thickness so let's say the thickness
73:58 - is two
73:59 - and the last argument here will be the
74:01 - line type so i'm going to just provide
74:03 - the line type as cv2
74:06 - dot
74:07 - line
74:08 - underscore aaa so what do you think will
74:11 - this
74:12 - text will be printed on our image or not
74:15 - so it will not print yet because we need
74:18 - to write on the frame this text so we
74:22 - need to just write frame
74:24 - is equal to and then put the text on the
74:28 - same frame which we are seeing right now
74:32 - so now this will work and let me just
74:34 - break this line so you will see all the
74:37 - code and now let's run the code and
74:40 - let's see what happens when we run the
74:42 - code so let me run this script and you
74:45 - will see here
74:47 - that now we are seeing the width and
74:50 - height on top of this video which is 1 2
74:54 - 8 0 and the height is seven twenty point
74:58 - zero so this is how you can show text on
75:02 - your video which you are capturing from
75:04 - the camera or from
75:07 - the video file now let me just comment
75:10 - these lines of code because they are
75:12 - changing the resolution of our video and
75:16 - it's not
75:17 - fitting this video a screencast so i
75:21 - have commented this code and now let's
75:24 - do something more interesting so now
75:27 - let's say we want to show the current
75:30 - date and time on the video and you might
75:33 - have guessed how to print it but let me
75:36 - show you if you don't know how to print
75:38 - the date and time on your live video so
75:43 - first of all we are going to import the
75:46 - package which is available inside python
75:50 - which is date time
75:51 - and then we are going to create uh this
75:54 - date time variable let's say the date
75:57 - time variable name will be
76:00 - date
76:01 - t and then first of all we are going to
76:04 - use the str method to convert the date
76:07 - and time to string
76:09 - and then there is a method inside this
76:13 - datetime library so we just need to
76:15 - write
76:16 - datetime.datetime once again and then
76:19 - the method called now which is going to
76:23 - show you
76:24 - the current date and time so once we
76:27 - have converted our current date and time
76:30 - to
76:31 - the
76:32 - string variable then we can pass this
76:35 - variable as the second argument and now
76:38 - let's run the script and let's see what
76:41 - happens once again so i am running the
76:45 - script and now you will see
76:47 - that it shows the current time
76:50 - and current date on the video itself so
76:54 - this is how you can put the text on your
76:58 - video you can even put some shapes which
77:02 - we have seen in the last video on this
77:05 - video itself so you can put the line or
77:08 - the rectangle or the circle
77:10 - on your video which you are capturing
77:14 - from the camera or some file so this was
77:18 - some kind of a mini project which we
77:20 - have created from the knowledge which we
77:23 - have gained from the last two videos in
77:26 - this video we will learn how to handle
77:28 - mouse event in opencv
77:31 - now mouse event can be anything for
77:34 - example right click event or left mouse
77:38 - button click event or left button double
77:41 - click event so there are many mouse
77:44 - event available in cv2 package
77:48 - now
77:49 - to list out all the events in the cv2
77:52 - package you can write this kind of code
77:56 - so first of all i'm going to create a
77:58 - variable called
78:00 - events and then i'm going to just
78:03 - iterate over
78:05 - all the events
78:07 - inside cv2 library so i'm going to just
78:10 - write
78:11 - i for
78:13 - i
78:14 - in dir
78:16 - inside our cv2
78:19 - package so this dir method is the
78:23 - inbuilt method which is going to show
78:26 - all the classes and member
78:29 - functions inside your cv2 uh package
78:33 - okay so we are iterating over all the
78:36 - function names or
78:38 - member variable names
78:41 - and then we want to see
78:43 - what are the events available inside
78:46 - this package so we can just filter those
78:49 - events using a condition so we are going
78:52 - to just say that we want to just see
78:56 - the variables or the member properties
79:00 - which have this keyword
79:03 - event in them
79:05 - so event
79:07 - in i
79:08 - and then we are going to just print out
79:11 - all the events so i'm going to just
79:13 - print out all the events
79:15 - and then i'm going to run this code and
79:18 - here you can see the list of all the
79:21 - events which are available inside your
79:24 - cv2 library
79:25 - so you can see there is an event called
79:28 - event flag
79:30 - r button for the right button for the
79:33 - mouse
79:34 - or there is event for left button double
79:39 - click event or the event for l button
79:43 - down event so there are
79:45 - many such events available here and we
79:48 - are going to use those events to listen
79:52 - for the mouse events
79:54 - so this is how you can uh print all the
79:57 - events which are available inside your
79:59 - cv2 library and now we will create a
80:03 - script or a program
80:06 - to listen for the mouse event
80:09 - so first of all we will create a mouse
80:12 - callback function
80:14 - which is executed when mouse event take
80:18 - place
80:19 - so in order to create this callback
80:22 - function we are going to just uh define
80:25 - a function and then we will give the
80:27 - name to our function for example
80:31 - click
80:32 - event function and this callback
80:34 - function
80:35 - generally takes few arguments so the
80:38 - first argument will be the event which
80:42 - is taking place when we click
80:44 - our mouse and then it's going to give us
80:47 - the x and y coordinate on the image
80:51 - where we are clicking with our mouse so
80:55 - we are going to get the x axis value and
80:57 - the y axis value whenever we click the
81:01 - mouse at certain position in our image
81:06 - also we will get the flags
81:10 - and we will get the param so for
81:14 - creating the mouse click callback
81:16 - function it has
81:18 - this kind of specific format which is
81:22 - same everywhere so these are the
81:25 - parameter it takes and then inside your
81:28 - callback function you can define the
81:30 - logic
81:32 - so let's say whenever
81:34 - i click the left button down
81:38 - then i want to show
81:40 - the x and y
81:42 - coordinates on the same image so i can
81:46 - just say if
81:47 - the event variable
81:50 - is equal to
81:51 - cv2
81:53 - dot
81:54 - event and then i will just
81:57 - look for the left button down click
82:00 - event so if this event occurs then
82:04 - i will first of all print the x and y
82:08 - axis values so let's print x
82:11 - comma
82:12 - y and you can also provide some space
82:15 - between x and y coordinates
82:18 - using this kind of square string
82:21 - and then what we are going to do is
82:24 - we are going to just uh put this x and y
82:28 - coordinate values
82:30 - on the same image which we are opening
82:34 - so we have already seen in the last
82:37 - videos how to put text on the videos we
82:41 - just need to create this
82:43 - font variable for the font and then
82:47 - there is a method called
82:50 - cv2.put text so we are going to just
82:53 - write
82:54 - cv2 dot
82:56 - put text which takes few argument first
82:59 - is the image now you can see this image
83:02 - shows error and this error says
83:04 - unresolved reference but don't worry
83:08 - when we write our code fully this error
83:12 - will go so first will be the image the
83:15 - second is the string which we want to
83:17 - put so let's say we want to put the
83:20 - string
83:21 - str
83:23 - for the
83:24 - x and y values so i'm going to just
83:26 - write x y
83:27 - and then we are going to just print the
83:31 - x value
83:32 - then concatenation operator and then
83:36 - comma
83:36 - and then once again concatenation
83:39 - operator y
83:40 - and don't forget to convert these
83:43 - coordinate values into the string using
83:46 - the str function
83:48 - so str function here
83:51 - and for
83:52 - the y axis also you need to use this str
83:57 - to convert it to the string value and
84:00 - then the string we pass as the second
84:02 - argument the third argument will be the
84:05 - location where we want to put the text
84:08 - and this location we already know from
84:12 - this x and y value so it's easy for us
84:15 - we are going to just say x comma y
84:18 - because we already know
84:20 - the
84:21 - x and y coordinate using this callback
84:24 - event
84:25 - the fourth argument will be the font the
84:28 - fifth argument will be the font scale
84:30 - let's say it's uh one
84:32 - and then the next argument is the color
84:35 - so let's say color we want to give here
84:37 - is
84:38 - 255 comma 255
84:42 - comma 0
84:44 - and the last argument i want to give
84:46 - here is the thickness let's say
84:48 - thickness we want to give here is 2 and
84:51 - then we will show this text on the image
84:54 - using cv2
84:57 - i am show method so i'm going to just
84:59 - write i am
85:01 - show and then the name of the image
85:04 - window for example image
85:06 - and the
85:08 - image itself which is img
85:11 - so right now this is showing error to us
85:15 - but when we will call this callback
85:17 - function using a standard function
85:20 - called set mouse callback then this
85:23 - error will go
85:25 - so i'm going to define the img variable
85:29 - first of all and let's say we want to
85:31 - create a black
85:33 - image using
85:35 - numpy so we will
85:36 - call np dot zeros method here so np dot
85:41 - zeros and the size of this image will be
85:46 - 512 comma 512
85:49 - comma three
85:51 - and the data type will be
85:54 - np
85:56 - dot u into
85:58 - 8 and once we have this image we are
86:01 - going to show this image
86:03 - using once again the i am show method
86:07 - and this
86:08 - image name will be the same
86:11 - image and the variable we want to pass
86:14 - here is the image variable which is the
86:16 - black image which we have created using
86:19 - this numpy zeros function now the next
86:22 - and the important step here is calling a
86:26 - method called set
86:28 - mouse callback a method
86:31 - so this method we are going to use
86:34 - to call our callback function which we
86:37 - have created which is click event
86:39 - function
86:40 - whenever somebody clicks on the image
86:44 - which we are showing using this i am
86:46 - show
86:47 - window
86:48 - so the first parameter it takes is the
86:51 - name of your image
86:53 - make sure that this name here which you
86:57 - uh take in the i am show method you can
86:59 - see i'm taking the same name here in the
87:02 - i am show method also here so the window
87:06 - name should be the same everywhere then
87:09 - only uh it will work so here also you
87:13 - need to uh just give the parameter first
87:17 - parameter here is the window name
87:20 - so the window name is image and the
87:23 - second parameter is the callback
87:25 - function which we want to call whenever
87:29 - this event take place so this is the
87:32 - callback function which we have created
87:35 - now the next step are the obvious steps
87:38 - which we have already
87:40 - seen so first of all we will call the
87:43 - weight key
87:44 - method to wait for the escape event
87:48 - and the second is the destroy all
87:52 - windows so we will destroy all the
87:54 - windows once we are finished
87:57 - so let's run this code and let's see
87:58 - what happens so
88:00 - now you can see the black
88:02 - image which is created by numpy zeros
88:05 - method and when i click on this image
88:09 - anywhere you can see the coordinates of
88:12 - the position where i have clicked is uh
88:15 - printed here so let's uh click here you
88:19 - can see when i
88:20 - uh give this left down button click
88:24 - event then
88:26 - the position of the x and y coordinate
88:30 - is printed on this black image so i'm
88:34 - clicking again and again this left down
88:37 - button and the position
88:40 - is
88:40 - printed okay so let me just close this
88:43 - window
88:44 - now what i want to do is let's uh just
88:48 - uh
88:49 - reduce the size of
88:51 - this
88:52 - font to 0.5
88:55 - then the
88:56 - font size will be little bit smaller
88:59 - now what i want to do is i want to
89:02 - listen for some other events so i will
89:04 - go to my callback function once again
89:07 - and i will add one more condition here
89:10 - so once again if
89:12 - event
89:13 - is equal to
89:15 - cv
89:16 - 2 and this time i want to listen for the
89:20 - right click event so i'm going to just
89:22 - write event write button down event okay
89:27 - so whenever
89:29 - somebody uh presses this
89:31 - right button
89:33 - down for the mouse then this
89:36 - event is going to be captured inside
89:39 - this condition now if you remember
89:42 - i have told you that image is shown in
89:44 - opencv in the form of bgr format and we
89:49 - already have this image you can see we
89:51 - have declared the image variable that's
89:54 - why this error is also gone so using
89:57 - this image we want to find out the
90:00 - red blue and green channel so now inside
90:03 - this condition what i want to do is i
90:05 - want to print out the bgr channels of
90:09 - the image wherever i click okay
90:12 - so you can
90:14 - first of all declare a blue
90:16 - variable name and then we have img
90:19 - variable which is this one and using
90:22 - this image variable we can get the
90:25 - blue channel using the coordinates so
90:28 - first of all you can provide y
90:30 - comma
90:31 - x we already have the y and x
90:34 - coordinates and then the channel for the
90:37 - blue color is channel 0 because it
90:40 - starts from
90:42 - blue bgr so blue and then green and then
90:46 - red okay so i'm going to just copy it
90:49 - two more times the second is green
90:53 - and the channel for it will be
90:56 - the
90:57 - one or index will be one here and for
91:00 - the red channel this index will be 2
91:03 - here so i'm going to just write
91:04 - red here and once again i'm going to
91:07 - just copy this code
91:10 - and this time what i want to do is
91:12 - instead of
91:14 - printing the
91:16 - coordinates i want to print the bgr
91:20 - channel so here i'm going to just write
91:22 - blue
91:23 - and then
91:24 - second will be the green channel and
91:27 - then the third
91:29 - will be the red channel so i'm going to
91:32 - just write comma and then concatenation
91:35 - operator s t r
91:38 - and then red channel okay so this will
91:41 - be uh
91:42 - the string we are going to name it as
91:46 - bgr
91:48 - and this string we will put here
91:51 - the color also we can change so the
91:54 - color for the coordinates will be
91:56 - different and the
91:57 - color
91:58 - for
91:59 - this event will be different 255 okay
92:03 - so it's going to print the bgr channels
92:06 - on your image
92:07 - now because we are creating the black
92:10 - image whenever i just click the right uh
92:14 - click mouse event you can see
92:17 - the bgr
92:19 - channels for this black image will be
92:21 - always 0 0 0 right when i click the left
92:26 - click then these are the coordinates
92:29 - when i click the right click these are
92:31 - the bgr channels so let's uh change this
92:35 - image from the black image to something
92:39 - visible so i already have the
92:43 - lena image so we can use this lamina
92:47 - image using the cv2 dot imread method so
92:50 - i'm going to just try
92:52 - cv2. im
92:55 - read
92:56 - and the first will be the name of the
92:58 - file which is
93:00 - lena dot jpg
93:03 - so now let's run this code once again
93:06 - and now i have this colored image so we
93:09 - will be able to
93:11 - see
93:11 - these functionality in a better way so
93:14 - first of all
93:15 - the left button click event you see the
93:18 - coordinate and when i click the right
93:21 - click button event then you can see the
93:24 - bgr
93:25 - channels are printed once again
93:28 - here you can see the bgr
93:30 - is different here also
93:33 - these bgr colors are different
93:36 - so you can see everywhere they are a
93:39 - little bit different because this is the
93:41 - colored image and the color differs
93:44 - at every pixel
93:46 - level
93:47 - so this is how
93:49 - the mouse click event works in the last
93:52 - video we have seen how to use mouse
93:55 - click event
93:56 - in opencv using python
93:59 - so we have seen how we can create a
94:01 - callback function which listens to a
94:04 - mouse click event and then how to use
94:08 - this callback function
94:10 - using
94:11 - the set mouse callback method
94:14 - now in this video i will show you some
94:17 - more examples about mouse click event
94:21 - so the first example i want to show
94:24 - about drawing
94:26 - a point
94:27 - and then connecting points
94:31 - using the line
94:33 - so to start with i'm going to just
94:36 - remove this if condition for
94:38 - the right
94:40 - down button click event
94:42 - and every time
94:44 - somebody clicks the left button down
94:48 - click event of mouse
94:50 - then what i want to do is every time
94:54 - the mouse is clicked down
94:57 - i want to draw a circle very small
94:59 - circle and when he clicks on the next
95:02 - point then i want to join
95:05 - those two points using a line
95:09 - so for that i will need a cv to circle
95:13 - so i'm going to remove
95:14 - this code which we don't
95:17 - need right now we just need this
95:20 - condition which listens for the left
95:23 - button down click event of mouse and
95:26 - then what we will do is we will just use
95:31 - cv2 to draw a circle so we will just
95:35 - write dot circle
95:37 - and first of all this
95:40 - circle
95:41 - method takes the image so we are going
95:43 - to pass the image and then the second
95:46 - argument is the coordinates x and y
95:49 - coordinate so we already have x and y
95:52 - coordinate using this callback function
95:55 - with the second and third parameters and
95:58 - then the third parameter will be the
96:00 - radius so i will take the radius three
96:02 - which is uh
96:04 - like very small
96:06 - which will uh give you a effect like a
96:10 - point on an image
96:12 - and then we can give the color so let's
96:14 - give 0
96:16 - comma 0 comma 255
96:19 - and then we will give the thickness
96:23 - now the thickness i'm going to give here
96:25 - is -1 and you might already know what
96:29 - this minus one do
96:30 - so this minus one whenever you give as a
96:33 - thickness it fills your circle or any
96:37 - closed
96:39 - shape okay so your
96:42 - closed circle will be filled with this
96:45 - color which you provide here
96:47 - now next what i want to do is i want to
96:50 - create an array of points
96:53 - so i'm going to
96:54 - just
96:55 - declare
96:56 - a variable called points and initialize
97:00 - it with an empty array
97:02 - now this
97:04 - empty array variable we can use inside
97:07 - our callback function
97:09 - and what we are going to do is we are
97:11 - going to just
97:13 - add or append every time this mouse is
97:16 - clicked so i'm going to just call an
97:19 - append method here and then we are going
97:22 - to append the x and y coordinate to this
97:26 - points array so we know that where this
97:30 - mouse is clicked and we are saving the
97:33 - coordinates wherever the mouse is
97:35 - clicked in the form of array now in the
97:38 - next step what we will do is if the
97:41 - mouse is clicked more than two times so
97:44 - we can just test the length of this
97:48 - array
97:49 - which is a point
97:51 - and if the length of this array is
97:54 - greater than or equal to 2 because the
97:57 - first
97:59 - click will be only a point so we cannot
98:01 - connect
98:02 - this point with a line but when we have
98:06 - two or more points then we can connect
98:09 - uh those points with a line right so if
98:13 - this points array length is greater than
98:17 - 2 then we are going to just create
98:20 - a line between those points or the
98:24 - circles in our case so i'm going to just
98:27 - call cb2 dot
98:29 - line method and first
98:32 - argument here will be
98:35 - image the second argument here will be
98:38 - the point 1
98:39 - so the coordinate of 0.1
98:42 - now we want to join the last two points
98:45 - right so we are going to just use this
98:49 - points array and then
98:51 - to get
98:52 - the last value of an array we use minus
98:57 - 1 here so here as an index we will give
99:01 - minus 1 which means the
99:03 - last element of an array and then we
99:06 - will join the second last element of an
99:09 - array so i'm going to just
99:11 - give
99:13 - this will be points variable not print
99:16 - so let's give the points variable and
99:19 - then we are going to pass the minus 2
99:22 - here which will be the second last
99:25 - element so last and second last element
99:27 - we want to join and then the next
99:30 - argument will be the color so let's say
99:34 - the color we want here is
99:37 - 255 comma zero comma 0
99:41 - and
99:42 - the next
99:43 - point will be the thickness so we
99:46 - will give the thickness of 5 here and
99:49 - then we will show this image using i am
99:51 - show method this code i have already
99:54 - shown you in the last video so i will
99:56 - not explain what this code is doing
99:59 - if you want to know more about this code
100:01 - you can see the last video
100:03 - and this time i will use the numpy zeros
100:08 - array which will be a black image so
100:11 - let's run this script and let's see what
100:12 - happens so i'm running the script and
100:16 - now i click on some position on this
100:20 - image and you can see this red circle
100:25 - is created this circle is created using
100:29 - cv2 dot circle method and because the
100:33 - radius is 3 the circle is very small and
100:37 - because the thickness is -1 the circle
100:40 - is filled with the color which you
100:43 - provide here
100:45 - now we have said that if the point is
100:49 - only one then we don't want to create
100:52 - any line if there are points which are
100:56 - two or more then we want to connect
100:59 - those point with the line so let's click
101:02 - here
101:03 - and you can see
101:04 - point one and point two are connected
101:07 - with a line
101:09 - i click here and you can see the last
101:12 - and the second last points are connected
101:15 - with the line that's why we have taken
101:18 - this minus 1 and minus 2 argument which
101:20 - means the
101:21 - last element of the array and the second
101:23 - last element of the array so when i
101:26 - click at any point it will be now
101:29 - connected with this blue
101:32 - line
101:33 - so this kind of
101:36 - line drawing you can use in
101:38 - satellite images where you want to
101:41 - connect two points together with the
101:44 - line now let's see the next example
101:47 - which i want to show you
101:49 - so in the next example what i want to do
101:52 - is
101:52 - i want to uh first of all read an image
101:56 - and then i want to click
101:58 - on any
101:59 - point on the image and then i want to
102:03 - show the color
102:05 - of the point which on which i have
102:08 - clicked
102:09 - using a second window
102:11 - so for this instead of using the numpy
102:15 - array which is the
102:16 - black
102:17 - image i will use
102:19 - the
102:21 - normal image which is the lana dot jpg
102:24 - image and now i will just remove this
102:28 - code from here so first of all i want to
102:30 - read the bgr channels so first of all i
102:33 - will just declare these variables first
102:37 - is blue and we have the image and in the
102:40 - last video we have already seen how we
102:42 - can get the bgr channels because we have
102:46 - the x-coordinate and the y-coordinate
102:49 - and we also know that blue is the first
102:53 - channel so we use the index 0 here to
102:57 - get the blue channel from this image at
103:02 - this
103:03 - coordinate which is x and y
103:05 - same we will do
103:07 - for the green channel so green
103:11 - i am
103:12 - g and then
103:14 - x comma y and then
103:17 - the channel index will be 1 here and
103:20 - then we will just
103:22 - get the red channel from this image and
103:24 - now what we are going to do is we are
103:27 - going to just draw a circle on this
103:29 - point where
103:31 - uh you will click this mouse uh down
103:35 - button click event so i'm going to just
103:37 - write cb2 dot circle
103:40 - and now i will not explain
103:43 - the parameters because you might already
103:45 - know
103:46 - what
103:47 - these parameters are in the next line
103:50 - what we are going to do is we are going
103:52 - to create
103:53 - a numpy
103:55 - zeros image and then we will pass our
103:58 - bgr channels which we got from
104:02 - the particular point on an image so
104:05 - let's create an image so i will just say
104:09 - my color image
104:11 - and then we are going to just use np
104:15 - for numpy and then we will just call a
104:19 - zeros method here
104:21 - and it takes three argument in the form
104:24 - of
104:26 - this list which is the size of
104:30 - your image let's say this size will be
104:33 - 512
104:34 - comma 512 and the channel will be
104:38 - three channels and then the next
104:41 - argument will be the data type so np dot
104:45 - u
104:46 - int
104:47 - it so we have a black image using this
104:51 - numpy zeros and now we want to fill this
104:55 - image with the
104:57 - bgr colors which we got from the
105:01 - particular point of the image
105:04 - so in the next line what we are going to
105:07 - do is we are going to just use this
105:09 - variable
105:10 - and then we are going to just
105:13 - write this kind of notation this means
105:16 - we want to fill every
105:19 - channel or every
105:21 - point of this list and then
105:25 - we will just pass our bgr channel values
105:29 - which we got
105:31 - from
105:32 - the image so blue
105:34 - green
105:35 - and then the
105:37 - red channel values we are going to pass
105:40 - so this will give us the bgr channel
105:43 - which will be the color
105:45 - of the point where we have clicked and
105:48 - now we have the new image with the color
105:52 - so we can show this image using
105:55 - a new window with let's say this is
106:00 - the color window okay so this is how you
106:03 - will get the
106:05 - new window with the color on which you
106:08 - have clicked
106:10 - so let's run this code and let's see
106:11 - what happens so i'm going to run this
106:14 - code and you can see
106:16 - this is the image which is the colored
106:18 - image
106:19 - let's see
106:20 - i click on this point and you can see
106:24 - the same color on which i have clicked
106:27 - is opened in the next window let's click
106:31 - here on the hat you can see it's going
106:34 - to give you the same color
106:36 - on which i have clicked let's click on
106:39 - the eyes
106:40 - and you will get the same color on which
106:43 - i have clicked
106:44 - let's see what happens when we just load
106:48 - a black image instead of this colored
106:51 - image so i'm going to just use
106:53 - this
106:55 - numpy zeros uh image which is the black
106:58 - image and let's run this code and now
107:01 - whenever i click on this every time i
107:04 - click on any point it'll be
107:07 - the black color window which will open
107:12 - so this is how
107:13 - you can use some examples
107:17 - to understand how
107:19 - mouse click events
107:21 - can work and you can
107:24 - use them to develop your
107:27 - applications
107:28 - in this video we will see some of the
107:30 - basic and arithmetic operations on
107:33 - images using opencv
107:36 - so let's get started so here i have this
107:39 - code some of this code you already know
107:42 - so you already know how to read the
107:43 - images using i am read method and then
107:47 - show it inside a window using i am show
107:49 - method
107:50 - and destroy all windows using this
107:52 - destroy all windows method but this code
107:55 - in between is little bit new so let me
107:58 - explain line by line what this code does
108:02 - so when you have this image using imread
108:06 - method or any other method
108:08 - you can use these attributes like shape
108:11 - size and d type to get different uh
108:16 - values from this image
108:18 - so image dot shape is going to return a
108:21 - tuple
108:22 - which contains the number of rows
108:26 - columns and the number of channels in
108:29 - this image the image dot size will
108:32 - return the total number of pixel which
108:35 - are there inside the image
108:38 - and image dot d type is going to return
108:42 - the data type of the image which you
108:45 - have obtained
108:47 - now here if you want to split your image
108:50 - in three channels then you can use cv2
108:54 - dot split method and pass your image as
108:56 - an argument and it's going to give you
108:58 - the bgr channel of your image
109:02 - now if you have bgr channels and you
109:04 - want to merge
109:06 - those bgi channel into an image then you
109:10 - can use
109:11 - cv2.merge method and pass
109:14 - these bgr channels in the form of tuple
109:18 - and it's going to give you the image
109:20 - which you can load using i am show
109:23 - method
109:24 - so let's run this code and let's see
109:27 - what we are getting using these
109:30 - attributes
109:31 - so you can see this messy5 dot jpg image
109:35 - is loaded
109:36 - and here you can see first of all the
109:39 - shape of the image so the shape returns
109:42 - the number of rows number of columns and
109:45 - the number of channels so number of rows
109:48 - here is 342 columns are
109:51 - 548 and number of channels are three
109:54 - here
109:55 - the number of a pixel which we have
109:57 - calculated using the size
110:00 - is
110:01 - this number which is 562248
110:04 - and the data type of an image is uint8
110:09 - so sometimes
110:10 - you need to debug the data type of
110:13 - your image and this attribute will be
110:17 - very useful in those cases when you need
110:20 - to debug if something is correct or
110:23 - wrong and because we have splitted this
110:26 - method using the split and
110:28 - re-merged these bgr channels using this
110:31 - merge
110:32 - method so we will at the end get the
110:36 - same image which we have at the
110:39 - beginning
110:40 - here in this code so there is no change
110:43 - in the code
110:45 - so once again let me just load this
110:47 - image and now let's talk about the roi
110:52 - of an image
110:53 - so roi stands for region of interest
110:58 - so sometimes you need to work with
111:00 - certain region of the image so let's say
111:04 - you only want to work with the face here
111:07 - or you only want to work with this ball
111:10 - okay so this is called the region of
111:13 - interest or in short form it's called
111:17 - roi
111:18 - so let's say we want to just work with
111:22 - this ball here so this will be our
111:26 - region of interest or roi
111:29 - and i want to just
111:31 - copy this ball to other place in
111:36 - this picture so i want to just copy this
111:38 - ball and place it on the other place
111:42 - let's say somewhere here okay
111:45 - so how we can do this
111:47 - so i already have the coordinates of the
111:51 - ball but you already know how to get the
111:53 - coordinates of
111:55 - some place in the image we have already
111:58 - discussed this in our previous video so
112:01 - i'm not going to show you how to obtain
112:04 - those coordinates but let's say i have
112:07 - those coordinates of the ball so i'm
112:09 - going to create a ball variable and we
112:12 - have our image so we will take our image
112:15 - and there are certain
112:17 - numpy indexing features which we can use
112:20 - here so i'm going to just write a
112:24 - 280
112:25 - colon 340 which is going to give you
112:30 - one point on the ball which is the
112:35 - upper left hand side of this ball and
112:38 - then we will give 330 here
112:41 - colon
112:43 - 3 90 which is going to give us the
112:46 - bottom right hand corner of this ball
112:50 - okay so now we have this ball so this
112:54 - this indexing is going to copy
112:57 - this ball all the pixels of this ball
113:01 - and then now we have the ball so we can
113:03 - place this ball
113:05 - on any place on this messy image which
113:09 - we are reading
113:11 - so what we can do is we can once again
113:13 - use img and using
113:17 - those numpy indexing
113:19 - features we can place this ball at some
113:23 - other place
113:24 - so let me just give
113:26 - those uh indexes here so let me give
113:30 - 273 colon 333 i have already tested this
113:35 - code so that's why i know exactly where
113:38 - i want to place this ball but if you are
113:42 - not sure where to place this
113:44 - ball then you might have to
113:47 - first
113:48 - calculate or know the coordinates where
113:50 - you want to place this ball and you
113:52 - already know how to find out the
113:54 - coordinates on an image
113:56 - and you will
113:58 - be able to place uh
114:01 - that roi or interest of region at some
114:04 - other place
114:05 - so what i'm
114:07 - doing here is i have just copied
114:10 - the ball
114:12 - and then i am placing the ball
114:15 - on this coordinate okay so i just need
114:19 - to just assign our ball on this
114:22 - coordinate and then this ball will be
114:25 - copied to this index on the image so
114:28 - let's see what happens when we run the
114:30 - code
114:31 - so now you can see we have copied this
114:34 - ball and we have placed this ball
114:37 - here
114:38 - on the image so this is how you work
114:41 - with the roi or region of interest okay
114:46 - so let me close
114:48 - this
114:49 - window now the next thing which i want
114:51 - to show here is how you can add two
114:54 - images so for that i need one more image
114:58 - so you can see in my project
115:01 - i have this five dot jpg
115:04 - and i have this other image which is
115:06 - opencv
115:08 - hyphen logo.png file which is of the
115:11 - same size as the
115:14 - messy.jpg image
115:16 - so i'm going to just write im g2
115:20 - and then once again cv2 dot
115:23 - im read method and then i am going to
115:26 - give the name of
115:28 - this file which is opencv hyphen logo
115:33 - dot png file okay so this is uh
115:37 - this file
115:38 - so this file we are reading
115:40 - and then there is a method called add
115:44 - okay so we are going to use this method
115:47 - here let's use this method cv2
115:51 - dot
115:52 - add
115:53 - and this method i'm going to show you
115:56 - what it does in a moment but this method
115:59 - takes two argument first is
116:02 - the first numpy array so let me show you
116:04 - what this method do first of all
116:06 - so this is the add method inside your
116:10 - cv2 package you can also see the
116:12 - documentation
116:14 - on the
116:16 - opencv
116:17 - dot org and what it does is it
116:20 - calculates the pre-element sum of two
116:23 - arrays or an array and a scalar okay so
116:28 - here we can just pass our two
116:32 - arrays which we got from the i am read
116:35 - method and pass
116:37 - here as the first and the second
116:40 - argument so i am g and i am g
116:43 - two are the
116:45 - one and two parameter and there are some
116:48 - other parameters also like uh output
116:51 - array input array mask and int which is
116:55 - the data type which we which are set by
116:58 - default so we are not going to
117:00 - set them
117:02 - so
117:02 - we are just using cv2.add method on
117:05 - these two images and then i just want to
117:10 - assign the new image which we have added
117:13 - to a new variable let's say this is dst
117:16 - for destination image
117:18 - and then
117:20 - we are going to just show this image
117:22 - using
117:23 - this
117:25 - i am show method okay so we have two
117:28 - images let me show you uh those images
117:30 - one by one first of all so this is the
117:32 - first image i have and the second image
117:35 - is opencv we have logo which is like
117:37 - this one okay
117:39 - so those two images we have and when i
117:42 - run this code after adding those two
117:45 - images using add method you will see
117:48 - first of all
117:49 - you will see this error and why this
117:52 - error is coming because
117:55 - you will see here that the size of
117:59 - those two input is not matching okay so
118:03 - in order to add two images you need to
118:07 - have the images or the arrays of same
118:10 - size and then only you will be able to
118:13 - add those two images so let's resize
118:16 - those two images into
118:19 - a size
118:20 - which uh is common to both of them so
118:24 - you what we are going to do next is we
118:26 - are going to resize those images so once
118:28 - again i'm going to just use img variable
118:32 - so what
118:33 - i get after the resizing i will once
118:36 - again assign to this img variable and
118:39 - there is a method called cv2
118:42 - dot resize
118:44 - and this helps us to resize the image so
118:47 - first of all we need to give the source
118:50 - which we want to resize and then we are
118:52 - going to give the size which we want to
118:56 - get so the number of columns and number
118:59 - of rows we can give here let's say we
119:01 - want to just resize this image to 5 1 2
119:05 - comma 5 1 2 which is the number of rows
119:08 - and number of columns right
119:11 - same we will do with the next image so i
119:14 - am g
119:16 - and then once again cv2 dot
119:19 - resize and then the
119:22 - source here will be image 2 and the size
119:26 - which we want here is again
119:29 - five one two comma five one two in the
119:31 - form of tuple
119:32 - so we have resized this image and this
119:35 - image which are of different sizes to
119:38 - the same size and now let's run the code
119:41 - once again and now you will see
119:44 - that these two images
119:46 - are merged now okay so you will be able
119:50 - to see the hand here and little bit foot
119:54 - and here the ball
119:56 - of uh this image one which is messy five
120:01 - and then we have the second image which
120:03 - is opencv which is added to the first
120:07 - image so this is how you can add two
120:10 - image
120:11 - using
120:12 - opencv
120:13 - now there is one more method which is
120:15 - called add weighted okay so this add
120:19 - method is going to just add these two
120:21 - images but if you want to add the weight
120:23 - for example you want to give the weight
120:25 - 90 to the first image and 10 percent to
120:28 - the second image there is one more
120:30 - method so let's go to the documentation
120:32 - once again and there is this method
120:35 - called add weighted method okay so this
120:39 - add weighted method takes uh
120:42 - several arguments here you can see first
120:44 - is the source of
120:46 - the first array and second argument is
120:49 - the alpha value alpha is the weight
120:53 - which you want to give to the first
120:55 - image okay the third argument is the
120:58 - source two so in our case this will be
121:00 - the image two the fourth argument is the
121:03 - beta
121:04 - beta is the weight
121:06 - which you want to give to the second
121:08 - image right so this weight you can can
121:11 - give from zero to one anything and this
121:16 - gamma is the scalar value which you want
121:18 - to provide and this second last
121:21 - parameter is the destination and the
121:24 - last is the d type or the data type here
121:27 - okay
121:28 - so this is the formula which
121:30 - this
121:31 - method is going to use so source
121:35 - multiplied by alpha
121:37 - and source 2 multiplied by beta plus
121:40 - gamma so this is the
121:42 - method which will be used using these
121:44 - arguments or simply you will use this
121:47 - kind of methods source multiplied by
121:49 - alpha plus source 2 multiplied by beta
121:53 - plus gamma which is the scalar you can
121:56 - add to the image okay so let's use this
121:59 - method so i'm going to just copy this
122:02 - method and then comment this
122:04 - and go to the next line and instead of
122:07 - using add i'm going to use the add
122:10 - weighted method okay
122:12 - so the first argument is the source
122:15 - which is the first source which is img
122:17 - in our case second argument is the
122:19 - weight
122:20 - so
122:21 - first
122:22 - this is the messy image right so we want
122:25 - to just give
122:26 - the weight here 90 or you can just give
122:30 - 0.9 here
122:31 - and for the second image we want to give
122:34 - the weight uh
122:35 - 0.1 okay
122:37 - so the sum of this weight and this
122:40 - weight
122:41 - will be 1 and also we are going to give
122:44 - the gamma value here as zero so we don't
122:47 - want to add any scalar value to uh this
122:50 - add weighted method so the next uh value
122:54 - here will be zero which is the value of
122:55 - gamma and let's run this code and you
122:58 - can see now now we have our messy image
123:02 - which is dominant here because it has
123:04 - the weight
123:05 - 0.9 which is 90 percent of the two
123:09 - and the opencv image have the weight 0.1
123:13 - which is 10
123:15 - of the two okay so the opencv image is
123:19 - light and the messy image is a little
123:22 - bit uh
123:23 - you know dominant here
123:25 - you can just give 0.5 and 0.5 so the
123:29 - weight of the two images will be the
123:31 - same and now you will see those two
123:34 - images
123:35 - in the same domination okay so 50 50
123:40 - now let's say we want to increase this
123:42 - value of opencv to 0.8 and
123:46 - the messy image weight will be 0.2 then
123:49 - the dominant image here will be opencv
123:52 - and in the background kind of thing you
123:55 - will see this messy image so this is how
123:57 - you can add two images with their weight
124:00 - and the scaler
124:02 - and that's it for this video so in this
124:05 - video you have seen some of the basic
124:07 - operations on the images and some of the
124:10 - arithmetic operations on the images
124:13 - which you can do
124:14 - using opencv
124:16 - in this video we will talk about bitwise
124:19 - operations on images using python and
124:22 - opencv
124:24 - so bitwise operations can be very useful
124:27 - when working with masks
124:30 - masks are binary images that indicates
124:33 - the pixel in which
124:35 - an operation is to be performed
124:39 - so let's see how we can perform bitwise
124:42 - operations on
124:44 - images
124:46 - so to start with i have here
124:49 - one image
124:50 - which is image underscore one dot png
124:53 - file and let me show you
124:55 - the image also so this image is half
124:58 - black and half wide
125:00 - now the second image i'm creating using
125:04 - numpy so first of all i have used
125:06 - np.zeros and i'm just creating this
125:10 - image with the same dimension as
125:13 - this image underscore 1 is having so 250
125:17 - comma
125:18 - 500 is the dimension of this image
125:21 - and the number of channels are three and
125:24 - this
125:25 - code is going to create a black image as
125:28 - you might already know from our previous
125:31 - videos
125:32 - now this code
125:34 - is
125:35 - just
125:36 - creating a white rectangle inside
125:40 - the black image which we got from
125:43 - numpy's zeros array okay
125:46 - so this is the dimension of the
125:48 - rectangle inside your black image and
125:51 - the color of the rectangle
125:54 - will be white because this is 255 comma
125:57 - 255 comma 255 and we are taking
126:02 - thickness as -1 that means
126:05 - your rectangle will be filled with white
126:07 - color
126:09 - now here i'm just
126:11 - showing both the images using i am show
126:13 - method and this code you might already
126:16 - know what this is doing
126:18 - from our previous videos so let me run
126:21 - this code and let's see what happens
126:23 - first of all
126:24 - so when we run this code you will see
126:28 - first image is this one which we have
126:31 - created using the numpy zero so this is
126:35 - img1
126:37 - and this is the black image and we are
126:39 - just creating a white rectangle
126:42 - inside this numpy zeros image
126:46 - and this is the second image which is
126:48 - half black and half
126:51 - wide
126:52 - now we want to perform some bit wise
126:56 - operations
126:57 - on these two images
126:59 - so let's see how we can perform these
127:01 - bitwise operations on these two images
127:05 - so to
127:06 - perform these bitwise operations
127:09 - we have some methods inside the opencv
127:13 - library so the first method will be bit
127:17 - and so i'm going to just create a
127:19 - variable called bit and
127:21 - with let's say
127:23 - like this
127:25 - and the method inside opencv is cv2 dot
127:30 - bit
127:31 - wise underscore and
127:33 - so this bitwise underscore and takes
127:37 - several arguments as you might see here
127:40 - the source
127:41 - of the first image the source of the
127:43 - second image and the destination which
127:47 - is none by default and the mask
127:50 - which is also optional
127:52 - so we are going to just provide our
127:54 - images here so i'm going to provide the
127:57 - i am g2 here first of all as the first
128:00 - argument and the second image will be
128:02 - img1
128:04 - and once we perform this bitwise
128:08 - and operation on these two images we are
128:11 - going to show the result in the form of
128:15 - windows i am going to create one more
128:17 - window which is
128:19 - cb2 dot i am show and i'm going to name
128:24 - this window as
128:26 - let's say
128:27 - bit
128:28 - and
128:29 - and the second argument will be our
128:32 - variable bit and which we got
128:35 - from the operation bitwise and on these
128:38 - two images so let's run this code once
128:41 - again and let's see what happens
128:43 - so now as a result we have the third
128:46 - image so let me just
128:48 - open
128:49 - all the images so this is our first
128:51 - image this is the second image and the
128:54 - last one is the result which is the
128:57 - bit and operation on these two images so
129:01 - now you might already know how the
129:03 - logical end works but those of you who
129:07 - might not know how the logical end works
129:11 - let me show you the truth table of
129:14 - logical end
129:16 - so this is the truth table of logical
129:19 - end
129:19 - and
129:20 - if
129:21 - the input a and input v b
129:25 - is zero then
129:27 - we get the result zero okay
129:30 - if input
129:31 - a and input b
129:34 - either of them is zero
129:36 - then also we get zeros right the result
129:40 - one we will only get when
129:43 - both the sources are one so a and b are
129:47 - one then only we will get a one in case
129:51 - of and logic so same and logic will work
129:55 - here
129:56 - so this is
129:58 - the zeros array right so we have created
130:02 - this black region from the zeros so here
130:06 - in these images black is performing as
130:10 - zeros
130:12 - and the white part is performing as 1
130:16 - so when
130:17 - 0 and 0 the result will be 0 right so
130:22 - from this truth table we have seen when
130:25 - the
130:26 - input
130:27 - is 0 and 0 the result is 0 same here we
130:31 - are seeing so when the image is black
130:34 - and black we get the result black when
130:37 - the input is white and black this means
130:40 - 0 and 1 the result will be once again 0
130:44 - using the logical end
130:46 - but when the input will be white and
130:50 - white that means 1 and 1
130:52 - the result will be wide that means the
130:56 - one okay so the only reason region white
131:00 - here is the result of this white and
131:03 - this white and the resulting image you
131:06 - can see here
131:07 - and all the other part is
131:10 - black because the and operation on 0 and
131:14 - 0 is 0 and 0 and 1 is also 0.
131:19 - so this is how bit wise end works let's
131:23 - see how
131:24 - bitwise or and other operation works so
131:28 - i'm going to just comment this code and
131:30 - now
131:31 - we are going to just create the bitwise
131:35 - or
131:36 - operation
131:37 - so for that i'm going to just uh
131:40 - instead of writing bit and i'm going to
131:43 - just write bit r
131:45 - and instead of bitwise and we are going
131:48 - to just write bit
131:50 - wise or here and then we will simply
131:54 - call
131:55 - this uh image using i am show method so
131:58 - we are
132:00 - just calling here
132:02 - bit or
132:03 - now let's run the code once again and
132:06 - let's see the result
132:08 - so you can see the result here so let's
132:11 - see the truth table so in the logical or
132:14 - if only one input is one then the result
132:18 - will be one so either a or b is one or
132:22 - both are one then the result will be one
132:25 - and if both inputs are zero then the
132:28 - result will be zero
132:30 - so same you will be able to see here
132:33 - so when
132:35 - the first source and the second source
132:38 - is
132:40 - zero the result is zero
132:42 - but when the first source and second
132:45 - source
132:46 - is one or white the result is white
132:50 - when the first
132:52 - source and the second source is
132:55 - zero and one or black or white the
132:58 - result is once again one or white here
133:00 - okay so this is how the logical r
133:04 - works on the image
133:07 - now let's uh see how
133:09 - the xor operation work on those images
133:13 - so i'm going to once again comment this
133:16 - code and this time i'm going to just
133:20 - perform the xor operation on these two
133:24 - images
133:32 - and now we are going to run this code
133:35 - once again
133:36 - and you will see this kind of result so
133:39 - once again let's see
133:41 - how the xor logic works so when both
133:46 - the inputs are 0 or both the inputs are
133:50 - 1
133:51 - then we will get the 0 and if either a
133:55 - or b is 1 then only we will get the
133:59 - result 1.
134:00 - so same you will be able to see
134:03 - here
134:04 - so when both
134:06 - first and second source is 0 the result
134:09 - is 0
134:10 - when
134:11 - both the first and
134:14 - second source is one you can see here
134:16 - and here
134:18 - the result is once again zero here right
134:21 - but when the input is zero and one
134:24 - result will be one and in this case also
134:27 - the black and white will result in the
134:30 - white image which is the logical
134:34 - xor operation
134:36 - so once again let's close this
134:39 - and now let me show you how
134:41 - the
134:42 - not operation work so i'm going to just
134:45 - comment this code and then
134:48 - i'm going to just use the bitwise not so
134:52 - here bit
134:54 - not let's say we will perform the bit
134:57 - not on the first image and the second
135:00 - image so i'm going to just write
135:03 - bit
135:04 - not
135:05 - on the first image because it only takes
135:08 - one argument bit not is just the
135:10 - opposite of
135:12 - the source so if you get the input 0
135:16 - then the result will be 1
135:18 - if you have the input 1 the result will
135:21 - be
135:22 - 0 so the opposite
135:24 - of the input so let's perform this
135:27 - operation on image 1 and image 2
135:31 - and let's comment this code and we are
135:33 - going to just show
135:35 - these two result windows using
135:38 - the i am show method and also i need to
135:42 - change
135:43 - this name otherwise we will
135:47 - face problems and here also i haven't
135:50 - changed the name of these i am show
135:53 - windows so let's change the name
135:56 - of these windows and let's run the code
135:59 - once again
136:00 - and now you will get these results
136:03 - so
136:04 - we will get the first result so the
136:07 - first
136:08 - bit not one is
136:10 - the not of
136:12 - the first image and bit not two is the
136:17 - not operation on the second image so you
136:19 - can see wherever we have white we got
136:22 - black and wherever we have black we got
136:25 - white so just the opposite of the input
136:28 - here also wherever we have the black
136:31 - region we got the white
136:33 - image here and wherever we have the
136:36 - white pixel we got the black pixel so
136:40 - this is how bitwise knot operation works
136:42 - on the images so these are some of the
136:46 - bitwise operations which you can perform
136:49 - on your images and as i said bitwise
136:53 - operations can be very useful when
136:55 - working with masks which we will see in
136:58 - the later videos
136:59 - in this video we will talk about track
137:01 - bars in opencv
137:03 - now track bars are really useful
137:05 - whenever you want to change some value
137:08 - in your image dynamically at runtime
137:12 - so let's see how we can use trackbars in
137:15 - opencv
137:16 - now to start with i have this simple
137:18 - code which you might know what it does
137:22 - so first of all i have imported cv2 as a
137:25 - cv
137:26 - and then i'm creating an image using the
137:29 - numpy zeros array with these dimensions
137:33 - and then i'm creating a named window
137:37 - with the name image
137:39 - so this might seem new to you because i
137:42 - haven't created a named window in the
137:45 - previous video
137:46 - so the named window you can use
137:50 - to create a window with a name and this
137:54 - time we have given the named window name
137:57 - as image now in this while loop we are
137:59 - just using this
138:01 - i am show method to call
138:03 - this window and then loading this image
138:07 - inside this named window
138:10 - now you might already know what this
138:13 - code does it just wait for the key
138:16 - and if the key is escape key then we
138:21 - will break out of this loop and in the
138:24 - last we are just destroying all the
138:26 - windows which we have created
138:28 - now in order to create a tracked bar you
138:32 - just need to use cv and then
138:35 - call a method called create track bar
138:38 - now the first argument here you need to
138:40 - give is the track bar name
138:44 - because you can create multiple track
138:47 - bars in your
138:49 - image window that's why you need to
138:52 - provide the name which is unique to this
138:55 - track bar
138:56 - so i'm going to just give the name to my
138:59 - track bar as b because
139:02 - what i want to do is i want to change
139:05 - the bgr values of
139:08 - the image using
139:10 - the drag bar so the first track bar will
139:13 - change the b channel values that's why
139:16 - this
139:17 - first argument is
139:19 - the
139:20 - track bar name which is b and the second
139:23 - argument here we will give is the name
139:26 - of the window so that one that is why we
139:29 - have created this named window so that
139:32 - we can provide the name of the window
139:35 - which is image in this case and that is
139:38 - how we know that in which
139:41 - window we need to add the track bar so
139:44 - in the image window which is this one we
139:47 - want to add the b track bar
139:51 - now the third argument here will be the
139:53 - value which is the initial value at
139:56 - which your track bar is set and the next
139:59 - value here will be
140:01 - the count which is the final value you
140:05 - want to set for your track bar
140:08 - now there is this last thing which we
140:11 - want to set here and this is the
140:14 - callback function which will be called
140:17 - whenever your trackbar value changes
140:21 - so here for example i'm going to create
140:25 - a
140:26 - callback function called
140:28 - nothing and this callback function
140:32 - definition or signature i'm going to
140:35 - create here so we can just create
140:38 - a
140:39 - callback function with a name nothing
140:42 - and this function can take
140:45 - this value x and this is the value of
140:49 - the current position of your track bar
140:52 - so we will see what it does little bit
140:55 - later and what we are going to do is we
140:58 - are going to just print the value of x
141:00 - so we will know the current position if
141:04 - this track bar is changed
141:06 - so this is the callback function which
141:09 - will be called whenever your track bar
141:12 - value changes
141:13 - same we will do with the other track bar
141:17 - so we will create the three track bars
141:20 - in the same window with the name b
141:24 - and the next track bar name will be the
141:27 - g
141:28 - and the last track bar name will be r
141:31 - okay so this will be
141:33 - capital r
141:35 - so now let's run this code and let's see
141:37 - what happens when we run this code so
141:39 - i'm going to right click and run this
141:41 - script
141:42 - and you can see here inside this named
141:46 - window with the name image
141:48 - we have this black image which we have
141:51 - created using numpy
141:53 - zeros array and now we have three track
141:56 - bars here
141:58 - with bgr names so these track bar values
142:02 - you can change
142:04 - using this uh scroller and as you can
142:07 - see here let me show you in this
142:10 - terminal whenever you change the value
142:13 - of
142:14 - any
142:15 - bar
142:16 - the corresponding value will be shown
142:19 - here
142:20 - using this callback function and inside
142:23 - this callback function we have the print
142:25 - statement okay so as i said whenever you
142:28 - change this value this callback function
142:31 - is called and it will print the value of
142:34 - the current
142:35 - track bar okay
142:37 - so for this functionality what we want
142:40 - to do is we want to get the current
142:43 - position of the track bar and because we
142:47 - can change the value of bgr channels
142:50 - from 0 to 255 that's why i have
142:54 - given the range between 0 to 255 to the
142:57 - track bars also so that you can change
143:01 - these bgr channel values
143:04 - so now in order to get the current value
143:07 - of your track bar first of all we will
143:10 - just check the value of the b track bar
143:15 - so we will just use cv dot get
143:18 - track bar
143:19 - position which is this method get track
143:22 - bar pos
143:24 - and then we just need to give the name
143:27 - of our track bar so let's say we want to
143:30 - check the position of
143:32 - track bar b then we will just say we
143:35 - want to have
143:37 - this track bar position
143:39 - with the name b
143:41 - and the second argument here will be the
143:44 - name of your window so in which window
143:48 - this
143:49 - uh
143:50 - track bar is present so the our track
143:52 - bar is present inside
143:54 - the image window right so same we will
143:58 - do for
143:59 - the g
144:01 - and r values also
144:07 - now we have the values of b
144:10 - g r channels from the track bar so now
144:15 - we want to set these values to our image
144:18 - so what we can do here is we can just
144:22 - write
144:23 - for example
144:25 - i am g
144:26 - inside these square brackets you can
144:29 - just
144:30 - give this kind of notation and then give
144:33 - the bgr channel value so i'm going to
144:36 - just write b comma
144:37 - g comma r that means we want to set the
144:41 - current b
144:43 - g r values to this image so let's run
144:47 - this code and let's see what happens now
144:49 - so i'm going to run this code and now
144:52 - when i change the blue channel values
144:55 - you can see this image becomes blue
145:00 - colored right
145:02 - let's bring it to 0 once again and now
145:04 - let's change the value of g so you can
145:08 - see this image color is changing to
145:12 - green and then we can try changing the
145:15 - red color and you can see when it goes
145:18 - to 255 the color of the image is red
145:23 - you can change the values
145:27 - of different track bars and the
145:30 - corresponding color will be displayed in
145:33 - this uh
145:34 - window here right so you can see the
145:37 - color is changing you can change any
145:40 - track bar here one more example i want
145:42 - to give here is how to add a switch
145:46 - using a track bar so for that i'm going
145:49 - to use one variable called
145:53 - switch
145:54 - and then here
145:56 - i can
145:57 - add first of all the name of the switch
146:06 - and in the next line we will once again
146:08 - call cv2
146:10 - dot
146:10 - create track bar
146:13 - with the name
146:14 - switch okay so now the name of our track
146:18 - bar will be switch
146:27 - so now we have added one more track bar
146:29 - to
146:30 - our named window
146:32 - and now here we will get the current
146:35 - position of this uh switch track bar so
146:39 - i'm going to name it as s
146:41 - and the name of the window is switch so
146:44 - we will just give the first argument of
146:47 - this get trackbar position as switch
146:51 - okay
146:51 - and the window name is image itself
146:55 - so now we can add some condition here so
146:58 - let's say
146:59 - if
147:00 - this position of the switch which we
147:02 - have if this position
147:05 - is equal to 0 because we only have 0 and
147:09 - 1
147:10 - in this last track bar so if this
147:12 - position is equal to 0 what we want to
147:15 - do is we want to set
147:18 - im
147:19 - g and then
147:21 - in the square bracket
147:23 - this colon and we don't want to change
147:25 - any value so we will say that img
147:29 - this square bracket colon is equal to 0
147:32 - which means that we don't want to do
147:35 - anything
147:36 - or in the other condition which is when
147:40 - your track bar is at position 1
147:44 - then only we want to change
147:47 - the bgr channel of the image okay so
147:51 - let's run this code and let's see what
147:53 - happens so i'm going to run this code
147:56 - and now you can see
147:58 - the position of this
148:00 - track bar switch is 0 and when
148:04 - i change it to 1 so let's change this
148:07 - position to 1 you can see the value to 1
148:11 - and when this position is at 0 you can
148:15 - change anything here any track bar
148:18 - nothing happens
148:19 - because
148:20 - this condition is met which means that
148:23 - we don't want to do anything as soon as
148:26 - we change the switch to 1 that means we
148:29 - want to change the bgr values you can
148:32 - see
148:33 - this color
148:34 - is changed inside the image so the 0 is
148:39 - just like
148:41 - off switch so we don't want to change
148:43 - any color
148:44 - and one is like on switch so when it's
148:48 - one
148:49 - the value of rb g channels can be
148:53 - changed now i want to give one more
148:55 - example of track bar to you so that's
148:57 - why i have created one more file which
149:00 - is python opencv trackbar example2 and
149:04 - this time i'm going to use just two
149:07 - track bars here so that's why i'm going
149:09 - to
149:10 - delete some of the code here so using
149:13 - the first track bar let's say i want to
149:15 - just change some values
149:17 - inside our image and i want to print
149:20 - that value on that image so let's say
149:22 - now our
149:24 - range is between 10
149:26 - to
149:27 - 400 okay so the lower range is 10 and
149:31 - the upper range is 400
149:33 - and using this track bar i want to print
149:37 - the current value on our image and also
149:40 - i want to have a switch which i can
149:43 - toggle
149:44 - and i want to change the color of the
149:47 - image from
149:48 - the colored value or colored image to
149:51 - the grayscale image
149:55 - so now our switch is between color to
149:58 - the grayscale image now in here what we
150:01 - want to do is we want to just assign
150:04 - this i am show value
150:06 - to the image variable itself and then we
150:09 - want to get the current position of the
150:12 - track bar so we will use
150:14 - this method to get the current position
150:17 - of the track bar and i'm going to name
150:20 - this current position as p o s variable
150:23 - and the name of this track bar let's
150:26 - change this name
150:28 - to something else let's say
150:31 - cp for current position and also here
150:35 - cp for the current position and the name
150:38 - of the named window is image itself so
150:40 - we are not changing
150:42 - it
150:43 - so now we have the current position so
150:46 - first of all we will just create the
150:49 - font and then we will just use the cv
150:54 - dot put text method you already know
150:57 - what this method does it just print the
151:00 - text on your image and then we will
151:03 - provide the parameters first argument is
151:06 - the image the second argument is
151:09 - the value which we get from the track
151:12 - bar so this is the position and because
151:14 - it's a number we need to convert it to
151:17 - the string using str method and then the
151:20 - position at which you want to
151:23 - show
151:24 - this
151:25 - text so let's say it's 50 comma 150 and
151:29 - then next is the font so i'm going to
151:32 - just give the font
151:34 - and then the next value is the font
151:36 - scale which is 4
151:38 - and the next value is
151:40 - for the color of the text so let's say
151:44 - the color here will be 0 comma 0 comma
151:47 - 255 and this should be cv dot font
151:51 - hershey complex let's change
151:54 - this font also let's say this is just
151:57 - the simplex
151:58 - font okay so this code is going to just
152:02 - print the color current position of the
152:04 - track bar on
152:05 - your image and then inside this
152:08 - condition what we want to do is we want
152:11 - to get the switch value so let's
152:15 - use this s variable and then
152:18 - get the current position of the switch
152:20 - using
152:22 - this switch name from the image window
152:25 - and then if the switch is at zero
152:28 - position then we want to do nothing so
152:31 - we will just pass
152:33 - this situation
152:35 - and in case the value of this switch is
152:39 - 1 then what we want to do is we want to
152:42 - change the image value from color to the
152:47 - grayscale value right so we can just
152:50 - write cv dot
152:52 - cbt color
152:55 - and the first argument is the image
152:58 - which we are loading and the second
153:00 - argument is cv dot
153:02 - color bgr to gray which is to convert
153:06 - this colored image to grayscale image
153:08 - but you can see here we are just
153:11 - creating a black colored image and in
153:14 - our project we also have this image so
153:16 - let's read this image so i'm going to
153:18 - just write the cv
153:21 - dot im
153:22 - read and then give the name of the image
153:25 - which is lena dot
153:27 - jpg so this is our colored image and
153:31 - this way we will be able to see
153:34 - the change of color to gray scale image
153:38 - in a better way
153:39 - let's run this code and let's see what
153:40 - happens
153:41 - and you can see
153:43 - image appears and disappears
153:46 - and there is an error so let's see what
153:48 - is the error so the error here is coming
153:51 - from this line so we need to read this
153:54 - image inside the while loop okay
153:58 - so this is
154:00 - why
154:01 - our error is coming and at the last
154:04 - we want to load this image after
154:08 - this if condition okay
154:10 - so now let's run this code once again
154:13 - and you can see
154:15 - this value is printed on our image which
154:20 - is 10 which is the value of cp
154:22 - and if we change this value it is
154:25 - changing on our image also right
154:29 - and once we change this 0 to 1
154:33 - then our image is converted
154:35 - from colored image to
154:37 - the grayscale image you can also change
154:41 - the font
154:43 - size here for example let's say it's
154:46 - 6 here
154:48 - and the thickness also if you want to
154:50 - change you can change it using
154:52 - this parameter let's say it's
154:55 - 10
154:56 - and let's run this code once again
154:58 - you can see the thickness and the size
155:02 - of the font is changed and you can see
155:04 - this
155:05 - value in a better way okay
155:07 - so this is how you can use track bars in
155:11 - opencv in this video we will see how we
155:14 - can perform object detection
155:16 - using hsv color space
155:19 - now we have already seen how to work
155:21 - with bgr or colored images or grayscale
155:26 - images
155:27 - and we have already seen how we can
155:30 - convert from colored images to grayscale
155:33 - images
155:34 - so there are more than 150 color space
155:37 - conversion methods
155:39 - in opencv
155:41 - and
155:42 - one of them is colored image to hsv
155:45 - image now what is hsv color space
155:49 - so hsv stands for hue
155:52 - saturation value so h stands for u
155:55 - as for saturation and v for the value
156:00 - now generally
156:01 - r g b
156:03 - in rjb color space
156:05 - are all correlated to the color
156:08 - luminance that is what we loosely call
156:11 - intensity
156:13 - in other words we cannot separate color
156:15 - information from
156:17 - luminance
156:18 - so hsv or hue saturation value
156:22 - is used to separate image luminance
156:26 - from color information
156:28 - so this makes it easier when we are
156:31 - working
156:32 - on or we need luminance
156:36 - in our images that is why generally we
156:39 - use hsb in the situation where color
156:42 - description plays a very important role
156:46 - now as i said hsv stands for hue
156:49 - saturation and value but what is the
156:51 - meaning of each and every
156:54 - single word in hsv now hsv is also known
156:58 - as the hex corn color model
157:02 - so this color space can be described in
157:06 - this kind of cylindrical cone
157:09 - model
157:10 - where hue
157:11 - is this
157:13 - circular angle which
157:16 - varies from 0 to 360 and hence just by
157:20 - selecting the range of hue you can
157:22 - select any color
157:24 - so you can see different colors are
157:27 - available at different angles so these
157:31 - colors are basically red yellow green
157:35 - cyan blue and magenta
157:38 - so hue is this angle in this cylindrical
157:42 - cone
157:43 - now we have saturation so the saturation
157:46 - is amount of color that is the depth of
157:50 - pigment or the dominance of hue
157:54 - and this value
157:55 - is described from the center towards the
158:00 - outer
158:01 - layer of this cylindrical cone so here
158:05 - you can see at the center
158:07 - this saturation start at zero and it can
158:10 - go up to one at the end of this
158:14 - cylindrical cone and this saturation can
158:17 - be increased from zero to hundred
158:19 - percent
158:20 - similarly the value is basically the
158:23 - brightness of the color so this
158:25 - brightness can be increased from 0 to 1
158:28 - from the bottom of the cone to the top
158:31 - of the cone
158:32 - so all these three value hue saturation
158:36 - and value
158:37 - can be used to pick any color as we can
158:42 - do with the bgr color space so this is
158:45 - the brief introduction about hsv color
158:48 - space and now let's see how we can use
158:51 - this hsv color space to detect an object
158:55 - in an image so here i have this simple
158:59 - code to load an image using imread
159:02 - method and show it inside a window so by
159:05 - now you might already know how this code
159:08 - works so let's run this code and let's
159:11 - see what does this code do
159:14 - so i have this image which is called
159:16 - smarties.png
159:18 - and here are some circles in different
159:21 - colors so we have blue circles or green
159:23 - or red
159:24 - orange and brown circles here inside
159:27 - this image
159:28 - so let's say we somehow want to detect
159:32 - only the blue circles or balls or green
159:35 - circles or balls how can we
159:38 - just detect only these
159:41 - balls let's say we just want to detect
159:43 - the green balls how can we achieve this
159:47 - using opencv we are going to see this
159:50 - using this hsv object detection
159:54 - and here we have one more window which
159:56 - is the tracking window which is coming
159:59 - from this code which is cv2 dot named
160:02 - window and the name of the window is
160:05 - tracking so this tracking window we are
160:08 - going to use little bit later when we
160:10 - will
160:11 - add the track bars to our image
160:14 - but let's say we want to uh use this
160:17 - image and detect
160:19 - these colored balls
160:21 - so first of all
160:23 - after this image is red what we want to
160:26 - do is we want to convert our colored
160:29 - image
160:29 - into our hsv image and by now you might
160:33 - already guess how to convert an image
160:36 - you can just write hsv is equal to cv2
160:40 - dot cvt color
160:43 - and then your
160:45 - frame name which is frame in this case
160:47 - and then cv 2 dot whatever color space
160:52 - you want to convert from and whatever
160:54 - color space you want to convert to so
160:56 - you can just write
160:58 - color underscore bgr to hsb so this is
161:01 - the property we are going to use now in
161:04 - the next step we will threshold the hsv
161:07 - image
161:08 - for our range of blue color so we are
161:11 - going to just define l underscore b
161:14 - for lower blue color and then we are
161:17 - going to use the numpy
161:19 - array so np dot
161:22 - array and inside this array we are going
161:24 - to define the lower range of blue color
161:27 - now by experience i know that these hsv
161:31 - value for lower blue color
161:33 - will be 110 comma 50
161:36 - comma 50 right but you might not have
161:41 - every time the idea what is the lower
161:44 - color range or the upper color range of
161:47 - some color so that is why later in this
161:50 - video we will use the track bar in order
161:53 - to perfectly uh define the lower and
161:56 - upper
161:57 - values for this hsv color space right
162:01 - so right now i'm just uh
162:04 - going with my experience so for the
162:06 - upper value i'm going to define the next
162:08 - variable which is ub
162:10 - is equal to np dot
162:13 - array and then once again i'm going to
162:15 - define
162:16 - these three uh
162:18 - color channels which is 130 comma 255
162:23 - comma 255 so this will be the upper
162:26 - limit for the blue color for our hsv
162:29 - image now in the next step what we are
162:31 - going to do is we are going to threshold
162:33 - the hsv image to get only the blue color
162:37 - lets say so i am going to just define a
162:39 - variable called
162:41 - mask here
162:42 - and then i am going to use cv2 dot
162:45 - in range method
162:48 - where i will provide first of all my hsv
162:52 - variable or image and then i will
162:55 - provide the upper and lower range for
162:58 - this function so my lower range is this
163:03 - numpy array for the blue color so i'm
163:05 - going to just say
163:07 - l underscore
163:08 - b is my lower range and
163:11 - u underscore b is my upper range now we
163:14 - have already seen how we can use bitwise
163:17 - and or bitwise operations on images so
163:21 - what we are going to do next is we are
163:24 - going to define a variable called res
163:27 - and then we will just call cv2
163:31 - dot
163:32 - bitwise and to mask the original image
163:36 - so here the first value will be
163:38 - our frame which which is the colored
163:41 - frame right so this is the
163:44 - frame which we have read from this image
163:47 - which is the smarties image so this is
163:49 - the source one source two will be the
163:52 - same
163:53 - so the
163:54 - frame itself will be the source two and
163:56 - what we want to do is we want to
163:59 - provide the mask of the
164:02 - lower blue color and the upper blue
164:04 - color values right so here we can just
164:07 - say mask
164:08 - is equal to whatever mask variable we
164:12 - have created so this is the attribute we
164:15 - can set in order to apply the mask for
164:19 - the
164:20 - lower blue value and the upper blue
164:22 - values so once we have
164:24 - this result uh frame what we can do is
164:28 - we can use this cv2 dot i am show method
164:32 - in order to show the mask let's say so
164:36 - we are going to show the mask and we are
164:38 - going to show the result using res
164:41 - variable so this is going to open three
164:44 - windows and let's see what happens when
164:48 - we run this code so we are going to run
164:50 - this code and this opens
164:52 - three windows here and now you can see
164:56 - the mask first of all so we are just
164:59 - detecting the blue colored uh balls
165:03 - using this mask that's why we have
165:06 - defined the lower boundation of the blue
165:10 - color and the upper boundation of the
165:12 - blue color right so that's why it's only
165:15 - detecting you can see
165:17 - the blue uh ball is here
165:20 - here and here and
165:22 - here also you can see the
165:24 - mask also detects
165:27 - only the blue values here right
165:30 - and then in the result you can see when
165:33 - we have applied this mask and we have
165:35 - masked
165:36 - all the other things other than the blue
165:40 - colored ball you can see only the blue
165:43 - balls here so the same method you can
165:46 - apply to detect any other colored ball
165:49 - from this image now as i said it's not
165:52 - easy to detect
165:54 - these lower and upper boundation for
165:57 - the
165:58 - colors
165:59 - so that's why you can use the track bar
166:02 - for
166:03 - adjusting these lower and upper
166:06 - boundation of any color
166:08 - so for that what we are going to do is
166:11 - first of all we will create a named
166:14 - window and then we are going to create a
166:17 - new window which we will use to adjust
166:20 - the lower and upper boundation of hsv
166:23 - values
166:24 - so now i'm going to just use a cv
166:28 - 2
166:29 - dot we have already seen how to create a
166:32 - track bar so i'm not going to explain in
166:35 - detail how this works but let's say
166:38 - this track bar name is
166:41 - lure hue for lh okay so this is the
166:45 - lower hue value and then
166:47 - the name of the window which is tracking
166:50 - which is this one so we are going to
166:52 - provide the name of the window and the
166:55 - next argument will be
166:58 - the starting and the ending value so we
167:00 - are going to define the start value 0
167:04 - and the end value let's say we are going
167:06 - to define the 255 here okay and the last
167:11 - thing we want to give here is the
167:13 - callback function which i have already
167:15 - created which is
167:17 - this function which is
167:20 - just no doing nothing we are going to
167:22 - just
167:23 - provide this callback function as a
167:26 - dummy function so it's not going to do
167:28 - anything so this is the track bar for
167:30 - the lower hue value similarly we are
167:33 - going to define
167:34 - the track bar for lower saturation and
167:37 - lower uh
167:38 - value and upper saturation
167:41 - upper value and upper hue okay so this
167:45 - will be lower saturation this will be
167:48 - lower value and then this will be
167:51 - u h which is upper hue
167:54 - and then this will be u as for upper
167:57 - saturation value and this will be
168:00 - upper value right so hsb lower values
168:05 - and hsv upper values
168:07 - so
168:08 - here we are going to set the initial
168:10 - value for the upper value so let's say
168:13 - everything is set to the maximum so 255
168:17 - 255 and 255 here okay
168:20 - so the lower values are set to zeros and
168:23 - upper values will be set to uh 255. now
168:27 - you already know how to get the values
168:30 - from attack track bar
168:32 - so
168:32 - you can use for example l underscore h
168:36 - for the lower
168:37 - q values is equal to cv 2 dot get track
168:42 - bar position so just use get track bar
168:46 - position method and then first of all
168:49 - give the name of the track bar from
168:50 - which you want to get the position so
168:53 - let's say we want to get the position
168:55 - from the lh track bar and then the name
168:59 - of the window which is tracking in our
169:02 - case so
169:03 - here is the second argument and
169:05 - similarly what we are going to do is we
169:08 - are going to define the other
169:10 - lower values and upper values so
169:19 - and also the name of
169:22 - your track bars
169:26 - so once you have the values of lower hsv
169:30 - and upper hsv you can provide these
169:33 - values here in place of these static
169:36 - values so first
169:39 - element of this array will be lh and
169:42 - then
169:43 - the ls variable and then the lv
169:46 - variables similarly for the upper
169:49 - boundation we will
169:51 - provide these three upper boundation
169:53 - variables
169:55 - and now when we will run our code let's
169:59 - see what happens so we are running our
170:01 - code and you can see
170:05 - these
170:06 - windows these three windows one is the
170:09 - mask
170:11 - other is the result and the third one is
170:13 - the frame and we also have these track
170:16 - bars in order to change the value of
170:19 - lower and upper
170:20 - hsv values so first of all let's set
170:24 - this mask for the blue color so i'm
170:26 - going to just move it
170:28 - to
170:29 - 110 as we have done
170:32 - in the last
170:34 - step and then this will be around 50
170:38 - and this also will be around 50 okay so
170:42 - let's move it to
170:44 - 50 and upper value here will be around
170:47 - 130 right so you can see once again
170:50 - using this track bar it's easier to
170:54 - adjust these lower and upper boundation
170:57 - and now you can see all the three uh
170:59 - blue colored balls so you can refine
171:02 - this object detection by moving these
171:05 - track bars a little bit uh left or
171:08 - little bit right you can see here now
171:10 - let's uh adjust this value to detect
171:13 - some other
171:14 - balls so let's say we want to detect
171:17 - the green balls so let's see what
171:19 - happens
171:20 - when we just change
171:23 - the saturation values here and you can
171:26 - see
171:27 - now you almost see
171:29 - the green values and the blue color is
171:33 - almost
171:34 - disappearing so you can see now there
171:36 - are only green
171:38 - uh
171:39 - balls which are detected and all the
171:41 - other
171:42 - balls are masked so you just need to
171:45 - play with this track bar for the lower
171:49 - hsv values and the upper hsb values and
171:52 - you will be able to detect the object
171:54 - whatever colored object you want to
171:56 - detect from the image
171:58 - now this is the object detection from
172:01 - the image
172:02 - similarly we can use the same method
172:06 - in order to track an object from a live
172:10 - video so i'm going to just
172:13 - escape to just close all the windows
172:16 - and in order to change this code for
172:20 - the video input what we can do here is
172:24 - we can just add this code so we are
172:27 - going to just
172:28 - add the
172:29 - cap variable which is the capture
172:31 - variable is equal to cv2 dot
172:35 - video capture so we are going to use
172:37 - this one and we are going to
172:40 - capture the video from our default
172:41 - camera which is at
172:44 - the index 0
172:46 - and then
172:47 - you already know how we can read the
172:50 - values from the camera input so i'm
172:53 - going to
172:54 - just
172:55 - comment this code and instead of reading
172:58 - the image what we are going to do is we
173:01 - are going to write
173:03 - underscore
173:05 - comma frame is equal to
173:08 - cap
173:09 - dot read which is going to read
173:12 - the
173:13 - frames from your default camera and at
173:16 - the end when you are done playing with
173:19 - your images you can just destroy
173:22 - this
173:23 - cap using the release method so you can
173:25 - just write cap dot release just going to
173:28 - release all the
173:30 - cameras you are
173:32 - just capturing right so now this is the
173:34 - three line code you need to use in order
173:38 - to capture the camera input and then
173:42 - track any uh
173:44 - object of any color so i'm going to run
173:47 - this code now and you can see
173:50 - i'm just holding a blue colored
173:54 - object here and i'm moving this object
173:57 - on the left and right
173:59 - and you can see only blue colored object
174:01 - is detected and every other
174:05 - frame value is masked so this is how you
174:08 - can do the object tracking of any color
174:11 - using the hsv
174:13 - color space so you can see the
174:16 - real image which is captured from the
174:18 - camera and then the mask and then the
174:21 - result of the mask and the real image
174:25 - in this blue colored object tracking so
174:29 - this is how you can do object detection
174:32 - and object tracking using hsv color
174:35 - space in this video we will see how we
174:38 - can perform simple thresholding on
174:40 - images
174:41 - using opencv
174:44 - so first of all what is thresholding
174:47 - so thresholding is a very popular
174:49 - segmentation technique
174:52 - used for separating an object from its
174:56 - background the process of thresholding
174:58 - involves comparing each pixel of an
175:02 - image
175:03 - with a predefined threshold value
175:06 - and this type of comparison
175:09 - of each pixel of an image to a threshold
175:12 - value divides all the pixels of the
175:16 - input image into two groups
175:19 - so the first group involves pixels
175:22 - having intensity value lower than the
175:24 - threshold value
175:26 - and the second group involves the pixels
175:30 - having intensity value greater than the
175:32 - threshold value and using the different
175:35 - thresholding techniques which are
175:38 - available in opencv
175:40 - we can
175:41 - give different values to these pixels
175:44 - which are
175:45 - higher than the
175:47 - threshold value and which have the
175:49 - intensity lower than the threshold value
175:53 - so let's see how we can use simple
175:56 - thresholding techniques on an image so
176:00 - to start with i have the simple code
176:03 - which loads an image
176:05 - on a window and this image is called
176:08 - gradient dot png so let me show you how
176:12 - this image looks like
176:14 - so this image looks like this so as you
176:17 - can see in this image
176:19 - we have on the left hand side the black
176:21 - values and when we gradually move from
176:24 - left to right we move towards the white
176:28 - value
176:29 - so on the
176:31 - left hand side the pixel value are
176:34 - closer to zero
176:36 - and on the right hand side the pixel
176:39 - values are closer to 255.
176:43 - so now we are going to
176:45 - just
176:46 - involve some thresholding techniques and
176:50 - we will see how
176:52 - these this image is affected by the
176:55 - thresholding techniques so first of all
176:57 - what we are going to do is we are going
176:59 - to uh define two variables one is
177:02 - underscore because the the result of the
177:05 - thresholding gives two result one is ret
177:10 - value and the second is the thresholded
177:13 - value of an image
177:16 - so i'm going to just say
177:18 - the second value which is given by the
177:21 - thresholding technique is th1
177:24 - is equal to
177:26 - cv
177:27 - dot
177:28 - threshold and this threshold method
177:31 - takes several values the first is the
177:34 - source so our source is
177:36 - image
177:37 - the second is the threshold value
177:41 - so as we
177:42 - have seen that our image
177:45 - have on the left hand side
177:47 - 0 pixel value and when we move towards
177:50 - the right its uh pixel value uh
177:54 - increases to 255 right so let's say our
177:58 - threshold here is
178:00 - 127
178:02 - and the maximum value of the threshold
178:05 - is 255 which is the white color on the
178:08 - right hand side
178:09 - and then
178:10 - the fourth value here will be
178:14 - the threshold type
178:16 - so there are several threshold type in
178:19 - simple thresholding we are going to see
178:22 - them one by one so the first
178:24 - thresholding type is cv2
178:27 - dot
178:28 - thrash
178:29 - binary so first of all let me show you
178:31 - how the result looks like and then i
178:34 - will explain what does this trash binary
178:38 - type
178:39 - does
178:40 - so what we are going to do is we are
178:42 - going to use one more cb2 dot iom show
178:45 - method to
178:46 - show this thresholded value into a new
178:51 - window so we are going to just show this
178:54 - value into a new window and we already
178:57 - have
178:58 - the original image in the image window
179:01 - so let's run this code and let's see
179:03 - what happens so you can see in this
179:06 - binary thresholding we are comparing
179:09 - each and every pixel of this original
179:12 - image to
179:14 - 127
179:15 - and if the value of the pixel
179:19 - is less than 127 the value is assigned
179:23 - to 0
179:24 - and if the value of the pixel is greater
179:28 - than 127
179:30 - the pixel value is assigned 255 that
179:33 - means wide
179:35 - so if the value of the pixel is zero it
179:39 - will look black and if the value of the
179:42 - pixel is 255 it will look
179:45 - wide
179:46 - so this is how binary thresholding works
179:50 - and by the name itself
179:52 - you can
179:53 - understand that this is just
179:56 - a binary thresholding so it's either 0
180:00 - or 1. now let's see the other type of
180:04 - thresholding technique
180:06 - so now i will just change the name of
180:09 - this variable as th2 and the next type
180:13 - of thresholding is called thresholding
180:16 - binary inverse and as the name suggests
180:21 - the thresholding binary inverse is going
180:23 - to give the inverse result of what you
180:27 - get
180:28 - from the trash binary
180:30 - so i'm going to once again
180:33 - use the i am show method to show the
180:36 - result of this
180:39 - thresholding binary inverse value and
180:42 - let's run the code and let's see what
180:43 - happens
180:44 - so this is the original image
180:46 - and then we have the thresholding one
180:50 - image and the thresholding uh inverse
180:53 - image so this image you got from
180:56 - the first thresholding which is by using
181:00 - thrash binary and the second image you
181:02 - got from this method which is thresh
181:06 - binary inverse and this thresh binary
181:09 - inverse image is just the inverse of
181:14 - what you get using the thresh binary so
181:17 - if the pixel value is lower than 127
181:22 - which is our threshold the
181:24 - pixel is assigned 255
181:27 - otherwise if the value is greater than
181:30 - 127 then the pixel value is assigned 0
181:34 - which is the inverse of what we
181:37 - got
181:37 - in the previous step now let's change
181:40 - this threshold to let's say 50 and here
181:44 - also let's say we change this threshold
181:48 - to
181:49 - 200 and let's see how this result
181:52 - changes when we change the threshold
181:54 - value so i'm going to run this code once
181:57 - again and you can see this is the result
182:00 - of thresh binary and now
182:03 - because our threshold is up to 50 that's
182:07 - why our result
182:09 - is
182:10 - like this
182:11 - so until
182:12 - the pixel value is 50 it's black
182:16 - otherwise if the pixel value is greater
182:19 - than 50 it's going to give you the white
182:23 - pixel value and the thresh binary
182:26 - inverse is going to give you
182:28 - the inverse value of what you get in the
182:31 - thresh binary
182:32 - step so i'm going to once again
182:35 - just close these windows and let's see
182:38 - the next uh thresholding type so i'm
182:41 - going to name my variable as three so
182:45 - the next thresholding type is called
182:48 - thresh trunk so this is this type
182:52 - and let's first of all
182:56 - see what is the result of this
182:58 - thresholding technique and then i'm
183:00 - going to explain what it does
183:03 - so we are going to just
183:05 - show this
183:07 - thresholded image into a new window and
183:10 - run
183:10 - the code and now we have the result
183:14 - so
183:15 - let's move it
183:17 - like this
183:18 - and we have here the original image and
183:21 - the
183:22 - result of the thresh trunk is this three
183:26 - so here what happens is
183:28 - up to the threshold the value of the
183:33 - pixels will not be changed so up to 200
183:37 - because our threshold is up to 200 so
183:40 - when the pixel value is up to 200 the
183:44 - pixel value will not change and after
183:47 - the threshold which is 200 the pixel
183:50 - value will remain
183:52 - the same which is 200 so from here to
183:55 - here the pixel value will remain 200
183:58 - let's change this threshold to some
184:01 - other value let's say 127
184:03 - and then let's uh run this code and you
184:06 - will see
184:07 - that now
184:09 - from
184:10 - black to
184:12 - 127 pixel value the value of this image
184:17 - will not change so original image up to
184:20 - half
184:20 - is the same
184:22 - and after the pixel value
184:26 - 127
184:27 - the value remains 127 okay so the pixel
184:32 - intensity value will remain 127 until
184:36 - the end so if the value is greater than
184:39 - 127
184:41 - the value will remain 127 and if the
184:45 - pixel value
184:46 - is lesser than 127 then the pixel value
184:51 - will remain unchanged so this is how the
184:54 - trash trunk works and let's see the
184:58 - other method
185:00 - which is let's say
185:02 - th4 and this is the method which is
185:07 - called thresh
185:08 - 2 0 so we are going to just
185:12 - use thresh to 0
185:14 - and then we are going to open this
185:17 - th4 into a new window
185:20 - and let's run this code and let's see
185:22 - what happens
185:23 - so now we have
185:25 - this result let's move this to the left
185:29 - and the result of the thresh to zero is
185:34 - this one so in thresh to zero
185:36 - thresholding
185:37 - whenever your pixel value is lesser than
185:41 - threshold
185:43 - the value assigned to pixel will be zero
185:46 - okay
185:47 - so when the pixel value is lesser than
185:50 - the threshold the pixel value is
185:52 - assigned to zero that's why you can see
185:55 - half of the image is black
185:58 - and when the pixel value is greater than
186:01 - 127 which is our threshold value
186:04 - the image or pixel value will remain the
186:07 - same so after 127 all the pixels
186:11 - will remain the same
186:14 - let's see the other technique which is
186:17 - called
186:18 - thresh to zero inverse which you uh
186:22 - already understood i think what it does
186:25 - so this is thresh to zero inverse and
186:30 - we can just change this variable name to
186:33 - th5
186:34 - and here we can just open it into a new
186:38 - window
186:39 - and i'm going to run this code once
186:42 - again
186:43 - let me move this
186:44 - here and the
186:47 - result here so you can see
186:49 - this
186:50 - thresh to zero inverse is just
186:53 - the opposite of the thresh to zero so if
186:57 - the value of the pixel is greater than
186:59 - the threshold value which is 127 the
187:02 - value will be assigned to zero otherwise
187:06 - if the value of the pixel is lesser than
187:08 - threshold the value of the pixel will
187:11 - remain the same so this is how some of
187:14 - the simple thresholding techniques
187:17 - works
187:18 - in opencv in the last video we have seen
187:21 - how we can perform simple thresholding
187:24 - in opencv using python using various
187:27 - thresholding techniques
187:29 - so we have used trash binary threshold
187:32 - binary inverse
187:34 - thresh trunk thresh to zero trash to
187:36 - zero inverse so these were all the
187:39 - simple thresholding techniques
187:42 - now in these thresholding techniques we
187:45 - were setting a global value of our
187:48 - threshold so in this example for example
187:51 - here the global value of threshold is 50
187:54 - here 200 here 127. so we were setting in
188:00 - simple thresholding the global value and
188:03 - this means that it is same for all the
188:07 - pixels in the image
188:10 - now in this video we are going to learn
188:12 - how to use adoptive thresholding
188:16 - so adoptive thresholding is the method
188:19 - where the threshold value is calculated
188:22 - for the smaller region so the threshold
188:26 - is not global for every pixel
188:29 - but it's calculated for a smaller region
188:33 - and therefore
188:34 - there will be different threshold value
188:37 - for different regions
188:40 - now why do we need this type of adoptive
188:43 - thresholding so using simple
188:45 - thresholding might not be a good idea in
188:49 - all the conditions so there might be
188:51 - conditions where the image has
188:53 - different lighting conditions in
188:56 - different regions
188:58 - and in those
189:00 - cases where the lighting conditions in
189:02 - the images
189:04 - varies from point to point in those
189:07 - cases we might want to use adoptive
189:10 - thresholding so as i said adoptive
189:13 - thresholding calculates
189:15 - the threshold for a smaller region of
189:18 - images so we get different thresholding
189:21 - values for different
189:23 - regions of the same image and as a
189:26 - result adoptive thresholding gives us
189:29 - better results for images with varying
189:33 - illumination so let me show you the
189:35 - problem with simple thresholding for
189:38 - the image which have
189:40 - different illumination at different
189:43 - regions
189:44 - so i have this image called sudoku dot
189:47 - png which i'm loading using i'm read
189:50 - method and then i'm just showing this
189:53 - image using i'm show method
189:55 - and then let's use the simple
189:58 - thresholding technique
189:59 - which is trash binary for this and we
190:03 - have set the global threshold value of
190:06 - 127 here
190:08 - and then we will see
190:10 - the
190:11 - result after this threshold is applied
190:14 - to the image so i'm going to run this
190:16 - program
190:17 - and let's see what happens so this is
190:20 - our original image and then this is the
190:23 - result which we got
190:25 - so
190:26 - on in the result you can see when we
190:28 - apply
190:29 - a same
190:31 - global threshold value some of the
190:33 - region of this image
190:35 - is black
190:37 - and other region of this image is
190:40 - visible right so because
190:43 - the image have different illumination
190:46 - value at different regions that's why
190:49 - we see half of the image
190:52 - which have the good illumination and we
190:55 - don't see half of the image which
190:57 - doesn't have the better illumination
191:00 - so in that case it's a better idea to
191:03 - use adoptive thresholding so let's see
191:06 - how we can use adoptive thresholding so
191:09 - here what i'm going to do is i'm going
191:11 - to declare a variable called th2 and
191:14 - then we use cv dot
191:17 - adoptive threshold so this is the method
191:20 - which we are going to use for performing
191:23 - adopting thresholding and this takes few
191:26 - arguments so first is the source so our
191:29 - source is the image variable now the
191:32 - second parameter here is the max value
191:35 - so the max value is the non-zero value
191:39 - assigned to the pixels for which the
191:41 - condition is satisfied so in our case
191:45 - the maximum value which we can provide
191:47 - to a pixel is 255 and we cannot go more
191:51 - than that right
191:53 - now the third parameter here is
191:56 - the adoptive method so this adoptive
192:00 - method is going to decide how the
192:03 - thresholding value is calculated and
192:05 - there are two types of adoptive methods
192:08 - which we can use so the first method is
192:11 - called cb2
192:13 - dot
192:14 - adoptive thrash mean c
192:17 - so what is the meaning of this adoptive
192:20 - thresh mean underscore c
192:22 - so using this method the threshold value
192:25 - is the mean of the neighborhood area and
192:29 - here is the documentation of these two
192:32 - methods so adoptive thresh means c
192:35 - gives us the threshold value using this
192:40 - function and this is going to give us
192:42 - the mean of the block size multiplied by
192:45 - block size
192:46 - neighborhood of x comma y minus
192:49 - c which is the constant and the second
192:53 - adoptive threshold type is this one
192:56 - which is adoptive trash gaussian
192:59 - underscore c and in this adoptive
193:02 - thresholding the threshold value is the
193:04 - weighted sum of neighborhood values
193:07 - where weights are the gaussian window so
193:11 - let's use the first adoptive method
193:14 - which is the adoptive thresh mean
193:17 - underscore
193:18 - c now the next parameter here is the
193:21 - threshold type so the threshold type
193:23 - which we are going to use is the thresh
193:26 - binary which we have also seen in the
193:29 - last video also and then the next value
193:32 - is the block size so block size decides
193:37 - the size of the neighborhood area
193:40 - so here we are going to give the block
193:42 - size 11
193:44 - and the next parameter here is the value
193:47 - of c so we have seen that we need to uh
193:51 - give the value of c also when we use the
193:56 - adoptive thresh mean c and adopt a
193:59 - thresh gaussian c so this is the value
194:02 - of c which we are going to give and c is
194:05 - just a constant which is subtracted from
194:08 - the mean in the case of uh this adopted
194:12 - thrash mean method or the weighted mean
194:15 - in the case of gaussian adaptive
194:18 - threshold okay so constant we are going
194:21 - to give here is two and now what we are
194:25 - going to do is we are going to just
194:28 - load this image which we got after
194:32 - applying this adaptive thresholding
194:35 - and lets uh just comment the
194:38 - other window so we will just see the
194:41 - original image and the adoptive
194:44 - thresholding result so i'm going to run
194:46 - this code
194:48 - and you can see the original image here
194:51 - and the result of adoptive thresholding
194:55 - which looks much better
194:57 - than the simple thresholding technique
195:00 - so let's uncomment uh this also so i'm
195:03 - going to uncomment
195:04 - this so we will see all the three result
195:08 - at the same time so this is the original
195:10 - image and you can see the simple
195:13 - thresholding gives us
195:15 - this value using the global threshold of
195:18 - 127 and adoptive thresholding gives us
195:23 - this
195:24 - value or this image which is much more
195:28 - readable
195:29 - than the simple thresholding technique
195:32 - image so this is how you can use
195:34 - adoptive thrash mean c method in a same
195:37 - way we are going to use
195:39 - the other adoptive
195:41 - thresholding technique which is called
195:44 - adoptive thresh
195:46 - gaussian c so instead of this we are
195:50 - going to use adoptive thresh gaussian c
195:53 - and then
195:55 - all the parameters we are going to leave
195:57 - as same and let's load the result of
196:02 - this type of thresholding
196:04 - which is stored in three
196:07 - so let's run this code and let's see
196:09 - what happens
196:10 - so
196:11 - we have already seen
196:13 - this image which is the simple
196:15 - thresholding this is the result of the
196:18 - adoptive thresholding mean c
196:21 - and this is the result of adoptive
196:24 - thresholding
196:25 - gaussian underscore c so both of the
196:28 - result looks good because the adoptive
196:31 - thresholding algorithm calculates the
196:34 - thresholding value for different regions
196:37 - so the thresholding value is not global
196:41 - for each and every pixel of the image
196:44 - and we have seen the two adaptive
196:46 - methods which are available in adoptive
196:48 - thresholding
196:50 - so in this way you can use adoptive
196:51 - thresholding in opencv in this video we
196:55 - will talk about a library called
196:57 - matplotlib which you can use with opencv
197:00 - images
197:01 - so first of all what is matplotlib
197:04 - so matplotlib is a plotting library for
197:07 - python which gives you a wide variety of
197:10 - plotting methods
197:12 - and on the official website which is
197:14 - matplotlib.org
197:17 - you can see match plotlib is a python 2d
197:21 - plotting library which produces
197:22 - publication quality figures
197:25 - so it's primarily a
197:28 - 2d plotting library
197:30 - but it's widely used with
197:33 - opencv to display graphs and images and
197:36 - histograms
197:37 - so we will see how we can use matplotlib
197:41 - with opencv
197:43 - it's also written here that for simple
197:46 - plotting the pi plot module provides a
197:50 - matlab-like interface
197:53 - so first of all let's see how we can
197:55 - install matplotlib and then we are going
197:58 - to see how to use matplotlib
198:01 - with opencv so to install matplotlib
198:05 - using pip you just need to open your
198:07 - terminal and then just give this command
198:09 - which is pip
198:11 - install
198:12 - matte
198:13 - plot
198:15 - lib
198:16 - and then press enter
198:18 - and in some seconds
198:20 - this matplotlib library will be
198:22 - installed using pip
198:25 - so now you can see matplotlib is
198:27 - installed on my windows operating system
198:30 - and to check it i'm going to just give
198:33 - the
198:34 - python command and here i'm going to
198:36 - import matplotlib so i'm going to just
198:38 - write from
198:40 - mat
198:41 - plot
198:42 - lib
198:43 - import pi plot
198:46 - as plt
198:48 - okay and then press enter and if this
198:51 - import doesn't give you any error that
198:53 - means
198:53 - it's imported successfully and you can
198:56 - start using matplotlib
198:59 - now as we are using pycharm ide let me
199:02 - show you how you can install matplotlib
199:05 - on pycharm so just open your pycharm ide
199:10 - and then here just click on file
199:12 - and then settings
199:14 - and then go to project colon your
199:17 - project name my project name is opencv
199:20 - examples and then click on interpreter
199:23 - and you can see other
199:25 - packages are already there and we just
199:28 - need to
199:30 - install the matplot
199:32 - lib package so just type here in the
199:34 - search matplotlib and you will be able
199:37 - to find matplotlib here in the result so
199:40 - just uh click on matplotlib and then
199:44 - just click on the install package so i'm
199:47 - going to just click on the install
199:49 - package and in some seconds matplotlib
199:52 - library will be installed in your
199:54 - pycharm ide so you can see this message
199:57 - which says package matplotlib installed
199:59 - successfully that means we can close
200:03 - this window
200:04 - and then you will be able to see
200:06 - matplotlib
200:08 - is available in your project interpreter
200:11 - so
200:12 - everything is fine and i'm going to just
200:14 - close this
200:15 - and now i will be able to import
200:18 - matplotlib so i'm going to just write
200:21 - from
200:22 - mat
200:23 - plot
200:24 - lib
200:26 - import pi plot
200:29 - as
200:30 - plt now in order to show the image which
200:33 - you read using the opencv i'm read
200:37 - method you can use this
200:40 - code so just write plt
200:43 - dot
200:44 - i am show so there is also a method
200:48 - inside your pi plot library which is
200:51 - available inside matplotlib and this
200:54 - mess third you can use to show the image
200:57 - which you
200:58 - have read from
201:00 - the opencv imread method so for now just
201:03 - write this kind of code and to show
201:06 - the matplotlib window you just need to
201:10 - write plt dot
201:12 - show so this is going to show this image
201:15 - using the matplotlib library so we are
201:19 - opening this image
201:21 - using the opencv i'm sure window as well
201:24 - as matplotlib window also so let's run
201:28 - this code and let's see what's the
201:29 - result which we are getting
201:32 - so you can see this is the image which
201:34 - is loaded using the matplotlib and this
201:38 - was our original image which is loaded
201:42 - using the
201:44 - opencv library and straight away you can
201:47 - see some difference so this is the
201:49 - original image which is the colored
201:51 - image and in the matplotlib
201:55 - window we also want the same result but
201:58 - it's giving us the different result
202:01 - and the reason behind this is
202:04 - opencv reads the image in the bgr format
202:08 - and the matplotlib reads the image in
202:12 - the rbg format
202:15 - so in order to show this kind of colored
202:18 - window using matplotlib you need to
202:21 - convert your image from bgr to rbg
202:26 - and then only you will be able to see
202:28 - this kind of colored image using matte
202:31 - plotlib so i'm going to just
202:34 - close
202:35 - these windows and now after i'm showing
202:38 - this image using the cb2 i'm show method
202:42 - i'm going to convert this image so i'm
202:44 - going to just write img is equal to
202:47 - cv 2 dot
202:50 - cv
202:51 - t color
202:52 - and then i am going to convert this
202:55 - image
202:56 - from
202:57 - bgr image so i am going to just write cv
203:00 - 2 dot color
203:02 - underscore bgr
203:05 - to rgb okay so our matplotlib library
203:11 - shows the image in the rgb format and
203:15 - the opencv reads the image in bgr format
203:18 - so now we have converted our image from
203:21 - bgr
203:22 - to rgb image and now we are showing this
203:26 - image
203:27 - using the matplotlib and let's run this
203:29 - code and let's see what happens now
203:31 - so now when we run this code
203:34 - you see both the image
203:36 - looks the same right
203:38 - now let's see that advantages of using
203:42 - matplotlib so you can see
203:45 - this is a quite static
203:48 - window but when you see in matplotlib
203:51 - when you hover over this image you can
203:53 - see x and y coordinates
203:55 - of the mouse point and this is helpful
203:59 - you can also save this image in the form
204:01 - of a png file so you can just press this
204:05 - and save this image wherever you want
204:07 - you can also zoom this image if this
204:11 - feature is available
204:12 - there is also configuration subplots
204:16 - options so you can
204:17 - you can just increase
204:20 - these values left bottom wherever you
204:23 - want to place your
204:27 - image you can
204:29 - do that these are some options which are
204:32 - available here you can also reset these
204:34 - options and you can see the coordinates
204:37 - here so because matplotlib is primarily
204:41 - a 2d plotting library so you can see the
204:44 - x coordinates and y coordinates and
204:47 - because this image is
204:49 - about 512 by 512
204:52 - pixels that's why here it's showing 0 to
204:57 - 512 and here also on the y-axis
205:01 - 0-512 so this is how you can load your
205:05 - image using matplotlib and now i'm going
205:08 - to show you one more thing and this is
205:11 - when you write plt dot
205:13 - x ticks here and then when you pass
205:16 - empty array here which is empty square
205:18 - bracket
205:19 - comma
205:20 - pl t
205:22 - dot y ticks
205:24 - and also here you pass
205:27 - mt array this is going to hide the tick
205:30 - value on x and y axis
205:33 - so now when i run this
205:35 - code
205:36 - and you can see now that
205:39 - these x sticks and y takes on x and y
205:43 - axis are gone so let me just
205:46 - comment this out once again
205:48 - and you will be able to see this x and y
205:51 - coordinates here
205:53 - on the image and when you use
205:56 - this
205:58 - code which is to hide the ticks on the x
206:01 - and y axis then you will see the image
206:05 - without these
206:08 - x and y axis ticks so if you remember in
206:11 - the last video we have seen how to use
206:14 - simple thresholding in opencv and we
206:18 - were using six windows to show
206:22 - these six different images
206:24 - using opencv
206:26 - now let's say you want to show all these
206:29 - six uh windows
206:31 - in one matplot
206:34 - lib window how you can do it with the
206:36 - use of matplotlib i'm going to show you
206:39 - so first of all we are going to import
206:42 - matplotlib
206:44 - import pie plot as plt
206:47 - and then what we are going to do is we
206:49 - are going to define
206:51 - the
206:52 - titles and then we are going to define
206:54 - these six different uh titles for six
206:58 - different images so first one is our
207:01 - original image second was the trash
207:04 - binary third was thresh binary inverse
207:07 - fourth was trunk fifth was two zero and
207:10 - six was to zero inverse
207:13 - in the same way we are going to define a
207:15 - variable called images and inside this
207:18 - square bracket we are going to pass
207:20 - first of all our original image and then
207:23 - th one comma th2 comma th3 comma th4
207:30 - comma
207:31 - th5 okay
207:34 - so these are the six value we want to
207:37 - show
207:37 - and these are the six titles of these
207:41 - images and now we are going to use the
207:44 - for loop so for
207:45 - i
207:46 - in
207:48 - x range so using the python x range we
207:52 - are going to just iterate over these six
207:56 - values so i'm going to just write x
207:58 - range and then the range we are going to
208:00 - provide here is six and then inside this
208:03 - for loop we are going to just call plt
208:05 - and we are going to call a method called
208:09 - subplot okay and this subplot method
208:13 - takes few arguments so first argument is
208:16 - the number of rows which we want to show
208:19 - in our matplotlib plot
208:22 - so
208:23 - because we have six images so we are
208:25 - going to divide these images into two
208:28 - rows
208:29 - and three columns so the first argument
208:32 - here is the number of rows and the
208:34 - second argument here is the number of uh
208:38 - columns and the third argument here will
208:40 - be the index of the image so the index
208:42 - of the image will be
208:44 - i plus
208:45 - 1
208:46 - and then we are going to write comma plt
208:50 - dot
208:51 - i am
208:52 - show so this is going to show this image
208:55 - and the index of the image so we are
208:58 - going to just write
209:00 - images and then square bracket i so this
209:03 - is going to give you a particular image
209:05 - at index i and then we want to show this
209:08 - image as a gray scale image so anyway
209:11 - when you use thresholding you use the
209:14 - grayscale image so
209:16 - you just need to write a gray here then
209:18 - we are going to show the titles
209:21 - of these images so we are going to just
209:23 - write plt dot
209:26 - title and then this title method takes
209:29 - the title name which we are getting
209:31 - using this titles array and then at the
209:35 - index i this is going to give you uh the
209:38 - title name which we have declared in
209:40 - this title array and at last if you
209:43 - don't want to show the text
209:45 - on the images you can give these two
209:48 - method which is plt dot x
209:52 - and the argument here is the empty list
209:55 - and also plt dot vitix and the argument
209:58 - is the empty list
210:01 - and at the end what we want to do is
210:04 - instead of
210:06 - using
210:07 - this kind of code we just want to show
210:10 - our
210:11 - window so we can just say plt dot
210:15 - show and this is showing us this error
210:17 - unresolved reference yes so this is when
210:22 - you are using uh python 2 but in python
210:25 - 3
210:26 - this x range is changed to a method
210:29 - called range and that's why it was
210:31 - giving us the error so let's run this
210:34 - script once again
210:36 - and you can see
210:37 - six different
210:39 - results and six different titles so
210:43 - these are all the titles which are shown
210:46 - here
210:47 - and then the result are shown
210:50 - under these titles so using matplotlib
210:53 - you can include
210:55 - multiple images into one window and this
210:59 - is very useful when you want to show
211:03 - multiple image
211:04 - at the same time in the same window so
211:07 - this is how you can use matplotlib
211:09 - library with
211:11 - opencv images and there is a lot of
211:14 - things which you can do with matplotlib
211:17 - so if you want to learn more you can
211:19 - just go to the official website which is
211:21 - matplotlib.org
211:24 - and then you will be able to see more
211:27 - documentation here
211:29 - in this video we are going to discuss
211:31 - about morphological transformations in
211:34 - opencv
211:36 - so we will discuss different
211:38 - morphological operations like erosion
211:41 - dilation
211:42 - opening and closing methods etc
211:46 - but first of all what are morphological
211:49 - transformations
211:51 - so morphological transformations are
211:54 - some simple operations
211:56 - based on the image shape
212:00 - now morphological transformation is
212:02 - normally performed on a binary image
212:06 - and when we perform morphological
212:08 - transformation
212:10 - there are two things which are required
212:13 - first is the original image
212:15 - and second is called a structuring
212:18 - element or a kernel
212:21 - which decides the nature of the
212:23 - operation
212:25 - now there are different type of mark for
212:28 - logical transformations and we are going
212:30 - to see them one by one
212:32 - now to start with i have this simple
212:34 - code which reads the image using opencv
212:38 - imread method and we are just loading or
212:42 - showing this image using matplotlib
212:46 - now if you are unfamiliar with
212:48 - matplotlib and how to use matplotlib to
212:51 - show images in the last video i have
212:54 - explained this topic in details so if
212:57 - you want to see that video about
213:00 - matplotlib you can see it and this is
213:04 - the code i have used in the last video
213:07 - also and i have explained this code in
213:10 - details in the last video so if you are
213:14 - confused what this code is doing just
213:17 - see the last video now there is one
213:19 - important thing to notice here is i am
213:22 - reading
213:23 - this image in a gray scale mode okay so
213:28 - either you can provide here as the
213:30 - second argument of i am read cv2 dot i
213:33 - am read underscore
213:35 - grayscale or you can provide simply 0
213:38 - here in order to read this image in
213:40 - grayscale so let's run this code and
213:43 - let's see what it does
213:45 - so as expected it's just opening the
213:48 - image in the grayscale mode using
213:52 - matplotlib
213:53 - now as i said
213:55 - normally we perform the morphological
213:58 - transformations on the binary images so
214:02 - that's why we need to provide a mask to
214:05 - our image using the simple thresholding
214:09 - so let's just do that
214:11 - so i'm going to just write underscore
214:14 - comma
214:15 - the mask so i'm going to name my
214:18 - variable as mask here
214:21 - and then i'm going to just write cb2 dot
214:24 - threshold
214:26 - and this threshold take few argument as
214:28 - you might already guess first is the
214:31 - image itself second argument is the
214:34 - value of the threshold so for now i'm
214:37 - going to just provide the threshold of
214:40 - 220 here the maximum value of threshold
214:44 - will be 255
214:46 - then the next argument here is the type
214:50 - of the threshold so we are going to
214:51 - provide
214:52 - cv
214:53 - 2
214:54 - dot thrash binary inverse so this is our
214:58 - mask so let's load the mask in the
215:02 - matplotlib
215:04 - window so i'm going to just provide in
215:06 - this titles array one more title which
215:10 - is
215:11 - mask
215:12 - and then we are going to see how
215:15 - this image looks like after the mask
215:18 - okay and here the range i'm going to
215:21 - increase it to 2 because now the array
215:24 - is of two elements
215:26 - and the subplot is also let's say one by
215:30 - two so we want to show
215:32 - two images and i'm going to just run
215:34 - this code and you can see this was the
215:38 - image which was the grayscale image and
215:40 - the second image is the masked image
215:44 - now if you see this image carefully let
215:47 - me just
215:49 - just increase the size of this image and
215:52 - if you see this image carefully after
215:55 - masking there are some black dots here
216:00 - on the balls
216:02 - and let's say we want to remove
216:04 - these dots which are there in between
216:08 - this white area this black dot or this
216:11 - black dot or you can see some black dots
216:14 - are there
216:15 - inside your ball in the bite area and we
216:20 - want to remove
216:22 - these dots from the balls
216:24 - for this we are going to use the
216:27 - dilation
216:29 - transformation
216:30 - so first of all what we are going to do
216:33 - is we are going to just write
216:36 - dilation which will be our variable name
216:40 - and then we are going to use this method
216:44 - called cv2 dot
216:46 - dilate
216:47 - okay
216:48 - so this method
216:50 - uses the source which is mask
216:54 - in our case
216:56 - and then the second thing is the kernel
217:00 - okay so let me
217:02 - explain what the kernel is
217:04 - so a kernel is normally a square or some
217:09 - shape
217:10 - which we want to apply on the image
217:14 - so we are going to define a kernel of
217:17 - numpy once which means we want to apply
217:21 - white
217:22 - square
217:23 - on our balls so you can see when we run
217:26 - our code once again it shows us error
217:29 - because this kernel
217:31 - is undefined so let me define this
217:33 - kernel first of all so i'm going to just
217:35 - say kernel
217:37 - is equal to
217:38 - np dot
217:40 - once
217:41 - and then we are going to define the
217:45 - shape of
217:46 - this kernel let's say this is of 2 comma
217:50 - 2
217:51 - size
217:52 - and then
217:53 - we will just say np
217:55 - dot
217:56 - u int eight so this is our kernel and
217:59 - kernel in this case is nothing but a two
218:03 - by two
218:04 - square shape and this square shape
218:08 - kernel is going to be applied
218:10 - on our image wherever these black dots
218:13 - are there so now we have defined this
218:16 - kernel so let's see
218:18 - after this kernel is applied on our
218:22 - masked image how it looks like so i'm
218:26 - going to just add one more title here
218:28 - which is dilation
218:30 - and then i'm going to add the image
218:34 - after the dilation is applied
218:36 - on our image and then we are just going
218:40 - to increase the range to three because
218:42 - now we have three images and let's say
218:45 - this plot contains
218:48 - images
218:49 - one by three so one
218:51 - row and three columns right
218:54 - so i'm going to just run this code once
218:56 - again
218:57 - and now you can see all these three
219:00 - images first was the original image
219:03 - second is the masked image and the third
219:07 - one is the image which we got after we
219:11 - applied the dilation let me just
219:14 - increase the size of
219:16 - this image somehow
219:18 - so now you can see
219:19 - that for example here there was a black
219:24 - dot and now it's reduced right the size
219:27 - of this black dot is reduced
219:30 - here also there was a black dot but its
219:33 - size also is reduced but still we can
219:37 - see these black dots here right
219:40 - so how we can
219:42 - remove these black dots completely
219:45 - so there is a third parameter which we
219:48 - can provide to this dilate method and
219:52 - it's called iteration so number of
219:55 - iterations
219:56 - so we can just provide iterations
219:59 - is equal to whatever
220:02 - the number of times we want to perform
220:04 - dilation on our image by default it's 1
220:08 - and you can provide let's say 2 here and
220:11 - let's see what is the result now
220:14 - so i'm going to just run this
220:16 - code again and now you can see
220:19 - those black dots which we can see here
220:22 - on the masked image
220:24 - are now gradually gone but still
220:28 - i can see some little dots on the images
220:32 - the small dots are already gone right so
220:36 - now what we can do here is we can
220:39 - increase the size of the rectangle so
220:42 - this rectangle is applied to our
220:46 - area which have these spots so we can
220:50 - increase the
220:52 - size of the rectangle and
220:55 - the bigger the rectangle is
220:58 - the better the result will be but there
221:01 - will be a problem which i'm going to
221:03 - show you so let's run this code and you
221:06 - can see now
221:08 - all the black dots
221:10 - from our image
221:12 - is gone so there was a black dot here
221:15 - which you don't see anymore and there
221:17 - was a black dot here here here and here
221:21 - and we don't see these black dots here
221:24 - but you might also observe that the size
221:29 - of this white area is also increased
221:33 - after we applied the dilation
221:36 - on this masked image so now
221:39 - this ball and this ball
221:41 - in the result after the dilation is
221:44 - merging here right so you can see it's
221:47 - merging because the size of our kernel
221:50 - is a big and when we apply dilation the
221:54 - pixel element is one if at least one
221:57 - pixel under the kernel is one that's why
222:00 - the shape of
222:02 - these balls
222:04 - are increasing so let's see how our next
222:08 - morphological transformation works which
222:10 - is called erosion and after that i'm
222:13 - going to explain you how
222:16 - this erosion works and what is erosion
222:19 - so i'm going to just declare a variable
222:21 - called erosion and then i'm going to
222:23 - just call a method called cv2
222:26 - dot erode so the method name is a road
222:30 - and the first argument here is the
222:33 - source the second argument here is the
222:36 - kernel as we have seen in dilate method
222:40 - and the third argument is the optional
222:42 - argument which is
222:45 - the iterations so
222:47 - for now we just apply
222:49 - one
222:50 - iteration which is by default
222:53 - also one and now we are going to just
222:55 - add this image to our matplotlib window
223:00 - so i'm going to add the title and the
223:02 - image and now i will increase the
223:05 - range of the array to 4 and let's say
223:08 - now we want 2 by 2
223:12 - metrics of these images right so let's
223:16 - run this code and let's see what happens
223:18 - so now you can see four results here
223:24 - and
223:25 - first was the original image second was
223:27 - the masked image third was the dilation
223:31 - so all the spots
223:33 - in the balls which are black are gone
223:37 - using the dilation but the size was
223:40 - increased
223:41 - and using the erosion
223:44 - you can see the sides of the ball
223:48 - eroded so the basic idea of erosion is
223:52 - just like soil erosion
223:55 - it erodes away the boundary of the
223:58 - foreground object so when this erosion
224:01 - is applied the kernel which we have
224:04 - defined
224:05 - slides through all the image and a pixel
224:09 - in the original image either
224:12 - one or zero will be considered as one
224:16 - only if all the pixels under the kernel
224:19 - is one otherwise it is eroded and this
224:23 - means this value will be set to zero
224:27 - which means
224:28 - this will be a black area so let's
224:31 - increase the number of iterations here
224:34 - so let's say we want to apply
224:37 - the erosion two times on the same image
224:41 - and i'm going to just
224:42 - run this code once again
224:44 - and you can see now
224:46 - these balls are eroded more let's say we
224:50 - want to increase this to five times
224:53 - and then run the code and you can see
224:56 - now these balls are really small because
224:59 - we have applied this erosion multiple
225:01 - number of times so let's say this is one
225:06 - once again and let's
225:08 - make this
225:10 - size of our kernel small 2 by 2
225:13 - rectangle size right so you can see now
225:16 - our result is better because all the
225:19 - spots
225:20 - from these balls are gone and
225:23 - these balls are not so much eroded now
225:27 - there are two more morphological
225:29 - transformation methods which are called
225:31 - opening and closing so we are going to
225:34 - first of all
225:35 - see how opening works i'm going to
225:39 - define a variable called opening and
225:42 - then
225:42 - i will call cb2 dot
225:45 - morphology
225:47 - x
225:48 - okay
225:49 - and then we will provide the source
225:51 - which is mask
225:53 - the second method is the type of
225:56 - morphological operation which we want to
225:59 - perform
226:00 - so
226:01 - in this we are going to just call cv2
226:04 - dot and then we can specify which type
226:07 - of morphological operation we want to
226:10 - perform on the image so just write morph
226:14 - and then the type of operation so we
226:16 - want to perform the morph open for the
226:19 - opening right and then the third
226:23 - argument here is the kernel which we
226:25 - have defined
226:27 - and now we are going to just add this
226:30 - opening
226:31 - to our matplotlib
226:34 - window let's add this and then let's do
226:39 - five here
226:40 - and then let's say our
226:43 - matplotlib is going to show these images
226:46 - in a two by three format okay so let's
226:50 - run this code and let's see what happens
226:53 - and let me increase the size of this
226:56 - image now
226:57 - and this is the result of the opening so
227:02 - what is opening in morphological
227:05 - transformations
227:07 - so opening is just another name of
227:10 - erosion followed by dilation so when you
227:13 - perform this opening of a logical
227:15 - operation first of all erosion
227:19 - is performed on the image and then the
227:22 - dilation will be performed on the image
227:25 - so
227:26 - you can see
227:27 - the effect of
227:29 - the erosion followed by the dilation
227:32 - still you see some spots here which can
227:36 - go if you
227:38 - can just increase
227:40 - the size of
227:42 - this block so let's rerun the code let's
227:46 - see what happens
227:48 - so now
227:50 - this image somehow looks better than the
227:53 - older image so opening is the erosion
227:57 - followed by
227:58 - dilation now
228:00 - there is a
228:02 - closing method also which is just the
228:04 - opposite of opening
228:07 - in the closing morphological
228:09 - transformation
228:10 - dilation
228:12 - is performed first on the image and then
228:15 - it is followed by the erosion so let's
228:18 - see if
228:20 - we get the better result when we perform
228:22 - the
228:23 - closing morphological operations and the
228:26 - morphological operation here will be
228:29 - close
228:37 - and run this code
228:39 - and now you can see
228:41 - the result here
228:43 - so in closing as i said
228:45 - first of all the dilation is applied and
228:48 - then the erosion is applied in the
228:51 - opening first of all erosion is applied
228:53 - and then the dilation will be applied
228:56 - now there are different type of
228:58 - morphological operations you can apply
229:01 - using this morphology x
229:04 - so for example
229:05 - i'm going to just use some of them so
229:10 - the main
229:12 - morphological operations other than
229:14 - opening and closing is
229:17 - let's say morphological gradient so i'm
229:19 - going to just say
229:21 - m g for morphological gradient
229:24 - and you just need to change the second
229:27 - argument here so cv2 morph underscore
229:32 - morphological gradient so we are going
229:35 - to just
229:36 - call this morph gradient and it's going
229:39 - to apply the morphological gradient and
229:42 - then the
229:43 - next is the top hat and the black hat so
229:47 - there are different
229:48 - uh
229:50 - morphological techniques you can apply
229:52 - so i'm going to show you one more and
229:55 - then
229:56 - i will leave you
229:58 - with the other techniques so
230:00 - t h for top hat and here also the second
230:04 - argument you just need to
230:06 - change it to top hat right otherwise you
230:10 - can see there are so many number of
230:13 - techniques you can apply on
230:16 - your image so there is gradient close
230:18 - open we have already seen
230:20 - black hat cross dilate
230:24 - uh ellipse erode
230:26 - hit miss
230:27 - wrecked and then top hat which is uh we
230:31 - are going to use right now right
230:34 - and then
230:35 - we can just add these two things to our
230:39 - list of titles and list of images
230:42 - so mg and then we have
230:46 - th for top hat
230:49 - and now we have eight images so range is
230:53 - increased
230:54 - to eight
230:55 - and let's say
230:57 - we just want to show them
231:00 - in two by four
231:02 - uh matrix here
231:04 - in the
231:07 - matplotlib window
231:09 - so you can see this is the result of
231:12 - morphological gradient
231:14 - so morphological gradient is the
231:17 - difference between the dilation and
231:20 - erosion of an image
231:22 - and this is the result of top hat
231:26 - that means it is the difference between
231:28 - the image and the opening of an image so
231:32 - this is how you can perform some of the
231:34 - morphological operations
231:36 - on the images
231:39 - now i will show you one more example i
231:41 - have a image
231:43 - called
231:44 - j
231:45 - dot png so i'm going to just
231:48 - load this image also and because this j
231:52 - dot png is already a binary image i
231:55 - don't need to apply this mask here so
231:57 - instead of this mask i can just directly
232:01 - use
232:02 - our image variable so i'm going to just
232:05 - write this and let's load this image two
232:08 - times because
232:10 - we already have defined this mask
232:13 - variable inside our
232:15 - title list and image list and now i'm
232:18 - going to just run this code so the
232:20 - original image
232:21 - of this j dot png looks like this
232:25 - and after
232:26 - we applied
232:28 - the dilation you can see the dilation
232:31 - increases the area of this j the erosion
232:35 - just erodes away the corners
232:38 - of
232:39 - this j right
232:41 - opening is going to apply
232:45 - the erosion first followed by
232:48 - the dilation and closing is going to
232:52 - first of all perform the dilation
232:53 - followed by the erosion
232:55 - this morphological gradient is going to
232:59 - give you the difference between the
233:01 - dilation and erosion of the image so
233:03 - it's going to give you
233:05 - this kind of result and you can see the
233:08 - top hat
233:09 - result here which is the difference
233:11 - between the input image and the opening
233:14 - of the image
233:15 - so this is how you can use different
233:18 - type of morphological transformations
233:21 - on your images in this video we will
233:24 - discuss about smoothing or blurring
233:26 - images in opencv
233:29 - so smoothing which is also known as
233:31 - blurring is one of the most commonly
233:34 - used operation
233:36 - in image processing the most common use
233:39 - of smoothing operation is to remove
233:42 - noise in the images now when smoothing
233:45 - or blurring images we can use diverse
233:47 - linear filters because linear filters
233:50 - are easy to achieve and are also
233:53 - relatively fast now there are various
233:56 - kinds of filters available in opencv for
233:59 - example homogeneous
234:01 - gaussian
234:02 - median or bilateral filters which we
234:05 - will see one by one
234:07 - so first of all we will see the
234:10 - homogeneous filter
234:12 - so homogeneous filter is the most simple
234:15 - filter and in homogeneous filter each
234:18 - output pixel
234:20 - is the mean of its kernel
234:23 - neighbors now in homogeneous filter all
234:26 - pixels contribute with the equal weight
234:29 - and that's why they are called
234:31 - homogeneous filters now those of you who
234:33 - don't know what the kernel is i have
234:36 - explained about kernel in the last video
234:38 - so you can see the last video and in
234:40 - simple word a kernel is a shape which we
234:44 - can apply or convolve over an image and
234:48 - you can use for example numpy to create
234:51 - this kind of squared kernel so in
234:54 - homogeneous filter the kernel looks like
234:57 - this image which you see on your screen
235:00 - so in homogeneous filter kernel k is
235:03 - equal to 1 by the width of the kernel
235:06 - multiplied by the height of the kernel
235:09 - so let's say we want to use a kernel of
235:12 - 5 by 5 then using this formula we will
235:16 - have k is equal to 1 by 25 and then we
235:21 - will have our kernel matrix of 5 by 5
235:25 - ones
235:26 - so let's create this kernel first of all
235:29 - and then we will see how to use this
235:32 - kernel
235:33 - for
235:34 - the image filtering using 2d convolution
235:38 - or homogeneous filter
235:41 - so what
235:42 - i have right now here
235:44 - is the simple code which loads this
235:47 - image using matplotlib and this code you
235:50 - might already know because i have
235:52 - explained in detail how matplotlib works
235:56 - and how to read the images using opencv
235:59 - one thing to note here is i'm just
236:01 - converting
236:03 - the image from
236:05 - bgr to rgb because
236:08 - matplotlib reads the images in the rgb
236:11 - format and opencv reads the images in
236:15 - the bgr format so this conversion is
236:18 - necessary so let's define our kernel so
236:21 - i'm going to just say kernel is equal to
236:24 - np
236:25 - dot
236:26 - once
236:27 - and then we are going to take the kernel
236:30 - of five by five so we are going to
236:32 - define this kernel five comma five off
236:36 - once so i am going to
236:38 - just say np
236:40 - dot float
236:42 - 32 here
236:44 - and then we are going to divide
236:46 - this kernel by 25 because our
236:49 - kernel is of 5 by 5
236:52 - because the formula which we have seen
236:55 - in that formula we have the kernel which
236:59 - was a matrix of ones
237:02 - and then we have the multiplication of
237:05 - one divided by the width and height of
237:09 - the kernel so that's why the
237:11 - multiplication of the width and height
237:13 - is 25 that's why i have taken 25 here
237:17 - so now we have our kernel so we can
237:20 - define
237:21 - our
237:22 - destination image using this kernel and
237:25 - we are going to use cv2
237:27 - dot there is a method called filter 2d
237:31 - which we are going to use which is used
237:34 - for this homogeneous filter
237:37 - so here the first argument is the image
237:41 - the second argument is the desired depth
237:44 - of the destination image so for now we
237:47 - are going to
237:48 - take it as a minus one the third
237:51 - argument is the kernel
237:54 - so now when we have applied this
237:57 - kernel on our image using 2d filter
238:01 - let's see what the output will look like
238:04 - so i will name this
238:06 - image as
238:08 - 2d convolution
238:10 - and the destination is the final image
238:15 - which we got using filter 2d
238:18 - and let's increase this range by 2 and
238:21 - let's say we want to show this image on
238:24 - matplotlib in
238:25 - 1 by 2 format okay so i'm going to just
238:29 - run this image so this is the result on
238:32 - the left hand side is the original image
238:35 - and on the right hand side
238:37 - is the
238:39 - 2d filter applied image so this is the
238:42 - image which we got by applying the
238:44 - homogeneous filter using filter 2d
238:47 - function
238:48 - so you can see on the corners here there
238:51 - is a little bit noise
238:54 - and after applying this
238:57 - 2d convolution
238:58 - over
238:59 - this image you can see all the corners
239:02 - are now smoothened and overall this
239:05 - image is now smooth and or blurred a
239:09 - little bit so these
239:11 - noises are removed or
239:14 - suppressed by this blurring so this is
239:17 - one way of blurring an image using
239:20 - filter 2d right filter 2d function
239:23 - now as in one dimensional signals images
239:26 - also can be filtered with various low
239:29 - pass filters or high pass filters etc so
239:34 - low pass filter helps in removing the
239:37 - noise or blurring the image etc
239:40 - and high pass filters helps in finding
239:43 - edges
239:44 - in the images now when you want to
239:46 - achieve image blurring
239:49 - we need to convolve over the image with
239:52 - the low pass filter kernel
239:55 - now there are some algorithm
239:57 - as i said there are various kind of
240:00 - algorithm available in opencv so we will
240:03 - also see them one by one so first
240:06 - algorithm is the blur method or it's
240:10 - also called the averaging so what i'm
240:12 - going to do is i'm going to define a
240:14 - variable called blur and then i'm going
240:16 - to call a method called cv2
240:19 - dot blur okay so this is the method
240:24 - which we will use to apply
240:26 - averaging
240:27 - algorithm for blurring the image and
240:30 - this takes two arguments one is the
240:33 - image and second is the kernel so the
240:36 - kernel we are going to apply is once
240:38 - again five by five
240:40 - and now we are going to just see with
240:43 - the result of this blurring method so we
240:47 - are going to just
240:49 - loaded using the matplotlib so range i'm
240:53 - going to increase it
240:55 - by one once again and let's see this
240:58 - these three images in one by three
241:00 - format on the matplotlib
241:04 - window so this is the result
241:07 - and you can see
241:09 - the original image
241:11 - the result which we got using the
241:13 - filter2d method
241:15 - and the result we got using the blur
241:18 - method which is also called averaging so
241:22 - the result is more or less looks the
241:24 - same to me because we have applied the
241:27 - same kind of kernel
241:29 - to
241:30 - both the functions
241:32 - so this is the result of filter2d
241:35 - function and this is the result of
241:37 - the blur function
241:39 - now there are more functions which are
241:42 - available
241:43 - in opencv so let's see uh them so the
241:46 - next algorithm which we are going to see
241:48 - is the gaussian filter algorithm
241:51 - so the gaussian filter is nothing but
241:54 - using different weight kernel
241:57 - in both x and y direction so in the
242:00 - result pixels located in the middle
242:04 - of the kernel have the higher weight or
242:07 - bigger weight and the weights decreases
242:10 - with distance
242:12 - from the neighborhood center
242:14 - so pixels located on the side have
242:17 - smaller weight and the pixels located on
242:21 - the center have the higher weight so
242:24 - when we take a five by five kernel its
242:28 - result is going to look like this which
242:31 - is shown in the image and now let's see
242:34 - how we can use this gaussian blur in our
242:38 - opencv code so i'm going to remove this
242:41 - semicolon which i somehow added here and
242:44 - let's uh declare a variable called g
242:48 - blur for gaussian blur
242:51 - and then we are going to use cv2 dot
242:55 - gaussian blur so the method name is
242:58 - gaussian blur
242:59 - and the argument here
243:01 - are same as the blur method so first
243:04 - argument
243:05 - is
243:06 - the image itself second argument is our
243:08 - kernel we are going to take the same
243:11 - kernel of five by five and the third
243:14 - argument here is the sigma x value which
243:17 - we are going to take 0 for now
243:20 - let's see the result of the gaussian
243:22 - blur method when it's applied
243:25 - to an image so i'm going to just define
243:28 - one more title which is g blur or
243:30 - gaussian blur or let's take this
243:33 - name which will be more clear and then
243:36 - our result image is g blur
243:39 - and let's increase the range to
243:42 - four and let's say we want to show this
243:45 - image in two by two format so two rows
243:48 - and two columns so i'm going to run this
243:50 - code and for opencv
243:54 - logo
243:55 - the results looks
243:57 - the same you can see
244:00 - for the 2d convolution of filter2d
244:02 - method or blur method using the gaussian
244:05 - blur you can see there is a little bit
244:07 - different between the blur method and
244:10 - gaussian blur
244:11 - method uh
244:13 - results the gaussian blur result is more
244:16 - better in my eyes than the
244:19 - blur method let's try this gaussian blur
244:22 - method with another image
244:25 - so i have this image
244:27 - called
244:28 - half tone underscore gaussian underscore
244:31 - blur
244:32 - and i'm going to run this
244:35 - code now with the new image
244:38 - and you can see
244:39 - the result now
244:41 - so
244:42 - this was the original image which have
244:45 - too much noise here so you can see
244:49 - the pixels
244:50 - here which have too much noise and after
244:54 - applying the gaussian blur you can see
244:56 - this eye image in a much better way and
245:01 - all the noise is removed so the gaussian
245:04 - blur method is designed specifically for
245:08 - removing the high frequency noise
245:11 - from the image like this one
245:14 - now let's see the next method which is
245:16 - called the median filter
245:18 - so median filter is something that
245:22 - replaces each pixel value with the
245:25 - median of its neighboring pixel so this
245:28 - method is great when dealing with
245:30 - something which is called salt and
245:33 - pepper noise now if you don't know what
245:35 - the salt and pepper noise is you can
245:38 - open the wikipedia and
245:41 - under this url or just search for salt
245:44 - and pepper noise
245:45 - wikipedia page and you can see
245:49 - more details about salt and pepper noise
245:51 - so you can see
245:53 - this is an image and there are some
245:56 - pixels which are distorted here so there
245:59 - are some pixels where the white uh
246:02 - dots are there or white noise is there
246:05 - and there are some places where the
246:07 - black noise is there so that's why it's
246:10 - called
246:10 - salt and pepper because we have
246:13 - white pixels which are distorted like
246:15 - salt and we have the black pixels which
246:18 - are
246:20 - which looks like pepper so that's why
246:22 - it's called salt and pepper
246:25 - noise so i have this
246:27 - same image which i'm going to use as a
246:30 - source now so it's called
246:33 - water.png in my case and now let's see
246:36 - how we can use the median blur method so
246:39 - i'm going to just define a new variable
246:42 - called
246:44 - median and then i'm going to use cv2 dot
246:48 - median blur method so this method is
246:51 - called median blur
246:52 - where the first argument is the image
246:56 - and the second argument here is the
246:58 - kernel size
247:00 - now one thing to note here is that the
247:03 - kernel size must be odd here so
247:07 - this must be a three or five or seven
247:10 - and so on except
247:12 - one okay so when you uh just give one
247:15 - it's going to show you the original
247:17 - image and let's say we just give three
247:20 - here
247:21 - as the kernel size or in our case we
247:24 - have the kernel size of five so let's
247:27 - take the five kernel size here
247:29 - so let's just show this result of the
247:32 - median filter in the plot layer window
247:36 - so i'm going to just increase the range
247:38 - 5
247:39 - and let's say this is 2 by 3 matrix now
247:43 - i'm going to run this code
247:45 - and now you can see the results of all
247:48 - the filtering method and you can see the
247:51 - best results you get using the median
247:54 - filter method
247:56 - so when you have this kind of salt and
247:59 - pepper dots on your images
248:02 - then you can use the median filter now
248:05 - let's see the last filter which is
248:07 - called the bilateral filter so by using
248:10 - all
248:11 - these filters for example homogeneous
248:13 - filter or averaging or the gaussian or
248:17 - the median filter we not only dissolve
248:19 - the noise but we also smooth the edges
248:24 - and sometimes we need to preserve the
248:26 - edges that means we need that all the
248:29 - ages
248:30 - must remain sharper even if
248:34 - the image is blurred
248:36 - so let me show you one example so i have
248:40 - this uh
248:41 - lena dot
248:42 - png image so i'm going to define a
248:45 - variable called bilateral filter and
248:48 - then there is a method called cv2 dot
248:51 - bilateral filter and this bilateral
248:54 - filter takes the first argument which is
248:57 - the image the second argument is the
249:00 - diameter of each pixel
249:02 - neighborhood that is used during the
249:04 - filter
249:05 - so let's take it as nine
249:09 - the third argument is the sigma color
249:12 - and the fourth argument is the sigma
249:14 - space
249:15 - so the sigma color is the filter sigma
249:18 - in the color space and sigma space is
249:21 - the filter sigma in the coordinate space
249:24 - so for this we are going to take this
249:27 - filter sigma color and sigma space as 75
249:31 - and 75 here
249:33 - and let's see
249:35 - it in the result window
249:38 - so bilateral filter
249:40 - and then the result bilateral filter and
249:44 - this gives me error because this image
249:46 - is called lena.jpg not png so jpg
249:51 - and then
249:52 - we need to increase this range by one to
249:56 - see all the six images
249:58 - and let's run this code and let's see
249:59 - what happens
250:01 - so you can see the result now so let me
250:03 - make it a little bit bigger so you can
250:05 - see them
250:06 - and from here also
250:09 - so now you can see
250:11 - by applying the bilateral filter
250:14 - the edges are preserved
250:17 - in a much better way so here you can see
250:20 - the hat border
250:22 - is blurred but here you can see in the
250:25 - result
250:26 - the border of the head are
250:30 - preserved so
250:31 - the images in which you need to
250:35 - preserve the borders
250:37 - then you can use the bilateral filter so
250:40 - bilateral filter is highly effective in
250:43 - noise removal while keeping the edge
250:46 - sharp so these are some of the methods
250:48 - and algorithms you can use to
250:51 - smoothen or blur your images using
250:54 - opencv in this video we will talk about
250:57 - image gradients in opencv
251:01 - so first of all what is an image
251:03 - gradient
251:05 - so an image gradient is a
251:08 - directional change in the intensity or
251:11 - the color inside the image
251:14 - now the image gradient of an image is
251:17 - one of the fundamental building blocks
251:19 - in image processing for example we use
251:23 - image gradients inside the image to find
251:26 - the edges
251:27 - inside an image
251:29 - now there are several image gradient
251:32 - methods available
251:34 - in opencv and we are going to see three
251:38 - of them
251:39 - first is the laplacian derivatives
251:42 - second is the sobel x method and third
251:46 - one will be the sobel y methods
251:50 - and all these methods which i mentioned
251:53 - are different gradient functions
251:56 - which uses different mathematical
251:58 - operations to produce the required
252:02 - image so the laplacian calculates the
252:05 - laplacian derivatives whereas sobel
252:08 - method is joint gaussian and
252:11 - differentiation operations but don't be
252:15 - overwhelmed with the details you just
252:17 - need to keep in mind that these are just
252:21 - the functions which we use
252:23 - for finding out the gradients of an
252:27 - image to analyze the image so let's use
252:29 - the first method which is called the
252:32 - laplacian gradient
252:34 - now to start with i have this initial
252:37 - code and you might already know what
252:39 - this code is doing so first of all i'm
252:42 - just reading this image messy5 dot
252:46 - jpg in the grayscale mode using the i'm
252:49 - read method and then i'm just loading
252:52 - this image using the matplotlib
252:55 - window so let's first see how the result
252:59 - looks like so this is going to look like
253:02 - this this is just a normal
253:05 - image of messy
253:07 - and let's see how we can apply the
253:10 - laplacian method
253:12 - to find out the laplacian gradient
253:15 - of an image
253:17 - so for that we are going to declare a
253:20 - variable called
253:22 - lap and then there is
253:25 - a function
253:27 - available inside your cv2 library which
253:31 - is called
253:32 - laplacian
253:34 - and this laplacian method takes few
253:36 - argument first argument is the image the
253:40 - second argument here will be the data
253:43 - type which
253:44 - we are going to
253:46 - use which is called cv2
253:48 - dot
253:50 - cv underscore
253:52 - 64. f
253:54 - so cv2 dot cv underscore 64 f
253:59 - is just a data type and we
254:02 - are using a 64 bit float due to the
254:05 - negative slope
254:07 - induced by transforming the image from
254:10 - white to black so you just need to keep
254:13 - in mind that this is just a data type
254:16 - which is 64 bit float and it supports
254:20 - the negative numbers which we will be
254:23 - dealing with
254:24 - when the laplacian method
254:28 - is run on our image now in the next line
254:32 - what we are going to do is we are going
254:33 - to take the absolute value of our
254:36 - laplacian
254:38 - image transformation and we are going to
254:40 - convert this value
254:43 - back to the unsigned 8-bit integer which
254:46 - is suitable for our output so i'm going
254:50 - to just write lab and then using the
254:53 - numpy
254:54 - uint method so np dot u int
254:59 - 8 and as an argument we are going to
255:02 - pass np dot
255:04 - absolute
255:05 - and then inside the absolute method we
255:09 - are going to just pass our image which
255:11 - is going to give us the absolute value
255:13 - of our laplacian image transformation
255:16 - which is going to convert
255:18 - this into the unsigned 8-bit integer now
255:22 - let's see the result of
255:24 - this laplacian gradient so i'm going to
255:27 - just
255:28 - add a new title to my title array which
255:31 - is called
255:32 - laplacian
255:33 - and also inside the images
255:37 - list i'm going to add this
255:39 - lap
255:41 - variable which contains this image right
255:45 - after the laplacian
255:47 - gradient is applied here and here the
255:50 - range will be 2
255:52 - and we are going to see it in 1 by 2
255:55 - format on the matplot
255:58 - lib window so here you can see the
256:01 - original image which is this one
256:04 - and after the laplacian gradient
256:07 - method is applied on this image you can
256:10 - see all the edges which are detected
256:14 - by this method when we applied this
256:17 - method on
256:19 - this messy5 dot jpg image and an image
256:23 - gradient as i said is the directional
256:26 - change in the intensity or the color
256:28 - in an image so let's close
256:32 - this window and there is one more
256:36 - argument you can provide here which is
256:39 - the kernel size so you can just say k
256:42 - size is equal to
256:44 - 5 this is the kernel size and i'm going
256:46 - to just run this program once again and
256:49 - you can see
256:50 - the kernel size is increased but our
256:54 - result is deteriorated right so let's
256:57 - reduce it to 3
256:59 - and then once again
257:01 - run this program and the result looks
257:05 - fine and if you
257:07 - apply k size is equal to 1 let's see the
257:10 - result
257:11 - and you can see you get
257:14 - the
257:15 - better result i think so for now i'm
257:17 - going to just use k size is equal to
257:20 - 3
257:21 - and now let's use the
257:24 - other two uh image gradient methods
257:28 - which are sobel x and sobel y
257:31 - so these methods which uh are called
257:34 - sobel x and sobel y are also called
257:37 - sobel gradient representation so let's
257:41 - just use them and then we will discuss
257:44 - how they are
257:45 - useful
257:46 - so first of all i'm going to declare a
257:49 - variable called
257:51 - sobel x and then i'm going to use
257:55 - the
257:56 - method inside this
257:58 - cv2 library which is
258:00 - called sobel so this is the method which
258:04 - takes again few arguments first is the
258:07 - image
258:08 - second is
258:09 - again this data type which is cv2
258:13 - dot
258:14 - cv underscore s64
258:17 - and
258:18 - the third argument here will be the dx
258:23 - so when you write one here this value
258:26 - can be one or zero so
258:28 - when you write one here
258:30 - that means we want to
258:33 - use the sobel x
258:35 - method okay
258:36 - and then the fourth argument here is the
258:39 - d
258:40 - y value okay so this is dx which is
258:46 - for the x direction
258:48 - and
258:49 - this is for the d y
258:51 - which is for the y direction and dx
258:55 - stands for the order of derivative x
258:59 - and the d y stands for order of
259:02 - derivative y
259:05 - now once again we are going to declare
259:08 - the
259:08 - sobel
259:10 - y
259:11 - variable so let's declare the sub y
259:14 - and then
259:15 - cv2
259:16 - dot
259:18 - sobel and this also takes a few
259:20 - arguments here the difference
259:24 - will be only the third and fourth
259:26 - argument so i'm going to
259:28 - just use the second argument same
259:31 - the third argument will be zero for
259:34 - sobel y
259:35 - and the fourth argument will be one
259:38 - right so this is the order of derivative
259:42 - x if it's 1 this is called the order of
259:45 - derivative which is in the x direction
259:48 - and in the second case it is in the y
259:52 - direction and the fifth argument here
259:54 - can be the k size as we have seen in the
259:57 - laplacian method so if you want you can
260:00 - provide the kernel size also here as the
260:04 - fifth argument but we are going to
260:06 - skip it for now
260:08 - now again we are going to convert these
260:11 - values into
260:14 - the
260:14 - unsigned int as we have done
260:17 - in the case of laplacian so what we are
260:20 - going to do is we are going to once
260:22 - again overwrite this variable sobel x
260:26 - and then we are going to use np dot
260:30 - u
260:31 - int
260:32 - 8 and in the parenthesis we are going to
260:34 - just write np
260:36 - dot
260:37 - absolute and then we are going to just
260:40 - pass the value inside the sobel x
260:42 - variable same we are going to do with
260:45 - the sobel y variable
260:48 - and now let's see the result how the
260:51 - result looks like so i'm going to just
260:54 - uh
260:55 - add these elements inside the title and
260:58 - the image list
261:00 - so let's add sobel x and sub y
261:04 - here and here also
261:07 - so sobel
261:09 - y
261:10 - and now
261:11 - let's increase the range to 4
261:14 - and let's see
261:15 - it in the form of 2 by 2
261:19 - matrix on the matplotlib window so i'm
261:22 - going to just run this code and you can
261:25 - see
261:26 - the result here so original image
261:29 - laplacian
261:30 - uh gradient and then sobel x and so blue
261:34 - y
261:35 - so you can see uh when you apply the
261:37 - sobel x
261:39 - gradient method
261:40 - the direction or the change in direction
261:43 - in the intensity
261:45 - is in the x direction
261:48 - and when you apply the sobel y method
261:51 - the change in direction in the intensity
261:54 - is in the y direction so this is like
261:59 - horizontal and this is in the vertical
262:03 - direction i have one more image which
262:06 - will illustrate this sobel x and so y
262:10 - gradient method
262:11 - in a better way i think and this is
262:14 - called
262:15 - sudoku so i'm going to just write sudoku
262:19 - dot png file file and hopefully i
262:24 - didn't do any mistake in the naming yes
262:27 - it works
262:28 - so you can see
262:30 - the laplacian result here
262:33 - and then sobel x and sub y uh result
262:36 - here
262:37 - so in the sobel x you can see more
262:40 - vertical lines so because sobel y
262:44 - is good for the directional change in
262:47 - the vertical direction so you can see
262:49 - more uh change in intensity in the
262:52 - vertical direction
262:53 - and using the subal y you can see the
262:57 - directional change in the intensity in
262:59 - the horizontal direction or the y axis
263:03 - you can also combine the result of sobel
263:06 - x and sub y images
263:08 - and how we can do this let's see so to
263:11 - combine these two result i'm going to
263:14 - just create one more method which is
263:16 - called
263:17 - sobel
263:18 - combined
263:20 - is equal to cv2
263:22 - dot we are going to
263:24 - use the bitwise or operator in order to
263:29 - merge these two images so
263:31 - we are going to just write bitwise or
263:34 - and then we are going to provide the two
263:36 - sources one is sobel x
263:39 - and the other is the subal y image so
263:43 - this is going to give us the bit wise or
263:47 - result of these two images and then we
263:51 - are going to just add
263:53 - this into the title list so let's say
263:56 - sobel combined
263:58 - and also in the image list
264:01 - so like this and let's just increase the
264:04 - range to five and let's see it in the
264:07 - form of
264:08 - two by 3
264:10 - on matplotlib
264:11 - so i'm going to just run this
264:14 - once again and you can see the result
264:18 - now
264:19 - so this here is the combination
264:22 - of sobel x and sobel y method and you
264:26 - can see now you can see
264:28 - the directional change in the
264:31 - vertical as well as in the horizontal
264:34 - direction because this
264:36 - is the combination of sobel y and sobel
264:38 - x
264:39 - images so this is how you can use the
264:42 - image gradients inside opencv in this
264:46 - video we will talk about kenny edge
264:48 - detector in opencv
264:51 - so first of all what is kenny edge
264:54 - detector
264:55 - so the kenny edge detector is an edge
264:58 - detection operator that uses multi-stage
265:02 - algorithm to detect a wide range of
265:05 - edges
265:06 - in images
265:08 - now this kenny edge detector was written
265:12 - and developed by john f kenney in 1986
265:18 - that's why it's named after
265:21 - his name which is kenny edge detector
265:25 - now the process of kenny edge detection
265:27 - algorithm can be broken down in five
265:30 - different steps
265:32 - the first step is to apply gaussian
265:35 - filter to smooth the image in order to
265:38 - remove the noise
265:40 - the second step is to find the intensity
265:43 - gradients of the image the third step is
265:46 - to apply the non-maximum suppression to
265:49 - get rid of spurious response to edge
265:54 - detection
265:55 - the fourth step is to apply double
265:58 - threshold to determine the potential
266:01 - edges
266:02 - and the fifth
266:04 - step is to track edges by hysteresis
266:08 - that is to finalize the detection of the
266:10 - edges by suppressing all the other edges
266:14 - that are weak or
266:16 - not connected to strong edges
266:19 - so
266:20 - this seems a little bit complicated but
266:23 - in opencv it's really simple to use
266:27 - so there is a built-in function
266:30 - in opencv which is called kenny and we
266:34 - are going to use this function so to
266:38 - start with i have this
266:40 - sample code which loads this image which
266:43 - is called messy.jpg
266:45 - using the matplotlib library i'm going
266:49 - to just run this to show you the results
266:51 - so this is the image and we want to
266:54 - detect the edges
266:56 - of this image so what we are going to do
266:59 - is we are going to first of all declare
267:02 - a variable called
267:04 - kenny and then there is a method as i
267:07 - already said inside your cv2 library
267:11 - which is called
267:13 - kenny method which takes few arguments
267:16 - so the first argument here is the image
267:20 - source itself the second argument and
267:23 - the third argument as you can see
267:25 - is the first threshold value and the
267:28 - second threshold value so this first
267:30 - threshold value and the second threshold
267:33 - value you need to provide
267:34 - for the hysteresis procedure so there is
267:39 - the last step as i mentioned
267:41 - and in that step hysteresis take place
267:44 - and for that procedure we need to
267:47 - provide the values of
267:49 - the threshold one and the threshold 2
267:54 - so for now i'm going to provide 100 as
267:58 - the threshold 1 and 200 as the threshold
268:02 - 2
268:02 - but
268:04 - later you might want to add a track bar
268:07 - in order to see
268:09 - the changes in the edges when you just
268:13 - move the track bar from left to right
268:16 - for the threshold one and the threshold
268:18 - two so this might be a small assignment
268:21 - for you you can just add the track bar
268:24 - and see
268:26 - how the edge detection changes when you
268:30 - change the value of threshold 1 and
268:32 - threshold 2 and i have already explained
268:34 - how you can use track bars
268:37 - with opencv
268:38 - so just watch that video and you will be
268:41 - good to go
268:42 - so now we have the result of kenny edge
268:46 - detection function so we are going to
268:48 - just add it to our list first to the
268:51 - list of titles and then second to the
268:56 - list of images and the range we are
268:58 - going to increase it to uh two and this
269:02 - we are going to just see the
269:05 - images in one by two format so i'm going
269:07 - to just run this python script and see
269:11 - the result so you can see
269:13 - we have the
269:16 - original image here which we have loaded
269:18 - in the grayscale and on the right hand
269:21 - side you can see the result of the kenny
269:23 - edge detection methods
269:25 - so you can see uh all the edges which
269:29 - are available here
269:31 - on this messy5 dot jpg image
269:34 - you can use uh this on the other images
269:38 - also so for example i have the
269:40 - lena dot jpg image let's see the result
269:45 - of that and
269:47 - this is the result of the kenny edge
269:50 - detection method on this lena dot
269:53 - jpg method
269:55 - so this kenny edge detection is really
269:58 - useful because
270:00 - in the last video we have seen how to
270:03 - find out the image gradients
270:05 - and let's see
270:07 - in comparison to those image gradient
270:11 - methods how kenny edge detection method
270:14 - performs
270:15 - so these are all the methods i have
270:18 - explained in the last video laplacian
270:21 - sobel x and sobel y and i have shown you
270:24 - how to combine the result of sobel x and
270:28 - sobel y and additionally i have added
270:31 - this line to the previous code which i
270:34 - have shown you in the last video
270:36 - which is edges is equal to cb2 dot kenny
270:40 - which gives us the result on the same
270:42 - image
270:44 - using the kenny edge detection method
270:47 - and i have added it to the title and the
270:49 - image right so let's run this uh script
270:52 - once again and let's see the differences
270:55 - in the result using all these
270:59 - methods
271:00 - so you can see all the six results
271:04 - this is the original image this is the
271:06 - result of the laplacian method this is
271:10 - the result of sobel x and this is a
271:13 - result of symbol y
271:15 - and this is the combination of sobel x
271:17 - and y and you can see kenny edge
271:19 - detection gives us the result which
271:21 - contains lesser noises so you can see
271:24 - there is a lot of noise present in the
271:27 - other matters you can see here
271:30 - all the noise is present which is
271:33 - removed using kenny edge detection or
271:37 - in the laplacian method also you can see
271:40 - some noises around but in the kenny edge
271:44 - detection method you can see you get the
271:47 - proper edges and more precise edges
271:51 - without any noise
271:54 - so this is the benefit of using kenny
271:57 - edge detection
271:58 - so this is how you can use kenny edge
272:01 - detection in this video we will discuss
272:03 - about image pyramids in opencv
272:06 - so till now normally when we have used
272:10 - images we have used the images of
272:12 - constant size
272:14 - but sometimes we need to work with the
272:18 - images of different resolution
272:21 - so for example
272:22 - if i have an image and i want to search
272:26 - the face inside an image this face can
272:30 - be of different sizes
272:33 - so using image pyramids we
272:36 - just create
272:38 - the images of different resolutions
272:42 - and then
272:43 - we search for
272:45 - the object for example face in all of
272:49 - these images so pyramid or pyramid
272:52 - representation is a type of multi-scale
272:55 - signal representation
272:57 - in which a signal or an image is subject
273:01 - to repeated smoothing and sub sampling
273:06 - so a normal pyramid when you create a
273:09 - pyramid of images it will look like this
273:12 - so let's say this is the original image
273:15 - at the bottom
273:16 - then
273:17 - when you down scale
273:19 - an image using a pyramid function it's
273:22 - going to give you
273:24 - this image which have the half
273:27 - resolution than the original image and
273:30 - then when you further go up
273:33 - it's going to give you the one fourth of
273:36 - the original image and then so on so 1 8
273:40 - or 1 16 of an image
273:43 - now there are two types of
273:46 - image pyramids which are available in
273:49 - opencv
273:51 - first is called gaussian pyramid and
273:53 - second is called laplacian pyramid so
273:57 - first we will discuss about the gaussian
273:59 - pyramid
274:00 - so gaussian pyramid is nothing but
274:02 - repeat filtering and sub sampling of an
274:06 - image
274:07 - now there are two functions available
274:10 - for the gaussian pyramid which is called
274:15 - pair down and pair up so let's uh see
274:18 - them one by one
274:20 - so
274:21 - i have this sample code which is just
274:24 - reading an image and then showing it
274:27 - using the i am show method now in order
274:29 - to uh
274:31 - use this
274:32 - pair down function you can just define a
274:35 - variable let's say
274:37 - l r for lower resolution and then you
274:41 - can use cv2 dot
274:43 - pair down so there are two functions you
274:46 - can see
274:47 - pair down and pair up so first of all we
274:50 - will see pair down
274:52 - and then we are going to pass our image
274:56 - as an argument here so i'm going to just
274:58 - pass our image as an argument
275:01 - and
275:01 - we are already showing the original
275:04 - image and let's show the
275:07 - image after we have reduced the
275:09 - resolution of this image using the pair
275:12 - down method so pair down
275:14 - is going to reduce the resolution of an
275:18 - image so i'm going to just
275:21 - use lr here and let's say this is the
275:25 - pair down
275:27 - one image okay so let's run this code
275:30 - and let's see what happens
275:32 - so you can see this is the original
275:34 - image and this is about
275:37 - you can see one fourth of this
275:41 - original image right so this pair down
275:45 - method is going to reduce the resolution
275:48 - of an image when you apply the same
275:52 - method on
275:54 - the
275:55 - second image so let's say this is
275:58 - lr
275:59 - one and then we create a second variable
276:03 - error lr2 and when we pass lr1
276:07 - as an argument for
276:09 - this method to create the lr2 method 2
276:13 - image
276:14 - then let's see what happens so this will
276:16 - be lr1
276:18 - and let's
276:19 - just
276:21 - say this is going to give us
276:23 - lr2
276:25 - the
276:26 - resolution of image will reduce further
276:29 - so let's see what happens
276:31 - so this was the original image this was
276:35 - uh the image which we got from the first
276:38 - period down method and then we get this
276:41 - image which we which is further reduced
276:45 - in resolution so this is the image after
276:50 - applying the pair down method second
276:53 - time
276:54 - on
276:54 - the lr1 image okay
276:58 - so you can see
276:59 - the resolution of image is reducing and
277:03 - it's creating a kind of pyramid and
277:06 - that's why it's called the image pyramid
277:09 - now there is a method called pair up
277:12 - also available in opencv so let's see
277:15 - what this pair up method do so as you
277:19 - can expect it's going to increase the
277:22 - resolution of the image
277:24 - so here i'm going to just say
277:27 - h r for higher resolution and then i am
277:31 - going to just say cv2
277:33 - dot
277:34 - pair
277:35 - up okay and it is going to increase the
277:37 - resolution of an image now let us say we
277:41 - want to increase the resolution of an of
277:44 - this image which is the smallest image
277:47 - which we got
277:48 - using the pair down method right so we
277:52 - are going to apply
277:53 - the
277:54 - pair up on the last image which we got
277:58 - using the pair down method
278:00 - and let's see what happens so when i'm
278:03 - going to
278:05 - use this hr2 here
278:07 - and this we got from pair up method and
278:12 - let's say this is the pair up one
278:14 - and i'm going to just run this code and
278:18 - you you are going to see that we have
278:21 - converted this image which was the
278:23 - smallest image to a higher resolution
278:26 - which resulted in this image
278:29 - but
278:30 - when you see
278:32 - this image carefully so let me just
278:35 - move this
278:36 - to this side and this was the original
278:39 - image so let me just minimize this
278:41 - so
278:42 - this image we have converted to this
278:45 - image using the pair up method so
278:48 - ideally
278:49 - this image should look like this but you
278:53 - have to remember that this pair up image
278:57 - is not going to be equal to
279:00 - this image because
279:02 - once you decrease the resolution using
279:04 - the peer down method
279:06 - you lose the information about that
279:09 - image so when you use pair up to just
279:14 - increase the resolution of this image
279:17 - then you
279:18 - can see the result
279:20 - looks little bit blurred because some of
279:23 - the information is loosed
279:26 - using the pare down method so you have
279:29 - to keep this in mind that when you want
279:32 - to increase the resolution after you
279:35 - have
279:36 - reduced the resolution you're not going
279:38 - to get the same result as you might
279:42 - expect that this image should look like
279:45 - this but they are not equal images so
279:48 - this image is just a higher resolution
279:51 - of this image and it has nothing to do
279:54 - with this image so these are the two
279:57 - methods which are available
279:59 - in gaussian pyramid now if you want to
280:04 - create a pyramid of multiple resolution
280:08 - instead of just using this pair up or
280:12 - pair down method repeatedly what you can
280:14 - do here is i'm going to just remove this
280:18 - and remove
280:20 - this code also so what i'm going to do
280:22 - is i'm going to copy
280:24 - the image into a new variable so i'm
280:27 - going to just say
280:28 - layer is equal to img
280:32 - dot
280:33 - copy there is a method available for
280:36 - copying the image which is a copy and
280:40 - then i'm going to create the gaussian
280:44 - pyramid array okay so i'm going to just
280:48 - create a variable called gp for a
280:51 - gaussian pyramid is equal to
280:54 - 10
280:55 - in square bracket i'm going to just pass
280:58 - this
280:59 - image here as the
281:01 - first element of this list
281:04 - then what i can do is i can use a for
281:07 - loop instead of just
281:09 - rewriting this pair down method again
281:12 - and again and you might already know how
281:14 - to use for loop in python so for i in
281:19 - range and here we can provide any uh
281:21 - range so let's say we want to create
281:25 - five uh image pyramid okay so five time
281:29 - we want to reduce the
281:31 - resolution so we are going to give six
281:34 - here because range goes uh
281:37 - the number minus 1 so whatever you give
281:40 - here minus 1 right
281:42 - so now what we are going to do is we are
281:45 - going to just use our
281:48 - layer
281:49 - parameter once again and then we are
281:51 - going to
281:52 - just call cv2 dot pair down method so
281:57 - pair it down
281:58 - and then we
282:00 - want to just
282:02 - say layer okay
282:04 - and then we want to append to the
282:08 - gaussian
282:10 - pyramid list okay so we are going to
282:13 - just say gp
282:14 - dot
282:15 - append and we are going to append
282:18 - the
282:19 - result of this pair down
282:22 - to our list which we have created here
282:25 - okay so this is going to uh just append
282:29 - this image to our
282:32 - list of images
282:33 - and then
282:35 - let's uh just show this image using cb2
282:39 - dot i am show method so cv 2 dot
282:43 - i am show
282:45 - and
282:45 - here we can just say s
282:48 - t r for converting
282:51 - the integer to the string because the
282:54 - first parameter you'd give to i am show
282:57 - is a string parameter that's why i'm
282:59 - converting the integer to the string and
283:02 - the second parameter is the image so
283:05 - let's pass this layer here okay so you
283:08 - have the original image which will be
283:12 - shown using this line of code and then
283:14 - you will see multiple number of images
283:16 - of different resolution using this code
283:19 - so let's run this code and let's see
283:21 - what happens so i'm going to run this
283:23 - code
283:24 - and you can see
283:25 - there are
283:27 - different images
283:29 - resulted using that code which we have
283:33 - written so this was
283:35 - the first image which is zero and then
283:38 - this is the second image and then this
283:41 - is the third
283:42 - fourth fifth and sixth so sixth you can
283:45 - see have a very small resolution
283:50 - so this is how you can
283:51 - use a pair down method multiple times
283:55 - using a for
283:56 - loop now what are laplacian pyramids so
284:00 - laplacian pyramids are formed from the
284:03 - gaussian pyramids there is no exclusive
284:07 - function
284:08 - for
284:09 - creating the laplacian pyramid so as you
284:12 - have seen that in gaussian pyramids
284:15 - there are two methods available pair up
284:18 - and pair down but there is no exclusive
284:21 - function for creating the laplacian
284:24 - pyramid so how we can create a laplacian
284:27 - pyramid if there is no function
284:29 - available
284:30 - for creating them
284:32 - so you can create a laplacian pyramid or
284:36 - a level of laplacian pyramid is formed
284:40 - by the difference between
284:42 - that level in the gaussian pyramid and
284:46 - the extended version of its upper level
284:50 - in the
284:51 - gaussian pyramid so this definition
284:54 - might be confusing to you guys so let me
284:56 - explain you with the code what i mean by
285:00 - this definition
285:02 - so what i'm going to do is first of all
285:05 - i'm going to take the top level layer of
285:10 - the gaussian pyramid so top level layer
285:14 - of the gaussian pyramid is the last
285:16 - image which is generated using this for
285:19 - loop so let's say
285:21 - we have
285:23 - six images or five images using
285:26 - this for loop so what we are going to do
285:29 - is because we have appended
285:32 - each and every image to this list right
285:36 - so we have all the images inside this
285:39 - list so we can
285:42 - just get the
285:44 - last image using the
285:47 - indexing so again i'm going to use
285:50 - the
285:51 - layer variable
285:52 - and then i'm going to
285:54 - just say gp for gaussian pyramid list
285:58 - and then there is
286:01 - the index 5 because last image will be
286:04 - available at the index 5 of this list so
286:09 - we get the last image of
286:12 - that
286:13 - gaussian pyramid
286:15 - and then let's show this image so i'm
286:17 - going to just say
286:19 - cv2 dot
286:22 - i am
286:22 - show and this is the upper level or the
286:26 - last image so i'm going to say upper
286:27 - level gaussian pyramid and then we are
286:30 - going to pass this layer variable here
286:33 - so this is going to show just the last
286:36 - image of
286:37 - this list and let's just comment this
286:40 - code out because we don't want to see
286:43 - all the images and then i'm going to
286:45 - create a new list for laplacian pyramid
286:49 - so i'm going to just say lp for
286:51 - laplacian pyramid and then i'm going to
286:54 - create a list
286:56 - using the layer
286:58 - variable itself as we have done for the
287:01 - gaussian pyramid list also so the first
287:04 - element here is
287:06 - the layer variable itself and now we are
287:09 - going to use the for loop
287:12 - and then i
287:14 - in
287:15 - range
287:16 - and this time what we are going to do is
287:19 - you might already know how to use
287:21 - the range function and if you don't know
287:24 - you can see you can give the stop
287:28 - integer here or you can give a multiple
287:33 - parameters here so you can see there is
287:36 - one more implementation of this range
287:39 - function so you can give
287:42 - the
287:42 - start parameter
287:44 - and the stop range so start is the
287:48 - starting point stop is the stopping
287:51 - point and also you can give the steps so
287:54 - this step means
287:56 - uh in what number you want to reduce
287:59 - okay so let us say we want to start from
288:02 - 5
288:03 - and then we want to go
288:06 - until 0 and we want to reduce in the
288:09 - step of minus one okay so five four
288:14 - three two one so let's uh print the
288:17 - value of i first of all if you uh
288:20 - might be interested in the result of
288:23 - this range function then let's us just
288:26 - run this code and let's see what happens
288:29 - so this is
288:30 - the images which we get but we are not
288:33 - interested in these images we are
288:35 - interested in the print function output
288:38 - so you can see the output of
288:40 - this
288:41 - print
288:42 - function code is five four three two one
288:45 - as i said
288:46 - uh the lower limit is not reached so if
288:50 - you give zero here then it's going to go
288:52 - until 1 and not 0. if you give 6 here
288:56 - then it's going to go until 5
288:58 - not 6.
289:00 - so let me repeat the definition of
289:02 - laplacian pyramid once again so
289:05 - laplacian pyramid is formed by the
289:08 - difference between
289:10 - that level in the gaussian pyramid and
289:13 - the extended version of its upper
289:17 - level in the gaussian pyramid so let's
289:20 - first create the extended version of
289:23 - that level
289:25 - so we are going to just create a
289:26 - variable called
289:28 - gaussian
289:30 - extend or extended and then we are going
289:34 - to extend the level of that image which
289:38 - are there in the gaussian
289:40 - pyramid list
289:42 - by using cv
289:44 - 2 dot dot pair up method and here what
289:48 - you need to give is the gaussian pyramid
289:52 - list and then
289:54 - we just need to get the index i
289:57 - from this so this line gives us the
289:59 - extended version of the upper level in
290:02 - gaussian pyramid now let's create the
290:05 - laplacian pyramid so
290:08 - laplacian is equal to cv2 dot
290:11 - subtract because we want to find out the
290:13 - difference between
290:15 - that level in the gaussian pyramid and
290:18 - the extended version of its upper level
290:22 - so i'm going to just say gp for gaussian
290:25 - pyramid and then we are going to just
290:28 - say i
290:30 - minus 1 as the first parameter and the
290:33 - second parameter is the extended version
290:36 - of
290:37 - the gaussian upper limit and now we can
290:41 - use the i am
290:43 - show method to show all these laplacian
290:46 - images so i am going to just say cv2 dot
290:49 - i am show and once again i am going to
290:52 - use str function to convert
290:55 - from a number to string and then
290:58 - in the next parameter i'm going to just
291:00 - pass the
291:01 - laplacian parameter here as an
291:05 - image source so what do you think will
291:08 - this
291:09 - code work so let's see what happens
291:12 - when we are going to run this code so
291:14 - you can see the laplacian pyramid looks
291:17 - just like
291:18 - the edge detection so all the edges are
291:22 - shown here on every uh
291:25 - image this is the first level this is
291:28 - the second level third level fourth
291:30 - fifth
291:31 - level so these images are called the
291:35 - laplacian pyramid
291:37 - now what is the use of
291:39 - creating those laplacian pyramids or the
291:43 - gaussian pyramids so the laplacian and
291:46 - gaussian pyramid helps us to blend the
291:50 - images and the reconstruction of the
291:53 - images so these are the two uh benefits
291:56 - of creating those laplacian and the
291:59 - gaussian pyramids so in the next video
292:02 - we are going to see how we can blend the
292:04 - images or how can we reconstruct the
292:08 - images
292:09 - using the opencv and
292:12 - the image pyramids in the last video we
292:14 - have seen what are image pyramids
292:17 - and i have told you there are two kinds
292:20 - of image pyramids in opencv
292:23 - one is called the gaussian pyramid and
292:26 - the other is called the laplacian
292:28 - pyramid and we have seen in the last
292:31 - video how we can
292:33 - create the gaussian pyramid and the
292:36 - laplacian pyramid now in the last video
292:39 - i have also told you some applications
292:42 - of image pyramids
292:44 - and one of the application of image
292:46 - pyramids is the image blending
292:50 - so let me show you one example
292:53 - so here in this code i have two images
292:56 - one is of apple and other is of orange
293:01 - and i want to blend or merge these two
293:06 - images
293:07 - so let me just run this code first of
293:10 - all
293:10 - so you can see there are two images
293:13 - first is
293:15 - of apple
293:16 - and other is of orange and i have also
293:19 - printed the shape of
293:22 - these two
293:23 - images so you can see the shape is
293:26 - similar 512 by 512 and orange image
293:31 - shape is also 512 by 512
293:35 - so
293:35 - here what i want to do is
293:38 - i want to blend half of the orange to
293:42 - half of the apple so let's say i want to
293:46 - just blend right hand side of this
293:50 - orange to the left hand half of this
293:54 - apple
293:55 - so how can i achieve this
293:58 - now you might say that i can just cut
294:01 - these two images into
294:04 - half and then i can stack these two
294:06 - images side by side and i will get the
294:10 - half and half of the two images
294:13 - and that's how i can just get the result
294:16 - so let's first of all try this technique
294:20 - first of all we are going to just uh
294:22 - create the half and half of the
294:25 - apple and orange images and we are going
294:27 - to just stack these images side by side
294:31 - so let's say i'm going to create
294:34 - the variable called apple underscore
294:37 - orange
294:38 - and then here there is a method in numpy
294:43 - so i'm going to just say numpy dot
294:46 - h stack
294:48 - so there is this method called h stack
294:50 - and here what i can do is in the form of
294:53 - tuple
294:54 - i can provide the half of my apple image
294:58 - so
294:58 - apple is the image variable name and
295:02 - then
295:03 - what i'm going to do is the half of this
295:06 - image because this image is 512 by 512
295:10 - so i'm going to just give this kind of
295:13 - expression colon comma and then colon
295:17 - 256 which is the half of the apple image
295:21 - on the left hand side right
295:24 - and then i'm going to just do the same
295:28 - with the orange image so i'm going to
295:30 - just take orange and then
295:33 - colon
295:34 - comma
295:35 - 256
295:37 - colon so one thing to observe here is i
295:40 - have taken colon
295:42 - before 256 in the apple image and i have
295:47 - taken
295:48 - colon after 256 in the
295:51 - orange image
295:53 - and then i'm going to just show this
295:56 - apple
295:57 - orange image
295:59 - and let's see what result we get
296:01 - when we run our code
296:04 - so these two are the
296:07 - apple and orange image and this is the
296:09 - result of adding the two halves of the
296:14 - orange and the apple image but still you
296:16 - can see this line which is clearly
296:19 - visible and from this line you can say
296:21 - half of this is orange and half of this
296:25 - is an apple
296:27 - so
296:28 - in image blending what we need to do is
296:30 - we need to
296:32 - blend this
296:33 - line also so the orange and the apple
296:37 - image should be merged or blended in a
296:41 - perfect way so for blending this half
296:44 - apple and half orange image
296:46 - what we can do is we can use the image
296:49 - pyramid techniques
296:51 - to
296:52 - blend these two images now in order to
296:55 - blend two images using image pyramids
296:58 - technique we need to follow five steps
297:02 - the first step is to load two images
297:06 - in our case these images are of apple
297:09 - and orange which we are already doing so
297:12 - first step is to load
297:14 - these two images
297:16 - the second step is to find out the
297:18 - gaussian pyramid of our apple and orange
297:22 - image
297:23 - the third step will be from these
297:26 - gaussian pyramids
297:28 - find out the laplacian pyramids okay
297:31 - so we will find out the gaussian pyramid
297:33 - in the second step and then in the third
297:36 - step we are going to find out the
297:38 - laplacian pyramids
297:40 - now in the fourth step we are going to
297:43 - join the left half of the apple and the
297:46 - right half of the orange in each levels
297:50 - of laplacian pyramid
297:53 - and finally in the fifth step what we
297:56 - are going to do is we are going to just
297:58 - join these image pyramids and
298:01 - reconstruct the original image
298:05 - so let's follow these steps one by one
298:08 - and let's see what result we get
298:12 - so as i said first step is already done
298:14 - which is just loading these two images
298:17 - and the second step would be to find out
298:21 - the gaussian pyramid so let me just uh
298:25 - just write this step generate gaussian
298:27 - pyramid for
298:29 - apple first of all and then we are going
298:31 - to find out the gaussian pyramid of the
298:34 - orange
298:35 - so first of all what i'm going to do is
298:37 - i'm going to just copy the apple image
298:40 - so i'm going to just say apple
298:42 - underscore
298:44 - copy is equal to
298:46 - apple
298:47 - dot copy so there is a method called
298:50 - copy which you can use
298:52 - to copy the this image so from this copy
298:56 - what we are going to do is we are going
298:57 - to generate the gaussian pyramid so i am
298:59 - going to once again name my variable as
299:03 - gp
299:05 - lets say underscore
299:07 - apple
299:08 - and then
299:09 - we are going to
299:10 - just pass our image which we have copied
299:14 - in the form of list so i'm going to just
299:16 - say
299:17 - apple copy here so these steps we have
299:19 - already seen in the last video how to
299:22 - create the gaussian pyramid and the
299:25 - laplacian pyramid of an image so i am
299:28 - not going to explain this in detail if
299:31 - you want
299:32 - the
299:33 - detailed explanation you can see the
299:35 - last video
299:37 - next what i'm going to do is i'm going
299:39 - to create a for loop and i'm going to
299:41 - just say for i
299:43 - in
299:44 - our range so i'm going to use the range
299:47 - function and
299:49 - we are going to
299:51 - use the six levels in this example so
299:54 - i'm going to provide the range
299:56 - up to six and then what we are going to
299:59 - do is we are going to just say apple
300:01 - copy or you might have named
300:04 - this variable as apple layer also
300:07 - because we are just creating multiple
300:10 - layer
300:11 - of the apple image for the gaussian
300:14 - pyramid right
300:15 - and then we are going to use
300:18 - the cv2 dot pair down method to create
300:22 - the
300:23 - gaussian pyramid okay this we have
300:26 - already seen in the last video and now
300:29 - as an argument we are going to pass our
300:32 - apple copy a variable here and in the
300:35 - next step what we are going to do is we
300:36 - are going to just append to our
300:39 - gp underscore apple variable which is
300:42 - our gaussian pyramid for the apple image
300:45 - and then we are going to just append
300:48 - this apple copy after
300:50 - we have applied pair down method on the
300:53 - same image so this is just giving us
300:56 - multiple
300:57 - layer of the
301:00 - apple
301:01 - image right the same method we are going
301:04 - to apply for the orange also so i'm
301:07 - going to just copy this code and then
301:10 - i'm going to just paste this code once
301:12 - again and this time this will be
301:15 - for
301:16 - orange
301:17 - and i'm going to just say this is the
301:20 - orange copy
301:22 - and we are going to copy from the orange
301:26 - image
301:27 - and then we are going to just generate
301:30 - the
301:31 - gaussian pyramid for the orange image
301:34 - and this will
301:35 - be passed here
301:37 - and also here
301:39 - and also here
301:41 - and also here
301:43 - and this gaussian pyramid orange will be
301:46 - passed here okay so we have generated
301:48 - the gaussian pyramid for the apple and
301:51 - the orange now we are going to generate
301:54 - the laplacian pyramid for
301:57 - apple and orange so this also we have
301:59 - seen in the last video so i'm going to
302:02 - just comment uh generate laplacian
302:05 - pyramid for
302:06 - apple first of all
302:08 - and to find out the laplacian pyramid
302:11 - for the apple what we are going to do is
302:14 - we are going to once again
302:16 - take
302:17 - our
302:18 - apple copy and then
302:21 - using our gaussian pyramid so lets
302:24 - take gaussian pyramid for the apple and
302:27 - we are going to
302:28 - use the
302:30 - fifth element of
302:32 - this list so what we have learned in the
302:35 - last video how we can find out the
302:37 - laplacian pyramid a level in the
302:39 - laplacian pyramid is formed by the
302:42 - difference between the level
302:44 - in the gaussian pyramid and extended
302:47 - version of its upper level in the
302:50 - gaussian pyramid so this difference we
302:52 - are going to find out in this step so
302:54 - i'm going to just say
302:56 - this is lp for the
303:00 - apple which stands for laplacian pyramid
303:03 - for the apple is equal to in the list we
303:06 - are going to just pass the apple copy
303:09 - and then we are going to use the for
303:12 - loop so fall i
303:13 - in
303:15 - the range so we are going to take the
303:17 - range and in the last video i have shown
303:20 - you how to take the range for the
303:22 - laplacian pyramid so we want to go from
303:26 - 5
303:27 - until 0 in the steps of minus 1 and then
303:32 - in the next step we are going to create
303:33 - the gaussian extended
303:36 - variables gaussian extended is equal to
303:39 - cv
303:40 - 2 dot
303:42 - pair up this time we are going to use
303:45 - the pair up method and then we are going
303:47 - to pass our
303:49 - gp apple which is gaussian pyramid for
303:51 - apple
303:52 - and then the index here will be i in the
303:56 - next step we are going to create the
303:57 - laplacian variables is equal to cv2 dot
304:02 - subtract so there is a method in cv2
304:05 - which is called subtract and then we are
304:07 - going to
304:08 - take our gaussian pyramid for the apple
304:11 - so gp apple and the index here will be i
304:15 - minus 1 and the second argument for this
304:19 - subtract
304:20 - method will be
304:21 - our
304:22 - gaussian extended variable so we are
304:24 - going to just pass this gaussian
304:26 - extended variable
304:28 - and in the next step we are going to
304:30 - just append to our laplacian pyramid for
304:33 - the apple so lp underscore apple
304:37 - dot append and we are going to just
304:40 - append this laplacian variable
304:42 - to
304:43 - the
304:45 - laplacian
304:46 - pyramid for the apple
304:48 - same we will do for
304:50 - the orange image also so we are going to
304:54 - generate the laplacian pyramid for the
304:57 - orange orange here and this will be the
305:01 - copy of
305:03 - the
305:05 - orange copy here and here also
305:08 - and
305:09 - then this will be
305:11 - the
305:12 - gp orange right
305:15 - this also will be gp orange
305:18 - this also will be gb orange
305:20 - and here instead of lp
305:23 - apple we are going to just say lp
305:26 - orange and then we are going to just
305:29 - pass this variable here also so now we
305:32 - have finished three steps
305:34 - one is to load both the images second is
305:37 - to find out or generate the gaussian
305:39 - pyramid and the third step is to
305:42 - generate the laplacian pyramid for both
305:45 - the images
305:46 - now the fourth step is
305:49 - to just join the half of
305:51 - these two images so what i'm going to do
305:55 - is now i'm going to just create one more
305:58 - variable which will be
306:00 - apple
306:01 - underscore
306:02 - orange
306:03 - underscore let's say pyramid is equal to
306:07 - and also we are going to create a
306:09 - variable called n and we are going to
306:12 - see uh later
306:14 - how to use this variable and then we are
306:17 - going to
306:18 - use the for loop and then we are going
306:21 - to create
306:22 - two
306:23 - variables one for the first image so i'm
306:27 - going to
306:28 - just say
306:29 - apple
306:31 - and then
306:32 - lap
306:33 - comma
306:34 - orange lab okay so these two variables
306:38 - i'm creating
306:40 - just
306:41 - same as this i in this for loop in
306:45 - zip so there is a method zip which we
306:48 - can use to zip
306:50 - the laplacian pyramid one which is for
306:54 - the apple and for the orange also so i'm
306:57 - going to just say
306:59 - lp for apple
307:01 - comma
307:02 - lp for the orange and inside this for
307:06 - loop first of all we are going to just
307:08 - uh increment the value of n
307:11 - by 1 each time so n
307:14 - plus
307:15 - equals
307:16 - 1 and in this next step we are going to
307:18 - find out the shape of the apple image so
307:22 - the apple image shape gives us three
307:25 - values first is columns so i'm going to
307:27 - just say ceo ls for columns then
307:31 - rows
307:32 - and then the third value is the number
307:35 - of channels and then we are going to
307:37 - just say apple lap
307:40 - dot shape
307:42 - in the next step we are going to just
307:44 - create a variable called laplacian and
307:46 - we are going to just join the
307:48 - two halves of uh
307:50 - these two images which we are getting
307:54 - inside the
307:56 - variable apple lab and orange lab so we
307:59 - are basically doing this step
308:02 - after applying the gaussian
308:05 - pyramid and the laplacian m pyramid on
308:08 - both the images so np dot at stake uh
308:12 - method we are going to apply
308:14 - in this step so i'm going to just write
308:16 - np dot
308:18 - h stack
308:19 - and then as an argument what we are
308:21 - going to do is we in the form of tuple
308:23 - first of all we are going to
308:25 - take our apple lamp variable which is
308:30 - this one and in the square bracket we
308:32 - are going to just write colon
308:34 - comma
308:35 - zero comma
308:37 - int
308:39 - so we are going to just type cast
308:41 - the number of columns in the apple
308:46 - shape so this we got from the shape of
308:49 - the
308:49 - apple uh index
308:52 - and then
308:53 - divided by two so we are going to just
308:55 - uh
308:57 - dividing the columns into half and same
309:00 - we will do for the orange lab so we are
309:03 - going to just say orange lap in the
309:06 - square bracket colon
309:08 - comma
309:09 - int
309:11 - and then once again
309:13 - in the
309:14 - parenthesis we are going to just say
309:17 - calls for the number of columns divided
309:19 - by 2
309:20 - and then colon as we have done
309:23 - in this step also and at last we are
309:26 - going to just append this laplacian
309:28 - variable to
309:30 - this
309:30 - list which we have created so
309:33 - apple underscore orange underscore
309:35 - pyramid dot
309:36 - append and then we are going to pass the
309:39 - laplacian variable here now the last and
309:41 - the final step is to reconstruct our
309:44 - image so let's reconstruct our image
309:48 - so now what we are going to do is we are
309:50 - going to once again create a variable
309:52 - called apple
309:54 - orange underscore
309:56 - reconstruct is equal to this will be the
310:00 - first index of our apple orange pyramid
310:02 - so i'm going to just say apple
310:04 - orange underscore pyramid and this will
310:07 - be the 0th index and once again we are
310:10 - going to use the for loop so for
310:12 - i
310:13 - in the range so we are going to
310:16 - go from
310:18 - 1
310:19 - until 6 and the default step is of 1 so
310:24 - we don't need to give the third argument
310:26 - and inside the for loop we are going to
310:28 - just take this variable once again and
310:31 - then we are going to apply
310:34 - the pair up method on this so cv2
310:37 - dot
310:38 - pair up and as an argument we are going
310:41 - to
310:42 - pass the same variable so we are going
310:45 - to just
310:46 - apply the pair up on this
310:49 - apple orange reconstruct from the zeroth
310:53 - index of the pyramid
310:55 - up to the sixth level and the last step
310:58 - will be to add
311:00 - all the layers so
311:02 - uh apple orange reconstructed once again
311:06 - or reconstruct
311:07 - is equal to cv2 dot
311:10 - add so this is
311:12 - also
311:13 - one method which is called add and here
311:15 - we are adding apple orange pyramid
311:19 - and the reconstructed apple orange
311:23 - image okay so this is this variable
311:27 - which we got
311:29 - by
311:30 - just adding the left and right halves of
311:32 - each level and then we are just
311:35 - reconstructing this image using the pair
311:39 - up method and thus just adding the
311:42 - pyramid level so this should be i think
311:44 - the index i right we cannot just add
311:49 - the list to the image directly okay so
311:52 - this will be
311:54 - at each layer we are just reconstructing
311:56 - and adding it to the image which we got
312:00 - by just
312:02 - adding addition of this half of the
312:04 - images now in the end let's try to just
312:08 - load this reconstructed apple orange
312:12 - image in the
312:14 - i am show window and let's hope it works
312:18 - i haven't checked it yet so
312:20 - i'm not sure it will work or not and you
312:23 - can see it's working in the first go so
312:26 - that's a good thing so you can see the
312:29 - difference
312:30 - so this
312:32 - result we got by just stacking
312:36 - this apple and orange image side by side
312:38 - but this
312:41 - line is clearly visible
312:43 - but when we applied the gaussian pyramid
312:46 - and the laplacian pyramid technique for
312:49 - blending the images then you can see
312:52 - this line
312:53 - is
312:54 - perfectly blended and this line is not
312:59 - any more visible so this is the
313:02 - perfect blending of the orange and the
313:05 - apple image
313:06 - so this is how you can use the laplacian
313:09 - and gaussian pyramids to reconstruct and
313:13 - blend two images together
313:16 - and result is in front of you so you can
313:19 - see how it can blend two images so
313:22 - perfectly so this is how you can blend
313:24 - images using image pyramid technique in
313:27 - this video we are going to understand
313:29 - what contours are and we are going to
313:32 - see how to find contours and how to draw
313:34 - contours
313:36 - so first of all what are contours
313:38 - so contours can be explained as the
313:41 - curve
313:42 - joining all the continuous point along
313:44 - the boundary which are having the same
313:47 - color or intensity
313:49 - now contours can be a useful tool for
313:53 - shape analysis
313:54 - or object detection or object
313:57 - recognition
313:59 - now for better accuracy we generally use
314:02 - binary image for finding the contour so
314:05 - first of all we are going to generate
314:08 - the binary image and then before finding
314:11 - out the contours we are going to apply
314:13 - the threshold or kenny edge detection to
314:16 - find the contours on the image so let's
314:20 - start with an example so here i have a
314:22 - simple code which reads an image and
314:25 - then converts this image into a gray
314:28 - scale image and then i'm just showing
314:31 - both the images using i am show method
314:34 - so let's run this code and let's see
314:36 - what result we get so this is the
314:39 - original image
314:41 - with
314:41 - these colors and after the conversion
314:44 - of this image to the grayscale image
314:47 - this is the result which we are getting
314:50 - and then we are going to find out the
314:52 - threshold or the kenny edge so
314:56 - in this example we are going to just
314:58 - apply the threshold so for applying the
315:01 - threshold on this image we are going to
315:04 - define first of all two variable ret
315:07 - comma
315:08 - thresh
315:09 - is equal to cv
315:11 - to
315:12 - dot threshold so there is a method
315:15 - called threshold which we have already
315:17 - seen how threshold work in detail in the
315:20 - previous videos so the first argument
315:23 - which this threshold method takes is the
315:26 - image so we are going to pass our
315:28 - grayscale image as the source the second
315:31 - argument is the threshold value so
315:33 - because it's a binary image let's set
315:36 - the threshold to 127
315:38 - which is around half of uh the 255 right
315:43 - the third argument is the maximum value
315:46 - so the maximum value here will be 255
315:50 - the next argument will be the type and
315:52 - type here will be zero
315:54 - so this is going to give us the trash
315:57 - hold value for this grayscale image and
316:01 - after finding out the threshold of this
316:03 - image we are going to find out the
316:06 - contours
316:07 - so for this we are going to define two
316:09 - variables one is contours and the second
316:12 - is the hierarchy because the method
316:16 - which we are going to use which is cv2
316:19 - dot
316:20 - find contours
316:22 - this is the method it's going to give us
316:25 - these two values contours and the
316:27 - hierarchy and we are going to see what
316:30 - our contours and hierarchy in details
316:33 - after applying this method on this image
316:36 - so the first argument will be the thresh
316:39 - which we got using this threshold method
316:42 - the second argument will be the contour
316:45 - mode so this is called the contour
316:49 - retrieval mode also and there can be
316:52 - several possibilities here which we can
316:55 - apply but for simplicity and in the most
316:59 - common case we use our etr underscore
317:03 - tree here okay as the mode the third
317:05 - argument here will be the method which
317:08 - we want to apply
317:10 - and this is also called the contour
317:12 - approximation method and here also
317:15 - several possibilities are possible
317:18 - but for now what we are going to use
317:20 - here is this will be cv2
317:24 - dot approx none so now as you are seeing
317:27 - here this find contour method gives us
317:30 - contours and hierarchy
317:32 - so the contour is a python list of all
317:36 - contours in the image and each
317:39 - individual
317:40 - contour is a numpy array of x comma y
317:44 - coordinates of boundary points of the
317:47 - object and the hierarchy is the optional
317:50 - output vector which is containing the
317:52 - information about
317:54 - image topology and this we are going to
317:57 - see in the later videos so for now we
318:00 - are only concerned about finding out the
318:03 - contours so for this as i said this
318:06 - contains the number of contours right so
318:09 - we can print out
318:12 - these number of contours is equal to and
318:15 - then we are going to just uh convert
318:18 - this number into the string and there is
318:21 - a method called
318:23 - length and then inside this length
318:25 - method we are going to pass our contour
318:28 - variable so this line is going to print
318:30 - out the number of contours which are
318:33 - found inside the image which we are
318:35 - providing so let's run this code and
318:38 - let's see what result we get so we
318:40 - already know that this gives us a
318:42 - grayscale image and the original image
318:46 - but we are interested in this
318:49 - print message and the number of contours
318:51 - which are found
318:53 - is nine inside the source image which we
318:56 - are providing here
318:58 - now we already found out the number of
319:01 - contours
319:02 - now we need to draw these contours on
319:06 - the image itself so how can we achieve
319:08 - this but before this let's see
319:11 - the individual contour also so i'm going
319:14 - to just print out the value of the first
319:17 - contour which will be at index 0. so
319:21 - let's run it once again and let's see
319:24 - what happens
319:25 - so we are running this
319:27 - code once again and you can see after
319:29 - printing out
319:31 - the number of contours it's going to
319:33 - give us
319:34 - the
319:35 - numpy array of the x and y coordinates
319:40 - so if we plot or join all these x and y
319:43 - coordinates we are going to get the
319:46 - boundary of the contour so now we are
319:49 - going to just take these contours and
319:51 - pass it to a method called draw contos
319:54 - which is going to draw or join all these
319:57 - coordinates of those contours so to
320:01 - get this we are going to just say cv2
320:04 - dot
320:05 - draw contours
320:07 - and then the first argument here will be
320:09 - our
320:10 - original image because we want to draw
320:12 - the contours on our original image so
320:15 - this will be
320:16 - the img
320:18 - and it's the original image and the
320:20 - second argument will be the contours so
320:22 - we are going to just pass the contours
320:25 - which we found inside the image the
320:28 - third argument will be the contours
320:31 - indexes
320:32 - so if we uh just give here -1 then it's
320:36 - going to draw all the nine contours
320:38 - which were found inside the image these
320:42 - all contours
320:44 - so first of all we will give minus one
320:46 - here as an argument and then we will see
320:49 - how to give other arguments
320:51 - as the numbers here also
320:54 - the fourth argument here will be i think
320:56 - the color so we are going to just give
320:59 - the color
321:01 - 0 comma 255 comma
321:04 - 0 let's say and
321:06 - the
321:07 - next argument will be the thickness so
321:09 - we are going to take the thickness three
321:11 - here so using this method what we have
321:14 - achieved is we have drawn
321:17 - the contours on the original image so
321:20 - let's run this code once again and let's
321:22 - see what result we get
321:24 - so you can see this was the grayscale
321:26 - image and this we have used for finding
321:30 - out the contours but the interesting
321:32 - image here is this one
321:35 - and here you can see all the contours
321:38 - are drawn on this image so all the green
321:43 - lines or green uh
321:46 - boundaries are all contours
321:49 - so because we have given -1 it has drawn
321:52 - all the contours on this image and we
321:56 - can also give the contour index so let's
321:59 - say we just want to
322:02 - find out the contour 0 which will be the
322:05 - first contour which is found inside the
322:07 - image we are going to just run this
322:11 - code once again and the first contour
322:14 - which was find out found out is this
322:17 - contours this uh p contour right
322:20 - in a similar way we can go
322:23 - up to
322:24 - eight so zero
322:25 - one and let's rerun this code again you
322:29 - will see that the second contour is this
322:33 - contour so this
322:35 - whole contour
322:37 - from the boundary of this image is the
322:40 - second contour and in a similar way you
322:42 - can go let's say 2
322:46 - i'm going to run this code once again
322:48 - you will see the next contour here and
322:50 - similarly you can go up to the index 8
322:53 - because the total number of contours are
322:56 - 9 and we are starting from the index
322:58 - zero that's why we can go up to eight so
323:02 - this value depends on the number of
323:04 - contours okay so because we found out
323:08 - the number of contours are nine so
323:10 - that's why we can go up to 8 and let's
323:13 - run this code and the last contour which
323:16 - was find out and we have drawn this
323:18 - contour here on this blue circle right
323:23 - now if we go beyond this index let's say
323:26 - we give nine here we are going to get
323:29 - the error right so you can go up to
323:32 - eight here and if you want to just draw
323:34 - all the contours then you can just give
323:37 - minus one here and it's going to draw
323:40 - all the contour on the image which you
323:43 - are providing so this is how you can
323:45 - find out the contours and draw contours
323:49 - on the images using find contour and
323:52 - draw contour methods in opencv in this
323:55 - video i'm going to show you how you can
323:57 - create a very basic and simple motion
323:59 - detection and tracking system using
324:02 - python and opencv
324:05 - so let me show you what we are going to
324:07 - achieve at the end of this video
324:10 - so
324:11 - i have
324:12 - this
324:13 - video which is a sample video and you
324:16 - can see some people are walking
324:18 - around inside this video
324:21 - now what i want to do here is i want to
324:24 - show these rectangles
324:26 - around
324:27 - these moving people or persons
324:31 - so this is tracking and when some
324:34 - movement occurs i also want to show this
324:37 - kind of status that status is movement
324:40 - because somebody is moving inside the
324:42 - video
324:43 - so if nobody is moving the status will
324:46 - be blank and if somebody is moving
324:50 - then the status
324:52 - will be movement
324:54 - so this is what we are going to achieve
324:56 - at the end of this video so we are going
325:00 - to try to track
325:03 - each and every person
325:04 - and also we are going to uh
325:07 - track this person with this rectangle
325:10 - and also we will show the status
325:14 - as movement when somebody moves inside
325:17 - the video
325:18 - so let's get started
325:21 - so to start with i have this basic code
325:24 - which just
325:26 - reads a video using video capture class
325:30 - and then if this video is valid then i'm
325:34 - going to just show this frame by frame
325:37 - inside i'm show window
325:40 - and i'm sure you might be knowing all
325:44 - this code because i've shown you step by
325:46 - step how to
325:47 - capture the video or how you can read
325:51 - the video frames using video capture
325:55 - method okay so this is just to uh load
326:00 - this video and show it frame by frame
326:03 - using i am show method so let me run
326:05 - this code first of all to start with so
326:08 - our original video looks like this so
326:11 - some people are moving but we want to
326:14 - track the movement of each and every
326:16 - person and also we want to show a
326:19 - rectangle around them whoever is moving
326:23 - so let's get started so
326:25 - under this
326:27 - video capture
326:28 - code line what i'm going to do is first
326:31 - of all i want to read
326:33 - two frames
326:35 - from
326:36 - the cap
326:37 - instance so i'm going to just copy this
326:41 - code and paste it here so this will be
326:45 - our frame 1 let's say
326:48 - and similarly i'm going to just read the
326:51 - second frame so
326:54 - simply we are just declaring to frame
326:57 - one after another okay and we don't need
327:01 - this uh code anymore so first of all i'm
327:04 - going to declare a variable
327:06 - diff and using
327:09 - a cv2 dot a b s diff method so absolute
327:14 - difference we are going to find out the
327:16 - difference between the first frame
327:19 - and the second frame
327:22 - so this method abs diff
327:25 - is for finding out the absolute
327:27 - difference between the first frame and
327:30 - the second frame
327:31 - now once we have the difference then we
327:35 - are going to convert this difference
327:38 - into
327:39 - a grayscale
327:40 - mode so we are going to just say
327:43 - gray is equal to cv 2 dot
327:48 - convert color so cvt
327:51 - color and the first parameter here will
327:54 - be our
327:55 - difference which we have found between
327:58 - the two frames so i'm going to just pass
328:00 - diff as the first argument and the
328:03 - second argument will be cv to
328:05 - dot we are going to convert this bgr
328:09 - color to the grayscale mode and why we
328:13 - are finding out the grayscale mode of
328:16 - this
328:17 - diff because we are going to find out
328:20 - the contour in the later stages and in
328:22 - the last video we have learned that it's
328:25 - easier to find out the contours in this
328:28 - grayscale mode
328:30 - as compared to the colored mode or the b
328:34 - gr mode
328:35 - so once we have this grayscale mode we
328:38 - are going to
328:39 - just blur
328:41 - our grayscale
328:43 - frame so we are going to just declare a
328:46 - variable called blur and then we are
328:50 - going to apply the gaussian blur on our
328:54 - gray
328:55 - variable so cv2
328:57 - dot
328:58 - gaussian blur the first parameter here
329:02 - will be
329:03 - gray so let's uh give this grape
329:06 - parameter which we have defined here the
329:09 - second parameter here is the k size or
329:12 - the kernel size so let's say we want to
329:14 - provide the kernel size 5 comma 5
329:18 - and the third parameter here will be the
329:22 - sigma x value so we are going to just
329:24 - pass
329:25 - 0 here as the sigma x value
329:28 - now we are going to find out the
329:30 - threshold so we are going to just say
329:34 - underscore because we don't need this
329:37 - first
329:38 - variable and then the second variable
329:41 - will be
329:42 - thresh is equal to cv to dot
329:45 - threshold and the first parameter which
329:48 - it takes is the source so we are going
329:51 - to pass our blurred
329:54 - image as the source
329:56 - and then the second parameter here will
329:59 - be
330:00 - the threshold value so we are going to
330:03 - just provide 20 here
330:05 - then the maximum threshold value will be
330:08 - 255
330:10 - then the type
330:12 - will be uh
330:14 - cv2 dot thresh binary so in the next
330:16 - step what we are going to do is we are
330:18 - going to dilate the threshold image to
330:21 - fill
330:22 - in all the holes this will help us to
330:25 - find out the better contours so there is
330:28 - a method called cv2.dilate so we are
330:31 - going to just declare a variable called
330:33 - dilated and then we are going to apply
330:36 - this uh method so cv2 dot
330:40 - dilate which takes few argument the
330:43 - first argument will be our thresholded
330:46 - version of the image the second argument
330:49 - here will be the kernel so kernel let's
330:52 - say for now we are going to provide none
330:55 - here okay so the kernel size will be uh
330:58 - none and then
331:00 - third parameter will be the number of
331:03 - iterations so let's provide the number
331:05 - of iterations and the number of
331:08 - iterations we are going to provide here
331:10 - will be three so
331:12 - if it doesn't work we can increase or
331:14 - decrease the number of iterations now in
331:16 - the next step what we are going to do is
331:18 - we are going to find out the contour so
331:21 - as you all know that contour or fine
331:23 - contour method is going to give you
331:26 - two result one is the contours and other
331:29 - is the hierarchy so we are going to just
331:32 - say contour and the second
331:34 - result we are going to just say
331:36 - underscore because we are not going to
331:38 - use this uh second result and then we
331:41 - are going to uh just say cv2
331:45 - dot find contours and we are going to
331:49 - find the contours on this dilated image
331:53 - so we are going to say dilated now the
331:55 - next argument here will be the mode so
331:58 - the mode which we are going to use here
332:01 - will be retter underscore tree so i'm
332:04 - going to just write retr underscore
332:08 - tree which is uh
332:10 - most commonly used and then the next
332:12 - argument here will be the method so the
332:15 - method here will be cv2 dot
332:20 - chain
332:21 - approx simple and once we have our
332:24 - contours we are going to just draw the
332:26 - contours because we already found out
332:29 - the contours so we are going to just say
332:33 - draw contos and the first argument here
332:36 - will be
332:37 - frame one because we want to apply all
332:39 - the contours on the original frame right
332:43 - so we are going to apply all the
332:46 - contours which we have found using all
332:48 - these method on the frame one
332:51 - and then the second argument here
332:54 - will be
332:55 - the contour so you can
332:58 - just give the contours here
333:01 - and the third argument here will be uh
333:05 - the contour id i can just say minus one
333:09 - which is going to apply all the contours
333:12 - and the third and the next argument will
333:15 - be the color so let's say we want to
333:17 - provide
333:18 - the green color so i'm going to just uh
333:21 - say 0 comma 255
333:24 - comma 0 and the next will be the
333:27 - thickness so let's say we want to
333:28 - provide the thickness of two here
333:31 - so now it's going to draw all the
333:33 - contours which we have found
333:36 - with the difference of frame one and
333:38 - frame two right
333:40 - and then we are going to just display
333:43 - this frame one so we can just say this
333:46 - is our feed
333:48 - and the result
333:50 - after applying the contour will be saved
333:53 - in the frame 1 which we will display now
333:57 - in the next step what we are going to do
333:59 - is we are going to assign the value
334:03 - inside frame 2 into frame 1 so we are
334:07 - going to just say frame 1 is equal to
334:09 - frame 2 and then inside our frame 2 we
334:14 - are going to read a new value so we are
334:16 - going to just say
334:18 - r e
334:19 - t
334:20 - comma frame 2
334:22 - is equal to cap dot read okay so we are
334:27 - reading the new frame
334:29 - in the variable frame two and before
334:33 - reading the new frame we are assigning
334:35 - the value inside the frame two to the
334:38 - frame one in this way we are reading the
334:41 - two frames and finding out the
334:43 - difference between uh the two frames so
334:46 - let's run this code and let's see if it
334:48 - works or not
334:49 - let's test this so you can see now
334:53 - there are these contours which are drawn
334:57 - around
334:58 - all the moving
335:00 - persons also there are some contours
335:04 - which are drawn around this rope which
335:06 - is also moving right so we have
335:10 - successfully determined the contours and
335:13 - we have already drawn
335:15 - these contours on the frame one
335:19 - but this was not the result we are
335:22 - looking for we want to draw the
335:23 - rectangle around these moving persons
335:27 - and also we want uh some noises to be
335:31 - removed so we not want to
335:34 - draw the contour on the moving
335:38 - rope let us say okay so how to remove
335:41 - these noises and how to draw these
335:43 - rectangles let's see
335:45 - so now in the next step what we are
335:47 - going to do is
335:49 - under or before we are drawing these
335:53 - contours we don't want to draw the
335:55 - contours now we want to draw the
335:58 - rectangles right so what we are going to
336:01 - do is we are going to iterate over all
336:04 - the controls so we are going to just say
336:06 - uh for
336:08 - contour so
336:10 - from contours we are going to find out
336:13 - contour in contours right so
336:17 - this is the list and we are iterating
336:21 - over this list so inside this for loop
336:23 - the first step will be
336:25 - to save all the coordinates of the found
336:29 - contours okay so we are going to define
336:34 - the
336:35 - x-coordinate then the y-coordinate and
336:38 - then we are going to
336:40 - just say
336:41 - width comma height and there is a method
336:45 - called bounding rect which we are going
336:48 - to apply on the contour so we are going
336:51 - to just say is equal to cv
336:54 - 2 dot bounding rect this is the method
336:57 - which we are going to apply which is
336:59 - going to give us the x and y coordinate
337:03 - and the width and height right and we
337:06 - are going to apply this bounding rect
337:09 - method on the contour which we are
337:12 - getting using this contours list now in
337:15 - the next step we are going to find out
337:17 - the area of the contour and we are going
337:20 - to just say if
337:22 - this area is less than certain value
337:26 - then we don't want to do anything we
337:28 - don't want to draw a rectangle or
337:30 - anything we just want to continue
337:33 - otherwise if this
337:36 - contour area is greater than
337:38 - let's say some kind of a person's area
337:42 - then we want to draw a rectangle on it
337:45 - so inside this for loop we are going to
337:48 - just define uh if condition so we can
337:51 - say if cv cv2
337:54 - dot
337:55 - contour area so there is a method called
337:59 - contour area which is this one where we
338:03 - can pass
338:05 - our contour so we are going to pass our
338:07 - contour and if
338:09 - the area of this contour let's say is
338:12 - less than
338:14 - 700 then we are going to just say
338:17 - continue so this code essentially mean
338:21 - that if the area of the contour is less
338:24 - than 700 then we are going to do nothing
338:28 - we don't want to draw any rectangle
338:30 - otherwise if the area is greater than
338:33 - 700 then we want to draw the rectangle
338:36 - so we are going to just say cv2 dot
338:40 - rectangle we have already learned how to
338:43 - draw a rectangle on an image using the
338:47 - rectangle method the first argument here
338:49 - will be
338:51 - the source which will be frame one the
338:54 - second argument will be the point one so
338:57 - we are going to just say
338:59 - point one will be x comma
339:02 - y the third argument will be point two
339:05 - so we are going to just say
339:07 - x plus w
339:09 - comma y
339:11 - plus
339:12 - h the next argument will be the color so
339:15 - let's say the color will be the same 0
339:18 - comma 255
339:20 - comma 0
339:22 - the next argument will be
339:25 - the thickness let's say we want to give
339:27 - the thickness
339:29 - 2 as we have done
339:31 - with the draw contour we have provided
339:33 - the thickness of two here right
339:36 - now in the next step we are going to
339:38 - just uh
339:40 - print some text on the image if some
339:43 - movement is observed so we can just say
339:47 - cv2 dot put text this also we have seen
339:50 - in the previous videos how to put text
339:53 - on an image so this time the source will
339:56 - be our frame one the second will be the
339:59 - text so we will just say
340:02 - uh status let's say
340:05 - and if there is some moment we are going
340:08 - to just say
340:10 - colon
340:11 - in the
340:12 - curly brackets we're going to just use
340:15 - the format method so this is just
340:18 - formatting the result
340:20 - using the string and we are going to
340:24 - just say movement the next argument here
340:27 - will be the origin so where we want to
340:30 - put this text let's say we want to put
340:32 - this text on
340:34 - 10 comma 20 coordinate and then the next
340:38 - argument will be the font phase so we
340:40 - are going to just say font face will be
340:43 - cv 2
340:44 - dot font font hershey simplex let's say
340:47 - so we are going to use uh this font
340:50 - and the the next argument will be the
340:53 - font scale so let me just
340:56 - do this on the next line so
340:59 - font scale will be
341:00 - let's say
341:02 - 1
341:03 - the next will be the color
341:06 - of the font so let's say the color will
341:09 - be 0 comma 0 comma 255
341:13 - and then the last argument will be the
341:16 - thickness so let's say the thickness
341:17 - will be 3 and this code is going to put
341:20 - the rectangle around your moving
341:22 - persons if the area of that
341:27 - contour is greater than
341:29 - 700 okay
341:31 - so let's run this code and let's see if
341:33 - it works or not so i'm going to just run
341:36 - this code
341:37 - and you can see
341:39 - that
341:40 - status is movement because all the
341:43 - persons here are moving and you can see
341:45 - these rectangles which are drawn around
341:48 - the moving persons and this noise which
341:51 - we were seeing in the previous result is
341:55 - also gone around the movement of uh this
341:58 - rope okay so sometimes uh this uh
342:02 - rectangle is drawn on the movement of
342:04 - the rope also so in this case you can
342:07 - also increase
342:09 - the expected area let's say we just want
342:12 - to
342:13 - find out the contours which are greater
342:15 - than 900
342:17 - and we can
342:18 - now
342:19 - you can see
342:20 - uh these rectangles are drawn around
342:23 - these moving persons
342:25 - with the area which have the
342:28 - contour area more than 900 so you can
342:31 - remove
342:32 - these kind of noises from
342:35 - the frame
342:37 - using this area so this was a very basic
342:40 - example how you can
342:43 - detect the motion and track your moving
342:47 - object
342:48 - inside your video using python and
342:51 - opencv in this video we are going to see
342:54 - how we can detect simple geometrical
342:56 - shapes using opencv
342:59 - so to start with i have this simple code
343:02 - which reads an image and then show it
343:05 - into uh i am show window so let's run
343:09 - this
343:10 - simple code first of all and let's see
343:12 - what it does
343:13 - so you can see i have this image which
343:16 - i'm loading into a opencv window using i
343:19 - am show method and here we have
343:23 - some shapes so we have a pentagon circle
343:27 - rectangle
343:28 - square triangle and this star shape
343:32 - right
343:33 - and let's say we want to detect using
343:36 - opencv which shape it is
343:40 - based upon the geometrical shape and we
343:43 - want to write the name
343:46 - on top of this shape
343:49 - so how we can achieve this let's see
343:51 - using opencv
343:54 - so
343:55 - as you can see if the first step is to
343:58 - read an image and then
343:59 - in the second
344:01 - line i'm just converting this image into
344:04 - a grayscale mode image so
344:08 - using this code i'm just converting this
344:10 - image into a grayscale mode and in the
344:13 - next step we are going to find out the
344:16 - threshold so i am going to just say
344:18 - underscore comma
344:20 - thrash
344:22 - is equal to
344:23 - cv
344:24 - 2 dot threshold so cv2 dot
344:28 - threshold and we are going to pass our
344:32 - image which is a grayscale image which
344:34 - we have converted as a source
344:37 - and then
344:38 - the next two values are the threshold
344:40 - values
344:41 - and the maximum
344:43 - value of
344:45 - the threshold so
344:47 - for now
344:48 - i am giving the threshold value 240
344:52 - because i know this will work but if you
344:55 - want to be more flexible you can always
344:58 - use the track bar to find out which
345:02 - threshold will work with your image
345:04 - the second value is the maximum value of
345:07 - the threshold and the next value will be
345:09 - the type so the type here will be cv2
345:13 - dot thresh binary so we are going to
345:15 - just say cv2 dot
345:17 - thresh binary
345:19 - now in the next step we are going to
345:21 - find out the contours
345:24 - so contours we have already
345:26 - seen in the last videos how to find out
345:29 - the contours and what are contours so
345:32 - for that i am going to define two
345:33 - variables
345:34 - one is contours variable other is
345:37 - the underscore variable because we do
345:40 - not need the second
345:43 - result
345:44 - and then i am going to just say cv2 dot
345:48 - find contours the first argument here
345:51 - will be
345:52 - the
345:53 - thresholded image and then the second
345:56 - argument here will be the mode and third
345:59 - will be the method so let's give these
346:01 - two values so cv2 dot
346:04 - retr
346:06 - 3
346:07 - and the method will be cv2
346:11 - dot
346:12 - chain approx none okay so let's
346:15 - give
346:16 - this method so this is the simple
346:19 - procedure to find out the contours
346:22 - inside an image
346:24 - now in the next step i'm going to
346:25 - iterate over all the contours so i'm
346:28 - going to just say
346:30 - for
346:31 - contour
346:32 - in contours so we are going to iterate
346:34 - over all the contours
346:37 - and then
346:38 - we are going to first of all use a
346:41 - method called cb2 dot approx poly dp
346:47 - so i'm going to just
346:50 - declare a variable first of all i'm
346:52 - going to just say
346:53 - approx is equal to cv 2 dot this method
346:58 - which i have mentioned which is called
347:01 - approx polydp so this method
347:04 - approximates a polygonal curves
347:07 - with a specific precision and the first
347:11 - argument which it takes is the curve so
347:14 - our curve here will be the contour which
347:17 - we have found on
347:19 - the shape the second argument here will
347:22 - be
347:23 - epsilon so epsilon is the parameter
347:26 - specifying the approximation accuracy
347:30 - so here what we are going to do is we
347:32 - are going to define
347:34 - epsilon is equal to 0.01
347:38 - and then we are going to multiply this
347:40 - number by
347:42 - cv 2 dot arc length so there is this
347:47 - method called
347:48 - arc length and what does this arc length
347:51 - method do it calculates a contours
347:54 - parameter
347:55 - or a curve length so here in this arc
348:00 - length parameter we are going to pass
348:03 - once again our contour variable and the
348:06 - second argument here will be if it's
348:09 - closed or the open contour so in our
348:13 - case we know that all the shapes which
348:16 - we want to detect are closed so we are
348:19 - going to just pass through here and the
348:21 - next argument in the approx poly dp
348:25 - method
348:26 - will be once again if it's a closed
348:29 - shape or the open shape so once again we
348:32 - are going to pass through here because
348:34 - all the shapes which we have are closed
348:38 - shapes
348:39 - now once we have this approximation
348:43 - we are just going to draw all the
348:45 - contours first of all so we are going to
348:47 - just say cv2 dot draw contours
348:51 - on which image on our original image so
348:54 - we are going to draw these contours on
348:57 - the original image
348:59 - and then we are going to pass the second
349:03 - argument and this will be
349:05 - our approximation so we can
349:08 - in the square bracket this is uh one
349:11 - other notation
349:12 - of uh just
349:14 - giving the number of contours as an
349:17 - argument to the draw contours method so
349:20 - in the square brackets you can just pass
349:23 - the approx
349:25 - the next parameter here will be the
349:27 - contour index so because we are
349:29 - iterating over all the contours
349:32 - that's why the index will always be zero
349:36 - because there will be only one contour
349:38 - which we are working at a time so this
349:42 - index will be zero
349:44 - the next argument here will be
349:47 - the color so you can give any color here
349:50 - i am going to give 0 comma 0 comma 0
349:54 - let's say
349:55 - and then
349:56 - the next will be the thickness so
349:58 - thickness i'm going to give here
350:00 - is 5
350:02 - now the next step is to print out the
350:06 - shape
350:07 - so which shape it is we want to print on
350:11 - the shape which shape it is in simple
350:14 - english let's say so for that we need to
350:17 - find out the coordinates on which we
350:20 - want to
350:21 - uh print this text on the shape so we
350:26 - need to find out the x and y coordinates
350:28 - so we can find this x and y coordinates
350:31 - using uh this approx
350:34 - variable and we
350:36 - can just say approx dot
350:39 - revel
350:40 - so this is a
350:43 - method called ravel
350:46 - and then the first index here will be
350:50 - the x coordinate and see in a same way
350:54 - we are going to just say approx dot
350:58 - ravel
351:00 - and on this method the second argument
351:04 - or the second index at index one
351:07 - will be the y coordinate
351:10 - so on these x and y coordinates we are
351:13 - going to print
351:14 - our uh text
351:17 - now in the next step what we are going
351:18 - to do is so because this approx poly dp
351:22 - is going to approximate the number of
351:25 - polygonal curves
351:28 - so based upon the number of polygonal
351:30 - curves
351:31 - we can just
351:33 - approximate which shape it can be
351:37 - so if
351:38 - this approx length so let's uh just find
351:42 - out the length of this
351:44 - approx and if the length of this approx
351:49 - variable is equal to 3
351:52 - then we are going to say that it's a
351:55 - triangle because triangle can be made
351:58 - with three points
352:00 - so this length of approx variable
352:03 - if it's equal to three then we are going
352:05 - to say that it's a triangle because
352:08 - if the number of curves here are three
352:11 - then most probably it's going to be
352:13 - a triangle
352:15 - so if we know that this is a triangle
352:18 - then we can easily
352:21 - just print or put text
352:23 - on
352:24 - that image so we are going to just say
352:26 - put text
352:27 - and the first variable here will be the
352:30 - image so we are going to put text on the
352:33 - image
352:34 - the second variable will be the text and
352:37 - we know
352:38 - that this will be a triangle so we are
352:41 - going to just say triangle
352:44 - here
352:45 - and then the next argument here will be
352:48 - the coordinates on which you want to
352:50 - print
352:51 - this text so
352:53 - we already found out the the coordinates
352:56 - at which we want to put this text the
352:59 - next argument here will be the font so
353:02 - we are going to just say cv
353:04 - 2 dot font hershey complex and the next
353:08 - argument here will be the font scale so
353:10 - let's say font scale will be 0.5
353:14 - and the next argument here will be
353:17 - the color so you can give any color
353:19 - let's uh say we just want to
353:23 - print this text in the black color
353:26 - itself so we are going to just say zero
353:28 - comma zero comma zero then using this
353:31 - logic we can also say that if the length
353:34 - of this approx is equal to 4 then it can
353:38 - either be a square or a rectangle so
353:42 - here if the approx
353:45 - length is 4
353:46 - then it can be a
353:49 - square or
353:50 - a
353:51 - rectangle
353:52 - but we don't know if it's a square or a
353:55 - rectangle so for now we can
353:58 - just write that it's a rectangle and we
354:02 - are going to
354:03 - decide if it's a rectangle or a square
354:07 - in the next step but let's define the
354:10 - other
354:11 - uh if else conditions also so this was
354:15 - l if similarly
354:17 - if number of approx points
354:20 - are
354:21 - 5 then we are going to say that it's a
354:25 - pentagon so we are going to print out uh
354:28 - the pentagon text on the x and y
354:31 - coordinates
354:32 - and if the number of points
354:35 - are 10
354:36 - then we are going to just say that it's
354:39 - a star shape so we're going to just say
354:43 - star because in the star the number of
354:46 - points are 10 and then we are going to
354:49 - say that in any other conditions so we
354:53 - are going to just say
354:55 - else and we are going to just remove
354:57 - this condition from here else in any
355:00 - other condition it's going to be a
355:03 - circle
355:04 - okay
355:05 - so if
355:06 - approx
355:07 - length is 3 it's a triangle if approx
355:11 - length is four it's a rectangle or a
355:13 - square a five pentagon if it's ten it's
355:17 - a star
355:18 - if it's uh nothing out of all these
355:22 - options then it's a circle you can also
355:25 - find out uh for example
355:27 - octagon or hexagon here if it's six it's
355:31 - a hexagon if it's eight it's a
355:35 - octagon and so on right
355:37 - now let's once again come to this step
355:41 - and in this step we
355:44 - just know that if the number of points
355:47 - are 4
355:48 - then it's a rectangle or a square but
355:51 - how can we find if it's a rectangle or a
355:55 - square
355:56 - so let's decide that now so what we are
356:00 - going to do
356:01 - for that is we are going to just say x
356:04 - comma y
356:06 - and then we are going to just say
356:10 - uh
356:10 - w comma h for width and height
356:14 - and there is a method called cv2 dot
356:18 - bounding
356:19 - rect which is going to uh give us the x
356:23 - and y coordinates and the width and
356:25 - height
356:26 - of the rectangle
356:28 - right
356:29 - so we are going to apply that method so
356:32 - cv 2 dot
356:34 - bounding rect on
356:37 - our approximate variable or approx
356:39 - variable which is going to give us the x
356:42 - and y
356:43 - coordinate and within height
356:45 - now based upon the width and height we
356:48 - can find out the aspect ratio so we are
356:51 - going to just say s
356:53 - pact
356:54 - ratio
356:57 - is equal to
356:59 - float first of all we need to type cast
357:02 - uh
357:03 - the width into a float so we're going to
357:05 - just say
357:07 - float w divided by height and this will
357:11 - be
357:11 - the aspect ratio of the rectangle now if
357:16 - this aspect ratio let's print out the
357:20 - aspect ratio also so we know what aspect
357:24 - ratio
357:25 - uh we are getting using the the
357:28 - rectangle or the square and we are going
357:30 - to just say if this aspect ratio is
357:33 - between uh
357:35 - 0.95
357:37 - and
357:39 - 1.05 then it's going to be a square
357:42 - right because
357:44 - the width and height are almost same
357:48 - okay so we just give
357:51 - some room for some noises that's why we
357:55 - are providing here ideally it should be
357:57 - a one aspect ratio should be one
358:00 - in order to have a square but let us say
358:04 - we are just
358:06 - approximating so
358:08 - we can just say if
358:10 - it's 0.9
358:12 - if it's greater than 0.95
358:15 - and
358:17 - if it's uh less than so aspect ratio is
358:21 - less than or equal to
358:24 - 1.05
358:26 - then it's a square okay in ideal
358:29 - situation you might want to give here
358:32 - one but in images it can be a little bit
358:36 - different so we are just giving
358:39 - this limit so if
358:40 - the aspect ratio falls in this limit
358:43 - then it's going to be
358:46 - square otherwise it's going to be a
358:48 - rectangle right and i'm going to just
358:51 - say that
358:53 - if this is the case then it's going to
358:55 - be
358:56 - uh square otherwise so in the else
358:59 - condition so let's
359:00 - give this
359:02 - else condition here
359:04 - else it's going to be a rectangle so
359:07 - let's uh
359:09 - print rectangle
359:11 - in
359:12 - the put text okay
359:14 - so this is the code which we have
359:19 - written and now finally what we are
359:22 - going to do we are going to just uh show
359:25 - the
359:26 - shapes image
359:28 - including all the contours and the text
359:31 - which we have put on these shapes so
359:35 - let's run this code and let's see if it
359:37 - works or not
359:39 - so you can see now
359:41 - it's going to
359:42 - work like this
359:44 - so all the contours are drawn across
359:48 - these shapes and you can see
359:51 - the text on top of these shapes so
359:55 - circle rectangle
359:57 - pentagon star triangle and squares what
360:00 - you can also do here is you can just
360:03 - change uh this
360:05 - text position
360:08 - using
360:08 - the x and y coordinates so let's say i
360:12 - just want to change this y position
360:15 - to just little bit top of the shape so i
360:20 - just added the minus five offset here in
360:23 - the y axis and now you can see it goes
360:26 - little bit up this text right so now
360:31 - it's much visible this text
360:35 - and you can see rectangle and square
360:37 - text is not going up because we have
360:40 - declared the local x and y here also so
360:44 - we can just say x
360:47 - 1 and a y 1 here and then run this code
360:50 - once again and you can see this
360:52 - rectangle and square text is also moved
360:56 - little bit up so i think the offset of
360:59 - five
361:00 - is okay to show these uh text on top of
361:05 - these shapes
361:07 - so this is how you can detect
361:10 - simple geometric shapes
361:13 - using opencv
361:14 - in this video we will discuss about
361:16 - histograms in opencv
361:20 - so what is a histogram
361:22 - so you can consider histogram as a graph
361:25 - or a plot which gives you an overall
361:28 - idea about the intensity distribution of
361:31 - an image
361:33 - so let me give you some examples and
361:36 - then i will be able to explain you
361:38 - better how histogram works and why they
361:42 - are useful
361:44 - so to start with i have this example
361:47 - which
361:48 - is a very normal example here i'm
361:51 - creating 200 by 200 pixel image using a
361:55 - numpy zeros which essentially mean that
361:59 - we are going to get a 200 by 200 pixel
362:03 - image of black pixels
362:06 - so let me uh just uh
362:08 - just start this uh
362:11 - example and you can see
362:13 - uh
362:14 - this is the final result so all the
362:18 - pixels here in this image
362:20 - are black and the size is 200 by 200
362:25 - now let's say we want to calculate or
362:28 - find out the histogram
362:31 - of this image so there are several ways
362:34 - of finding out histogram of an image so
362:38 - let's see uh them one by one
362:40 - so first of all we are going to find out
362:43 - the histogram using the matte plot lib
362:46 - uh because uh
362:48 - the plot using matplotlib you can draw
362:52 - easily
362:53 - so let's use that first of all so for
362:56 - that what i'm going to do is i'm going
362:58 - to use plt because i have already
363:01 - imported this matplot
363:04 - library as plt so pltr dot hist there is
363:09 - a function called
363:11 - plt.hist
363:13 - which calculates the histogram
363:16 - of an image and because it's
363:18 - just a grayscale image or it's just a
363:21 - black image
363:22 - so
363:23 - it's
363:24 - easier to find out the histogram so you
363:27 - what you can do here is the first
363:30 - argument here will be
363:32 - your image or your
363:34 - source so
363:36 - i'm going to just say image
363:38 - dot ravel okay so there is a method
363:42 - called
363:43 - ravel the second argument here will be a
363:46 - maximum number of
363:48 - pixel values so i'm going to just say
363:50 - 256
363:52 - the third argument here will be the
363:54 - range so the range will vary from
363:58 - 0 to 256
364:00 - okay so this is all you need to find out
364:03 - the histogram using the matplot lib
364:07 - and you just need to show this plot in a
364:11 - matplotlib window so you can just say
364:14 - plt dot show
364:17 - so that's it so let's run this code and
364:20 - let's see what happens
364:22 - so you see this plot using matplotlib
364:26 - and also our original
364:29 - image so as
364:31 - we have created the image
364:34 - of 200 by 200 pixel of black pixels so
364:39 - all the intensity of this graph you can
364:43 - see is zero so you can see here 200
364:47 - multiplied by 200 is equal to
364:50 - 40 000 so these are the number of pixels
364:54 - so on the y-axis you will see total
364:56 - number of pixels and here the intensity
365:00 - so intensity starts from
365:03 - 0 to 256. so this graph is showing how
365:07 - many number of pixels inside an image
365:11 - which have
365:12 - this uh pixel values so in our example
365:16 - all the pixels inside this image have
365:18 - the pixel value zero that's why
365:21 - this graph is like this so
365:24 - all the 40 000 pixels inside the image
365:28 - have the pixel value 0. so you will get
365:31 - this type of
365:33 - histograms so once again
365:36 - the histogram is a graph or a plot which
365:40 - gives you the overall idea about the
365:43 - intensity distribution of an image
365:47 - now histogram is just another way of
365:50 - understanding the image
365:51 - by looking at the histogram of an image
365:54 - you can get the intuition about the
365:57 - contrast brightness
365:59 - intensity distribution etc now let's uh
366:02 - improve this example which we have so
366:05 - i'm going to just close uh this window
366:09 - and let's say i want to add some
366:12 - white pixel also inside this image so
366:15 - what i'm going to say is i'm going to
366:17 - just cv2 dot
366:20 - rectangle so i'm going to just
366:22 - add the rectangle inside this image and
366:26 - the source here will be
366:28 - the img variable
366:30 - then where i want to introduce this
366:33 - rectangle so i want to introduce this
366:35 - rectangle at this point which will be
366:39 - let's say which starts from
366:42 - 0 comma 100
366:44 - and the second point here will be let's
366:46 - say 200 comma 200 okay so this will be
366:50 - ah 200 and the next value here will be
366:54 - the color so let us say we want to add
366:56 - the
366:57 - white pixels so this will be
367:00 - 255 which will be the maximum value and
367:04 - then the next argument will be the
367:06 - thickness so i am going to just say -1
367:08 - which will fill
367:09 - this rectangle inside this image
367:13 - so when i run
367:14 - now this
367:17 - you will see
367:19 - this graph and this image so you can see
367:23 - half of this image contains black pixels
367:26 - and half of this image contains the wide
367:29 - pixels and we already know that the size
367:32 - of this image is 200 by 200 that's why
367:36 - here in the graph you will see
367:39 - 20 000 pixels are
367:43 - black which means that 20 000 pixels
367:46 - have the pixel value 0
367:49 - and 20 000 pixels have the pixel value
367:53 - 255 that's why you see
367:56 - this
367:57 - here so you can see you can easily find
367:59 - out the pixel intensity of an image
368:03 - easily using histograms
368:05 - now
368:06 - next we are going to add some more
368:09 - pixels into this image
368:11 - and this time what we are going to do is
368:14 - we are going to add the rectangle
368:16 - inside the same image so lets say it
368:19 - goes from 0 comma 50 to
368:22 - 100
368:23 - comma 100
368:25 - and the
368:26 - color here we are going to provide the
368:29 - pixel value of
368:31 - 127 let's say okay so which is the half
368:35 - of uh
368:36 - 0 and 255
368:38 - approximately so i'm going to run this
368:41 - uh
368:43 - example once again
368:45 - and now you will see this kind of image
368:48 - so you can see half of the pixels here
368:51 - are white that means 20 000 pixels have
368:54 - the pixel value of 255 so you can see
368:57 - here
368:58 - now
368:59 - around 15 000 pixels here
369:02 - in the half of this image have the pixel
369:05 - value of zero that's why you can see uh
369:08 - this
369:09 - line here and we have added
369:12 - the rectangle of uh pixel value 127 also
369:17 - so around
369:18 - you can see around 5000 pixels
369:22 - here have the pixel value of 127. so
369:26 - this is how uh the histogram is going to
369:29 - uh work so let's use now the original
369:32 - image so some kind of uh image instead
369:35 - of this black or white image so now what
369:39 - i'm going to do is i'm going to uh just
369:42 - once again declare a variable and then
369:44 - i'm going to just say cv2 dot
369:48 - im
369:49 - read
369:50 - and we are going to read some files so
369:53 - let's say i have this
369:56 - lenna.jpg image so i'm going to just uh
370:00 - read that
370:02 - i hope the extension is correct jp g
370:06 - and we are going to read this image in
370:08 - the grayscale mode so i'm going to just
370:10 - say 0 here
370:12 - and now i'm going to run this
370:16 - example once again
370:17 - and you can see this lena image is
370:21 - loaded in the grayscale mode
370:24 - and here
370:25 - is the histogram of this image so these
370:29 - are all the pixel
370:31 - intensities inside this image so you can
370:34 - see from this uh graph that most number
370:38 - of pixels
370:40 - contained inside this image have the
370:43 - pixel value around 150. now you can also
370:47 - find out
370:48 - the pixel intensity of different colors
370:52 - so
370:53 - till now we have been just using uh the
370:56 - grayscale mode or black or white uh
370:59 - pixels but you can also
371:03 - use the same histogram
371:05 - for the bgr values also so let's see how
371:09 - we can undo that
371:11 - so what
371:12 - we are going to do is let me just remove
371:15 - this code or i am going to just leave it
371:17 - commented
371:19 - and here i am going to just say b comma
371:23 - g comma
371:24 - r
371:25 - and there is a method we have already
371:28 - seen which is called cv
371:30 - dot split
371:32 - which is going to split
371:34 - your
371:35 - image
371:36 - into bgr values so we are going to just
371:40 - give the source which is
371:42 - our image
371:44 - and then if you want to uh show
371:47 - these bgr values you can
371:50 - just show in
371:52 - the i'm sure window so bg
371:55 - [Music]
371:56 - r and here also
371:58 - b
372:00 - g
372:00 - [Music]
372:01 - and r and when you want to uh
372:05 - show the histogram of bgr values then
372:09 - also you can use matplotlib dot hist
372:13 - method you just need to change this
372:16 - source from image to bgr so b
372:20 - g
372:21 - and
372:22 - r
372:23 - okay
372:24 - so now what we are going to do is we are
372:28 - going to run our code and let's see what
372:30 - happens
372:33 - so it's giving me uh this error because
372:36 - i'm reading this image in the grayscale
372:38 - mode so i'm going to
372:40 - remove
372:42 - this extra parameter from i am read
372:45 - because we want to read this
372:48 - image in the color form and then only we
372:50 - will be able to get the bgr channels
372:53 - right in the gray scope scale mode there
372:55 - are no bgr channels so i'm going to run
372:58 - this script once again
373:00 - and let's see what happens so you can
373:03 - see this histogram of blue channels and
373:06 - green channels and the red channels and
373:09 - these are the images which are loaded in
373:12 - these different uh channels so this is
373:15 - the
373:15 - image which is loaded in the
373:18 - blue channel and this is the green and
373:22 - this is the red channel and you can see
373:25 - uh the histogram of each channel
373:27 - differently
373:28 - using uh matplotlib so let me just
373:33 - close all these windows
373:35 - now there is a method in cv2 also which
373:39 - is called calc hist which is going to
373:42 - give you the histogram
373:43 - of an image so for that what you can do
373:48 - is i'm going to uh just
373:51 - just uh comment all uh the this code
373:55 - because i just want to show how you can
373:57 - use the
373:58 - cv2
374:00 - calc hist
374:01 - method okay
374:03 - so
374:04 - what you can do is you can use
374:08 - a
374:09 - method so let's say
374:12 - hist
374:13 - and then
374:14 - cv
374:16 - dot
374:17 - calc hist and this method takes a few
374:20 - arguments
374:22 - so the first argument here will be the
374:24 - image so it's the source which you give
374:28 - but the only special thing is you just
374:31 - give this image in the square brackets
374:35 - okay
374:35 - the second argument here is the channel
374:38 - so it is the index of channels for which
374:41 - we calculate the histogram
374:44 - so here in our case because we are going
374:48 - to uh read the image in grayscale mode
374:51 - we can just give the channel 0 here so
374:55 - for one channel you can give 0 here for
374:57 - different channel you can give 0 1 2
375:01 - value the next argument here is the
375:03 - image mask so to
375:05 - find histogram of full image it is given
375:09 - as a none because our
375:12 - because our image is loaded in the
375:15 - gray scale mode so we can give here none
375:19 - the next value is the hist size so this
375:23 - his size is the representation of bin
375:27 - counts and this is also given in the
375:30 - square bracket so we are going to just
375:32 - say 256 here
375:34 - the next argument is the range so range
375:38 - will vary from 0 to 256 so minimum and
375:42 - the maximum range of the x-axis you can
375:45 - say so 256
375:48 - and then we can just show this
375:52 - hist or histogram
375:54 - inside the plt so
375:58 - plt dot plot method so dot
376:02 - plot
376:03 - and then we can just give this
376:08 - histogram value here okay so let's run
376:11 - this code and let's see what happens
376:13 - so you can see
376:14 - you get the histogram
376:17 - of this image using the opencv calc hist
376:21 - method
376:22 - and what are the uses of the histogram
376:25 - so a histogram can tell you whether or
376:28 - not your image has been properly exposed
376:31 - so when you take a digital image
376:34 - it's very useful it it can also tell you
376:38 - whether the lighting conditions were
376:41 - flat or harsh when you took that image
376:44 - and using the histogram you can also
376:47 - make the adjustments uh which will work
376:50 - best for your digital images so this uh
376:54 - the usefulness of the histograms we will
376:57 - see in the later videos this was just
377:00 - the basics about the histograms in
377:03 - opencv
377:05 - in this video we will discuss about
377:07 - template matching in opencv
377:10 - so first of all what is template
377:12 - matching
377:14 - so template matching is a method of
377:16 - searching and finding the location
377:19 - of a template image
377:21 - inside a larger image
377:24 - in opencv
377:26 - there is a method called match template
377:29 - for achieving this purpose
377:32 - so let's get started and let's see an
377:34 - example about it
377:36 - so i have this simple code which just
377:40 - loads this image and let's see uh what
377:43 - this image looks like so this is the
377:47 - image and this is the messy image
377:50 - and what i want to do is i want to match
377:54 - the face template which i have which
377:58 - looks like this which is
378:00 - the smaller
378:02 - template which is also
378:05 - available inside this image so this will
378:08 - act like a template for us and we will
378:12 - try to find
378:13 - this template inside this larger image
378:17 - so let's get started and let's see how
378:19 - we can search this template inside this
378:22 - larger image
378:24 - so
378:25 - first of all what we need to do is
378:27 - obviously we need to load
378:31 - this image and also load our template
378:35 - so before loading our template image
378:39 - i'm going to just convert my original
378:42 - image which is the larger image
378:44 - into the
378:46 - grayscale image so i have declared this
378:48 - variable gray underscore image and then
378:51 - i'm going to just say cv 2 dot cvt
378:56 - color
378:57 - which is going to
378:59 - convert my image img
379:02 - and let's convert this image into cv2
379:05 - dot
379:06 - color
379:07 - underscore bgr to gray
379:09 - now
379:10 - let's load our
379:13 - face image
379:15 - which is called messy underscore face
379:18 - dot jpg so i'm going to
379:21 - just change this name merge c underscore
379:24 - face dot jpg and this will be
379:28 - our face image or you can also say this
379:32 - is a template and i'm going to also load
379:34 - this image
379:36 - as a grayscale image so i'm going to
379:38 - just pass the second argument in the
379:40 - read method
379:41 - as 0 which is going to load this messy
379:44 - image as a grayscale image now in the
379:46 - next line we will simply
379:49 - use this method which is called match
379:52 - template and we are going to
379:56 - save it into some variable so
379:58 - we can just say res
380:02 - is equal to cv2 dot
380:05 - match
380:06 - template which is this method which
380:08 - takes few argument
380:10 - first is our
380:12 - image so i'm going to pass our grayscale
380:15 - image here
380:16 - the second argument here will be the
380:18 - template which we are trying to search
380:21 - inside this image
380:23 - so this will be our template
380:25 - the third is the method
380:28 - so the method can be a
380:32 - several method there are several methods
380:35 - available for the template matching so i
380:38 - want to show you these method
380:40 - for the template matching so you can see
380:43 - a type of template matching operations
380:46 - and there is separate formula involved
380:50 - in order to match that template inside
380:53 - that image so so for now we are going to
380:56 - use this method which is a tm underscore
380:58 - c co f underscore
381:01 - normed
381:03 - dot tm
381:05 - underscore c co f
381:07 - norm which is this method now let's try
381:10 - to print this result and let's see what
381:13 - is the content inside this result so i'm
381:16 - going to
381:17 - just print the content inside this
381:20 - result which we got so i'm going to run
381:23 - this code and this image is loaded but
381:26 - for now we are interested in
381:29 - this
381:31 - array matrix which you are seeing here
381:34 - so you can see when you observe these
381:37 - values carefully
381:40 - you will see uh all are
381:42 - relatively uh smaller values so
381:46 - you can see uh
381:48 - 0.2 0.2 almost
381:51 - every value is around
381:54 - until 0.3 so the maximum value i can see
381:57 - here is 0.3 so let me just show this
382:01 - image once again and the
382:03 - the template also
382:05 - so what this result contains is these
382:09 - all values
382:10 - and there will be
382:13 - one value which contains
382:16 - the number for example 0.8 or
382:21 - the brightest point okay so if
382:25 - here this
382:27 - matrix contains a value
382:29 - which have the value 1 it is the
382:32 - brightest point and it will be there
382:35 - inside this image after applying this
382:37 - match template method which will be
382:40 - around this point at this point
382:42 - at which uh
382:44 - this template matches so top left corner
382:48 - of this template so at the point at
382:51 - which this left top corner of this image
382:55 - will match inside this large image
382:58 - there will be a brightest point there
383:02 - and that brightest point will be
383:04 - reflected inside
383:06 - this
383:07 - image in the form of this decimal number
383:12 - and all the other values will be
383:14 - slightly uh
383:16 - darker darker values okay so that's how
383:20 - this matrix from this matrix we will
383:23 - come to know the the top left corner of
383:26 - the template inside this larger image
383:30 - so now how can we filter out
383:34 - that value which is the brightest point
383:37 - inside this matrix so all the points
383:41 - you can see looks like
383:43 - under 0.3 but there are some points here
383:47 - you can see three dots and there are
383:50 - thousands and thousands of values
383:53 - will be available here all the values
383:56 - are not printed okay
383:59 - so what we are going to do is we are
384:01 - going to try to find out the brightest
384:04 - point so this we can find out with the
384:07 - numpy method uh there is a method called
384:10 - where
384:11 - using which we can find out uh or filter
384:15 - out those values which are greater than
384:18 - certain number so i'm going to uh first
384:21 - of all
384:22 - declare a variable called
384:25 - threshold is equal to i'm going to
384:27 - declare the value of threshold initially
384:30 - as 0.8 which will be
384:33 - relatively brighter point
384:36 - inside the
384:37 - matrix which we are getting using this
384:40 - result uh
384:42 - variable right and then there is a
384:45 - method called uh
384:47 - where numpy where so i'm going to
384:49 - declare once again
384:50 - l o c variable and then p dot
384:54 - where method and here we are going to
384:57 - pass
384:58 - our
384:59 - result which we got and we are going to
385:02 - filter out using
385:04 - this expression so this will be a
385:07 - boolean expression so i'm going to just
385:09 - say
385:10 - give me all those values which are
385:13 - greater than
385:14 - or equal to the threshold inside this
385:18 - result matrix okay so this
385:22 - where method is going to uh
385:24 - just evaluate this expression each and
385:27 - every value will be evaluated and if
385:30 - this value inside the matrix
385:33 - is greater than
385:35 - 0.8 which is our threshold then it's
385:38 - going to uh give those values to us so
385:42 - let's print out those values after the
385:45 - filtering out of most of the values
385:48 - and let's uh just print this loc
385:51 - variable also so i'm going to run this
385:53 - code once again
385:55 - and
385:56 - you can see here
385:58 - this is the matrix which we
386:01 - got
386:02 - so you can see this is the array which
386:04 - we got so still
386:06 - we can increase this threshold in order
386:08 - to find out only
386:10 - one point so there are several points
386:13 - available here so let's say i'm going to
386:16 - increase this
386:18 - value to 0.9 and let's run this code
386:21 - again
386:22 - and you will see
386:24 - only two points 85 and 220 so this is
386:28 - what we were expecting so we wanted to
386:31 - find out uh this point which will be the
386:34 - brightest point
386:36 - uh
386:37 - inside this
386:38 - result matrix so once we got the
386:41 - brightest point uh which will be around
386:44 - here which will be the top left corner
386:48 - as i said of this template
386:51 - and it will be located somewhere here in
386:53 - the original image
386:55 - then we can draw the rectangle
386:58 - around this
387:00 - original image
387:02 - same as the size of this template so
387:05 - this will be the easier task because we
387:08 - already know
387:09 - the width and height of
387:12 - this template we already know how to get
387:15 - the width and height of this template
387:17 - and same size rectangle we just want to
387:21 - draw on this original image so let's see
387:25 - how we can do this
387:27 - so
387:28 - there is already a method so i'm going
387:31 - to just declare two variables width and
387:34 - height
387:35 - and you already know uh the method so
387:39 - template dot shape is going to give you
387:42 - the shape of
387:44 - your
387:45 - image right so i'm going to just say
387:48 - template dot shape
387:51 - and then inside the square brackets we
387:54 - are going to just give two colons and
387:57 - minus one this means
387:59 - that we want to get the
388:02 - column and the
388:04 - rows value in the reverse order so width
388:07 - and height that's why
388:09 - uh i have given this minus one
388:12 - index here now in the next step what we
388:15 - are going to do is uh we are going to uh
388:18 - just draw all the rectangles
388:20 - uh where the template is is matched so
388:24 - uh
388:25 - by seeing this template image and the
388:28 - original image we know that there is
388:30 - only one messy face inside this image
388:34 - but let's say there are several number
388:36 - of uh
388:38 - matched templates inside our original
388:41 - image
388:42 - for that we need to iterate over
388:45 - the result which we got after applying
388:48 - the filter on the result
388:51 - so for that we are going to just iterate
388:54 - over
388:55 - that result in our case as we know that
388:58 - there is only one point
389:00 - so
389:01 - we don't even need to iterate over it
389:04 - but
389:05 - if there are multiple number of meshed
389:08 - templates then this
389:11 - for loop will be uh
389:13 - handy so for uh
389:15 - pt
389:17 - in
389:17 - your
389:20 - loc variable so we are going to just say
389:23 - zip which is going to iterate over
389:26 - this loc variable so s tricks
389:29 - l o c
389:31 - and then we are going to
389:33 - find out the width and height here also
389:36 - so we are just reversing
389:39 - the
389:40 - x axis and y-axis right so we are going
389:43 - to just say
389:45 - colon cool minus 1 here
389:48 - and then
389:48 - once again inside this for loop so cv 2
389:52 - dot
389:53 - rectangle method and the first argument
389:56 - here will be our original image because
389:58 - we want to draw the rectangle on the
390:00 - original image
390:02 - the second
390:03 - argument will be the first point
390:06 - of the rectangle so the first point will
390:09 - be this one pt which we
390:12 - are getting using the loc uh
390:16 - variable so as you all know that the
390:19 - first point here
390:21 - will be the top left corner
390:24 - of the rectangle and the second point
390:27 - here will be
390:28 - the bottom right corner so how can we
390:31 - get the bottom right corner
390:33 - we will get the bottom right corner
390:36 - using this pt uh
390:39 - variable and then
390:42 - on the zeroth index we are going to just
390:45 - add the width
390:47 - comma
390:48 - on the first index so pt
390:51 - uh
390:52 - square bracket first
390:54 - we are going to add the height
390:57 - okay
390:58 - so essentially we have just found out
391:02 - the width and height of our template and
391:05 - we are getting the second point
391:08 - using this addition on the first point
391:12 - width and height so it's going to give
391:14 - us this bottom right uh corner of this
391:18 - template or this point so this is how we
391:21 - are getting uh our two points to draw
391:25 - the rectangle
391:26 - now the third and fourth variable will
391:29 - be a simple which are the color so you
391:32 - can just say 0 comma 0 comma
391:36 - 255 which will be the green color
391:39 - and the width let's say 2 here so we
391:42 - want to give the width 2 here so let's
391:45 - run this code and let's see what happens
391:47 - so i am going to run this code and you
391:49 - can see
391:50 - this red
391:52 - rectangle is drawn
391:54 - on the face of the messy and you can
391:58 - here also see
391:59 - this rectangle will match our template
392:03 - image so whatever
392:05 - image is inside this rectangle will be
392:09 - exactly same as our template and once
392:13 - again you can see the result let me
392:15 - explain
392:16 - this code once again
392:18 - so if
392:20 - this point this threshold will be uh
392:24 - 0.08 let's say in the case of 0.09
392:28 - threshold we are only getting two values
392:31 - uh this 85 and 220 right that's why we
392:36 - are seeing
392:37 - the clear rectangle here
392:40 - when we are
392:43 - giving the threshold 0.8 here let's see
392:46 - what happens so i'm going to run this
392:48 - code once again you can see there will
392:50 - be
392:51 - this rectangle but it will be much
392:54 - thicker
392:55 - why it's
392:56 - much thicker because we are getting
392:59 - several number of values one two three
393:02 - four five six seven eight nine so we are
393:05 - getting the nine points on the x-axis
393:07 - and the y-axis so this for loop will
393:10 - iterate nine times and this rectangle
393:14 - will be drawn
393:15 - nine times on the image and that's why
393:18 - this rectangle is much thicker let's uh
393:23 - just change this value to 0.9 once again
393:26 - and you will see this rectangle is
393:30 - you know
393:31 - the single rectangle that's why it's
393:33 - much thinner right
393:36 - now
393:37 - when you give this value let's say we
393:39 - give the value
393:40 - 0.3 so
393:43 - most of the point as you can see here
393:45 - have the value 0.3 and when we run this
393:49 - code
393:50 - you will see so many rectangles here so
393:53 - that's why this thresholding is
393:56 - essential for us to find out the
393:58 - brightest point or the value which have
394:02 - the maximum value right so that's why we
394:06 - were filtering out
394:08 - this these points and finding out
394:12 - the values more than 0.9
394:16 - threshold
394:18 - and
394:19 - about the method so let's try different
394:22 - methods so let's try to give different
394:25 - methods here these two method behave
394:27 - little bit differently
394:29 - so uh we can start with this uh
394:33 - tm c
394:35 - c o r r
394:37 - normed
394:38 - and uh we can apply it here and it's
394:40 - going to give us uh this kind of uh
394:43 - result you can see we are getting uh
394:46 - several uh points here after filtering
394:48 - so let's try to
394:51 - increase this value to
394:54 - 0.95 and let's rerun this code
394:57 - and let's see what happens so now you
394:59 - are getting
395:01 - four values
395:03 - you can
395:04 - also filter that out let's say 0.99
395:10 - now let's see what happens so now you
395:12 - are getting
395:13 - only two values okay so you need to uh
395:16 - try to change this value to the maximum
395:20 - point so try to change this value and
395:22 - you will get
395:24 - uh this kind of rectangle only one
395:26 - rectangle so every method is going to
395:29 - give you different
395:32 - result and that's why you need to
395:36 - try all the result not all the result
395:38 - will give you the perfect
395:41 - rectangle or template matching so you
395:43 - need to try
395:44 - different methods on your
395:47 - images so this is how you can do
395:49 - template matching in opencv in this
395:52 - video we will understand the concept
395:54 - behind half transform
395:57 - so first of all what is half transform
396:00 - so half transform is a popular technique
396:03 - to detect any shape if you can represent
396:06 - that shape in a mathematical form
396:09 - half transform can detect the shape even
396:12 - if it is broken or distorted a little
396:15 - bit
396:16 - now this explanation might seem a little
396:20 - bit confusing so let me explain it by an
396:23 - example
396:24 - so let's say you have an image of this
396:28 - road and you want to detect these lane
396:32 - lines in
396:34 - this road image so the first step in
396:36 - order to detect these lane lines in this
396:40 - road is to find the edge pixels using
396:45 - kenny edge detection or any other edge
396:47 - detection method now after you found out
396:50 - the edges using any edge detection
396:53 - method you want a geometrical
396:57 - representation of that edge and in order
397:00 - to find out the geometrical
397:02 - representation for example you want to
397:05 - find out the slope of this edge or its
397:08 - intercept you can use half transform to
397:12 - represent these pixels or edges
397:16 - in the mathematical or geometrical form
397:19 - so after you find out the edges
397:22 - using any edge detector you just have
397:25 - the sequence of pixels so you can loop
397:28 - through all the pixels and somehow
397:31 - figure out the slope and intercepts but
397:34 - it's a very difficult task so we want
397:37 - some mechanism that gives more weightage
397:40 - to pixels that are already in line
397:44 - and this is what we can achieve using
397:47 - half transform so let's begin and let's
397:50 - start with the lines so a line in the
397:53 - image can be represented by two
397:56 - coordinate systems
397:57 - first is using the cartesian coordinate
398:01 - system
398:02 - and using this equation you can
398:04 - represent a line which is y is equal to
398:07 - m x plus c and you can also represent
398:11 - this line using polar coordinate system
398:15 - using this equation which is x
398:17 - cos theta plus y sine theta
398:20 - is equal to r or rho sometimes
398:24 - so let's start with uh this equation
398:27 - first which is a cartesian coordinate
398:30 - system equation which is y is equal to
398:33 - m x plus c
398:35 - so uh
398:37 - when you represent a line
398:39 - in x and y coordinates which is also
398:42 - called the x y space
398:46 - this equation looks like this so y is
398:48 - equal to mx plus c
398:50 - where m is the slope of the line
398:53 - and c is the intercept of this line so
398:57 - if you know the values of m and c
399:00 - you can represent this line in the x and
399:03 - y coordinates now in half transform you
399:06 - can represent this line in other form
399:10 - also and this is called the m c space or
399:13 - the half space
399:15 - so
399:16 - using this equation when you take m on
399:20 - this axis and c on this vertical axis
399:25 - then this is
399:26 - called the m c space so earlier we have
399:29 - represented this line in the x y space
399:32 - and now we are saying that we want to
399:34 - represent this
399:36 - using the m c coordinate where m
399:39 - is on the horizontal line and c
399:42 - is on the vertical line
399:44 - so when you represent
399:46 - this
399:47 - simple line
399:49 - in the mc space or the half space
399:52 - it can be represented as a point so this
399:55 - line can be represented as a point so we
399:59 - all know that a line is a collection of
400:01 - points and managing the collection of
400:04 - points
400:05 - is tougher than managing a single point
400:07 - so if you want to manage a collection of
400:11 - point and if you were to manage a single
400:14 - point which will you prefer and then
400:17 - obvious answer will be to manage the
400:19 - single point and this is what this mc
400:22 - space is doing
400:24 - it's representing a line in the form of
400:28 - a point in mc space or the half space
400:32 - and the opposite of this concept is also
400:35 - possible so if you can represent a point
400:38 - using these coordinate in the x y space
400:42 - then it can be represented as a line in
400:45 - the m c space okay and the formula now
400:50 - will turn into uh this equation which is
400:53 - c is equal to minus x a m
400:56 - plus y a right so you can represent a
401:02 - point and if you have the x and y
401:04 - coordinate in the mc space you can
401:07 - represent this as a line
401:11 - and this will be the equation where x
401:14 - will be the slope now and y will be the
401:18 - intercept earlier m was the slope and c
401:22 - was the intercept but when you just
401:25 - transform or
401:27 - just represent this point into m c space
401:31 - then your x becomes or minus x becomes
401:35 - the slope and
401:37 - y becomes the intercept so how does
401:40 - these concepts are going to help us
401:43 - so the half transform is all about doing
401:47 - what we have learned converting points
401:51 - in the x y space to the lines in the m c
401:54 - space or the half space
401:57 - so for example you can see four points
401:59 - one
402:00 - two three four
402:02 - which are joined by a line right so you
402:06 - can represent these four points and you
402:10 - can join all these four points and it's
402:13 - our representation of a line
402:16 - and here slope is equal to m and
402:19 - intercept is equal to c in the x y space
402:23 - the same
402:25 - line you can represent in the m c space
402:28 - uh using these four lines okay so every
402:32 - point is a line in the mc space
402:36 - and you see the intersection point here
402:39 - which is on the mc coordinate so you
402:43 - have taken an edge detected image
402:46 - and for every point that is a non-black
402:50 - point you draw lines in the mc space
402:54 - and obviously when you draw these lines
402:56 - these lines will intersect with
402:59 - each other and these intersections mark
403:02 - are the parameter of a line okay so in
403:06 - the mc space you can represent each and
403:09 - every point
403:10 - as a line and they will intersect on a
403:15 - single point and now this intersection
403:18 - point can be used to uh draw a line so
403:22 - this was the representation of points in
403:25 - a line
403:26 - using mc space using a cartesian
403:29 - coordinate system
403:31 - now let's apply these same concepts
403:34 - which we have learned using the
403:36 - cartesian coordinate system
403:38 - uh into a polar coordinate system so
403:42 - as we all know that in the polar
403:45 - coordinate system we can represent a
403:48 - line using this equation also which is r
403:52 - is equal to x multiplied by cos theta
403:55 - plus y multiplied by sine theta
403:58 - or in other form you can also represent
404:01 - this
404:02 - equation like this where y is equal to
404:06 - minus
404:07 - cos theta by sine theta multiplied by x
404:10 - plus
404:11 - r divided by sine theta so this is your
404:15 - x y space where line can be represented
404:19 - like this
404:21 - and we are going to transform or
404:23 - represent
404:25 - this line using this equation
404:28 - into
404:29 - the r theta space or
404:32 - the half space okay so this line using
404:35 - this equation can also be represented as
404:38 - a point in r theta or the half space
404:42 - like this
404:43 - so let's take an example about this
404:47 - so as i said the equation was r is equal
404:50 - to x multiplied by cos theta plus y
404:53 - multiplied by sine theta where this
404:56 - theta is the angle of the line and r is
404:59 - the distance from the origin to the line
405:02 - so let's say we
405:04 - want to represent a point
405:06 - which is from x y space
405:09 - into a half space into r theta space so
405:13 - we give the values of x 0 and y 0 which
405:16 - will be the first point we can represent
405:20 - this point in the form of line in the
405:23 - half space or the r theta space
405:27 - in this formation which looks like a
405:30 - sine
405:31 - curve using this equation so this is for
405:35 - the one point representation
405:38 - in xy space to
405:40 - a line representation in the half space
405:43 - so let's say you have a multiple points
405:45 - so we take three points
405:48 - then uh it's going to look like this so
405:51 - let's say x 0 is equal to 8 and y 0 is
405:54 - equal to 6 x 1 is equal to 4 y 1 is
405:57 - equal to 9 and x 2 is equal to 12 and y
406:00 - 2 is equal to 3 so we have
406:03 - three points in the xy space
406:07 - they can be represented in the half
406:09 - space using three lines and as we have
406:12 - seen in the cartesian coordinate system
406:15 - these points can be represented in uh
406:19 - these lines in the half space in the
406:22 - polar coordinate system also using these
406:25 - curved line and this intersection is
406:29 - going to represent
406:30 - a line
406:31 - in the half space so which
406:34 - representation we are going to use in
406:36 - order to use the half transform so this
406:39 - equation is not able to represent the
406:42 - vertical lines that's why
406:45 - generally we use
406:46 - this equation or a polar coordinate
406:49 - system in order to use a half transform
406:53 - so the half transform algorithm
406:56 - involves these four important steps
406:59 - in the first step edge detection is done
407:01 - using canning edge detector or any edge
407:04 - detection method and the second step
407:07 - mapping of the edge points to the half
407:09 - space
407:10 - is done and all these edge points are
407:13 - stored in an accumulator and the third
407:16 - step interpretation of accumulator to
407:19 - yield lines of infilite length is done
407:22 - and this interpretation can be done by
407:24 - thresholding or any other constraint the
407:28 - fourth step involves the conversion of
407:31 - infinite line to finite lines now opencv
407:35 - implements two kind of half line
407:37 - transforms the first is the standard
407:40 - half transform which is done using huff
407:44 - lines method the second type is the
407:48 - probabilistic half line transform which
407:51 - is done by half lines p
407:54 - method so this is the half lines method
407:57 - and this is the half lines p method in
408:00 - the last video we have seen
408:02 - a brief theory introduction about half
408:05 - line transform
408:07 - so i have told you that opencv
408:09 - implements two kind of half line
408:12 - transforms
408:13 - one is a standard half line transform
408:16 - using half lines method
408:18 - and the second is the probabilistic half
408:22 - line transform
408:23 - using half lines
408:25 - capital p method
408:27 - so we are going to
408:29 - use the half line method
408:31 - in this video and see how we can use
408:34 - this half line method to detect the
408:37 - lines
408:38 - inside an image using half transform
408:42 - now i also told you that there are four
408:45 - steps associated with half transform so
408:50 - the first step was the edge detection
408:52 - step using any edge detection method
408:56 - preferably kenny edge detection the
408:58 - second
409:00 - step is the mapping of edge points to
409:03 - the half space and store these edge
409:06 - point to an accumulator
409:09 - the third step was the interpretation of
409:11 - accumulator to yield lines of infinite
409:14 - length and the fourth step was the
409:17 - conversion of these lines to the finite
409:21 - lines
409:22 - so let's say we have this image
409:26 - of this
409:27 - sudoku.png and you can see all these
409:30 - lines here which we want to detect so
409:32 - this is the line and this is the line so
409:35 - all these lines we want to detect using
409:38 - the half line transform so i have
409:41 - already written this code so i'm going
409:44 - to go step by step
409:46 - to explain how this code works
409:50 - so in the first step you just need to
409:52 - import the normal cv2 and the numpy as
409:56 - np
409:57 - then here i'm just reading this image
410:00 - using i'm read method
410:02 - in the next step i'm converting this
410:05 - image into a grayscale image and storing
410:08 - it
410:09 - into this variable which is gray
410:11 - because
410:12 - for kenny edge detection it's preferred
410:15 - to have gray scale images rather than
410:19 - your normal colored images
410:22 - now in the next step we are applying the
410:26 - kenny edge detection method on this gray
410:28 - scale image so here this cv2 dot kenny
410:32 - method takes
410:33 - these arguments first argument is the
410:36 - image second and third argument is the
410:38 - first threshold and the second threshold
410:41 - so i'm giving the first threshold as 50
410:44 - and the second threshold here as
410:46 - 150 and the fourth argument here i'm
410:49 - giving aperture size is equal to three
410:53 - now in the next step
410:55 - i'm using this half lines method this is
410:59 - the normal hub transform
411:01 - method which is implemented in opencv
411:05 - now this half line method takes few
411:08 - argument the first argument is
411:11 - the image so we are just
411:15 - just passing this edge detected image to
411:19 - the first argument of this half lines
411:23 - method the second argument here is the
411:25 - row value this row value is the distance
411:29 - resolution of the accumulator in pixels
411:33 - normally it's taken as 1.
411:36 - the third value is the theta value which
411:39 - is the angle resolution of accumulator
411:42 - in radians so for that we are just using
411:46 - numpy so np dot pi divided by 180 so
411:50 - this is
411:51 - also typical in this method and the next
411:54 - argument here is the accumulator
411:56 - threshold parameter
411:59 - so what does this mean it's a threshold
412:02 - so only those lines are returned that
412:05 - get enough vote that means that those
412:08 - lines will be returned which have
412:11 - threshold greater than
412:13 - this value so starting value i have
412:15 - taken
412:16 - here as 200 as threshold so now this
412:20 - half lines method is going to return the
412:23 - output vector of lines
412:26 - now i have explained you how polar
412:29 - coordinate works
412:31 - for the half transform in the last video
412:34 - so
412:35 - these
412:36 - lines will be in the polar coordinates
412:40 - so each line is represented by two or
412:42 - three element vectors either rho and
412:46 - theta or rho theta and volts so as you
412:50 - can see this is the output vector of
412:53 - lines so i'm going to iterate over each
412:56 - and every uh line vector and what it
413:00 - gives is the first element of this
413:04 - line is going to give you these two
413:07 - values rho comma theta it's going to
413:10 - give you rho comma theta or rho comma
413:12 - theta comma vote right so right now i'm
413:17 - using just two parameters here row comma
413:20 - theta
413:21 - so rho is the distance from the
413:24 - coordinate 0 comma 0 which is the top
413:28 - left corner of the image
413:30 - and the theta is the line rotation angle
413:34 - in radians so all this rho and theta i
413:38 - have explained you in the last video and
413:40 - we have seen how we can represent
413:43 - these row and theta values in the half
413:47 - space
413:48 - so first of all what we are going to do
413:50 - is once we get the row and theta value
413:54 - is we are going to uh
413:56 - just get the cos theta value and the
413:59 - sine theta value because we want to
414:02 - convert these polar coordinates into the
414:05 - normal cartesian coordinates
414:08 - for the line method because this line
414:11 - method as you uh can imagine takes uh
414:14 - these coordinates right which are the
414:17 - cartesian coordinates so this is the
414:19 - point one parameter and this is the
414:22 - point two parameter so x1 y1 and x2 y2
414:26 - so first of all we are just getting the
414:29 - cos theta value and theta here is this
414:32 - theta
414:34 - so cos theta we are just assigning to a
414:36 - and the sine theta value we are just
414:39 - assigning to b
414:41 - and we are just uh multiplying this a
414:46 - to the row so this will give us the x0
414:50 - value and the y zero value when you
414:53 - multiply b
414:54 - uh by rho so this row is this row value
415:00 - so this x0 and y0 is going to give you
415:03 - the origin which is
415:05 - zero comma zero or top left corner of
415:09 - the image
415:11 - but we want the lines
415:14 - not the top left corner of the image
415:17 - so how we can get these x 1 and y 1
415:21 - coordinate and x 2 and y 2 coordinate
415:24 - this is
415:26 - given in this equation
415:28 - so once you get your x 0 and y 0 value
415:32 - you can get the value of x 1 and y 1
415:35 - coordinate using this equation so you
415:38 - just need to type cast
415:40 - everything into
415:41 - integer
415:42 - so this equation x 1 value stores the
415:45 - rounded off value of
415:47 - rho
415:48 - as i have shown here so this r represent
415:52 - rho so rho multiplied by cos theta cos
415:55 - theta we have already uh taken in the a
415:58 - variable so we are essentially here
416:01 - multiplying
416:03 - the rho multiplied by cos theta minus
416:07 - thousand multiplied by sine theta sine
416:09 - theta value is the value of the b right
416:14 - so x0 plus 1000 multiplied by minus b
416:19 - here okay
416:21 - y1 we get using this equation so y1 is
416:25 - equal to int
416:26 - in the bracket
416:27 - y0 plus 1000 multiplied by a which is
416:31 - essentially this equation which is rho
416:34 - multiplied by sine theta plus thousand
416:38 - multiplied by cos theta so these two
416:40 - values are going to give you the first
416:43 - coordinates
416:44 - and similarly we are going to get the x2
416:46 - and y2 coordinate using these two
416:48 - equations
416:49 - so
416:50 - here everything is same
416:52 - just this
416:54 - minus is
416:56 - nu right so in this equation you just
416:59 - need to replace a plus by minus and you
417:02 - get the x2 value
417:04 - same you have to do in the case of
417:07 - y2 so in this equation if you just
417:10 - replace this plus by minus you will get
417:13 - the y to value
417:15 - and we have already seen how to use the
417:17 - cb2 dot line method it takes a few
417:21 - argument as you can see here first is
417:24 - the image so image is our original image
417:27 - second is the x1 and y1 coordinate which
417:30 - is the first point comma the second
417:32 - point so
417:33 - as you already know that a line is a
417:37 - collection of point so you need at least
417:40 - two point to create a line right so this
417:44 - is
417:44 - the coordinates of the first point and
417:46 - this is the coordinates of the second
417:48 - point
417:49 - the next argument here is the color so
417:52 - color i have taken simply zero comma
417:54 - zero comma 255 and the last parameter
417:57 - here is the thickness of the line which
417:59 - i have taken to here
418:02 - and the next line of code you already
418:06 - know i think so after this line we come
418:09 - out of the loop and we are just
418:12 - plotting all the lines using this loop
418:15 - on the original image and once we
418:18 - get all these lines on the original
418:21 - image we are just showing it using i am
418:23 - show method and
418:25 - at the last we are just destroying our
418:27 - window once we are done with the image
418:30 - so let's uh run this uh code and let's
418:33 - see what happens so i'm going to run
418:35 - this code and you can see all these
418:38 - lines are plotted here
418:41 - let's see uh the
418:44 - kenny edge detected uh image also so i'm
418:47 - going to just uh
418:48 - after the kenny edge detection i'm going
418:50 - to once again
418:52 - add this
418:54 - i'm show method to show the kenny edge
418:56 - detected image also
418:58 - so you can see here
419:01 - this is the kenny edge detected image
419:03 - all the edges are detected and based
419:05 - upon
419:06 - all these
419:07 - lines which are detected here these
419:10 - lines are drawn
419:12 - but the problem here is these lines are
419:16 - of infinite length so
419:18 - there is no end to this line this these
419:20 - lines just go from the
419:22 - start or the corner of the image to the
419:26 - other corner of the image so you can see
419:28 - they start from here and go to
419:31 - the next corner they don't
419:34 - just uh
419:35 - stop here so in this
419:39 - half transform you uh
419:41 - see that even for the line with two
419:43 - argument it takes a lot of computation
419:46 - and we don't even get the correct result
419:50 - so this problem can be
419:53 - solved using the other method which is
419:55 - implemented
419:57 - using this half line
419:59 - p method which is the probabilistic half
420:03 - line transform which i'm going to show
420:05 - you in the next video so how we can get
420:08 - the better result using half line p
420:11 - method we are going to see
420:13 - in the next video in the last video we
420:16 - have seen how to use standard half
420:18 - transform
420:19 - using half lines method in opencv
420:23 - now in this video we are going to see
420:25 - how to use probabilistic half line
420:28 - transform
420:29 - using a method called half lines capital
420:32 - p method in opencv
420:34 - so let's go to our editor and this was
420:38 - the code we have written last time and
420:40 - we have used half lines method
420:43 - for detecting lines inside this image
420:47 - which was the sudoku image so let's run
420:50 - this uh example really fast to see what
420:54 - was the result
420:56 - which we got last time so this was the
420:58 - result which we got last time
421:00 - and
421:01 - the problem with this result is you can
421:04 - see
421:04 - these lines just go from one end to the
421:08 - other end
421:09 - and in this kind of half transform
421:12 - uh you will be able to see that even for
421:16 - the lines
421:17 - which
421:18 - have two argument it takes a lot of
421:21 - computation
421:22 - so in opencv there is also a method
421:27 - called
421:28 - uh half lines capital p which stands for
421:31 - probabilistic half lines transform and
421:34 - this probabilistic half line transform
421:37 - is
421:38 - an optimization of
421:40 - the normal half transform which we have
421:44 - seen in the last video so let me close
421:47 - this
421:48 - example and let's open
421:50 - the example which we are going to see in
421:52 - this video and you can see in this uh
421:56 - example we have
421:57 - used this huff
421:59 - lines capital p method so when we use
422:02 - this half lines capital p method it
422:04 - doesn't take all the points into
422:07 - consideration
422:09 - instead it takes only the random subset
422:12 - of the points
422:13 - which is sufficient for the line
422:15 - detection
422:16 - so
422:17 - let's go through this code from the top
422:20 - uh to the end so as you can see i have
422:23 - imported these two uh
422:26 - packages cv2 and numpy as np
422:30 - and then i'm reading this image so doku
422:32 - using i'm read method
422:34 - and then i'm converting this image to
422:37 - the grayscale image using
422:40 - cvt color method in cv2 now the next
422:43 - step is to find out the edges of the
422:46 - images this we have also seen in the
422:49 - last video so
422:51 - until here everything is same
422:54 - so once we got the edge detected image
422:57 - using kenny edge detection
422:59 - instead of using the half lines method
423:02 - we are now using this half lines
423:06 - capital p method
423:08 - and it takes few arguments the first
423:10 - argument is your edge detected image the
423:13 - second argument is the row which is the
423:16 - distance resolution of the accumulator
423:18 - in pixels the third argument is the
423:21 - theta value which we have taken np dot
423:24 - pi divided by 180 which is
423:27 - the angle resolution of the accumulator
423:30 - in radians
423:32 - the next value is the threshold so
423:35 - right now we have taken this threshold
423:37 - as
423:38 - 100 and this threshold is the
423:42 - accumulator threshold parameter
423:45 - which means that only those lines are
423:47 - returned
423:48 - that get enough vote
423:50 - that means greater than
423:53 - the threshold value
423:55 - the extra two argument here are a little
423:59 - bit different from
424:00 - the half lines method so you can see
424:03 - all
424:04 - these arguments are almost same these
424:08 - four arguments but there are two extra
424:11 - arguments here or parameter here which
424:13 - we need to provide so the first
424:16 - parameter here is the min
424:18 - line length
424:20 - and this we have taken hundred so this
424:23 - min line length is the minimum length of
424:26 - the line which means that line segments
424:29 - shorter than this length which is 100 in
424:32 - our case
424:33 - will be rejected
424:35 - the next argument is the maximum line
424:39 - gap and it is the maximum allowed gap
424:43 - between the line segments to treat them
424:47 - as a single line
424:48 - so these are the two extra arguments we
424:51 - have taken and this half lines capital p
424:53 - method is going to return again the
424:56 - output vector of the lines but the
424:59 - difference between this return value
425:03 - from
425:04 - half line p method and the
425:07 - half lines method is
425:09 - you can see here this line
425:12 - at index 0 is going to directly give you
425:15 - the values of x1 y1 and x2 y2 which are
425:19 - the two points which we will be able to
425:22 - join
425:23 - and we will be able to draw the line
425:25 - using cv2 dot line method in the last
425:29 - video i have shown you that you have to
425:31 - do so much calculation in order to find
425:34 - this x1 y1 and x2 y2 and this
425:38 - probabilistic half line transform method
425:41 - is going to do our job easy and it's
425:44 - going to directly give us these four
425:46 - values so you don't need to do anything
425:49 - you just need to pass these x 1 y y1 and
425:51 - x2y2 value to the
425:54 - cb2.line method
425:56 - so cv2.line method is going to take
426:00 - the first argument which is the image
426:02 - and then the second argument is the 0.1
426:04 - coordinate which is x1 and y1 which we
426:07 - got from the line
426:10 - variable at index 0.
426:12 - and the third parameter here is the 0.2
426:16 - which are the coordinate of the 0.2
426:18 - which is here x2 and y2
426:22 - the next argument here is the color
426:25 - which we have taken right now 0 comma 2
426:28 - 55 comma 0
426:31 - and the last parameter here is the
426:34 - thickness of the line
426:36 - so we have taken two here
426:38 - and the next three line
426:40 - are going to just show this image first
426:42 - of all all these lines which we found
426:46 - out are drawn on the image this image
426:49 - which is the original image and then we
426:52 - are just showing this image after
426:54 - drawing all the lines which we got using
426:57 - half lines p method on the original
427:00 - image and then we are
427:03 - just loading this image using this uh
427:06 - i'm sure method and then after we are
427:08 - done we are just destroying all the
427:10 - windows
427:11 - so let's see what result we get
427:14 - after this
427:17 - script is run so i'm going to run this
427:19 - script
427:20 - and this is the kenny edge detected
427:23 - image
427:24 - and this is the image you got
427:27 - when you apply this half lines p method
427:31 - on your kenny edge detected image
427:34 - so you can see these lines are no longer
427:38 - going to the end to end
427:40 - these are
427:42 - more uh you know accurately detecting
427:46 - all the lines
427:48 - which are there in this doku uh image
427:52 - you can see some lines are broken here
427:54 - so that's why these lines are not even
427:58 - you know drawn because they are not even
428:01 - detected by kenny edge detection so this
428:03 - one or this one are not detected by
428:06 - kenny edge detection so that's why these
428:09 - lines are not drawn so let me show you
428:12 - these results side by side so this was
428:14 - the result which we got after applying
428:17 - the hof line transform method which is
428:20 - half lines
428:22 - on our kenny edge detected image and you
428:24 - can see all these lines here and this is
428:28 - the result which we got after applying
428:31 - half lines p method which is the
428:34 - probabilistic half line transform
428:37 - so
428:38 - these two methods are available
428:40 - in opencv to detect these lines
428:44 - in an image now let's go back to our
428:48 - script and here instead of this image
428:51 - which is the sudoku.png image i have one
428:54 - more image which is called
428:57 - road dot
428:59 - jpg and this is the image which contains
429:03 - a road and inside this road we have some
429:06 - lane lines so you can see
429:09 - this result now here
429:11 - which is the road and these are the lane
429:15 - lines which are detected using this half
429:18 - lines
429:19 - p method
429:21 - so in case of
429:23 - lane line detection you can use this
429:25 - half line p method but you need to
429:28 - decide your roi or region of interest
429:32 - because you can see some lines are
429:34 - detected here here here and here
429:37 - so you just need to uh
429:39 - you know define your line of interest
429:42 - region
429:43 - and you will be able to detect all the
429:47 - lines or lane lines on the road so maybe
429:50 - in the next video we are going to
429:53 - see how
429:54 - we can detect these lane lines on the
429:57 - road accurately without
430:00 - these noises which we are seeing
430:03 - here
430:04 - on the other
430:06 - part of the image so we just need to
430:08 - detect these lane lines and nothing more
430:13 - and we will
430:14 - uh do the same on our video so on the
430:18 - video in which these lane lines are
430:21 - there and we just need to continuously
430:23 - detect these lane lines so in the case
430:26 - of let's say self-driving car you need
430:28 - to
430:30 - detect these lane lines we are going to
430:32 - see how to detect these lane lines in
430:35 - the last videos we have learned some
430:37 - important concepts in opencv
430:41 - now in this video and the next few
430:43 - coming videos i'm going to create a
430:46 - simple
430:48 - project which uses most of these
430:50 - concepts which we have learned
430:52 - in the previous videos
430:55 - so what we are going to do is we are
430:56 - going to create a very simple lane
430:59 - detection
431:00 - system
431:02 - so first of all we will start with a
431:05 - still image you can see there is an
431:08 - image which contains this road and this
431:11 - road contains
431:13 - lanes so what we want to achieve is we
431:16 - want to
431:17 - detect these lanes on which our vehicle
431:21 - is traveling so first of all we will do
431:24 - this with this image and gradually we
431:27 - will move towards the video frames so
431:31 - first of all we will see how to detect
431:33 - these lanes
431:35 - in this image and then we will see how
431:37 - to detect these lanes
431:39 - in the moving video
431:42 - so let's get started so i have created
431:45 - this new project in my pycharm ide you
431:48 - can use any other editor of your choice
431:51 - and first of all obviously you just need
431:53 - to install
431:54 - opencv python package and
431:57 - matplotlib package
432:00 - once you have done that i will create a
432:03 - new
432:04 - file here so i'm going to just right
432:05 - click here and create a new file
432:08 - and i'm going to name this file as
432:10 - detector
432:11 - dot py file so here we are going to
432:14 - import a few uh packages for example
432:18 - matplotlib so matplotlib
432:21 - dot pi plot
432:22 - as plt so let's say as
432:26 - plt
432:28 - also we are going to import the cv2
432:32 - package
432:33 - and we are going to import numpy so an
432:36 - import
432:37 - numpy
432:38 - as np
432:40 - in the next uh section what we are going
432:43 - to do is we are going to simply uh load
432:45 - an image so i am going to create an
432:47 - image variable so image is equal to cv2
432:51 - dot im read and we are going to
432:55 - read our image which is the road image
432:59 - so road dot
433:00 - jpg
433:02 - now in the next line we are going to
433:04 - convert this image
433:06 - into
433:07 - the rgb format because we are going to
433:10 - load this image using
433:12 - matplotlib
433:14 - so i am going to just write once again
433:17 - image so i am going to overwrite this
433:20 - image variable with the converted image
433:23 - so cv 2 dot
433:26 - cvt color and the source is our
433:30 - image so this is the variable and then
433:33 - cv2 dot color
433:36 - from
433:37 - bgr to rgb right so this is what we want
433:41 - to use
433:42 - now in the next line what we want to do
433:45 - is we want to load our image using
433:49 - plt dot i am show method and at last we
433:53 - are going to just say plt
433:55 - dot show so
433:58 - this is how we are going to just load
434:00 - our image so i'm going to right click on
434:03 - this file and then
434:05 - run this script and you can see this
434:08 - road jpg image is loaded now on this
434:11 - plot you also see
434:13 - these values
434:15 - and one things to observe here is
434:18 - horizontally these values goes from 0
434:22 - to 1200 something
434:24 - and vertically normally in the graphs
434:27 - you will see that values increases from
434:30 - the bottom to top but in matplotlib this
434:34 - value goes from top to bottom right so 0
434:38 - is at the top and then the maximum value
434:41 - will be at
434:43 - the bottom
434:44 - so this is a one thing to note because
434:47 - we are going to define our region of
434:50 - interest and that will be based upon
434:54 - these values now in the next step we
434:56 - want to define
434:58 - our region of interest so once again let
435:02 - me just run this
435:04 - code once again and one thing to notice
435:07 - here is this
435:09 - lane in which our vehicle is traveling
435:13 - is parallel so there are two
435:16 - parallel lines and eventually they are
435:19 - going to
435:20 - merge
435:22 - here right so all the lanes on which the
435:25 - vehicle travels
435:27 - have the same pattern so this lane and
435:30 - this lane
435:32 - are parallel to each other and they're
435:34 - going to merge at some point so it's not
435:38 - merging but it seems to be merging at
435:41 - some point
435:42 - so we can define our region of interest
435:46 - from this point
435:48 - to this point and from this point to
435:51 - this point so this region of interest
435:54 - will be
435:56 - the triangle so this region of interest
435:59 - we are going to define for our vehicle
436:01 - will mask any other uh obstruction for
436:04 - example this is also one lane line for
436:08 - us it's not important because this is
436:10 - the other side of the lane so here uh
436:14 - the vehicle will come in the opposite
436:16 - direction so this is our region of
436:19 - interest so it will mask out this lane
436:23 - line or any other
436:25 - lines or distortions which we have
436:28 - in this picture we are going to just
436:30 - mask them and we are going to just
436:32 - concentrate on
436:34 - this triangle so let's do this first so
436:38 - first of all we are going to
436:40 - find out the shape of the image so i'm
436:43 - going to just
436:45 - print and then we are going to just say
436:48 - image
436:48 - dot shape and also we are going to uh
436:53 - just define the height and width of the
436:55 - image so i'm going to
436:57 - just say okay so let's print this value
437:00 - and let's see what happens
437:02 - so what's at 0 and what's at 1 so you
437:06 - can see
437:07 - it prints
437:08 - 704
437:09 - as our
437:11 - height
437:12 - and 1279
437:14 - as the width so this is what i'm just
437:18 - taking from this
437:20 - image shape method so it's going to
437:22 - return this kind of tuple so at 0th
437:25 - index there will be height and at the
437:28 - first index there will be
437:31 - the width and as i said it starts
437:35 - from 0 to
437:37 - 704 from top to bottom
437:40 - and
437:40 - horizontally it goes from 0
437:43 - to
437:44 - 1279 from the left hand side to the
437:47 - right hand side right
437:49 - so once we have the width and height we
437:52 - can define our region of interest so we
437:55 - are going to uh
437:56 - define a variable called
437:59 - region of interest vertices and here
438:03 - we are going to
438:05 - provide some values
438:07 - so we are going to provide three points
438:09 - which will be the three points of our
438:12 - region of interest so as i said
438:15 - that our region of interest we want is
438:18 - this point which is the left
438:20 - bottom corner this point which is the
438:23 - right bottom corner and somewhere in the
438:26 - middle
438:26 - of this image
438:28 - so here so in the image because
438:31 - the
438:32 - vertical
438:34 - height starts from zero so i'm going to
438:36 - just say
438:37 - zero comma
438:39 - height and the second point will be the
438:41 - half of the width and half of the height
438:43 - which will be the center of the image so
438:45 - i'm going to just say width divided by 2
438:48 - comma
438:49 - height divided by
438:52 - 2 and this will be
438:54 - inside these uh
438:57 - parentheses and the third point
439:00 - will be
439:01 - the
439:02 - next corner so this will be
439:05 - width
439:06 - and then the height so let's try to see
439:09 - these points in our
439:12 - matpot plotlib window so the first point
439:15 - here is 0 comma
439:17 - 704 which is this point
439:20 - the second point is somewhere here which
439:23 - is the half of the height and half of
439:24 - the width and the third point will be
439:27 - here which is width
439:29 - comma height which is 700 comma
439:33 - 1279 which is this one right so this
439:36 - will be our region of interest
439:39 - now we are going to define
439:41 - one function
439:43 - to
439:44 - mask
439:45 - every other thing other than our region
439:48 - of interest so i will just define this
439:50 - function def region of interest and this
439:53 - is going to take two parameter first
439:56 - will be the image and second will be the
439:59 - vertices so vertices
440:03 - and inside this function let me just
440:06 - minimize this terminal also so you can
440:09 - see the function so inside this function
440:11 - in the first step we are going to define
440:13 - a blank matrix that matches the image
440:16 - height and the width so this will be the
440:19 - easy step we are going to define a
440:21 - variable called mask and we are going to
440:23 - use np dot
440:25 - zeros like method which uh is going to
440:29 - take one parameter which will be
440:31 - our image
440:33 - matrix now in the next step we are going
440:35 - to just uh retrieve the number of color
440:38 - channels from the image this will be
440:41 - the easy step also so channel count
440:45 - and then we are going to just say image
440:49 - dot shape and at the second index we are
440:51 - going to find out the channel because we
440:53 - have seen that image dot shape is going
440:56 - to give you uh three values height width
440:59 - and the channel count so this channel
441:01 - count
441:02 - is coming from this index
441:05 - now in the next step what we are going
441:08 - to do is we are going to create
441:10 - a match color with the same color
441:14 - channel counts so i'm going to just say
441:17 - match underscore mass underscore color
441:21 - this will be our variable name and then
441:23 - we are going to just
441:25 - take 255
441:27 - comma
441:28 - and then multiply it by the
441:32 - channel count so let's multiply it by
441:34 - the channel count so this is going to
441:36 - create a match color with
441:38 - the same color channel counts now in the
441:40 - next step we are going to fill inside
441:42 - the polygon using
441:44 - the fill poly method because we have our
441:47 - region of interest and we want to mask
441:50 - every other thing other than our region
441:52 - of interest so we are going to just say
441:54 - cv2 dot
441:56 - fill poly which is going to take few
441:59 - arguments first will be our mask
442:02 - second will be the vertices which we are
442:05 - providing using the second argument and
442:09 - the third argument will be
442:11 - our
442:12 - match mask color variable so we are
442:14 - going to pass this variable as the third
442:18 - argument
442:19 - and in the next step we are going to
442:21 - just return the image
442:24 - only where the mask pixel matches so i'm
442:27 - going to just say
442:29 - masked
442:31 - underscore image is equal to cv 2
442:36 - dot
442:36 - bit wise and so we're going to just
442:40 - apply bitwise and using
442:43 - this bitwise and method and the first
442:46 - argument here will be the image and the
442:48 - second argument is the mask which we
442:52 - got using this zeros like method right
442:57 - and in the last step we are going to
442:59 - just return this so i'm going to just
443:02 - write
443:03 - return
443:04 - this masked image and that's it so we
443:08 - are going to just uh apply our region of
443:11 - interest on the image using this method
443:14 - and then we are going to just get our
443:17 - image which contains region of interest
443:20 - and any other thing will be masked so
443:23 - now it's time to use this method so we
443:27 - are going to just use this method
443:29 - using
443:30 - this variable i'm going to just define a
443:32 - variable called let's say
443:35 - cropped image or masked image whatever
443:38 - you want to write here so
443:40 - let's say cropped
443:42 - underscore
443:43 - image
443:44 - and then we are going to just use this
443:47 - function which is a region of interest
443:49 - function which takes
443:51 - this argument so because we have already
443:53 - read our
443:55 - image in the image variable we are going
443:57 - to pass this as the first argument
444:00 - and the vertices is simply
444:03 - our region of interest variables so this
444:06 - region of interest variable we are going
444:08 - to pass using numpy
444:11 - dot array method
444:13 - and let's uh split this line so we will
444:16 - be able to see what i am doing inside
444:18 - this np.aria method so first of all
444:22 - the first argument will be our region of
444:25 - interest
444:27 - variable which is this one region of
444:29 - interest vertices so in the square
444:31 - bracket
444:32 - we are going to just pass region of
444:35 - interest vertices and the second
444:37 - argument here
444:39 - will be np
444:40 - dot int 32 so np dot
444:44 - int
444:45 - 32
444:46 - and now we are going to just uh show
444:48 - this image using our matte plot lib
444:52 - window so let's run this code and let's
444:55 - see what happens when it runs and there
444:57 - is a problem here so let's see what the
445:00 - problem is so you can see uh this
445:02 - problem is coming from this line and
445:04 - most probably this region of interest
445:07 - has some problem so you can see we have
445:10 - passed this first element as the tuple
445:13 - second element as the tuple and the
445:15 - third element also we need to pass as a
445:18 - tuple and that's why it's giving us the
445:20 - problem so
445:21 - i have just fixed it and let's see what
445:24 - happens when we run this code again
445:26 - and you can see our
445:28 - image is now masked
445:30 - with our region of interest so we have
445:33 - defined our region of interest uh from
445:36 - this point to this point to this point
445:38 - so now we have only this region of
445:41 - interest so we will be able to easily
445:43 - find out this lane line and this lane
445:46 - line inside our region of interest and
445:49 - any other distraction will be
445:52 - marked now right
445:54 - so this is the first step which we have
445:57 - achieved which is masking our image and
446:01 - just applying our region of interest on
446:03 - the image
446:04 - in the next step we are going to uh see
446:07 - how we can
446:08 - apply the edge detection and find out
446:11 - the lane lines
446:13 - on
446:14 - the image
446:15 - in the last video we have started our
446:17 - simple project of detecting lane lines
446:20 - on the road using opencv
446:22 - and we came to the point where we were
446:25 - able to define our region of interest
446:28 - and our result
446:29 - was looking like this so let me run this
446:32 - project
446:33 - so we have defined this region of
446:35 - interest and now
446:37 - the
446:38 - only thing which remains here is to
446:40 - detect these
446:42 - lane lines
446:44 - so we will once again
446:47 - go to the next step and the next step
446:49 - will be
446:50 - to find out the edges
446:53 - and then we are going to apply
446:56 - half line transform to draw the lines
446:59 - so first thing first what i'm going to
447:01 - do is i'm going to just move this region
447:04 - of interest function which we have
447:05 - created in the last video
447:07 - on the top of this script so we can see
447:11 - uh this other code clearly this code
447:14 - which we have written
447:16 - so
447:17 - we have this region of interest function
447:20 - which we have created then
447:22 - we have just created this region of
447:25 - interest variable and then we just
447:28 - used our region of interest function
447:31 - using this region of interest vertizes
447:34 - variable
447:35 - so the next step as i said is to find
447:38 - out the
447:39 - edges and for that we need to first
447:43 - convert our image into a grayscale image
447:46 - so i'm going to just say a gray
447:50 - image
447:51 - and then we all know how to find out the
447:54 - gray scale image out of
447:57 - an image so we just need to write
448:00 - cvt color and the source is our cropped
448:03 - image so we are going to pass our
448:05 - cropped image and then we are going to
448:08 - just convert it into a grayscale image
448:10 - using
448:11 - cv2
448:12 - dot
448:14 - color
448:15 - underscore
448:16 - rg
448:17 - b2 gray so let's do this
448:21 - so once we got our grayscale image we
448:23 - can apply kenny edge detection on this
448:26 - image so i'm going to just write
448:28 - kenny image
448:31 - and then i'm going to just say
448:33 - cv2 dot
448:35 - kenny which is
448:36 - the function which we want to use which
448:39 - takes few parameter first parameter will
448:42 - be our gray scale image the second
448:44 - parameter
448:46 - will be the first threshold and the
448:48 - second threshold so generally
448:50 - uh we are going to take here 100 as the
448:53 - first threshold and 200 as the second
448:57 - threshold
448:58 - now
448:59 - in the next step we are going to just uh
449:03 - display this image
449:05 - on
449:06 - our
449:07 - matplotlib window and let's see what
449:10 - happens once
449:11 - we
449:12 - apply the skinny edge detection method
449:15 - on the image
449:16 - so now you can see this result which
449:19 - detects all the edges and here you can
449:22 - see the lean line edges are detected but
449:26 - there is one more thing here which is
449:29 - the edges of our region of interest
449:32 - are also detected so how to solve this
449:35 - how to remove these edges because these
449:38 - edges doesn't interest us the
449:41 - interesting
449:42 - uh
449:43 - edges here in this image
449:45 - for us are these edges
449:48 - which are of the lanes road lanes right
449:51 - so to solve this problem we can apply
449:54 - the scanning edge detection before
449:56 - we
449:57 - find out the region of interest so i'm
449:59 - going to just copy this code and paste
450:01 - it just before
450:03 - we
450:04 - apply this region of interest method
450:06 - which we have created in the last video
450:09 - so now in our kenny edge detection uh we
450:12 - will pass the gray scale image but here
450:16 - instead of this uh
450:18 - cropped image which we were getting in
450:20 - the last step from
450:22 - this variable we directly are going to
450:25 - pass our image which we have read using
450:29 - the i am read method right so let me
450:32 - just
450:33 - remove all these line breaks so you will
450:36 - be able to see the code at once
450:39 - so here you can see i have directly
450:42 - passed now
450:44 - this image variable to the
450:47 - cvt color method so we get the
450:50 - grayscale image of the original image
450:53 - and then we apply the kenny edge
450:55 - detection on the original image and then
450:58 - we are applying
450:59 - the region of interest method which we
451:02 - have created in the last video
451:04 - now because we are applying uh
451:09 - this uh
451:10 - region of interest method on the
451:12 - grayscale image or the edge detected
451:15 - image
451:16 - therefore
451:17 - we don't uh need this channel here so we
451:21 - can comment out this code which was kind
451:24 - counting out the number of channels
451:27 - and for the
451:30 - grayscale image and the kenny has
451:32 - detected the image we just take this
451:34 - match mask color as 255 because it's
451:37 - only one color right we don't
451:40 - need any color channels here because we
451:43 - are just passing the grayscale image
451:45 - which has only one color so that's why
451:47 - we don't need any channel because there
451:50 - will be only one channel
451:52 - and that's why i have commented this
451:54 - code and the value of the match mask
451:57 - color will be 255 now once you do that
452:00 - let's try to run this code and let's see
452:03 - what happens
452:04 - once again we need to load the cropped
452:08 - image not the kenny image so just
452:10 - replace this
452:12 - variable here in the i am show
452:15 - method and let's run this code once
452:16 - again
452:17 - so you can see now uh
452:19 - there is some mistake here because we
452:22 - were expecting
452:23 - the edge detected image and we are
452:26 - getting this image so let's see what's
452:28 - the problem is
452:29 - so the problem i see here is because
452:33 - we have applied this region of interest
452:35 - on the original image which we don't
452:37 - want now we want to apply this
452:41 - region of interest on the kenny edge
452:43 - detected image so we have to pass
452:46 - as the first variable of the region of
452:48 - interest method the skinny uh edge
452:51 - detected image not the original image
452:53 - right so once again for you you can see
452:56 - this code region of interest method and
452:59 - all this code at a one glance let's run
453:02 - the code and let's see what happens so
453:04 - now we get the better result so we have
453:08 - these edges which are detected by the
453:10 - kenny edge detection
453:12 - for only the lane lines inside our
453:15 - region of interest and now it will be
453:17 - easier to draw the lines on these edges
453:22 - which we have detected so the next step
453:25 - will be to draw the lines on these edges
453:29 - using the half line transform
453:32 - so we have in the previous videos have
453:36 - already seen how to use the half line
453:39 - transform so i'm not going to go into
453:42 - the details so let's uh just directly
453:46 - jump into uh using that half line
453:49 - transform
453:50 - so what we are going to do is in the
453:53 - next line after we have
453:56 - got our
453:57 - cropped image we are going to just
454:01 - define
454:02 - a variable called
454:04 - lines and we are going to use this half
454:07 - line transform probabilistic half line
454:09 - transform method so here cb2 dot
454:13 - half line transform
454:16 - and this will be this method which takes
454:18 - few argument first argument will be the
454:20 - image
454:21 - so i'm going to pass this cropped image
454:25 - here
454:26 - the second argument here will be the
454:29 - value of row so let's provide this row
454:33 - value variable value which will be 6 in
454:37 - our case then in the next parameter we
454:40 - have to pass the value of theta and
454:43 - theta will be
454:45 - equal to np
454:47 - dot pi which is the method inside the
454:50 - numpy library so np dot pi
454:53 - divided by
454:55 - 60 so i'm going to pass here divide by
454:57 - 60
454:58 - then the next parameter here will be the
455:02 - threshold so the threshold value we are
455:04 - going to provide here will be 160. the
455:07 - next parameter here will be lines which
455:10 - is equal to none by default so i'm going
455:12 - to provide this variable
455:15 - lines is equal to and then uh we are
455:18 - going to pass the empty uh numpy array
455:21 - so i'm going to just say
455:23 - uh numpy and p dot
455:26 - array and then we are going to just pass
455:28 - the blank square bracket here the next
455:31 - two parameters will be the min line
455:34 - length so let's provide this uh
455:37 - min line length and let's say we want 40
455:41 - as the minimum line length and the max
455:44 - line gap so let's provide that also max
455:47 - line gap and this will be
455:49 - let's say initial value for that will be
455:52 - 25
455:53 - so now after applying this half line
455:56 - transform
455:57 - you know that it's going to return the
455:59 - line vector of
456:01 - all the lines which are detected inside
456:05 - our image which we have provided as the
456:08 - source here so if you don't know what
456:11 - are these parameters which i'm using
456:13 - here you can see
456:14 - my uh last videos about probabilistic
456:18 - half line transform and you will be able
456:20 - to know what they actually mean
456:23 - now once we got our line
456:26 - vectors then we can draw the lines
456:30 - easily
456:31 - and for that we are going to define our
456:33 - next function
456:35 - which is to draw the lines so i'm going
456:37 - to just define this function
456:40 - with the name
456:42 - draw
456:43 - the
456:45 - lines for example and it's going to take
456:48 - few parameters so
456:50 - let's pass these parameters first
456:52 - parameter will be the image or
456:55 - the original image
456:56 - the second parameter will be the line
457:00 - vectors which we have
457:02 - found out
457:03 - and that's it so there are these two uh
457:07 - parameters we are going to pass here
457:10 - now uh
457:11 - inside this function what we are going
457:13 - to do is we are going to first of all uh
457:15 - copy our image so i'm going to just say
457:17 - i'm g
457:18 - is equal to np dot copy
457:22 - and then we are going to just make a
457:25 - copy of
457:27 - the image variable which we are
457:29 - providing
457:30 - and then or you can
457:32 - write here copy image whatever i'm just
457:35 - just reassigning this copied image to
457:38 - the same variable but you can define a
457:41 - new variable here for the copied image
457:44 - also
457:45 - now in the next line we are going to
457:47 - create a blank image that matches
457:50 - the original image size so the dimension
457:54 - should be equal
457:56 - so
457:56 - for this we can just say
458:00 - line
458:02 - image and then we are going to just say
458:05 - np dot zeros inside these parentheses we
458:09 - are going to provide the shape
458:12 - of our image right so you can provide
458:15 - the shape of our image using the image
458:17 - variable so first of all it's going to
458:19 - take the
458:21 - height and then the width and then the
458:23 - number of channels so because we know
458:26 - that this is a colored image
458:28 - which we are
458:30 - working with so we are going to just say
458:33 - img.shape
458:36 - and we all know that the 0th
458:39 - index parameter here will be the height
458:42 - the second parameter img
458:44 - dot shape
458:46 - the
458:47 - value at the first index will be the
458:50 - width and the number of channels for the
458:52 - colored image are always three so we are
458:55 - going to provide the third parameter as
458:57 - three here so this is in the form of
458:59 - tuple i'm providing
459:01 - and the next parameter here will be the
459:03 - data type or d type so let's uh provide
459:06 - that d type is equal to numpy dot
459:10 - u in
459:13 - okay so u int
459:15 - eight
459:16 - not one in eight u into eight so this
459:19 - will be uh
459:21 - the second parameter so once we have
459:24 - this image which is exactly same as the
459:28 - size of our original image
459:31 - we are going to loop around
459:34 - these line vectors
459:36 - and then we are going to draw the lines
459:39 - right so let's loop around these uh line
459:42 - vectors and uh draw all these lines
459:45 - which were found
459:47 - so for that we are going to use the for
459:49 - loop and then we are going to say line
459:52 - in lines
459:54 - and these this lines variable is coming
459:58 - from
459:59 - this lines variable so we are going to
460:01 - use this draw lines function and we are
460:04 - going to pass this lines vector as the
460:06 - second parameter here so this is how
460:09 - uh this line variable is coming here
460:13 - so now
460:14 - inside this for loop we are going to uh
460:17 - just define one more for loop because
460:21 - this line is going to give us uh
460:24 - four parameters which is uh the
460:27 - coordinates of the first point in the
460:30 - line and the coordinates of the second
460:32 - point in the line so we are going to
460:34 - just once again
460:36 - say for
460:38 - x1 which is the first coordinate of the
460:42 - first point and the y
460:44 - one and then similarly x2
460:47 - and the y2 so
460:49 - this will be
460:51 - the line coordinate in the line
460:54 - uh which we got from the line vector
460:57 - and then inside this for loop we are
461:00 - going to draw a line and drawing line is
461:03 - really easy by using
461:06 - cv2.line method which takes a few
461:09 - parameters as you already know the image
461:12 - and then the second parameter is the
461:15 - coordinate of the first point which we
461:17 - already have
461:18 - using this iteration which is x1 comma
461:23 - y1
461:24 - and third parameter here will be
461:28 - the
461:29 - coordinates of the second point x2 comma
461:32 - y2 and then you can provide the color
461:36 - and thickness so let's uh provide this
461:39 - color so the color here i'm going to
461:41 - take
461:42 - let's say 0 comma 255 comma 0 you can
461:46 - take any color
461:48 - here
461:49 - and the thickness so the thickness here
461:51 - i'm going to take is uh
461:54 - let's provide this parameter thickness
461:56 - is equal to
461:57 - 3 okay
461:59 - so this is the thickness of the line
462:02 - which we want to draw and here i think
462:04 - this blank image should uh
462:07 - be uh
462:08 - given because we want to draw the line
462:10 - on the blank image and then merge it
462:13 - with the original image so here we have
462:15 - to provide this line image or you can
462:18 - say
462:19 - this is the
462:20 - blank image which is more appropriate in
462:24 - this case so we want to draw the line on
462:27 - the blank image which is of the same
462:29 - size of the original image and now once
462:32 - we draw these lines on the blank image
462:35 - we can merge this blank image and the
462:37 - original image which will give us
462:40 - the
462:42 - line which are drawn on the original
462:44 - image so outside this for loop
462:47 - we are going to merge the image with the
462:50 - lines into the original image so our
462:53 - original image is the
462:55 - image
462:56 - itself so image variable is the our
462:59 - original image and then
463:01 - we are going to just say cv to dot
463:05 - add weighted this function also we have
463:07 - seen in the last videos
463:09 - and this is the function which we use to
463:14 - merge two images with some weights
463:17 - so the first parameter here will be
463:19 - image now the second parameter here will
463:22 - be the value of alpha so which
463:25 - we are going to give here 8 this is like
463:28 - a weight
463:29 - to an image
463:30 - which we want to provide
463:32 - and then the third parameter here will
463:34 - be the second image so we want to merge
463:36 - the blank image with the original image
463:39 - the fourth parameter is the value of
463:42 - beta so this value we are going to
463:45 - take as one and the last value will be
463:48 - of uh gamma so gamma
463:51 - we are going to take as zero 0.0 here
463:54 - okay so this add weighted also we have
463:57 - seen
463:58 - in the last video how to use it so i'm
464:00 - not going into the details
464:03 - and at last once we have the
464:07 - lines on the image then we are going to
464:10 - simply return it so let's uh return this
464:13 - uh image img so once we have this
464:16 - function we are going to call this
464:18 - function after applying the half line
464:21 - transform method which is the
464:23 - probabilistic half line transform so
464:25 - here in the next line we are going to
464:27 - just define a variable called image
464:30 - with
464:31 - lines let's say
464:34 - is equal to our method which is draw the
464:37 - lines method the first argument is the
464:40 - original image so we are going to pass
464:41 - the original image the second argument
464:44 - is the line vector which we got from
464:46 - this method right
464:48 - so the original image and the line
464:52 - vector variable which we got here
464:55 - at last we are going to just
464:58 - see what is the result
465:00 - which we got after applying this draw
465:03 - the lines method
465:04 - on the original image
465:07 - so let's run this code and let's see
465:09 - what happens
465:12 - so now you can see let me just maximize
465:15 - it
465:15 - you can see this line is drawn on our
465:19 - image
465:20 - so
465:21 - this is the first line and this is the
465:23 - second line so we got the result which
465:25 - we wanted if you want to change the
465:27 - thickness or
465:29 - the color of this
465:31 - line on the image you can just change it
465:34 - using this draw lines method so this
465:38 - is the line and thickness parameter so
465:41 - for example i want to change
465:43 - this to
465:44 - 255 here
465:46 - some different color and the thickness
465:48 - let's say four
465:50 - and i'm going to run this code and now
465:52 - you can see this yellow color here right
465:56 - so you can change the thickness and the
465:58 - color using uh this method so let's say
466:02 - for now we want
466:04 - the
466:04 - red color
466:06 - so we are going to go with this
466:09 - red color on the lane lines so this is
466:12 - what we wanted to achieve we wanted to
466:14 - draw the lane lines on these lanes and
466:18 - we have achieved this
466:20 - in the next video we are going to see
466:22 - how we can apply the same concept on a
466:26 - live video or on a video of
466:30 - this road for example so for example
466:32 - this car is running on the road and we
466:36 - want to continuously
466:38 - draw these lines on
466:40 - the lane lines how we can achieve this
466:42 - using opencv we are going to see in the
466:45 - next video
466:47 - in the last two videos we have seen how
466:49 - we can detect the lane lines on the road
466:52 - using opencv
466:55 - now till now we have only worked with
466:58 - this image and in this video we are
467:00 - going to try to apply what we have
467:03 - written
467:04 - not on an image but with the video
467:08 - frames
467:09 - but you have already learned in the
467:11 - previous videos that a video frame is
467:14 - like an image so
467:16 - a video contains
467:18 - many number of frames
467:20 - so if we apply the same technique on
467:23 - each frame we will be able to detect
467:26 - these lane lines on the video frames
467:29 - also
467:30 - so let's apply that concept on our
467:33 - script what we have till now
467:36 - so right now i have added this test dot
467:40 - mp4 video inside my project so let me
467:43 - show you how it looks like
467:45 - so our video looks like this so we are
467:48 - going to apply all these concepts which
467:50 - we have applied on an image
467:52 - on this video so let's get started
467:55 - so i hope you have this code which we
467:58 - have written in the last two videos the
468:00 - only thing we need to do here is we need
468:03 - to read the video instead of an image
468:07 - and then apply
468:09 - those
468:10 - concepts on the frame instead of an
468:13 - image
468:14 - so we till now have two functions region
468:17 - of interest draw the line and we have
468:19 - this code so this all code we are going
468:22 - to enclose inside the function
468:26 - so that it will be easier to apply all
468:30 - this code on the video frames
468:33 - now as you already know that
468:35 - this
468:36 - will not be used because we are reading
468:39 - the videos so we don't need to read the
468:41 - image obviously so we are going to
468:44 - comment these two lines out so we don't
468:47 - need to convert
468:49 - bgr to rgb because we are going to use
468:53 - this native cv2 library not the
468:57 - matplotlib library for which we have
469:00 - converted this bgr to rgb
469:03 - image so now we are going to define a
469:05 - function so let's define this function
469:07 - and i'm going to name this function as
469:09 - process and it's going to take
469:12 - an argument which will be
469:14 - the image argument
469:16 - and all this code which is under this
469:19 - which we have written in the last video
469:21 - we are going to enclose this code inside
469:24 - this process function we don't need
469:27 - these two lines because we are not going
469:29 - to use matplotlib for processing this
469:32 - video so i'm going to remove these two
469:34 - lines
469:35 - and i'm going to just give
469:39 - a space here for this code so it can be
469:42 - enclosed inside this function
469:45 - now at last
469:47 - or at the end of this function we are
469:49 - going to just
469:50 - return
469:51 - this image with lines so we are going to
469:54 - return this image with lines
469:57 - using this process function that means
469:59 - on every frame we are going to draw the
470:01 - lines and return it using this process
470:04 - function next we are going to read the
470:05 - video using the video capture functions
470:08 - so i am going to declare a variable cap
470:10 - is equal to cv2
470:12 - dot
470:13 - video capture
470:15 - and then we are going to just pass one
470:18 - argument which will be our
470:21 - video file which is test
470:24 - dot mp4 in our case so test dot mpu4
470:28 - and then once we have this video we are
470:31 - going to check if the video
470:34 - frame is available using the while loop
470:36 - so let's use this while loop and we are
470:39 - going to check
470:41 - if
470:42 - cap
470:43 - dot is open
470:45 - is valid or not so is opened and this
470:48 - function is going to return the boolean
470:50 - value so if this video frame is still
470:54 - available is going to return true and
470:56 - whatever we write inside the while loop
470:59 - is going to be executed now in the next
471:02 - line we are going to just read every
471:05 - frame so we all know from our previous
471:09 - videos that this cab dot read
471:12 - returns two
471:14 - uh result or two variables one is ret
471:18 - and the other is
471:20 - the
471:20 - frame and we are going to just say cap
471:24 - dot read and then we are going to apply
471:27 - our process function on this frame so we
471:30 - are going to once again
471:33 - take this frame variable and we are
471:35 - going to overwrite this frame
471:38 - with the
471:39 - lines on the frames so this we are going
471:42 - to get from our process functions let's
471:45 - call the process function and pass the
471:47 - frame variable
471:49 - inside it okay so this frame is going to
471:53 - go to the process function it's going to
471:55 - process everything and then the final
471:58 - result
471:58 - which we get is going to be saved once
472:01 - again into the frame variable with the
472:04 - actual lines on the frame in the next
472:06 - line we are going to just show our
472:08 - result using cb2 dot i am show method
472:12 - and we are going to just pass the frame
472:14 - variable here in the next line we are
472:16 - going to just write the code for the
472:19 - quitting from this loop so we are going
472:21 - to just say if cv2
472:25 - dot weight key
472:27 - is going to be 1 and then we are going
472:30 - to apply the end operator and then write
472:32 - 0xff
472:34 - for the cross platform functionality and
472:37 - then we are going to just say is equal
472:40 - to
472:41 - ord so whenever somebody presses the q
472:47 - key then we are going to exit from this
472:50 - loop and then in the next line we are
472:52 - going to just say break so break out of
472:55 - the loop
472:56 - the last two line in the last two line
472:58 - outside this loop we are going to just
473:02 - uh
473:03 - call the release function
473:06 - on the cap variable and we are going to
473:09 - destroy all the windows in the cv2 so we
473:12 - are going to just say
473:13 - destroy
473:14 - all windows that's it hopefully this is
473:17 - going to work so let's uh
473:19 - just run this script once again and
473:21 - let's see what happens and here we got
473:24 - the error and it's coming from this line
473:26 - which is cv2 dot i am show we forgot to
473:30 - give the first argument here which will
473:32 - be the name of this window we're going
473:34 - to just say
473:36 - frame here and let's run this script
473:39 - once again
473:40 - and let's see what happens and you can
473:42 - see on this video
473:44 - on this lane line our lines are drawn
473:48 - right so this is the result which we
473:51 - were expecting we can improve this
473:54 - result by adjusting few uh variables so
473:57 - we are going to first of all press q to
474:00 - quit and let's uh change some of these
474:04 - values here in the half line transform
474:07 - so we are going to just say that the max
474:09 - line gap is going to be 100 we are going
474:12 - to reduce the threshold value to 50 and
474:16 - draw value to 2 okay inside this uh hof
474:20 - lines p method and let's run this code
474:23 - so let's see what result we get
474:25 - this is also okay let's improve it
474:28 - little bit more in the kenny edge
474:30 - detection we can reduce this
474:34 - threshold value here to 120 the second
474:37 - threshold value and let's run this code
474:39 - once again
474:40 - and now we get the better result
474:43 - so the problem might be
474:46 - the edge detection so we have reduced
474:49 - our
474:51 - second threshold and now we get the
474:54 - better result you can see
474:56 - on this middle lane the lines are drawn
474:59 - clearly so this is how you can write a
475:01 - simple script to detect lane lines on
475:04 - the road
475:06 - i hope you have enjoyed this video and i
475:08 - will see you in the next video
475:22 - we have already seen how to use half
475:24 - line transform to detect lines in opencv
475:28 - in this video we are going to see how we
475:30 - can use half circle transform to detect
475:34 - circles in an image
475:37 - now as you can see here i have this
475:40 - small example which loads an image and
475:43 - shows it into
475:45 - the i am show window so let me run this
475:48 - code and let me show you how this image
475:51 - looks like so you can see
475:53 - there are
475:54 - so many smarties here inside this image
475:58 - and all the smarties
476:00 - are
476:01 - circle form right they are not
476:05 - the perfect circles but they are in the
476:09 - form of circles and we want to detect
476:12 - all these circles forms
476:15 - inside the image
476:17 - we can use half
476:18 - circle transform for that so let's see
476:21 - how we can use
476:23 - this half circle transform to detect the
476:26 - circles in the image
476:28 - so a circle is represented
476:29 - mathematically by this equation which
476:32 - you see
476:33 - on your screen
476:35 - so here
476:37 - x center and the y center are the
476:40 - coordinates of the center
476:43 - and r here is the radius of the circle
476:46 - so if you know these three
476:49 - parameters then you can draw a circle so
476:53 - the coordinates of the circle and the
476:55 - radius of the circle we need to detect
476:59 - so now let's see how half circle
477:02 - method is applied using opencv
477:06 - so you might observe here that i have
477:09 - created a copy of this original image
477:12 - which i have read using this i am read
477:15 - method in the next step i'm going to
477:18 - just
477:19 - convert this image into a grayscale
477:21 - image so i'm going to just write
477:24 - gray is equal to cv
477:26 - dot
477:27 - cvt
477:28 - color which is going to take
477:31 - two parameters first is the source and
477:34 - second is
477:35 - the method so we are going to convert
477:39 - the color bgr to gray
477:42 - now in the next step because our half
477:44 - circle method works better with
477:48 - the
477:49 - blurred images so we are going to uh
477:52 - create this blurred image using median
477:55 - blur so i'm going to
477:57 - just say
477:58 - gray so we are going to overwrite this
478:00 - gray variable
478:02 - with cv2 dot
478:04 - median blur which is going to take a few
478:07 - arguments first is the image itself so
478:10 - we are going to pass gray here and the
478:13 - second
478:14 - is the
478:15 - k size or the kernel size so we are
478:18 - going to initially provide the kernel
478:20 - size of 5 here
478:23 - now we are going to apply our half
478:26 - circles
478:27 - method so i'm going to declare
478:30 - this
478:31 - circle's variable
478:33 - and then i'm going to just call this
478:35 - method which is called cv dot
478:39 - half circles method so
478:41 - this is the method
478:43 - and you can see it takes few parameters
478:46 - so we are going to
478:48 - give these parameters one by one first
478:51 - is the obvious one which is the image so
478:55 - we are going to
478:56 - provide the gray uh scale image here
478:59 - which is already
479:00 - blurred so the second parameter here is
479:03 - the method which we want to use
479:06 - currently the only implemented method is
479:09 - half gradient method so the choice is
479:13 - very simple here we are going to
479:16 - just provide this cv
479:18 - dot
479:19 - huff gradient
479:21 - method the third parameter here will be
479:25 - a dp dp is the inverse ratio of
479:28 - accumulator resolution to the image
479:31 - resolution
479:32 - so for example when dp is equal to 1 the
479:35 - accumulator has the same resolution as
479:39 - the input image and if the dp is equal
479:42 - to 2 then accumulator has the half as
479:46 - big
479:47 - as width and the height so we are going
479:50 - to take this dp value as
479:54 - one
479:55 - the next parameter here will be min dist
479:58 - it is the minimum distance between the
480:01 - center of the detected circles
480:04 - okay so here we are going to give
480:08 - initially the value of 20 and later we
480:12 - will adjust this value if
480:14 - the circles are very near to each other
480:18 - the next parameter which we are going to
480:20 - give here is the value of parameter 1
480:23 - and parameter 2 or param 1 or param 2.
480:26 - the param 1 is the first
480:29 - method specific parameter
480:32 - in case of half gradient
480:34 - it is higher threshold of the two
480:37 - passed to the kenny edge detector
480:41 - param2 is the second method specific to
480:45 - the method which we have provided here
480:47 - which is the half gradient method it is
480:51 - the accumulator threshold for the circle
480:54 - centers at the detection stage so we are
480:58 - going to provide the value of uh the
481:01 - param1 and param two here so let's start
481:04 - with the param one value and we are
481:07 - going to provide param one value is
481:09 - equal to 50 and param 2 value
481:13 - is equal to
481:15 - let's say 30 so those param1 and param 2
481:19 - parameters are specific to this method
481:22 - which
481:23 - we are using
481:25 - the next parameter which we are going to
481:27 - pass here is the
481:28 - min radius and the min radius is the
481:32 - minimum
481:34 - circle radius
481:35 - and we are going to just start with the
481:37 - zero so we are going to say that
481:39 - anything which is greater than
481:42 - zero we are going to just draw it
481:45 - and then we are going to provide the max
481:49 - radius if this max radius is greater
481:52 - than or equal to zero it uses the
481:54 - maximum image dimension
481:57 - if it's only greater than zero it
482:00 - returns
482:01 - center without finding the radius so
482:04 - this also we are going to start with 0
482:06 - let me just break this function so you
482:09 - can see all the parameters here so this
482:12 - half circle method is going to give us
482:15 - the circle vector
482:16 - which we can iterate upon but first of
482:20 - all we need to convert
482:22 - those circle parameters which we got
482:25 - using this circles variable that is
482:29 - x and y coordinate and the radius into
482:32 - an integer so to do that we are going to
482:37 - just declare a parameter called
482:40 - detected circles
482:41 - and then we are going to use numpy
482:44 - to convert them into an integer so i'm
482:47 - going to just say np dot
482:49 - u
482:50 - in 16
482:52 - and then in the parenthesis i'm going to
482:54 - just use np
482:57 - dot around and we are going to pass our
483:00 - circles
483:02 - parameter which we got using the half
483:05 - circles method now in the next step we
483:07 - are going to iterate over those detected
483:10 - circles so we are going to just say
483:13 - uh
483:14 - for
483:15 - and because
483:16 - this circle vector is going to give us x
483:20 - y and the radius we can directly uh just
483:23 - extract those values so we're going to
483:25 - just say x comma y comma r
483:29 - and then
483:30 - in
483:31 - our detected circles
483:34 - and those circles will be
483:37 - at this index so 0 comma colon and then
483:41 - inside this for loop we are going to
483:43 - first of all draw the circle and also we
483:46 - are going to draw the center
483:48 - so to draw the circle we already know
483:52 - that
483:53 - we have this circle method available
483:56 - which takes view parameter first is the
484:00 - uh image so we are going to pass the
484:03 - copy of this image here so let's pass
484:06 - this copy which is output the second
484:08 - argument here will be the center which
484:10 - are the coordinate of the center which
484:12 - we already got
484:14 - in the form of x comma y so we will give
484:17 - them
484:18 - uh in the form of tuple the third
484:20 - argument is the radius so radius
484:22 - is extracted in the r parameter here so
484:25 - we're going to pass
484:27 - the radius here and then the next
484:30 - parameter will be
484:32 - the color so let's start with
484:35 - let's say green color
484:38 - and then the thickness so we are going
484:40 - to give the thickness of let's say
484:43 - 3 here
484:45 - similarly when we use the same circle
484:48 - method and we want to draw the center
484:51 - then
484:52 - we know that this is the center so these
484:55 - are the coordinates of the center and if
484:58 - the radius is very small let's say 2
485:01 - then it's going to just draw a small
485:04 - point right so that's why i have given
485:07 - very small value for example 2 here so
485:11 - it's going to just draw a very small
485:13 - circle which will look like a
485:16 - dot on the circle that's why this value
485:20 - is very small and we
485:23 - are going to just say that this will be
485:26 - also three and let's change the color of
485:29 - this dot let's say
485:30 - this will be this color
485:33 - okay so we are just drawing those circle
485:37 - on the copy of the image which is called
485:41 - the output
485:42 - so let's run this code and let's see
485:44 - what happens when we run this code
485:47 - and you can see
485:48 - this dot
485:50 - is
485:52 - first of all drawn on each circle which
485:55 - is detected which is in the form of
485:58 - yellow
485:59 - and also in the form of green
486:02 - all the circles are drawn so you can see
486:05 - this circle is drawn so every uh
486:09 - circular shape is uh
486:12 - you know enclosed by
486:14 - the detected circle
486:17 - we also strangely detected this circle
486:21 - uh somehow because uh open c we think
486:24 - that this is also a circle i have one
486:27 - more image which is
486:29 - shapes.jpg so we are going to
486:32 - just uh
486:34 - see that also so i'm going to just say
486:36 - shapes dot jp g let me show you this
486:40 - image first of all so it looks like this
486:42 - so it has only one circle and some other
486:45 - shapes right so we are going to just run
486:48 - this code once again
486:50 - and you can see it just detect this
486:53 - circle and it just draws a small dot on
486:57 - the center
486:58 - and all the other shapes are undetected
487:02 - so this is how you can detect the
487:04 - circles inside an image using half
487:08 - circle transform
487:10 - in this video we are going to discuss
487:12 - about the basics of face detection using
487:15 - har feature based cascade classifiers
487:19 - so object detection using har feature
487:22 - based cascade classifiers
487:24 - is an effective object detection method
487:28 - proposed by paul viola and michael jones
487:31 - in their paper now haar feature based
487:34 - cascade classifier is a machine learning
487:37 - based approach
487:39 - where a cascade function
487:41 - is trained for a lot of positive and
487:45 - negative images now what are these
487:47 - positive and negative images
487:50 - so first a classifier is trained with
487:53 - few hundred sample views of particular
487:57 - object that is a face or a car
488:00 - or any other object
488:02 - that is called a positive example so
488:05 - whatever you want to detect if you train
488:07 - your classifier with those kind of
488:10 - values so for example if you want to
488:12 - detect face
488:14 - then you need to train your classifier
488:16 - with the number of images which contain
488:19 - faces so these are called the positive
488:22 - images which contains the object which
488:25 - you want to detect similarly we want the
488:27 - classifier
488:29 - to train with the negative images that
488:32 - means the images which doesn't contain
488:35 - the object which you want to detect so
488:37 - in our case
488:38 - for example we want to detect the face
488:41 - then the image which doesn't contain the
488:44 - face
488:45 - then it is called the negative image and
488:48 - if the image contains face or number of
488:50 - faces then it's called the positive
488:53 - image and after a classifier is trained
488:56 - it can be applied to a region of
488:58 - interest in
489:00 - an input image and the classifier
489:02 - outputs
489:03 - a 1 if the region is likely to show the
489:07 - object or zero otherwise so let's see
489:10 - how we can use har cascade detection in
489:14 - opencv
489:15 - so opencv comes with a trainer as well
489:19 - as a detector so if you want to train
489:22 - your classifier for any object for
489:24 - example a watch or a car or a train or
489:29 - anything
489:30 - then you can use
489:32 - this classifier also on opencv's github
489:37 - page you can find some trained
489:39 - classifier xml files
489:42 - so let me show you these classifiers on
489:45 - the opencvs github page so here is the
489:50 - opencv repository and inside this
489:53 - repository you can see this data folder
489:56 - and then go to
489:57 - har cascades i will just share the link
490:01 - with you in the description so you can
490:03 - directly navigate to
490:06 - this website and this location
490:09 - and you can see plenty of
490:11 - trained classifiers are available inside
490:15 - this repository
490:16 - so for our example we want to detect the
490:20 - face so we are going to use this trained
490:24 - classifier which is called heart cascade
490:27 - underscore
490:28 - frontal face underscore default dot xml
490:32 - file so you just need to open this file
490:35 - and then download it you can just
490:38 - click on the raw
490:40 - icon here this button and once this raw
490:44 - file is open you can just right click
490:47 - and save it on your computer so you can
490:51 - just say save page as and then you can
490:54 - just save this inside your opencv
490:58 - project
490:59 - so i have already saved this file inside
491:02 - my opencv project you can see uh this
491:05 - file here which is a xml file which i
491:08 - have downloaded using this repository so
491:11 - as you can see here i have this code
491:13 - which is the minimal code to load an
491:16 - image and show it using opencv window
491:19 - now in the next step
491:21 - what i'm going to do is
491:23 - before this reading we are going to just
491:27 - define our classifiers so because it's a
491:30 - face classifier i'm going to name my
491:32 - variable as face
491:34 - cascade
491:36 - and then
491:37 - in opencv there is a method called so
491:40 - i'm going to just call this method
491:42 - and there is a method called cascade
491:45 - classifier so
491:47 - this is this method called cascade
491:50 - classifier where you can provide your
491:54 - classifier name which is the xml file so
491:57 - just provide your trained classifier
492:01 - file name in our case it's hard cascade
492:04 - underscore
492:05 - frontal face underscore default dot xml
492:08 - so once we have our classifier we read
492:12 - the image and then because this
492:15 - classifier will work with the grayscale
492:17 - images we are going to convert our image
492:20 - into a grayscale image and it will be
492:23 - really easy to convert our image to a
492:26 - grayscale image now once we have our
492:28 - grayscale image the next step is to
492:30 - detect the faces inside this image
492:33 - so for that we are going to declare this
492:36 - variable let us say faces and then we
492:39 - are going to
492:40 - use this result which we got
492:43 - using this
492:45 - cascade classifier and then
492:48 - we can call a method called detect multi
492:52 - scale so we are going to just call this
492:56 - method which takes few argument first is
492:58 - the image so we are going to provide our
493:02 - grayscale
493:03 - image here and the second argument we
493:06 - are going to use here will be the scale
493:08 - factor so the scale factor parameter
493:11 - specifies how much the image size is
493:14 - reduced at each image scale so to start
493:18 - with we are going to provide a 1.1
493:22 - value here and then the next parameter
493:24 - which we are going to provide here will
493:26 - be the min
493:27 - neighbors parameter so min neighbors
493:31 - parameter is going to specify how many
493:33 - neighbors each candidate rectangle
493:36 - should have to retain it so we are going
493:39 - to provide this value 4 here to start
493:42 - with and if
493:44 - it doesn't give us the proper result we
493:47 - are going to change it and the last step
493:49 - here will be to iterate over all the
493:52 - faces which we have detected and then
493:55 - draw a rectangle on them so this face
493:58 - variable will be the vector of rectangle
494:01 - where each rectangle contains
494:04 - the detected object in our case this
494:06 - will be the detected face so the
494:09 - rectangle may be partially outside the
494:12 - original image if
494:14 - it's on the corner so the if
494:16 - the object or the face is on the corner
494:21 - then this rectangle may be a little bit
494:23 - outside the original image so we are
494:26 - going to iterate over this
494:28 - faces
494:30 - object and here we are going to get the
494:34 - parameter x comma y comma
494:38 - w comma h which means
494:40 - the values of x and y and
494:43 - the width and height of the rectangle of
494:47 - the object in our case this is the faces
494:51 - right so we got
494:54 - all the four parameters for drawing the
494:56 - rectangle and then we can just call cv2
495:01 - dot
495:02 - rectangle method to draw the rectangles
495:05 - the first parameter here will be the
495:07 - image the second parameter will be the
495:10 - point 1 which will be x
495:12 - comma y which we got using this
495:15 - faces
495:17 - vector
495:18 - and then
495:19 - we need to give the second point which
495:22 - will be x plus w
495:25 - comma y plus
495:27 - height okay
495:29 - and then the next two parameters are the
495:31 - color and
495:33 - the thickness so we are going to give
495:35 - the color 255 comma 0 comma 0 here and
495:39 - the thickness to start with we are going
495:41 - to give a 3 here that's it so it's this
495:44 - simple to detect faces inside the images
495:49 - using har cascade classifiers
495:52 - so now i'm going to run this code and
495:54 - let's see what happens
495:56 - so you can see this is the face so this
496:00 - is how you can detect the face or a
496:02 - multiple number of faces inside an image
496:05 - let's try to detect the face
496:08 - inside a video
496:10 - so i'm going to just close uh this
496:13 - window and now we are going to try to
496:16 - detect the face inside a video so this
496:18 - will be nothing different than this
496:22 - approach we just need to apply this
496:24 - approach on each and every single frame
496:28 - so instead of
496:29 - this
496:30 - code we are going to
496:33 - use the video capture method to capture
496:35 - the video so you can see i have this
496:38 - test.mp4 video here so we are going to
496:41 - define a cap variable is equal to cv2
496:46 - dot
496:47 - video capture and then in the
496:50 - parenthesis we are going to provide
496:52 - the
496:53 - test dot mp4 file here or if you have
496:56 - the camera you can provide zero here as
497:00 - the parameter and then all this code we
497:03 - are going to just enclose inside a while
497:06 - loop so we are going to just say that
497:11 - while cap
497:12 - dot is opened so if cap dot is opened is
497:16 - going to give us a true value then we
497:20 - are going to read the frame so
497:24 - underscore
497:25 - let us say
497:27 - the parameter name will be img in this
497:30 - case also
497:32 - normally we take the variable name frame
497:34 - here because
497:35 - we are reading each and every frame
497:38 - and then i'm going to just say cap dot
497:41 - read okay
497:43 - cap dot read this means we are reading
497:46 - every frame and let's uh enclose this
497:49 - code also
497:51 - inside this while loop so i'm going to
497:53 - just provide a little space here so
497:56 - basically we are getting
497:59 - every frame and then applying the same
498:02 - procedure on each and every frame and at
498:05 - last outside our while loop we are going
498:08 - to release
498:09 - our
498:10 - cap so we are going to just say cap dot
498:14 - release and here instead of using this
498:17 - cv2 dot weight key we are going to
498:20 - provide a condition if
498:22 - cv2 dot weight key and in the
498:26 - parenthesis we are going to provide 1
498:28 - and 0 x f f is equal to our d
498:34 - and we are going to listen for
498:36 - the key uh queue so if somebody presses
498:39 - the
498:40 - key q
498:42 - then we are going to break out of this
498:44 - while loop so let's run this uh
498:47 - script and let's see what happens when
498:49 - we run the same script on a video so
498:52 - this is the video
498:53 - and this is uh in this video so you can
498:56 - see in this video the face is detected
498:58 - in real time in the real
499:01 - live video so this is how you can use
499:04 - haar based cascade classifiers to detect
499:08 - faces or any other object
499:10 - inside an image in the last video we
499:13 - have seen the basics of face detection
499:16 - using haar feature based cascade
499:18 - classifiers
499:20 - in this video we are going to extend our
499:22 - code to detect eyes using the same har
499:26 - cascade classifier
499:28 - so for that first of all you need to
499:31 - download the pre-trained class
499:34 - classifier for the eyes from the same
499:37 - source which i have shown you last time
499:39 - also
499:40 - which is the github repository of opencv
499:44 - again i'm going to
499:46 - give you this link in the description so
499:48 - you can directly come to this page
499:51 - and this time we are going to
499:53 - download this xml file with the name
499:56 - har cascade i
499:58 - underscore tree underscore i glass dot
500:01 - xml file
500:03 - so this is the pre-trained classifier
500:07 - for detecting eyes so you can just click
500:10 - on draw and
500:12 - then save it as
500:14 - this same file name in your project okay
500:18 - so i have
500:20 - already downloaded this xml file you can
500:23 - see here our cascade i underscore tree
500:26 - underscore iglast.xml file and now we
500:29 - are ready to
500:30 - write our code so this is the code which
500:32 - we have written
500:34 - last time so if you don't know how this
500:36 - code works you can see the last video
500:39 - i'm going to just extend this code to
500:41 - detect eyes
500:43 - so first thing first we need to create
500:47 - the
500:48 - cascade classifier for the eyes so
500:51 - instead of face cascade we are going to
500:54 - name it as
500:56 - i
500:57 - cascade and this file name will be
501:00 - the file which we have downloaded which
501:02 - is
501:03 - i underscore tree underscore i class dot
501:06 - xml file
501:07 - so once we have our classifier
501:10 - then
501:11 - in the last video we have already seen
501:13 - how to detect faces
501:15 - so our region of interest will be the
501:18 - face this time
501:19 - because the eyes will not be present
501:23 - outside the face right so eyes will
501:25 - always be present inside the face so our
501:28 - region of interest
501:29 - will be the face and face
501:32 - we have already detected last time so
501:34 - this
501:35 - face will be now our region of interest
501:39 - so go inside this for loop where we are
501:42 - iterating over this
501:44 - face
501:45 - variable and then we are going to create
501:48 - our roi so i am going to create this
501:51 - variable which is called roi underscore
501:54 - gray and
501:56 - this will be the original grayscale uh
502:00 - image which we have
502:02 - created here but we just want the face
502:05 - out of this image so we can
502:08 - just index the space using y colon y
502:12 - plus
502:13 - h
502:14 - comma
502:15 - x colon
502:17 - x plus w which is the width so this line
502:20 - is going to give us the grayscale region
502:23 - of interest but we also want the colored
502:26 - image also so we are going to just say
502:29 - roi color
502:31 - which will be
502:32 - the colored roi and here instead of gray
502:36 - we are going to take the direct image
502:40 - which will be
502:41 - before
502:42 - we have converted
502:44 - this bgr image to the grayscale image so
502:46 - we have the colored roi and the
502:49 - grayscale roi once we have this we will
502:52 - follow
502:53 - the same
502:55 - concept which we have applied for
502:57 - detecting the faces so so we are going
502:58 - to use this detect multiscale method so
503:02 - i'm going to
503:03 - just write eyes
503:06 - is equal to because we already have our
503:09 - i cascade which is a classifier so we
503:12 - are going to use this variable and then
503:15 - use this method called detect multiscale
503:18 - and then we are going to simply uh pass
503:21 - our
503:22 - roi gray which we got using the faces
503:26 - now we are going to iterate over those
503:30 - eyes so inside this for loop we are
503:33 - going to create one more for loop to
503:35 - iterate over all the eyes
503:37 - which are found on the face so far and
503:40 - then this will be e x
503:43 - comma e y
503:45 - comma e w comma
503:49 - e
503:50 - h for x y coordinate and the width and
503:52 - height now we will just say in
503:55 - eyes
503:56 - and then
503:58 - we are going to just draw this rectangle
504:00 - which is also very simple cb2 dot
504:03 - rectangle
504:05 - and then we are going to pass our image
504:08 - first of all which will be our colored
504:10 - roi image which is
504:12 - this one so here we will pass
504:15 - this roi color
504:17 - and then
504:18 - the first point in the rectangle which
504:21 - will be
504:22 - ex and ey so i'm going to just say ex
504:26 - comma e y
504:28 - and the second point will be
504:31 - e x plus e w which is x plus width so we
504:36 - are going to just write this e x plus
504:38 - e w comma
504:41 - e y plus e h which is the y coordinate
504:45 - and the
504:46 - height
504:47 - the next parameter will be the color so
504:50 - let's uh provide the color let's say
504:52 - this will be 0 comma 255 comma 0
504:57 - and then the next parameter will be the
504:59 - width so let's say the width we want
505:01 - here
505:02 - is five so that's it so hopefully this
505:05 - code is going to work out of the box we
505:07 - don't need to do anything else
505:10 - we just need to define our classifier
505:14 - and then we just need to
505:16 - use this detect multiscale
505:19 - method to detect the eyes and then we
505:21 - just need to draw the rectangle on all
505:24 - the eyes which are detected so let's run
505:26 - this code and let's see what happens so
505:28 - we are going to see you can see eyes are
505:30 - detected but there is some problem
505:33 - because
505:35 - something is wrong so i'm going to just
505:37 - quit this script and see what's going
505:41 - wrong here so you can see
505:43 - this should be x e x comma e y
505:47 - and then
505:48 - our problem will be solved hopefully so
505:50 - i'm going to run this code once again
505:52 - and you can see the eyes are properly
505:55 - detected so this is how you can detect
505:57 - eyes in the face
506:00 - using opencv and har cascade classifiers
506:04 - in this video we are going to try to
506:06 - understand how we can find out the
506:08 - corners
506:10 - inside an image using a method called
506:13 - harris corner detection
506:15 - now first of all what are corners
506:18 - so corners are the region in the image
506:21 - with large variation in intensity in all
506:24 - the direction
506:26 - now this harris corner detector was
506:28 - first introduced by chris harris
506:31 - and mike stephens in their paper in
506:35 - 1988
506:38 - now detecting corners using harris
506:40 - corner detector contains three main
506:43 - steps
506:45 - so the first step is to determine which
506:48 - windows produces very large variation in
506:52 - intensity when we move
506:54 - in the x direction and the y direction
506:58 - now what are windows here
507:01 - so windows in this case means that
507:04 - let's say we want to just find out this
507:07 - corner here so windows will be
507:10 - your small
507:11 - box here
507:13 - and then you check for the intensity
507:17 - when you move in the
507:20 - vertical direction and also
507:23 - in the
507:24 - horizontal direction
507:26 - so you check for
507:28 - the change or large variation
507:31 - in the intensity when you move in the x
507:33 - direction and when you move in the y
507:36 - direction in the second step with each
507:39 - such window which we found a score r is
507:43 - computed
507:44 - so this r value which is computed
507:48 - is going to give us the estimate or
507:51 - give us an idea about where this corner
507:54 - is located depending upon the value of r
507:59 - and in the third step after applying a
508:02 - threshold to this score
508:04 - the important corners are selected and
508:07 - marked
508:08 - so let me explain you
508:10 - all these steps one by one what do i
508:14 - mean by detecting the windows and
508:17 - calculating the value of ours let's see
508:21 - step by step
508:22 - so as i said in the first step we
508:24 - determine which windows produces
508:27 - very large variation in the intensity
508:31 - in the x direction and in the y
508:34 - direction so let's say a window or a
508:37 - center is located at the position x
508:40 - comma y and let's say the intensity of
508:44 - the pixel at this location is i
508:48 - x comma y
508:49 - so if this window is slightly shifted
508:53 - to a new location and let's say this
508:56 - displacement is u comma v
509:00 - then the intensity of the pixel at this
509:03 - location
509:04 - will be x plus u
509:06 - and y plus v because our displacement is
509:11 - u comma v so we are just adding it uh to
509:15 - the x value and the y value and hence
509:18 - the difference between the shifted
509:21 - intensity
509:22 - and the original intensity will be the
509:26 - difference in the intensities of the
509:29 - windows shift
509:31 - so for a corner this difference will be
509:35 - very large and that's how we detect the
509:39 - corners using
509:41 - this harris corner detection method
509:45 - now as you can see here
509:47 - this value will be
509:50 - given in the
509:52 - e
509:53 - u comma v format so we have to maximize
509:56 - this function for the corner detection
509:59 - and this we can achieve by applying a
510:02 - taylor expansion
510:05 - to
510:05 - this equation which is given here and by
510:09 - using some mathematical steps
510:11 - so i'm not going to go deep into the
510:14 - mathematical steps but after applying
510:17 - the taylor expansion you will get
510:20 - this kind of approximate value where m
510:25 - is equal to
510:26 - this value and here in this equation i x
510:30 - and i y are the image derivatives in the
510:34 - x and y direction respectively
510:38 - so this can be easily found out using
510:41 - the
510:42 - cv
510:43 - dot sobel method in opencv
510:47 - now
510:48 - comes the second step and in this step
510:51 - we find out or calculate the score for r
510:56 - so this r is equal to this value and the
511:00 - m we have already uh seen how we can get
511:04 - this m value in the first step right
511:08 - so in this equation
511:11 - d e t m
511:13 - is equal to lambda 1 multiplied by
511:16 - lambda 2
511:18 - and trace m
511:19 - is equal to lambda 1
511:21 - plus lambda 2 where lambda 1 and lambda
511:24 - 2
511:25 - are the eigenvalues of m so again if you
511:28 - want to go into the details you can
511:31 - refer to some book or you can go to the
511:34 - wikipedia page to learn more about this
511:38 - equation so once we got the value of r
511:42 - then based upon the value of r
511:45 - we can make some decision and this we
511:48 - can do
511:49 - in the third step
511:51 - so if the value of r is very small that
511:55 - means
511:56 - the value of lambda 1 and lambda 2
512:00 - are also very small
512:02 - and we can conclude that the region is a
512:06 - flat region and not the corner
512:09 - if the value of r is less than 0 that
512:12 - means lambda 1 is very large in
512:15 - comparison to lambda 2 or vice versa and
512:19 - that means it's an edge and not the
512:22 - corner
512:24 - and if the value of r is large which
512:28 - happens when lambda 1 and lambda 2 are
512:31 - large and this means
512:33 - that this region is a corner
512:36 - so if the value of r is very large that
512:40 - means the region is a corner and that's
512:43 - how harris corner detector detects if
512:46 - it's a corner or a edge or a flat
512:50 - area so this was the theory about harris
512:53 - corner detector let's see how we can use
512:57 - this harris corner detection concept
513:00 - inside opencv
513:02 - using our python code
513:05 - so i have this
513:07 - script already written here so just
513:10 - import cv2 and numpy
513:12 - and then we are reading this image
513:14 - called crossboard underscore image.png
513:17 - using i'm read method
513:19 - and after
513:21 - we
513:22 - read this image i'm just showing the
513:24 - original image so we have the original
513:27 - image and the output at the end to
513:30 - compare
513:31 - now in the next step i'm converting
513:34 - this image into a grayscale image to get
513:37 - the better
513:39 - results
513:40 - and because the cv2 dot
513:43 - corner harris method
513:45 - takes
513:46 - the grayscale image in the float
513:50 - 32
513:52 - format that's why we need to convert our
513:55 - image into float 32 format so that's why
514:00 - we are using numpy.float32
514:02 - to convert this image into
514:05 - uh floating point values because our
514:08 - corner harris method which we are going
514:11 - to use in the next step
514:13 - is going to take this kind of value and
514:16 - not
514:17 - the value which comes directly from the
514:20 - conversion of this image to the
514:22 - grayscale image so this step is
514:25 - necessary for
514:27 - the harris corner method
514:30 - and in the next uh step we are just
514:32 - applying the cv
514:34 - 2 dot corner harris method
514:37 - which takes few arguments first is our
514:40 - image
514:41 - in the floating point so this we have
514:44 - passed and the second parameter here is
514:48 - called the block size
514:50 - so here i have given the value 2 here so
514:53 - block size
514:54 - means
514:55 - the window
514:56 - in the first step so we have seen we
514:59 - have to define the window right so
515:02 - for example we define this block size 2
515:05 - that means neighborhood size is equal to
515:08 - 2 that means for each pixel value block
515:12 - size multiplied by block size that means
515:14 - 2 by 2 neighborhood is considered
515:18 - the next parameter here is called the k
515:21 - size and it's the aperture parameter for
515:24 - the sobel operation
515:26 - and then we have the next parameter here
515:30 - and this next parameter is called the k
515:34 - which is the harris detector free
515:37 - parameter in the equation
515:40 - so after applying this harris corner
515:44 - method to our image we get this
515:47 - destination image and to get the better
515:51 - result we need to dilate this result so
515:55 - we apply cv2.dilate method on our
515:59 - uh
516:01 - image which we get
516:02 - using the harris corner so this image
516:05 - are marked through the dilated corners
516:09 - and then in the next step we are
516:10 - reverting back to the original
516:13 - image with optimal threshold value and
516:17 - we are just
516:19 - just marking all our corners with this
516:23 - color so basically we want to mark all
516:26 - the corners with the red color here
516:29 - and in the next step we are just
516:32 - showing our result in the i am show
516:35 - window
516:36 - and at last we are destroying all the
516:38 - windows so let's run this code and let's
516:41 - see what happens when we run this code
516:44 - and
516:46 - we will see the results
516:47 - so you can see this is the original
516:49 - image which have so many corners
516:53 - and all the
516:55 - corners are detected and
516:58 - it's
516:59 - marked with this red color here so this
517:03 - is how you can find out and mark all the
517:06 - corners
517:07 - using harris corner detection
517:10 - in opencv in the last video we have seen
517:12 - how we can use harris corner detector
517:16 - in order to find out the corners inside
517:18 - an image in this video i'm going to show
517:21 - you
517:22 - how you can use sheet tomasi corner
517:25 - detector method to detect the corners
517:28 - inside an image
517:30 - so in late 1994
517:33 - j she and c thomasi made a small
517:37 - modification in the harris corner
517:40 - detector method in their paper which was
517:43 - called good features to track so this
517:46 - shi thomasi method is similar to harris
517:49 - corner detector apart from the way the
517:52 - score of r is calculated which we have
517:55 - seen in the last video
517:58 - so this sheet omasi method gives us
518:01 - better result in comparison to harris
518:04 - corner detector and also when you use
518:08 - this c to marsy method we can find the
518:12 - top and corners which means we can
518:15 - provide the number of corners we want
518:18 - and this might be useful in cases where
518:22 - we don't want to detect all the corners
518:25 - inside an image
518:27 - so let's see in the code
518:29 - how we can implement this sheet tomasi
518:32 - corner detector
518:34 - in opencv
518:35 - so here i have already written all the
518:38 - code so let me explain you
518:40 - all the lines of the code one by one
518:43 - so as you can see here i'm just
518:45 - importing the libraries in the first two
518:48 - line and in the next line i'm just
518:51 - reading
518:52 - the image using i'm read method and then
518:56 - i'm converting this image into a
518:58 - grayscale image
519:00 - using this cbt
519:02 - color method so i'm converting this
519:05 - image from bgr to grayscale image
519:09 - now as i said the paper
519:11 - which was published by she and tomasi
519:15 - was named good features to track that's
519:18 - why in opencv
519:21 - this method is also called good features
519:25 - to track
519:26 - so
519:27 - here in this line we are just using this
519:30 - method cv dot good features to track
519:34 - which takes few arguments so first
519:37 - argument here is our input image which
519:40 - is a grayscale image which we are
519:42 - providing as the first parameter
519:45 - the second parameter is the maximum
519:47 - number of corners
519:49 - so here we can limit the number of
519:52 - corners we want to detect so for example
519:55 - i have given 25 here that means we just
519:58 - want to detect 25 corners and if
520:03 - there are
520:04 - more than 25 corners which are present
520:08 - in the image they will not be shown so
520:11 - this value means maximum number of
520:13 - corners to return and if there are more
520:17 - corners than
520:18 - the corners found then the strongest
520:22 - corners will be returned right
520:26 - now the third parameter here is called
520:30 - the quality level
520:32 - so this is the parameter characterizing
520:36 - the minimal expected quality of the
520:39 - image corner
520:40 - the next parameter here is the min
520:43 - distance which is the minimum possible
520:46 - euclidean distance between the returned
520:50 - corners so i have taken
520:52 - 10 here as the minimum distance and
520:56 - 0.01 as the quality level now once all
521:02 - the corners are detected using this good
521:05 - features to track method we convert
521:08 - those corners into
521:11 - the integer values
521:13 - and here int 0 is a mere alias for
521:18 - int 64. and once all the corners are
521:22 - detected we iterate over all the corners
521:26 - and then we find out the value of x and
521:29 - y
521:30 - using this i
521:32 - and then it's easier to just draw the
521:37 - circles
521:38 - over these
521:39 - values using the cb
521:42 - dot circle method so this cb dot circle
521:45 - method takes
521:46 - few arguments first is the input image
521:49 - so the second parameter is the center
521:51 - the third parameter is the radius of the
521:53 - circle which we want to provide the
521:56 - fourth parameter is the color we want to
521:59 - provide and the fifth parameter
522:02 - is
522:03 - the thickness and if it's minus one that
522:05 - means we want to fill
522:07 - the color inside that circle
522:11 - and at last once all the circles are
522:14 - drawn on the corner which are detected
522:17 - then we are just showing this using i'm
522:20 - sure method
522:22 - so let's see how
522:23 - this works in the case of sheet omasi
522:27 - method on an image
522:30 - so i'm going to run this code so you can
522:32 - see
522:32 - all the corners inside this image are
522:36 - detected and because we have just
522:39 - provided this number 25 here so maximum
522:43 - number of corners which will be detected
522:45 - here will be 25
522:48 - and the rest of them will not be shown
522:51 - so if we increase the value of the
522:54 - maximum number of corners let us
522:56 - increase it to 100 let us say so i am
523:00 - going to just increase it to the value
523:02 - 100 you will see more number of circles
523:07 - are drawn on an image now let's compare
523:10 - the result of the harris corner detector
523:13 - and shea tomasi corner detector so on
523:17 - the left hand side you can see the
523:19 - original image and this middle image
523:22 - shows the harris corner detector
523:25 - method and you can see all the corners
523:28 - are detected using harris corner
523:30 - detector
523:31 - and
523:32 - using the sheath omasi corner detector
523:35 - it gives us better result and we can
523:38 - control the number of corners we want to
523:40 - detect
523:41 - and you can see all the important
523:44 - corners are detected using
523:47 - sheet omasi corner detector in a better
523:50 - way
523:51 - so that's how she tomasi corner detector
523:55 - works in this video we are going to see
523:57 - how to use background subtraction method
524:00 - in opencv
524:02 - so first of all what is background
524:04 - subtraction so background subtraction is
524:08 - common and widely used technique for
524:11 - generating the foreground mask which is
524:15 - also known as the binary image
524:17 - containing the pixels belonging to the
524:21 - moving object of a scene
524:23 - when
524:24 - these images are captured using a static
524:29 - camera and as the name suggests
524:31 - background subtraction calculates the
524:34 - foreground mask performing the
524:36 - subtraction between the current frame
524:39 - and the background model containing
524:42 - the static part of the scene so for
524:46 - example the background subtraction
524:48 - method can be used in the case of
524:51 - visitor counter where
524:53 - you have a static camera capturing the
524:56 - number of visitors
524:58 - entering or leaving the room
525:00 - or you have a traffic camera
525:03 - which wants to count the
525:06 - various telematic data
525:09 - from the moving car or moving car data
525:13 - which is captured by that traffic camera
525:17 - now there are several algorithms which
525:20 - were introduced for the purpose of this
525:23 - background subtraction and opencv has
525:26 - implemented few of them which we are
525:28 - going to see
525:29 - one by one
525:31 - so as you can see here i have this
525:34 - example which is a very simple example
525:37 - of just
525:39 - taking a video and then we are
525:42 - extracting each and every frame of that
525:44 - video and showing it into
525:48 - a window so using i am show method i am
525:51 - just showing each and every frame of
525:54 - that video
525:55 - so this you already know from the
525:58 - previous videos how to
526:00 - capture the video frames from a video
526:04 - file or
526:05 - the live camera so when i run this code
526:09 - you will see that there are few uh
526:11 - persons which are moving here and we
526:15 - want to uh detect all those moving uh
526:19 - persons which are moving in the image so
526:23 - for that we are going to use a few
526:26 - methods which are available in opencv so
526:30 - let's first write some code and i will
526:32 - explain you
526:34 - what this code is going to do
526:37 - so i'm going to define a variable after
526:41 - this line of code and i'm going to
526:44 - define a variable name
526:46 - f g b g for foreground
526:49 - background and then i'm going to
526:52 - just call cv
526:55 - dot b g
526:56 - s e g m so b g s e
527:00 - g m
527:01 - and then i'm going to call a method
527:03 - called create background subtraction m o
527:06 - g method so this create background
527:09 - subtraction mog method
527:11 - is a gaussian mixture based background
527:14 - and foreground segmentation algorithm so
527:17 - using this line what we are doing is we
527:20 - are just creating a background object
527:23 - of the function using this method create
527:27 - background subtraction emoji
527:30 - now uh this method has some optional
527:33 - parameters
527:35 - like history number of gaussian mixtures
527:38 - and
527:39 - threshold but all of them are set by
527:43 - default so you don't need to set
527:46 - anything
527:47 - specifically unless you want to change
527:49 - some of the optional parameters so i'm
527:51 - going to leave everything as default and
527:53 - i'm not going to give
527:55 - any
527:56 - argument here for this method and then
528:00 - after i captured each and every frame
528:03 - inside this while loop what i'm going to
528:06 - do is i'm going to create a new variable
528:08 - called fgmask for
528:11 - foreground mask so i'm going to just
528:13 - write fgmask is equal to and for getting
528:17 - the foreground mask we are going to just
528:20 - call
528:21 - a method called apply on this fgbg or
528:26 - the background subtractor image so we
528:28 - are going to just take
528:30 - fgbg and then we are going to call
528:34 - a method called apply here and it takes
528:37 - one argument which is the frame
528:39 - which we are capturing okay
528:43 - so we have applied this method and then
528:46 - we are just
528:48 - getting the foreground mask using the
528:52 - apply method on this background
528:54 - subtractor
528:55 - variable
528:56 - and that's it so this is your foreground
528:59 - mask so
529:01 - when i just use one more
529:04 - i'm sure window and this is
529:07 - for the fg frame let's say so fg uh
529:11 - mask frame let's say okay so fg mask
529:14 - frame
529:15 - and we are going to just
529:17 - pass this argument here
529:20 - so let's see what result we get after we
529:23 - apply create background subtractor mog
529:25 - method
529:27 - so you can see this normal image here
529:30 - and also you will see
529:33 - uh these
529:35 - moving persons
529:37 - in the foreground mask right so you have
529:40 - subtracted the background for from the
529:43 - foreground
529:44 - and you can easily detect the moving
529:48 - persons here inside this image using
529:51 - this mask you will also observe that
529:54 - there is a very uh little noise not much
529:58 - when you use this kind of subtraction
530:01 - using
530:02 - create
530:03 - background subtractor mog method there
530:05 - is one more method which is called
530:07 - background subtractor mog2 which is also
530:11 - gaussian mixture based background for
530:13 - and program segmentation algorithm so
530:16 - let's use that method also so this
530:19 - method is directly available under cv2
530:23 - so you just need to write cv dot create
530:26 - background subtractor mog2
530:29 - okay and everything will remain the same
530:32 - so it's going to return you the
530:34 - background subtractor variable which you
530:37 - can use
530:38 - uh with this apply method to get the
530:41 - foreground a mask okay so let's see how
530:45 - this method performs so you can see the
530:48 - result which is quite different from the
530:52 - first method which we have used so in
530:54 - the previous case we have
530:56 - to create the background subtractor
530:58 - object
530:59 - and here in this method you have an
531:02 - option of detecting the
531:04 - shadows so there is an optional
531:07 - parameter which you can give
531:10 - into this method which is this create
531:13 - background substructure
531:15 - mog2 which is called detect shadows so
531:20 - by default this detect shadows is true
531:23 - that's why you see the shadows there if
531:27 - you just write the text shadows is equal
531:29 - to false then it's going not going to
531:32 - detect the shadows so i'm going to just
531:34 - run
531:35 - the code once again
531:37 - and you can see now
531:40 - shadows are less visible right so let's
531:44 - run the default case once again so let's
531:46 - say we just write true here
531:49 - and you will see the shadows in the gray
531:54 - color right so these shadows in the gray
531:57 - color and when we
531:59 - just make it false so the text shadows
532:03 - false
532:04 - you will not see that gray color okay so
532:06 - shadows are
532:08 - displayed in the gray color so if you
532:10 - don't see any gray color then shadows
532:13 - are not detected this is a noise which
532:16 - is detected but not the shadow okay
532:20 - so this is uh the difference between
532:24 - the first background subtractor method
532:26 - and the background subtractor mog2
532:28 - method there is
532:31 - one more method which is called the
532:33 - background subtractor
532:35 - gmg
532:37 - so this algorithm which we are going to
532:39 - use so let's use this uh method first of
532:43 - all which is called background
532:46 - subtractor
532:47 - g
532:48 - m
532:49 - g
532:50 - which is available under cv
532:52 - dot bg segment as the first method so
532:56 - just write b g s e g m dot create
533:00 - background subtractor gmg method so this
533:03 - create background subtractor gmg method
533:06 - algorithm combines statistical
533:09 - background image estimation and p pre
533:12 - pixel
533:13 - biasing segmentation
533:16 - let's see how this method performs when
533:18 - we just use this gmg method
533:22 - and when you will see here there is
533:24 - nothing on this uh
533:28 - foreground mask frame so to get the
533:30 - better result you need to apply
533:33 - morphological opening to the result to
533:36 - remove the noises
533:37 - so we are going to
533:39 - do just that so i'm going to just
533:41 - overwrite this
533:43 - fgmask
533:45 - frame
533:47 - using a method called cv dot
533:50 - morphology x this also we have seen in
533:54 - the previous videos right so the first
533:58 - uh
533:59 - parameter here will be
534:01 - fg mask parameter the second parameter
534:04 - here will be uh the
534:07 - op so
534:09 - cv2 dot
534:11 - morph open we are going to use the morph
534:14 - open
534:15 - uh method and then the third parameter
534:18 - will be the kernel so we need to define
534:20 - the kernel also so for defining the
534:23 - kernel let's define the kernel outside
534:25 - this while loop so i'm going to just
534:27 - write
534:29 - kernel which takes few argument first is
534:32 - the shape so we are going to say we want
534:36 - the
534:36 - more
534:37 - eclipse shapes so i'm going to just
534:39 - write
534:41 - more
534:42 - eclipse and then the kernel size will be
534:46 - let's say three comma three okay so we
534:48 - are going to apply this kernel
534:51 - using this morphology x
534:54 - method
534:55 - and when we are going to run this code
534:57 - you can see uh these kind of results
535:01 - which are not as good as
535:04 - you have seen
535:05 - in the first method now let me show you
535:08 - the last background subtraction method
535:11 - which is called
535:13 - the background subtractor knn method so
535:19 - this method is available
535:21 - under cv2 directly so we are going to
535:24 - just
535:25 - comment this kernel
535:26 - code because for this method we don't
535:29 - need to define any kernel so we can just
535:32 - write cv
535:33 - dot create background subtractor and
535:36 - then at last you just need to write
535:38 - k and n in capital okay
535:42 - and it also takes few optional
535:44 - parameters like history and other
535:47 - parameters but these are optional
535:49 - parameters so for now we are not going
535:51 - to set any uh
535:53 - parameter and let's see the result which
535:56 - we get
535:57 - using this k n method so i'm going to
535:59 - run this code
536:01 - and you can see
536:02 - this knn method result it also shows the
536:06 - shadows in the form of gray pixels so
536:09 - whatever
536:10 - gray pixels you see here in this image
536:14 - are the shadows in this method also
536:17 - there is an optional parameter which is
536:19 - called detect shadows which is set by
536:21 - default to true so when you make it
536:24 - false
536:25 - the shadows will not be detected so you
536:28 - can see
536:29 - no gray pixels are visible now when you
536:32 - make it true
536:34 - then the gray pixels will be uh visible
536:37 - and those gray pixels indicates the
536:41 - shadows
536:42 - right so these are the few methods which
536:45 - you can use for the background
536:47 - subtraction in opencv
536:50 - in this video we will talk about an
536:52 - object tracking method which is called
536:54 - mean shift so first of all what is
536:57 - object tracking
536:59 - so in simple words object tracking is
537:02 - the process of locating a moving object
537:05 - over time using a camera and what is
537:09 - mean shift
537:10 - the idea behind mean shift is really
537:12 - simple
537:13 - consider you have a set of points it can
537:16 - be
537:17 - a pixel density like histogram
537:20 - back projection and you are given a
537:23 - window which is a very small window
537:26 - which can be a circle or rectangle or a
537:28 - square and you have to move that window
537:31 - to an area of maximum pixel density or
537:35 - maximum number of points so in the image
537:39 - you can see this illustration
537:42 - very easily so essentially the working
537:45 - of mean shift algorithm can be
537:47 - summarized in following points
537:51 - so in the first step we pass the initial
537:54 - location of our target object and the
537:57 - histogram back projected image to a mean
538:00 - shift function
538:02 - and then in the second step as the
538:05 - object moves the histogram back
538:07 - projected image also changes and in the
538:11 - third step the mean shift function moves
538:15 - the
538:16 - window to the new location with the
538:19 - maximum probability density so we will
538:22 - see all these steps with the help of an
538:25 - example
538:26 - so here i have the simple code where i'm
538:29 - loading
538:30 - a video which is called slow traffic
538:33 - small dot mp4
538:35 - and i'm just iterating over each and
538:38 - every frame of that video so this code
538:42 - till now you already know how it works
538:44 - so i'm going to just run this code
538:47 - and let's say i just want to
538:50 - track this window of the white car okay
538:53 - so let me just run this
538:56 - video once again so i want to track this
538:58 - window of the white car or window in
539:01 - general of each and every car let's say
539:04 - okay
539:05 - so how can i track this window
539:09 - using mean shift algorithm let's see
539:13 - so as i said the first step is
539:16 - the passing of the initial location of
539:19 - our target so this can be you can say a
539:22 - disadvantage of mean shift that you have
539:26 - to provide the initial location
539:28 - of your target in our case that target
539:31 - is the
539:33 - car window so what i have done is i have
539:36 - just calculated the initial position of
539:39 - the white car window
539:41 - and
539:43 - that we are going to see in the next
539:45 - step so first of all we are going to
539:47 - take the first frame of our video so the
539:50 - first frame of our video can be uh
539:53 - retrieved by uh
539:55 - this code so ret comma
539:58 - frame
539:59 - is equal to
540:01 - cap we have this cap function
540:04 - and we will
540:05 - read the first frame using the read
540:08 - method
540:09 - and this is going to give you the first
540:11 - frame of the video so this is our first
540:14 - frame
540:15 - now once we have our first frame
540:18 - we are going to define the initial
540:20 - location of the car window in our case
540:24 - we want to track first of all the
540:27 - white car window right so i'm going to
540:30 - define four variables first two are x
540:33 - comma y
540:35 - and the next two are width and height so
540:39 - and because i have already calculated
540:41 - the initial position of the window i'm
540:43 - going to hard code
540:45 - this uh position of the window so 300
540:49 - uh 200
540:51 - comma 100 comma 50 okay so this is the
540:55 - hard-coded
540:56 - value which
540:58 - i have already calculated which is the
541:00 - initial position
541:01 - of the car window
541:03 - now we can say that this x y and width
541:08 - and height is our
541:10 - track window so we are going to define a
541:12 - variable
541:14 - called
541:15 - track
541:16 - underscore
541:18 - window let's say and then we are going
541:21 - to just pass all these four variables x
541:23 - comma y
541:25 - comma
541:26 - with comma height okay so let's pass all
541:30 - these four variables
541:32 - and in the next step we are going to
541:34 - define our region of interest
541:37 - so
541:38 - let's define this region of interest
541:42 - with the variable called roi
541:45 - and we already have our first frame so
541:47 - we are going to take our
541:49 - first frame and then
541:51 - we are going to pass that window so y
541:54 - colon
541:56 - y plus
541:57 - height
541:58 - comma
542:00 - x
542:01 - colon
542:02 - x plus width so this is our window or
542:06 - the position of the window
542:09 - so as i said in the first step we will
542:11 - pass the initial location of our target
542:14 - object
542:14 - and the histogram back projected image
542:18 - of the mean shift function
542:20 - so histogram back projection in simple
542:23 - words creates an image of the same size
542:27 - but of a single channel as of our input
542:32 - image in our case this will be our frame
542:36 - where each pixel corresponds to the
542:38 - probability of that pixel
542:41 - belonging to our object that is the
542:44 - output image will have our object of
542:48 - interest or region of it in more white
542:50 - color compared to the remaining part
542:54 - of that image
542:56 - so this is a back projection so for
542:59 - calculating the histogram back
543:01 - projection there are some steps which
543:03 - are involved so we are going to uh
543:06 - follow all these steps to calculate the
543:09 - histogram back projection but first of
543:11 - all let's uh just
543:14 - see
543:15 - the region of interest because we
543:17 - already have
543:18 - a region of interest so i'm going to
543:20 - just write cv
543:21 - dot i'm sure and
543:24 - our region of interest so let's say
543:27 - how our region of interest looks like
543:30 - so
543:31 - i have
543:32 - this video and this image which is our
543:36 - region of interest right
543:38 - so this is the initial position i'm
543:41 - going to pass
543:42 - to our mean shift function right
543:46 - so now in the next step what we are
543:49 - going to do is we are going to
543:52 - uh
543:52 - define the histogram back projection so
543:55 - we already have our
543:58 - roi
543:59 - so in the next step we are going to
544:02 - just convert this roi to the hsb
544:06 - color space so i'm going to just write
544:08 - hsv underscore roi hsb we have already
544:12 - learned in the previous video so i'm
544:14 - going not going to go into the details
544:17 - of hsv color space i'm going to just try
544:21 - it cv
544:23 - dot cb
544:25 - t
544:26 - color
544:27 - which is going to convert this uh
544:30 - image into the
544:32 - hsv color space so our input image will
544:36 - be the roi and
544:38 - the next parameter will be cv
544:41 - dot
544:42 - color underscore bgr
544:45 - to
544:46 - hsv
544:48 - okay so we are converting this image to
544:50 - the hsp color space and then we are
544:53 - going to calculate the mask so let's
544:57 - say we define a variable called mask
545:00 - and for the mask we are going to just
545:02 - write
545:03 - cv dot in range so this also we have
545:08 - learned in the hsv
545:10 - tutorial
545:11 - so if you want to learn more about all
545:13 - these functions
545:15 - you can just go to that video so first
545:18 - parameter we are going to pass is our
545:20 - hsv
545:22 - image
545:23 - and the second parameter and the third
545:25 - parameter will be the lower and the
545:27 - upper bound
545:29 - so the lower limit will be 0
545:32 - dot comma 60 dot
545:35 - comma 32 dot okay so let's pass this
545:40 - and the upper limit so let's define the
545:43 - upper limit also so the third parameter
545:46 - will be the upper limit in the form of
545:50 - the tuple but we need to use the numpy
545:52 - for that right so numpy dot
545:55 - array
545:56 - and inside that we just passed this
545:59 - tuple value which will be
546:01 - 180
546:03 - dot
546:05 - 255.2255
546:07 - okay so 180 dot comma
546:10 - 255 dot comma 255.
546:14 - so why we use the in range function
546:16 - because for the histogram only hue is
546:20 - considered from hsv right so the first
546:24 - uh
546:25 - channel right and also to avoid the
546:28 - false value due to low light or
546:32 - low light value
546:34 - we
546:34 - use the in range function okay so these
546:38 - low light values are discarded using the
546:42 - inner range function and then in the
546:44 - next step we are going to calculate our
546:48 - histogram value so i'm going to define
546:51 - this variable called roi hist
546:54 - this also we have learned in the
546:56 - previous videos i'm not going to go into
546:58 - the details so i'm going to just use the
547:01 - function called calc hist
547:04 - which takes the first parameter which
547:07 - will be the image so i'm going to just
547:09 - pass
547:10 - our
547:11 - hsv roi so just pass hsb
547:15 - underscore roi the second value here
547:18 - will be the channels so we are just
547:21 - using only hue channel the first channel
547:24 - in the hsb space so we are going to just
547:27 - write 0 here ask now the next parameter
547:30 - will be the mass so we have already
547:31 - calculated the mass so we are going to
547:34 - just pass
547:35 - this mask
547:36 - parameter here the next parameter will
547:39 - be the hist size so as we have already
547:41 - learned in the previous videos that this
547:44 - hist size uh
547:46 - starts from 0 to 179 so essentially 180
547:51 - values
547:52 - and then we just need to pass the ranges
547:55 - so as i said it starts from zero to one
548:00 - and now in the next step we are going to
548:01 - just normalize these uh values using
548:05 - the normalized functions so this
548:07 - normalize function takes few values
548:09 - first is the source so the source is our
548:12 - roi
548:13 - hist variable
548:15 - the next value is the destination so
548:19 - let's say
548:20 - we have the same destination we just
548:22 - want to overwrite this roi hist value
548:26 - the next parameter here will be the
548:28 - value of alpha so alpha will start from
548:32 - 0
548:33 - and the value of beta will be 255 so we
548:36 - want to normalize these values
548:38 - between 0 to 255
548:41 - okay
548:42 - and then the next
548:44 - value
548:45 - will be the norm type so the norm type
548:49 - we are going to take
548:50 - is
548:51 - cv
548:52 - dot
548:54 - norm
548:55 - min max okay so we are going to just
548:58 - take this one
548:59 - norm min max
549:02 - so all these steps which we have
549:04 - written here is going to give us the
549:07 - histogram back projected image
549:10 - now once we have this histogram
549:13 - back projected image
549:15 - we are going to
549:17 - use this histogram back projected image
549:20 - uh which is also going to change with
549:23 - the moving object
549:25 - so now in the next step we are going to
549:28 - go inside our while loop
549:31 - and read each and every frame one by one
549:35 - and first of all what we are going to do
549:38 - is we are going to
549:39 - calculate the hsv value of the frame as
549:43 - we have done
549:44 - with the first frame also right
549:47 - so we are going to just uh take the
549:50 - frame
549:51 - and then calculate the hsb
549:54 - roi value let's say
549:56 - this time we are going to name it as hsb
550:00 - and we are going to pass
550:02 - frame as the source instead of this roi
550:05 - value
550:07 - now in the next step we are going to use
550:09 - a function called calculate
550:12 - back project so let's define
550:15 - the variable called dest for destination
550:19 - and then cv dot
550:20 - calc back project which is the function
550:24 - for calculating the back projection and
550:27 - this function takes few argument first
550:30 - is the number of images so we only have
550:34 - our hsb image so we are going to pass
550:37 - in the form of the list the second
550:39 - argument will be the channels so as i
550:43 - said we just want to use the hue values
550:46 - here so only one channel so we are going
550:49 - to just write
550:50 - uh 0 so because channel starts from 0
550:54 - 1 2 so that's why i have written
550:56 - 0 here the third parameter is the hist
551:01 - value so in this case our hist value is
551:05 - the roi hist which we have calculated
551:08 - the next parameter is
551:10 - the ranges so we will start from 0 to
551:14 - 180
551:16 - as we are
551:17 - talking about the hsv color space
551:21 - and the next value will be the scale
551:24 - so let's say scale for now we take one
551:28 - as the scale so this is going to give
551:31 - you the back projected image and then in
551:34 - the next step we are going to apply the
551:36 - mean shift to get
551:38 - the new location so i'm going to just
551:41 - write ret comma
551:43 - track window so i'm going to just say
551:46 - track window which is
551:49 - this variable which we have already
551:52 - defined
551:53 - and then we are going to just use cv
551:57 - dot
551:58 - mean
551:59 - shift
552:00 - which is going to take few arguments
552:02 - first is
552:04 - the image which is the destination image
552:07 - which we got
552:09 - from
552:09 - the back project function calc back
552:12 - project
552:14 - and next argument will be our
552:17 - track image which is the track window so
552:21 - we have to define this
552:22 - term criteria so i'm going to just write
552:26 - term
552:28 - c rit for criteria
552:31 - and then we have to define this outside
552:34 - this while loop
552:36 - so i'm going to
552:37 - go here
552:39 - and we are going to set up the
552:41 - termination criteria either
552:43 - 10 iterations or
552:46 - move by
552:47 - at least one point okay so
552:50 - we are going to define that criteria so
552:53 - here
552:54 - in
552:55 - the curly brackets we are going to just
552:59 - say
553:00 - cv dot term criteria esp or cv
553:06 - dot term criteria count so because we
553:10 - want to either provide the termination
553:13 - criteria for either 10 iterations so we
553:17 - just give 10
553:18 - or we want to uh terminate by moving at
553:22 - least one
553:24 - point this is the criteria for
553:26 - the mean shift and we are providing
553:29 - these two criteria so once we have our
553:32 - track window for the car
553:34 - we can draw a rectangle with the help of
553:38 - this track window and this will be
553:40 - visible
553:41 - on our
553:43 - video so
553:45 - we are going to draw that uh
553:47 - window we are going to just say x
553:50 - comma y
553:52 - comma
553:53 - w
553:54 - comma h for
553:56 - x y width and height and this will be
553:59 - our
554:00 - track window
554:01 - and then we are going to just draw a
554:04 - rectangle so i'm going to
554:06 - just say we have the
554:08 - final
554:11 - image and then we are going to just
554:13 - write cv dot
554:16 - rectangle which is going to take
554:19 - the frame
554:20 - and then
554:21 - the
554:22 - point for the first point of the
554:26 - rectangle and the second point of the
554:28 - rectangle which are the coordinates of
554:30 - that point so the first
554:32 - point coordinates will be x comma y
554:36 - and the second point coordinate will be
554:39 - x
554:40 - plus width
554:42 - comma y plus height
554:44 - okay
554:45 - and then the next value will be the
554:48 - color let's say we want to use 255 here
554:52 - and the
554:54 - thickness so thickness we want to take
554:57 - three here for example
555:00 - now we can just uh show this final image
555:04 - using i'm sure method so till now we
555:07 - were just uh
555:09 - showing our original frame so we can
555:11 - just say
555:13 - let's say we want to show the final
555:16 - image here
555:17 - also if you want to see
555:20 - the back projected image you can just uh
555:24 - use
555:25 - this destination so we can print this
555:28 - destination
555:29 - uh
555:30 - image also and see how does this back
555:32 - projected image looks like so let's run
555:36 - this code
555:37 - and you can see this car window is
555:40 - dragged right so as the car moves this
555:43 - window also moves
555:45 - once this car goes out of the scope it
555:48 - tracks the other window so this is how
555:51 - the mean shift algorithm works in opencv
555:56 - now as i said this mean shift has few
556:01 - disadvantages or limitations
556:04 - the first limitation is
556:06 - the size of the target window does not
556:09 - change so as we have seen
556:12 - once this car is coming near to us the
556:17 - size of this window is not changing it
556:19 - remains always same
556:22 - so this is one problem
556:24 - the second problem is we have to give
556:27 - the initial position of our region of
556:30 - interest for example if initial position
556:33 - of the region of interest is not known
556:36 - then
556:37 - it will be really hard to apply mean
556:40 - shift method so there are
556:42 - these two mean limitations of this mean
556:46 - shift algorithm and we are going to try
556:48 - to solve these uh limitations in the
556:50 - next video when we learn
556:52 - cam shift which stands for continuously
556:55 - adoptive mean shift
556:58 - in the last video we have learned how to
557:00 - use mean shift algorithm to find and
557:03 - track objects in the video in this video
557:06 - we are going to learn camshaft algorithm
557:09 - to track the object in the video
557:12 - so if you have seen the last video we
557:14 - have written this code so we are going
557:17 - to use all this code which we have
557:20 - written in the mean shift uh video
557:22 - tutorial
557:23 - and first of all let me just run this
557:27 - mean shift code which we have written in
557:29 - the last video and we have discussed
557:31 - about this problem of this rectangle
557:34 - which always remains the same
557:36 - even if the object is coming closer to
557:40 - the camera so we need to adopt the
557:43 - window size with the size and rotation
557:45 - of the target so once again the solution
557:48 - came from opencv labs and this time they
557:52 - introduced
557:53 - an algorithm which is called cam shift
557:56 - which stands for continuously adoptive
557:59 - mean shift so this camshaft algorithm
558:02 - applies mean shift first and then once
558:05 - the mean shift converges it updates the
558:08 - size of the window in addition it also
558:11 - calculates the orientation of the best
558:14 - fitting eclipse to it now let's talk
558:17 - about the implementation part of the cam
558:19 - shift so as i said all the code which we
558:22 - have written in the last video will
558:23 - remain the same
558:25 - except one thing which is we have used
558:28 - this mean shift algorithm in the last
558:31 - video and in this video we are going to
558:33 - use the cam shift shift so just write cv
558:37 - dot
558:38 - cam shift and all the parameters also
558:41 - will remain the same which is
558:43 - destination track window and the
558:46 - termination criteria so let's run this
558:49 - code once again and let's see what
558:52 - result came out of this algorithm so you
558:55 - can see this rectangle is changing its
558:57 - size
558:58 - according to the target now this result
559:01 - which we have seen can be better because
559:04 - the camshaft function returns a rotated
559:08 - rectangle that is our result and also
559:11 - the box parameters which are used to be
559:14 - passed as the search window in the next
559:18 - iteration
559:19 - so
559:20 - here when we see the result inside the
559:24 - ret variable so let's print the result
559:26 - inside the ret variable i'm going to
559:29 - just
559:30 - print it using the print function now
559:33 - let's run this code and let's see what
559:35 - this ret variable prints on the
559:38 - terminal so let me just
559:40 - press escape
559:42 - so what is this result so here you will
559:44 - see the value of x and y
559:47 - and also you will see these three values
559:49 - which are your width height and the
559:52 - value of rotation so in camshaft you can
559:56 - also rotate your rectangle according to
560:00 - your object size
560:02 - so now we are going to use all these
560:05 - parameters which are there inside this
560:08 - ret variable and we are going to try to
560:11 - draw
560:12 - the rectangle which might be uh rotating
560:16 - so there will be a different approach
560:18 - other than this
560:20 - rectangle we are going to use that
560:23 - approach to print
560:25 - those
560:26 - points which we got using the ret
560:28 - variable so let's draw that rectangle
560:32 - so
560:33 - here we are going to define a variable
560:35 - called pts
560:37 - and there is a function called cv
560:40 - dot
560:41 - box points so we are going to use
560:44 - that
560:46 - function here which is
560:48 - box points and it takes a few arguments
560:52 - we just need to uh
560:54 - pass our ret variable here so we are
560:57 - going to just pass our ret so let's see
561:00 - what values this is going to give us so
561:03 - i'm going to just print this pts value
561:06 - so i'm going to just print the value of
561:08 - pts
561:09 - now let's run this uh code once again
561:13 - you won't see anything
561:15 - and you will see these values right so
561:19 - it's going to give these floating point
561:22 - values which we need to convert it into
561:26 - the integers and the error was due to
561:29 - this because this is no longer defined
561:31 - right
561:32 - so for that we are going to just convert
561:35 - these points pts
561:37 - into
561:38 - the integer values so i'm going to
561:40 - overwrite this variable pts and then
561:43 - there is a function in numpy which is
561:46 - called int
561:47 - 0
561:48 - and here when you pass
561:51 - this pts variable it's going to convert
561:54 - those point into the integers and now we
561:57 - can just draw our rectangle
562:00 - but remember this is a rotating
562:02 - rectangle so we cannot use this normal
562:04 - rectangle function so we need to use the
562:07 - other function for
562:09 - drawing those points so i'm going to
562:11 - define this final image variable once
562:14 - again and then i'm going to use cv dot
562:17 - poly lines so there is this function
562:20 - called polylines which can you can use
562:23 - to draw those lines which you get using
562:25 - this points variable so we are going to
562:28 - just uh pass the frame first of all so
562:31 - we need to pass the frame as a first
562:33 - parameter the second parameter will be
562:36 - our pts value
562:38 - and then the third parameter will be
562:42 - the
562:44 - closed or not closed so when we pass
562:47 - true here then this rectangle will be
562:51 - closed right then we need to pass the
562:54 - color so you can pass any color here
562:56 - let's say it zero comma 255 comma zero
563:00 - and then you can also pass the thickness
563:04 - so let's say we just need to give the
563:08 - thickness of two here okay
563:10 - so this is our final image and now we
563:14 - are going to run this
563:17 - example once again and let's see what
563:18 - happens
563:19 - so you can see
563:21 - this rectangle is drawn and it can
563:24 - rotate also with the object so this is
563:27 - how
563:28 - camshaft algorithm works with opencv i
563:31 - hope you've enjoyed this video and i
563:33 - will see you in the next video

Cleaned transcript:

first of all let's see what is computer vision because opencv is an open source computer vision library so computer vision is the way of teaching intelligence to machines and making them see things just like humans so what happens when a human see an image he will be able to recognize the faces which are there inside the images so in its simplest form computer vision is what allows computers to see and process visual data just like humans computer vision involves analyzing images to produce useful information so to give you some examples a selfdriving car it can detect the lanes using computer visions or you might have wondered how facebook detects images when you upload the images of you with your friends it becomes possible by facebook's face and image recognition technology so now let's see what is opencv so opencv which stands for open source computer vision is a library of programming functions mainly aimed at realtime computer vision it is originally developed by intel and then it was later supported by a developer called willow garage and now it is supported and maintained by itc's now opencv is available on mac windows and various linux operating systems so we can say that opencv is a crossplatform library now you can work on opencv using c c plus plus or python and we will be using python to learn opencv now opencv is a open source and free library which is licensed under bsd license and it said that it is very easy to use and install that we will see when we will install opencv on various operating systems now because opencv primarily deals with computer vision that means dealing with mainly images or videos so i wanted to show you how a digital image is seen by a computer so digital images are typically stored in the form of matrix now if you have heard about ppi or pixel per inch which refers display resolution that means how many individual pixels are displayed in one inch of digital image so when a computer sees a picture it sees it in the form of pixel matrix now there are two type of digital images one are called grayscale images and other are called colored images so in grayscale images each pixel represents the intensity of only one shade that means how bright or dark the pixel is in other word it is said that it has only one channel so on the right hand side you can see a grayscale image and on the left hand side you can see a colored image so in colored images we have three channels that is r g b which stands for red green blue so grayscale images have one channel and colored images have three channels your standard digital camera have three channels that means red green blue channels so we will learn more about images and how we can process images using opencv in the later videos now there is one more thing which i want to show you is numpy so we are going to learn opencv using python so when you will install opencv library for python on your operating system numpy will be automatically installed with this library so first of all what is numpy so numpy is a highly optimized library for numerical operations now as i told you digital images are 2d arrays of pixels and numpy library is a general purpose array processing package library so it provides a high performance multidimensional array object and tools for working with these arrays which makes the processing of images easier now all the opencv array structures are converted to and converted from numpy arrays and in addition you can use more convenient indexing system rather than using for loops so when you want to learn opencv using python you need to have some knowledge about numpy also so if you have some knowledge of numpy library it's good but don't worry i will teach you step by step so you will not miss anything so that was a brief introduction about computer vision and opencv in this video i'm going to show you how you can install opencv for python on your windows operating system so obviously you need to install python on your windows operating system in order to install opencv for python so first of all i'm going to show you how you can install python on your windows operating system and then we will see how to install opencv using python now if you have already installed python on your windows operating system you can skip about five minutes of this video and go directly to the point where i am going to show you how you can install opencv for python so let's get started so first of all open your favorite browser on your windows 10 operating system and then search for python and the first link which will appear here will be from python.org so we are going to click on that link and once this python.org website is open you just need to scroll down a little until you see this downloads section and you can see at the time of making this video python 3.7.0 is the latest version of python available so we are going to click on this link which says python 3.7.0 and you will be redirected to this page which says python 3.7.0 and now i'm going to scroll down until i see the files here and you will see there are various kinds of installer available here we are going to install the python using the executable installer so we are going to choose this option which says windows x86 hyphen 64 executable installer and now i will wait for this executable to be downloaded and once this executable is downloaded you just need to click on this exe file and i'm going to minimize the browser here so you can see python's 3.7.0 setup window has been started and on the first window you will see two options here one is install now and other is customize installation so what we are going to choose is this option which says customize installation because when you choose this install now option python will be installed at this part which i don't want to use you can see it's a long path which i don't want to remember so i will use this option which says customize installation and i will also check this option which says add python 3.7 to path so now let's click on customize installation and next you will see this optional feature window and you can see there are some optional feature which this python installer will install for example documentation pip it will install which is a python package manager idle ide python test suit and other feature it's going to install so i'm going to leave everything as default and then i'm going to click next and now this next window will open which says advanced option here i'm going to check this option which says install for all users and i'm going to leave other check boxes as checked and then you will see this section here which says customize install location so i want to install python on my c directory so what i'm going to do is i'm going to open the windows explorer and i'm going to go to the c directory here and once the c directory is open i'm going to right click here and i'm going to create a new directory and i'm going to name this directory as python and then i'm going to press enter and this path i'm going to give here in the customize install location so i'm going to just give this part which says see colon slash python and then backslash python37 37 here means that we are going to install 3.7 version of python so now python will be installed at this location on my computer and then i'm going to click on the install button here and then you will see the installation will start and it will be finished in a few seconds so just wait for the installation to complete and after some time i can see this message which says setup was successful so i'm going to click on this close button which is going to close this installer so now in order to check whether python is installed on our windows operating system or not we are going to search for python here and you will see few options here one is this python 3.7 terminal other is idle ide so first of all we are going to click on this option which says python 3.7 64bit which is going to open this kind of terminal so this is a python terminal and here we can for example print something so i am going to just write print and in the parenthesis and in between the double quotes i can just write hello world and then press enter which is going to in return print hello world that means python 3.7 terminal is working so i'm going to close this terminal now and once again i'm going to search for python here and this time i'm going to select this option which says idle okay so just select this option which says idle and in the parenthesis python 3.7 64bit so this idle is an ide which comes with python installation at the time of installation we have chosen this option to install idle that's why we can see this option here and also this is an interactive shell so you can once again write a print and inside the parenthesis you can just write for example once again hello world and then press enter and it's going to give you this kind of output here so now python interactive shell is working and idle ide is also working so i'm going to close this idle ide and now i want to check whether python is working using my command prompt or not so i'm going to right click on this windows button and then i'm going to click on command prompt and here i'm going to first of all write python and then press enter and you can see this python option is working now even on your command prompt right so here also you can just write print and inside the parenthesis you can just print hello world and then press enter and it prints hello world in return now once python is installed on your windows 10 operating system we are going to install opencv using pip now pip is automatically installed on your windows operating system with the python installation so you don't need to separately install pip on your windows operating system it comes automatically with your python installation so to verify this first of all i'm going to check the python version so you can just give this command python hyphen hyphen version and then you can check the pip version so you can just give this command pip hyphen hyphen version so just give this command and it's going to give you the version of pip which is installed on your windows operating system so to install opencv using pip you just need to give this command pip install open cv hyphen python and i'm going to press enter so you can see opencv related packages are downloading now so now opencv python package is installed using pip on my windows operating system now you will observe one more thing here and that is numpy package will be automatically installed with your opencv python package so now once opencv python package is installed we can verify it by just opening our python shell so i'm going to just give a python command to open the python shell and then here i'm going to just write import cv2 okay so once you give this command it should not give you any error and if this import gives you error that means opencv is not correctly installed on your operating system now after importing you can just check the version of opencv which you have installed using cv2 dot underscore underscore version underscore underscore and then press enter and it's going to give you the version of opencv which is installed on your operating system and in our case this is 4.0.0 at the time of making this video now you can check the same by writing your code inside a python file also so here i have opened my visual studio code editor and i have already created sample.py file and here also i'm going to import the cv2 package first of all and then i'm going to print the version of cv2 using this print statement so i'm going to just write cv2 dot underscore underscore version underscore underscore and then save this script and to open the terminal inside visual studio code you can just press ctrl shift p and then type toggle integrated terminal so just type toggle integrated terminal and then click on this first option which says toggle integrated terminal this is going to open the terminal inside your visual studio code editor so here you can run your python script using the python command so python and then name of the script which is sample.py in my case and then press enter and it's also going to give you the version of opencv which is installed on your operating system so this is how you can install opencv for python on your windows operating system in the last video we have seen how we can install opencv for python now from this video we will actually start writing some code now moving forward i will be using pycharm ide to demonstrate how opencv works but you are free to use any ide or any other editor in order to use opencv now on a pycharm ide you need to install opencv little bit differently so if you are using opencv you just need to create a project inside opencv and then you just need to click on file and then go to the settings now once the settings window opens you just need to go to the project and then it'll say after colon your project name so my project name is opencv examples that's why it's written here opencv examples so project colon your project name so just click on this section and then click on project interpreter and on the right hand side you will see all the packages which comes preinstalled when you create a project inside pycharm ide now we want opencv python package so to install opencv python package on pycharm you just need to click on this plus button here and then you just need to type open cv hyphen python now the first result you can see here is opencv hyphen python and the version which is available right now is 4.0.0 0.21 which is the latest version so to install this package for your pycharm ide you just need to click on install package button and then after some time you will see this message which says package opencv hyphen python installed successfully in the green bar that means opencv package is installed successfully so you can close this window and now you will be able to see opencv hyphen python is added to your packages and also numpy is added to your packages which comes with your opencv python package so i'm going to just click ok and now you will be able to import this cv2 package in your python script now in this video i'm going to show you how you can read images and write images using cv2 package now let me show you where you can find some sample images for your project so you can open the browser and then go to this url github.com forward slash opencv so just go to this url and then under this opencv project in github you will be able to see these repositories you just need to choose this repository which says opencv and then you can scroll down and all the images you will find inside the samples folder so i'm going to go inside the sample folder and then inside the sample folder you just need to go inside the data folder so here you will find many sample images and videos and other files which you can use in your project for the learning purpose so you can use these images in order to develop your example so what i generally do is i just go to this repository which is under the url github.com forward slash opencv forward slash opencv and then i either download the zip file of this project or clone this github repository on my operating system and once you clone or download this repository it will look like this so it will be downloaded as this folder which is opencv hyphen master and once again you can go to the samples folder here and inside the samples folder you can go to the data folder and you will find all those images which i have shown you on the github repository now to start with we will be using this image which is lena dot jpg so i'm going to just copy this image for now and then i'm going to go to my pycharm ide and then i'm going to just paste this image inside my project so this jpg image will be directly available inside my project folder now let's see how we can read images using the cv2 module so you just need to use cv2 and there is a method called i am read which enables you to read the images so the first argument which you need to give here is the image name so i'm going to give the image name which is lenna dot jpg and the second argument here is a flag so there are three flags you can give here you can either give 0 or one or minus one flag here so this second argument is a flag which specifies the way images should be read so let me show you all the flags here so the first flag is cv2 dot i am read underscore color or you can give the integer value of it which is one and whenever you give this flag as the second argument of imread function it's going to load the colored image if you give this flag which is cb2 dot i'm read underscore grayscale or if you give this integer value which is 0 it's going to load your image in grayscale mode and the third flag is i am read underscore unchanged or the value 1 which is going to load your image as it is including the alpha channel so for now we are going to just give here zero flag which means we want to load our image in grayscale so now let's run the code and let's see what happens until this point so you can see our code runs fine without giving any error now let me give any random name here as the file name and once again run this code and once again you will see that there is no exception which is thrown here so even if you give the wrong file path or file name here this function is not going to give you any error now in case of wrong path or wrong file name let's say i'm going to just assign this value to a new variable which is img and let's print the value of this img using the print method and then let me run the code once again and you can see whenever you will give the wrong file name here or wrong path here as a result of this method you will get none so you can check the value of image and if it is equal to none then you know that you have done something wrong or you have given some wrong file name or wrong path here let's give the correct file name so i'm going to give the correct file name and then run the code once again and now this time you will see it's going to give you a matrix which means that it has read all the pixels from this image and then assigned it to our img variable and the result you can see in the form of this matrix so until now we have just read the image now we want to display our image so in order to display our image what we can do is we can use a cv 2 dot i am show method so just write i am show here which is going to show your image so the first argument here will be the name of your window in which your image will open so you can give any name here for example i'm going to give image name here to my window and then the second argument is the image variable which you have read using the i am read method so i'm going to just pass img variable which is this variable here so now will it show the image let's check so i'm going to run the code once again and you can see the image is shown for a millisecond and then it disappears so now we need to add something which will wait for the image to disappear so i'm going to add one more method here which is cb 2 dot weight key so this cv2 dot weight key is the keyboard binding function and the argument which it takes is the number of milliseconds for which you want to show your image window so let's give 5000 value here which means we want to show the image for 5 seconds and at last what we are going to do is after we have done seeing our image we will destroy the window which we have created so you can just give this method cv to destroy all windows so destroy all windows simply destroys all the windows which we have created there is one more method which is destroy window and this method you can use to destroy a particular window which we will see little bit later but for now we will just use this method which says destroy all windows so now let's run our code and let's see what happens so now this time you can see our image is loaded for five seconds and our image is loaded in grayscale mode now if you give here zero as the argument of weight key method then let's see what happens so i'm going to run my code and now you will observe that your window will not disappear after five second or any number of seconds it's going to wait for the closing of this window which we can close from this close button and you can see it's loaded in the gray scale mode i'm going to close this window and here as an argument of i am read image the second argument i want to give here right now is one which means the colored image and let's run the code and you can see this image is loaded in the colored mode now let's also check the 1 argument which is unchanged so it's going to just load the image as it is with alpha channels so let me just close this image once again so now we have understood how we can read an image using i am read function so let's see how we can write an image to a file using a function called i am right so we are going to just use cv 2 once again and then there is a method called im write which you can use to write an image in the form of a file so the first argument here will be the image name whatever you want to give here so here let's say we want to give the name to our image as lena underscore copy dot png so the image will be saved as the file name lena underscore copy dot png file and the second argument which it takes is the image you want to save so let's save the same image which we have read using the i am read function inside this img variable and pass it as the second variable here and let's run the code and let's see what happens so our image is loaded using this i am show function and now when i close this window here you will observe one more file will be created here so let me just close this window and now you can see as soon as i close this window this method is called and after this this i am right method will be called and when this method is called this image is created with the name lena underscore copy dot png we can also open this image and you can see it has the same image which we have seen in the case of lena dot jpg so let me close these two images so now we have understood how we can read the images and write the images using i am read function and i am right function so let's make our code little bit better and what we want to do here now is let's say if somebody presses an escape key then only we want to destroy all the windows without saving it into a new file otherwise if somebody presses the s key then we are going to save this file with the new name let's say lena copy dot png file so i'm going to just capture the output of my weight key so just create a new variable let's say k so now when we press any key this key will be captured in this variable now as you know every key has its own value so we are going to just use uh if condition and we are going to just check whether the value of k is equal to 27 which means that we have pressed the escape key and if somebody have pressed the escape key we are going to simply destroy all the windows otherwise let's give the second condition which is l if k is equal to ord and this is a built in function and it takes one argument which is the key name which we want to press so let's say somebody presses the s key and if somebody presses the s key we just want to save the image which we have read using the i am read function to a second image which we call lena underscore copy dot png and then we will simply destroy all the windows which we have created using im show method so let's run the code and let's see what happens so i am going to run the code so this image is loaded and as soon as i press escape button you can see this image disappears that means this condition is met and this method is called and all the windows will be destroyed without saving the image let's delete this image and let's see what will happen when we press the save key so let me just delete this image and now let's run the code once again and this time i'm going to press the s key and once again you can see the windows are destroyed but this image is created once again using this function that means this time this condition is fulfilled and this image is created and after that all the windows are destroyed so this code is working fine for me but in the documentation it's also written that if you are using a 64bit machine it's better to use this notation with your weight key method which is weight key and this mask here and then once again when we run our code it works as it is but in case if it doesn't work you can try using this mask here so this is how you can read and write images using opencv in this video we will see how to read display and save videos using cameras so often we have to capture live stream from camera so first of all we will see how we can capture the live stream from the camera the same method you can use to display the video from a video file so let's get started and let's see how we can capture the live stream from your default camera so i'm going to just create a variable called cap and then inside your cv2 package there is a class called video capture we are going to take this class and create an object of it and as an argument here you can provide either the input file name so for example if you want to just read the video from a particular file you can give the file name for example my file dot avi or mp4 or you need to provide the device index of your camera from which you want to read so by default this index will be either zero or in many devices it's also minus one so first of all we are going to try with zero device index which in most cases works so if this device index 0 doesn't work try with 1 now if you have multiple cameras and if you want to use the other camera then you can also try one for the second camera or two for the third camera and so on so we are going to use the default camera which is at device index zero so this is the argument we need to provide here and then we are going to create a while loop in order to capture the frame continuously so let's create a while loop here and this loop we are going to run indefinitely so we are going to just say that while this loop is true we want to capture the frames so we are going to just define these variable ret and frame and then using this cap instance we are going to call a method called read now this read method is going to return true if the frame is available and this frame will be saved into this frame variable so here the true or false will be saved if the frame is available this ret will be true otherwise it will be false and this frame variable will actually capture or save the frame now in order to show this captured frame we can use i am show so i'm going to just use a cv 2 dot i am show which is going to show this frame inside the window first of all you can give the name to your window for example frame and then second argument will be the frame which you are reading which is this variable here now in the next step we have seen in the last video also we are going to use the cv2 dot weight key in order to wait for the user input and if this input will be q we will quit the window and destroy all windows so we are going to just say cv2.weight key and the argument here will be 1 and i have told you you need to provide this mask for 64 bit machines so you can provide this mask and then we are going to just see if this key which is pressed is q or not so we are going to use the ord method for this and we will just check if the q key is pressed and if this q key is pressed we are going to break from the loop and we will come out of the loop and after we come out of the loop the first thing we need to do is to actually release the capture variable so this is important after reading your video you need to release the resources so you need to just call this method cap.release and then we will just destroy all windows so let's run this script and let's see what is the output so i'm going to run the script and you can see in this window the input from my default webcam of my laptop right now i'm just showing some book in front of this camera that's why you will see this book and as soon as i press q key our window will be destroyed and we come out of this script now let's say you want to change the frames to gray so we want to convert our video input from the colored image to the grayscale image for that what you can do is you can define a variable called grey or anything else and then there is a method called cv2 dot cvt color which is to convert color and the first argument which it takes is the source so in our case the source is the frame which we are capturing from the cap dot read method the second argument is actually the conversion what we want to do so we will just call cv2 dot color underscore and by default the default colored image is captured as bgr image that means blue green red channel images and we want to convert it to a grayscale image so we will just write bgr to gray this means we want to capture the bgr image to the grayscale image and now this is going to give us the grayscale image and this input we can just transfer to the i am show method as the second argument of this i am show method so let's run this script once again and let's see what is the output of the script and now you can see the video captured is in grayscale image and as soon as i press q it's going to release all the captured resources and then destroy all windows now as i said if you want to display the image from a video file you just need to give the name of the video file for example name and then the extension which is let's say avi or mp4 or any other format now using this cap instance you can read few properties about the video which is captured and the first property is if the video is open or not so in case whenever you provide the file name and the file path is wrong then this is going to give you false so there is a method called is opened and this means if the file name of the video which you want to provide here is correct this is going to give us true otherwise this is opened is going to give us false in case the file path is wrong or the index which you give here for the device is wrong so let's give any random index here and then let's see what happens so i'm going to run the script and you will see nothing will happen because this is opened is going to give you false let's print that also and let's verify with the print statement the same thing so i'm going to just use this and then run the program once again and you can see it prints false that means you cannot capture the video using this index so my device is at index 0 by default so i need to give this index name otherwise for example i provide the wrong file name here also it's going to give us the false value there is a method called cap dot open also so if this cap is open gives you false you can try opening your capture video using cap.open also now there are other properties which you can read using this cap instance and the property you can read using a method called get so you can just write cap dot get and as an argument of this get method you can provide the prop id so there are different prop ids which you can read so let's say we want to read the property which is called frame width and frame height which is going to give you the height and width of your frame so for this you just need to write cv2 dot cap underscore prop underscore frame underscore width this is going to give you the width of your frame and if you want to get the height of your frame you can use gap underscore prop underscore frame underscore height and this whole list you can find on the official documentation of opencv so i will provide you this link where you can see different values of the prop id so right now i have used this id and this id but there are several number of ids available here which you can use to read the property of your frame so let's use print method to just print out what property we are reading and let's once again run this script and here you can see in the output you can see the value 640 and 480 which is the width and height of your frame by default now let's see how we can save the image which we have captured from our webcam or the default camera so as we already know that we read frame by frame when we capture the videos from your default camera so for creating the capture you have used video capture class and for saving the video we are going to create the video writer class so i'm going to first of all create a variable called out for output and then i'm going to call a class called video writer so let's call this class which is video writer and now this class takes few argument the first argument is the name of your output file so for example i can just give the name output dot avi the extension of the file the second argument here is the 4cc code now 4cc is a 4 byte code which is used to specify the video codec and if you want to know more about 4cc code you can visit this website which is 4cc.org forward slash codec dot php and here you can find several 4cc codes so for now what we are going to do is we are going to just get the four cc code using a class called video writer underscore 4cc so i'm going to declare a variable 4cc and then i'm going to use cv2 to call a class called video writer 4cc so as an argument of this class you just need to provide the 4cc code so for example i can give this kind of code so you can provide this argument which is xtrx and then your four cc code which is x with in this case or otherwise what you can do here is you can also give this code in this format so for example x comma then second argument is v and then the third argument is i and the fourth argument is d so you can either give this type of notation or you can just use asterix and then in single quotes you can just write xvid or any other code here and then this 4cc code we are going to pass as the second argument the third argument is the number of frames per second so let's say we just want to use uh 20 frames per second and the fourth argument is the size so we already know that the size in which we are capturing is 640 by 480 so we are going to provide this in the form of tuple so 640 comma 480 so this will be the size of the video which will be saved in this file now inside our loop as we have seen we are just reading the frame here in the frame variable and this is the boolean variable if the frame is available it's true otherwise it's false right so first of all we are going to check if its value is true or false so we can just write if ret is equal to true then only we are going to just save this file into the output file so i'm going to just put everything inside this uh if condition otherwise we are going to break out of this loop so i'm going i'm going to just say else break now inside this if condition we can just write this frame into a file using a method called out dot write so out is the instance of video writer so i'm going to just use out dot write and then we are going to just pass the frame which we have captured which is inside the frame variable and now at last we are going to release all the resources using the out instance which is the instance of video writer so i'm going to just write out dot release and then let's run the script and let's see what happens so one thing to note here is our video will be saved as it is that is in the bgr mode that is in the colored mode so let's run the code and let's see what happens so i'm going to just start my script once again and now i'm going to just press q so you can see here our video is shown in the grayscale and our video will be saved in the original from format because we are saving every frame before the conversion so it will be saved in the original format so i'm going to just uh close this uh script and as soon as i close the script you can see the output.avi file and in order to verify this file i'm going to go to the project and here i'm going to start this file using let's say vlc media player and you can see it shows the output of the output.avi file so this is how you can read videos display and save videos using the default camera or the video file in this video we will learn how to draw different geometric shapes using opencv so to start with i have this code and i have already explained what this code does so this i am read is used to read an image and then we are just showing this image into a window using this i am show method and then using the wait key we will wait for the closing event and the destroy all window will destroy all the windows which we have created so this we have already seen now let's say we want to draw some geometric shapes on this image so to start with let's learn how to draw a line on our image which we have read from this read function so what we are going to do is we will overwrite this image so we have already uh created this image variable so what we are going to do is we will draw a line on the same image so i'm going to just write img is equal to cv2 dot lines and you can see in the suggestion this line method takes few arguments so the first argument is the image itself the second argument is the starting coordinates of point one the third argument is the ending coordinates of point two and then the fourth argument is the color and fifth argument is the thickness so let's use this line method and then give these arguments one by one so we want to write to the image which we have read using this file so the first argument is the image variable and the second argument is the coordinates so the coordinates should be given in the form of tuple so let's say we start with 0 comma 0 coordinate and the ending coordinates will be let's say 255 comma 255 okay the fourth argument will be the color and the color you need to give in the bgr format so if you want to uh give the blue color then you can just write 255 comma zero comma zero because first is the blue color second is the green color and third is the red channel color so if you specify here 255 in the first channel that means the blue channel then it's going to draw the blue line if you give here 255 and then you make other channels 0 then it's going to draw the green line and if this 255 comes at last and the other channels are 0 then it's going to draw the red line so let's say we want to draw the red line that's why i have given 255 here and the next argument is the thickness so the thickness you provide in the numbers so starting from one one is the lowest thickness you can increase the thickness two or three or let's say 5 or 10 so it's going to increase the thickness based upon this number so let's say we want to give the thickness to our line 5. so this is going to draw a red line on our image so let's run the code and let's see what happens so you can see our image is loaded in the grayscale that's why you don't see any color on the line but our line is created here so let us load this image in the colored format by changing this argument to 1 and let's run the code once again and you will see the image is loaded in the colored format and the line color is red now if you want to change the thickness of this line you can just increase this number and if you want to change the color of this line you can change it using these color channels so let's change the line color to green let's say and i'm going to run the code and you can see the thickness of the line is increased and now the color of the line is green now if you want to draw the line with any other color you can just go to your favorite browser and search for rbg color picker but always remember our image will be loaded in the bgr format so in the reverse order so blue green and then the red channel so let's say we want this uh color here and it's the rbg channels are this so i'm going to just copy all these channels and then i'm going to give these channels in the reverse order so first of all 147 then 96 and then third channel is the 44 and then i'm going to run my code and you can see you get the same color which you have chosen here so this is how you can change the color of your lines now there is a function called arrowed line let's say we want to use this function which is called arrowed line and this is going to draw the arrow line as it says so let's say we want to just uh draw this arrowed line in blue color so i'm going to just give the color channels here and then run the code and this arrowed line is overlapping on the previous line that's why you don't see the previous line so let's change the coordinate of this line so let's draw this line in this coordinate which is going to draw the straight line in my opinion let's see what happens when i run the code and you can see it draws the straight line from left to right which is the arrow line and this was our original line now let's see how to draw the rectangle so to draw the rectangle we will do the same we will just overwrite on the same image so we will just say image is equal to cv2 dot rectangle which is a method and you can see what are the argument it takes so the first argument is the image itself the second argument is the point one and point two this point one and point two coordinate i am going to explain in a bit the third argument is the color which is same as line and the fourth argument is the thickness so let's use this rectangle function to draw the rectangle so first of all i'm going to just pass the image variable here the second argument is the top left vertex coordinates so let me just draw something here so you will be able to understand in a better way so when you want to draw a rectangle using opencv this here is a top left vertex coordinates which is x1 and y1 and this is here the lower right vertex coordinates so the top left vertex coordinates you give in the second argument so let's give some coordinates here so 3 84 comma 0 and the lower right coordinates i want to give here is let's say 510 comma 128 so let's say we want to give the red color so i'm going to just write 0 comma 0 comma 255 and the thickness i want to give here is 5 and i'm going to just remove this because it'll just create problems and now let's run the code and you can see the rectangle is drawn with the red color of thickness five you can change the thickness of this rectangle by changing the value of the thickness and then you can run the code and now the thickness of this rectangle line is increased now one more thing you can provide here is instead of giving the thickness value if you write here minus one then it's going to fill the rectangle with the color which you provide here so when we give minus one here let's see what happens so now we get the filled rectangle because we have provided minus one option here so if you provide minus one then your rectangle or whatever shape you are creating will be filled with the color which you specify here so let me just change the thickness to 10 once again and now let's see how we can draw the circle so to draw the circle we once again use cv2 dot circle function and once again you can see what are the argument which it takes so the first argument is the image the second argument is the center of the circle the third argument is the radius of the circle and the fourth and fifth argument is the color and the thickness once again so once again we will provide the image the second argument is the center of the circle so let's give the center of the circle which is the coordinate on which you want to give the center so i'm going to provide let's say 447 comma 63 here and the third argument is the radius so radius we want to provide here is let's say 63 and the fourth argument is the color so let's uh use 0 comma 255 comma 0 which is going to draw the green color and then let's give minus 1 here so our circle will be filled with green color and let's run the code and let's see what happens so you can see this circle is drawn here and this circle is filled with the green color now let's see how we can put some text into the image so to put the text on our image we will once again use the image variable and overwrite on it and then we will use a method called put text so this is the method which we are going to use the first argument is the image the second argument here is the text which we want to put so let's say we want to just print opencv on our image so we can just write opencv as the second argument the third argument is the starting point of your text so you need to give the coordinates where you want to start your text from so the coordinates i want to give here is let's say 10 comma 500 and then the next argument is the font face so the font face you need to give here using a variable so i'm going to create a variable let's say font and then there are many font faces available using cv2 so you can just write cv2 dot font in capital and you can see what are the options available here i'm going to choose the first one itself which is font hershey simplex font and then we are going to pass this font as the fourth argument the fifth argument here will be the font size so let's say i want to give the font size four here the sixth argument here is the color of your font so let's say i want to just draw 255 255 255 which is going to give us a whitish kind of color the next argument we will give here is the thickness so let's say i want to provide the thickness 10 here and the next argument you can give here is the line type so let's say i want to give the line type cv2 dot capital line underscore and now let's run our script and let's see what happens so you can see here opencv is printed in the white color of thickness 10 and if you want to change this color you can change it from here so i'm going to just put the first channel as 0 and now this color is changed to yellow color now one more thing i want to show here is how you can create an image using numpy zeros method so either you can use a image which you read from i am read method or what you can do here is i'm going to just comment this code and we can create an image using the numpy zeros method so i'm going to create this img variable and then i'm going to use the numpy module so just import this numpy as import numpy as np and then we are going to use this mp to call the zeros method now in order to create a black image using this zeros method you need to give the first argument in the form of list and inside this list the first element will be the height second will be the width and third will be three so let's say we want to provide the height uh 512 we want to provide the width also 512 and the third argument will be uh 3 and the next argument you give here is the d type or data type so you can just write np dot u int 8 here so this method is going to give you a black image of the size 512 by 512 so let's run our code and let's see what happens so you can see now you can see the black image and on our black image the line is drawn the arrowed line is done and the text and the circle and the rectangle are drawn here so this is how you can draw different geometric shapes on your image there are several other methods you can use for example cv2 dot polyline method or cv2 dot eclipse method to draw eclipse and polygon on your image so just try those method to draw different shapes on your image so in this video we will see how to set some properties to our captured images so in the video capture lesson we have seen that when we create a cap variable using the video capture class we can get many properties using the cap dot get method so we were able to get the width of the frame and the height of the frame similarly we can use the cap dot set function to set some values so you can just write cap dot set and then you can set the values of the property generally all the properties which you can read like this you can also able to set those property using the set method now this notation you can also give in the form of numbers so every property here has a defined number so for example instead of using cv2 dot cap prop underscore frame width you can just write 3 here and that will work also so every property has a number associated with it so using that number either you can just let's say we want to set the width and height either you can write this as the first argument and the second argument is the actual width you want in the video right or you can just give the number of that property and then give its value so let's say we want to change the width of this video to let's say one two zero eight and then let's uh just set the height so cap dot set and the associated number for the height parameter will be four so three for width and four for the height and let's say we want to just move it to 720 and then we will once again print the value of the width and height and this time we are going to just give their associated numbers which is three and four so let's run this program you might already know this program what this program is doing so it's just capturing the video from your default device at index 0 and then it's just showing all the frames using this i am show method in a window so now i'm going to run this script and let's see what happens so when i run the script you can see the size of this frame is changed so let's see in the terminal also you can see before the original size of the video we are capturing is 640 and 480 so width was 640 and the height is 480 now once we have changed the width and height you can see the width is changed to one two eight zero and the height is changed to 720 so even if i have given here one two zero eight the default camera will automatically set its value according to its resolution so let's just close uh this video and let's say we want to just change this value to some random number so let's say 700 by 700 will it work or not so let's run the script once again and let's see what happens so the script is running and you can see that even though we have provided the 700 and 700 the camera will automatically take the resolution which is available for your default camera so the resolution remains the same even though we have set the different value to it so you need to keep in mind even though you can give any value here but the camera will only set the resolution which is available for it so let's give a very big value here so i'm going to provide let's say 3000 here and height also 3000 and let's run the script once again and let's see what happens so when we run the script you will see the resolution is changed but the resolution will change to the maximum resolution of my default camera which is 1280 and 720 this is the maximum resolution which is available for my webcam so let me just close this window so this is how you can set some values so there are many values you can set using this set method you just need to go to the documentation and then search for the value you want to set so in the last two videos we have seen how to capture videos from our default camera device or how to add geometric shapes on the images now in this video we are going to combine the knowledge we have gained in the last two videos so if you haven't seen the last two videos i will recommend you to watch those videos and then come to this video so in this video we will see how we can just draw something on a video and more specifically the aim of this video is how to show the current date and time on a live video so now in the last video we have seen how to draw shapes on images and we have also seen how to put text on our images right so let's say we just want to print the value of width and height on the default camera and let me just remove this line which we have used to convert the bgr image to the grayscale image so we will just see the colored bgr image so now what we want to do is we want to print the width and height which we get from these properties on our video which we are capturing so in the last video we have already seen that we can use a method which is cv 2 dot put text yeah so this method we have seen in the last video and first of all we will define the font which we will pass to our put text method so the font i'm using here is cv2 dot font hershey underscore simplex and now the first argument here will be the frame which we are capturing because every frame is just like an image and a video is the combination of multiple images so the first argument here will be the frame the second argument here will be your text so the text which we want to show here is let's say width and height so let's define a variable which we want to show on our video so let's say the variable name is text and first of all i'm going to define the width so just say width and then we are going to provide the value of the width using the concatenation operator now because this value will be in integer and we want to convert it to the string so we will use the str method to convert the integer to the string and then we can pass the width here inside our string variable once again we will use the concatenation operator and then let's provide some space here and then we will just write the height colon and then once again the concatenation operator and once again we will use this string method and inside this string method we will now take the hide okay and now we will pass this text to our put text argument now the third argument is the coordinates so let's say i want to just put this text at the coordinate 10 comma 50 the fourth argument is the font which we have already declared the fifth argument is the thickness so let's say the thickness we want is one and then the color so let's say the color we want is 0 comma 255 comma 255 and then the thickness so i think the thickness comes after the color and the value 1 we have set for the font scale so you can change the font scale one two three four any font you can change it from here so this value one is for the font scale and the value we are providing right now is for the thickness so let's say the thickness is two and the last argument here will be the line type so i'm going to just provide the line type as cv2 dot line underscore aaa so what do you think will this text will be printed on our image or not so it will not print yet because we need to write on the frame this text so we need to just write frame is equal to and then put the text on the same frame which we are seeing right now so now this will work and let me just break this line so you will see all the code and now let's run the code and let's see what happens when we run the code so let me run this script and you will see here that now we are seeing the width and height on top of this video which is 1 2 8 0 and the height is seven twenty point zero so this is how you can show text on your video which you are capturing from the camera or from the video file now let me just comment these lines of code because they are changing the resolution of our video and it's not fitting this video a screencast so i have commented this code and now let's do something more interesting so now let's say we want to show the current date and time on the video and you might have guessed how to print it but let me show you if you don't know how to print the date and time on your live video so first of all we are going to import the package which is available inside python which is date time and then we are going to create uh this date time variable let's say the date time variable name will be date t and then first of all we are going to use the str method to convert the date and time to string and then there is a method inside this datetime library so we just need to write datetime.datetime once again and then the method called now which is going to show you the current date and time so once we have converted our current date and time to the string variable then we can pass this variable as the second argument and now let's run the script and let's see what happens once again so i am running the script and now you will see that it shows the current time and current date on the video itself so this is how you can put the text on your video you can even put some shapes which we have seen in the last video on this video itself so you can put the line or the rectangle or the circle on your video which you are capturing from the camera or some file so this was some kind of a mini project which we have created from the knowledge which we have gained from the last two videos in this video we will learn how to handle mouse event in opencv now mouse event can be anything for example right click event or left mouse button click event or left button double click event so there are many mouse event available in cv2 package now to list out all the events in the cv2 package you can write this kind of code so first of all i'm going to create a variable called events and then i'm going to just iterate over all the events inside cv2 library so i'm going to just write i for i in dir inside our cv2 package so this dir method is the inbuilt method which is going to show all the classes and member functions inside your cv2 uh package okay so we are iterating over all the function names or member variable names and then we want to see what are the events available inside this package so we can just filter those events using a condition so we are going to just say that we want to just see the variables or the member properties which have this keyword event in them so event in i and then we are going to just print out all the events so i'm going to just print out all the events and then i'm going to run this code and here you can see the list of all the events which are available inside your cv2 library so you can see there is an event called event flag r button for the right button for the mouse or there is event for left button double click event or the event for l button down event so there are many such events available here and we are going to use those events to listen for the mouse events so this is how you can uh print all the events which are available inside your cv2 library and now we will create a script or a program to listen for the mouse event so first of all we will create a mouse callback function which is executed when mouse event take place so in order to create this callback function we are going to just uh define a function and then we will give the name to our function for example click event function and this callback function generally takes few arguments so the first argument will be the event which is taking place when we click our mouse and then it's going to give us the x and y coordinate on the image where we are clicking with our mouse so we are going to get the x axis value and the y axis value whenever we click the mouse at certain position in our image also we will get the flags and we will get the param so for creating the mouse click callback function it has this kind of specific format which is same everywhere so these are the parameter it takes and then inside your callback function you can define the logic so let's say whenever i click the left button down then i want to show the x and y coordinates on the same image so i can just say if the event variable is equal to cv2 dot event and then i will just look for the left button down click event so if this event occurs then i will first of all print the x and y axis values so let's print x comma y and you can also provide some space between x and y coordinates using this kind of square string and then what we are going to do is we are going to just uh put this x and y coordinate values on the same image which we are opening so we have already seen in the last videos how to put text on the videos we just need to create this font variable for the font and then there is a method called cv2.put text so we are going to just write cv2 dot put text which takes few argument first is the image now you can see this image shows error and this error says unresolved reference but don't worry when we write our code fully this error will go so first will be the image the second is the string which we want to put so let's say we want to put the string str for the x and y values so i'm going to just write x y and then we are going to just print the x value then concatenation operator and then comma and then once again concatenation operator y and don't forget to convert these coordinate values into the string using the str function so str function here and for the y axis also you need to use this str to convert it to the string value and then the string we pass as the second argument the third argument will be the location where we want to put the text and this location we already know from this x and y value so it's easy for us we are going to just say x comma y because we already know the x and y coordinate using this callback event the fourth argument will be the font the fifth argument will be the font scale let's say it's uh one and then the next argument is the color so let's say color we want to give here is 255 comma 255 comma 0 and the last argument i want to give here is the thickness let's say thickness we want to give here is 2 and then we will show this text on the image using cv2 i am show method so i'm going to just write i am show and then the name of the image window for example image and the image itself which is img so right now this is showing error to us but when we will call this callback function using a standard function called set mouse callback then this error will go so i'm going to define the img variable first of all and let's say we want to create a black image using numpy so we will call np dot zeros method here so np dot zeros and the size of this image will be 512 comma 512 comma three and the data type will be np dot u into 8 and once we have this image we are going to show this image using once again the i am show method and this image name will be the same image and the variable we want to pass here is the image variable which is the black image which we have created using this numpy zeros function now the next and the important step here is calling a method called set mouse callback a method so this method we are going to use to call our callback function which we have created which is click event function whenever somebody clicks on the image which we are showing using this i am show window so the first parameter it takes is the name of your image make sure that this name here which you uh take in the i am show method you can see i'm taking the same name here in the i am show method also here so the window name should be the same everywhere then only uh it will work so here also you need to uh just give the parameter first parameter here is the window name so the window name is image and the second parameter is the callback function which we want to call whenever this event take place so this is the callback function which we have created now the next step are the obvious steps which we have already seen so first of all we will call the weight key method to wait for the escape event and the second is the destroy all windows so we will destroy all the windows once we are finished so let's run this code and let's see what happens so now you can see the black image which is created by numpy zeros method and when i click on this image anywhere you can see the coordinates of the position where i have clicked is uh printed here so let's uh click here you can see when i uh give this left down button click event then the position of the x and y coordinate is printed on this black image so i'm clicking again and again this left down button and the position is printed okay so let me just close this window now what i want to do is let's uh just uh reduce the size of this font to 0.5 then the font size will be little bit smaller now what i want to do is i want to listen for some other events so i will go to my callback function once again and i will add one more condition here so once again if event is equal to cv 2 and this time i want to listen for the right click event so i'm going to just write event write button down event okay so whenever somebody uh presses this right button down for the mouse then this event is going to be captured inside this condition now if you remember i have told you that image is shown in opencv in the form of bgr format and we already have this image you can see we have declared the image variable that's why this error is also gone so using this image we want to find out the red blue and green channel so now inside this condition what i want to do is i want to print out the bgr channels of the image wherever i click okay so you can first of all declare a blue variable name and then we have img variable which is this one and using this image variable we can get the blue channel using the coordinates so first of all you can provide y comma x we already have the y and x coordinates and then the channel for the blue color is channel 0 because it starts from blue bgr so blue and then green and then red okay so i'm going to just copy it two more times the second is green and the channel for it will be the one or index will be one here and for the red channel this index will be 2 here so i'm going to just write red here and once again i'm going to just copy this code and this time what i want to do is instead of printing the coordinates i want to print the bgr channel so here i'm going to just write blue and then second will be the green channel and then the third will be the red channel so i'm going to just write comma and then concatenation operator s t r and then red channel okay so this will be uh the string we are going to name it as bgr and this string we will put here the color also we can change so the color for the coordinates will be different and the color for this event will be different 255 okay so it's going to print the bgr channels on your image now because we are creating the black image whenever i just click the right uh click mouse event you can see the bgr channels for this black image will be always 0 0 0 right when i click the left click then these are the coordinates when i click the right click these are the bgr channels so let's uh change this image from the black image to something visible so i already have the lena image so we can use this lamina image using the cv2 dot imread method so i'm going to just try cv2. im read and the first will be the name of the file which is lena dot jpg so now let's run this code once again and now i have this colored image so we will be able to see these functionality in a better way so first of all the left button click event you see the coordinate and when i click the right click button event then you can see the bgr channels are printed once again here you can see the bgr is different here also these bgr colors are different so you can see everywhere they are a little bit different because this is the colored image and the color differs at every pixel level so this is how the mouse click event works in the last video we have seen how to use mouse click event in opencv using python so we have seen how we can create a callback function which listens to a mouse click event and then how to use this callback function using the set mouse callback method now in this video i will show you some more examples about mouse click event so the first example i want to show about drawing a point and then connecting points using the line so to start with i'm going to just remove this if condition for the right down button click event and every time somebody clicks the left button down click event of mouse then what i want to do is every time the mouse is clicked down i want to draw a circle very small circle and when he clicks on the next point then i want to join those two points using a line so for that i will need a cv to circle so i'm going to remove this code which we don't need right now we just need this condition which listens for the left button down click event of mouse and then what we will do is we will just use cv2 to draw a circle so we will just write dot circle and first of all this circle method takes the image so we are going to pass the image and then the second argument is the coordinates x and y coordinate so we already have x and y coordinate using this callback function with the second and third parameters and then the third parameter will be the radius so i will take the radius three which is uh like very small which will uh give you a effect like a point on an image and then we can give the color so let's give 0 comma 0 comma 255 and then we will give the thickness now the thickness i'm going to give here is 1 and you might already know what this minus one do so this minus one whenever you give as a thickness it fills your circle or any closed shape okay so your closed circle will be filled with this color which you provide here now next what i want to do is i want to create an array of points so i'm going to just declare a variable called points and initialize it with an empty array now this empty array variable we can use inside our callback function and what we are going to do is we are going to just add or append every time this mouse is clicked so i'm going to just call an append method here and then we are going to append the x and y coordinate to this points array so we know that where this mouse is clicked and we are saving the coordinates wherever the mouse is clicked in the form of array now in the next step what we will do is if the mouse is clicked more than two times so we can just test the length of this array which is a point and if the length of this array is greater than or equal to 2 because the first click will be only a point so we cannot connect this point with a line but when we have two or more points then we can connect uh those points with a line right so if this points array length is greater than 2 then we are going to just create a line between those points or the circles in our case so i'm going to just call cb2 dot line method and first argument here will be image the second argument here will be the point 1 so the coordinate of 0.1 now we want to join the last two points right so we are going to just use this points array and then to get the last value of an array we use minus 1 here so here as an index we will give minus 1 which means the last element of an array and then we will join the second last element of an array so i'm going to just give this will be points variable not print so let's give the points variable and then we are going to pass the minus 2 here which will be the second last element so last and second last element we want to join and then the next argument will be the color so let's say the color we want here is 255 comma zero comma 0 and the next point will be the thickness so we will give the thickness of 5 here and then we will show this image using i am show method this code i have already shown you in the last video so i will not explain what this code is doing if you want to know more about this code you can see the last video and this time i will use the numpy zeros array which will be a black image so let's run this script and let's see what happens so i'm running the script and now i click on some position on this image and you can see this red circle is created this circle is created using cv2 dot circle method and because the radius is 3 the circle is very small and because the thickness is 1 the circle is filled with the color which you provide here now we have said that if the point is only one then we don't want to create any line if there are points which are two or more then we want to connect those point with the line so let's click here and you can see point one and point two are connected with a line i click here and you can see the last and the second last points are connected with the line that's why we have taken this minus 1 and minus 2 argument which means the last element of the array and the second last element of the array so when i click at any point it will be now connected with this blue line so this kind of line drawing you can use in satellite images where you want to connect two points together with the line now let's see the next example which i want to show you so in the next example what i want to do is i want to uh first of all read an image and then i want to click on any point on the image and then i want to show the color of the point which on which i have clicked using a second window so for this instead of using the numpy array which is the black image i will use the normal image which is the lana dot jpg image and now i will just remove this code from here so first of all i want to read the bgr channels so first of all i will just declare these variables first is blue and we have the image and in the last video we have already seen how we can get the bgr channels because we have the xcoordinate and the ycoordinate and we also know that blue is the first channel so we use the index 0 here to get the blue channel from this image at this coordinate which is x and y same we will do for the green channel so green i am g and then x comma y and then the channel index will be 1 here and then we will just get the red channel from this image and now what we are going to do is we are going to just draw a circle on this point where uh you will click this mouse uh down button click event so i'm going to just write cb2 dot circle and now i will not explain the parameters because you might already know what these parameters are in the next line what we are going to do is we are going to create a numpy zeros image and then we will pass our bgr channels which we got from the particular point on an image so let's create an image so i will just say my color image and then we are going to just use np for numpy and then we will just call a zeros method here and it takes three argument in the form of this list which is the size of your image let's say this size will be 512 comma 512 and the channel will be three channels and then the next argument will be the data type so np dot u int it so we have a black image using this numpy zeros and now we want to fill this image with the bgr colors which we got from the particular point of the image so in the next line what we are going to do is we are going to just use this variable and then we are going to just write this kind of notation this means we want to fill every channel or every point of this list and then we will just pass our bgr channel values which we got from the image so blue green and then the red channel values we are going to pass so this will give us the bgr channel which will be the color of the point where we have clicked and now we have the new image with the color so we can show this image using a new window with let's say this is the color window okay so this is how you will get the new window with the color on which you have clicked so let's run this code and let's see what happens so i'm going to run this code and you can see this is the image which is the colored image let's see i click on this point and you can see the same color on which i have clicked is opened in the next window let's click here on the hat you can see it's going to give you the same color on which i have clicked let's click on the eyes and you will get the same color on which i have clicked let's see what happens when we just load a black image instead of this colored image so i'm going to just use this numpy zeros uh image which is the black image and let's run this code and now whenever i click on this every time i click on any point it'll be the black color window which will open so this is how you can use some examples to understand how mouse click events can work and you can use them to develop your applications in this video we will see some of the basic and arithmetic operations on images using opencv so let's get started so here i have this code some of this code you already know so you already know how to read the images using i am read method and then show it inside a window using i am show method and destroy all windows using this destroy all windows method but this code in between is little bit new so let me explain line by line what this code does so when you have this image using imread method or any other method you can use these attributes like shape size and d type to get different uh values from this image so image dot shape is going to return a tuple which contains the number of rows columns and the number of channels in this image the image dot size will return the total number of pixel which are there inside the image and image dot d type is going to return the data type of the image which you have obtained now here if you want to split your image in three channels then you can use cv2 dot split method and pass your image as an argument and it's going to give you the bgr channel of your image now if you have bgr channels and you want to merge those bgi channel into an image then you can use cv2.merge method and pass these bgr channels in the form of tuple and it's going to give you the image which you can load using i am show method so let's run this code and let's see what we are getting using these attributes so you can see this messy5 dot jpg image is loaded and here you can see first of all the shape of the image so the shape returns the number of rows number of columns and the number of channels so number of rows here is 342 columns are 548 and number of channels are three here the number of a pixel which we have calculated using the size is this number which is 562248 and the data type of an image is uint8 so sometimes you need to debug the data type of your image and this attribute will be very useful in those cases when you need to debug if something is correct or wrong and because we have splitted this method using the split and remerged these bgr channels using this merge method so we will at the end get the same image which we have at the beginning here in this code so there is no change in the code so once again let me just load this image and now let's talk about the roi of an image so roi stands for region of interest so sometimes you need to work with certain region of the image so let's say you only want to work with the face here or you only want to work with this ball okay so this is called the region of interest or in short form it's called roi so let's say we want to just work with this ball here so this will be our region of interest or roi and i want to just copy this ball to other place in this picture so i want to just copy this ball and place it on the other place let's say somewhere here okay so how we can do this so i already have the coordinates of the ball but you already know how to get the coordinates of some place in the image we have already discussed this in our previous video so i'm not going to show you how to obtain those coordinates but let's say i have those coordinates of the ball so i'm going to create a ball variable and we have our image so we will take our image and there are certain numpy indexing features which we can use here so i'm going to just write a 280 colon 340 which is going to give you one point on the ball which is the upper left hand side of this ball and then we will give 330 here colon 3 90 which is going to give us the bottom right hand corner of this ball okay so now we have this ball so this this indexing is going to copy this ball all the pixels of this ball and then now we have the ball so we can place this ball on any place on this messy image which we are reading so what we can do is we can once again use img and using those numpy indexing features we can place this ball at some other place so let me just give those uh indexes here so let me give 273 colon 333 i have already tested this code so that's why i know exactly where i want to place this ball but if you are not sure where to place this ball then you might have to first calculate or know the coordinates where you want to place this ball and you already know how to find out the coordinates on an image and you will be able to place uh that roi or interest of region at some other place so what i'm doing here is i have just copied the ball and then i am placing the ball on this coordinate okay so i just need to just assign our ball on this coordinate and then this ball will be copied to this index on the image so let's see what happens when we run the code so now you can see we have copied this ball and we have placed this ball here on the image so this is how you work with the roi or region of interest okay so let me close this window now the next thing which i want to show here is how you can add two images so for that i need one more image so you can see in my project i have this five dot jpg and i have this other image which is opencv hyphen logo.png file which is of the same size as the messy.jpg image so i'm going to just write im g2 and then once again cv2 dot im read method and then i am going to give the name of this file which is opencv hyphen logo dot png file okay so this is uh this file so this file we are reading and then there is a method called add okay so we are going to use this method here let's use this method cv2 dot add and this method i'm going to show you what it does in a moment but this method takes two argument first is the first numpy array so let me show you what this method do first of all so this is the add method inside your cv2 package you can also see the documentation on the opencv dot org and what it does is it calculates the preelement sum of two arrays or an array and a scalar okay so here we can just pass our two arrays which we got from the i am read method and pass here as the first and the second argument so i am g and i am g two are the one and two parameter and there are some other parameters also like uh output array input array mask and int which is the data type which we which are set by default so we are not going to set them so we are just using cv2.add method on these two images and then i just want to assign the new image which we have added to a new variable let's say this is dst for destination image and then we are going to just show this image using this i am show method okay so we have two images let me show you uh those images one by one first of all so this is the first image i have and the second image is opencv we have logo which is like this one okay so those two images we have and when i run this code after adding those two images using add method you will see first of all you will see this error and why this error is coming because you will see here that the size of those two input is not matching okay so in order to add two images you need to have the images or the arrays of same size and then only you will be able to add those two images so let's resize those two images into a size which uh is common to both of them so you what we are going to do next is we are going to resize those images so once again i'm going to just use img variable so what i get after the resizing i will once again assign to this img variable and there is a method called cv2 dot resize and this helps us to resize the image so first of all we need to give the source which we want to resize and then we are going to give the size which we want to get so the number of columns and number of rows we can give here let's say we want to just resize this image to 5 1 2 comma 5 1 2 which is the number of rows and number of columns right same we will do with the next image so i am g and then once again cv2 dot resize and then the source here will be image 2 and the size which we want here is again five one two comma five one two in the form of tuple so we have resized this image and this image which are of different sizes to the same size and now let's run the code once again and now you will see that these two images are merged now okay so you will be able to see the hand here and little bit foot and here the ball of uh this image one which is messy five and then we have the second image which is opencv which is added to the first image so this is how you can add two image using opencv now there is one more method which is called add weighted okay so this add method is going to just add these two images but if you want to add the weight for example you want to give the weight 90 to the first image and 10 percent to the second image there is one more method so let's go to the documentation once again and there is this method called add weighted method okay so this add weighted method takes uh several arguments here you can see first is the source of the first array and second argument is the alpha value alpha is the weight which you want to give to the first image okay the third argument is the source two so in our case this will be the image two the fourth argument is the beta beta is the weight which you want to give to the second image right so this weight you can can give from zero to one anything and this gamma is the scalar value which you want to provide and this second last parameter is the destination and the last is the d type or the data type here okay so this is the formula which this method is going to use so source multiplied by alpha and source 2 multiplied by beta plus gamma so this is the method which will be used using these arguments or simply you will use this kind of methods source multiplied by alpha plus source 2 multiplied by beta plus gamma which is the scalar you can add to the image okay so let's use this method so i'm going to just copy this method and then comment this and go to the next line and instead of using add i'm going to use the add weighted method okay so the first argument is the source which is the first source which is img in our case second argument is the weight so first this is the messy image right so we want to just give the weight here 90 or you can just give 0.9 here and for the second image we want to give the weight uh 0.1 okay so the sum of this weight and this weight will be 1 and also we are going to give the gamma value here as zero so we don't want to add any scalar value to uh this add weighted method so the next uh value here will be zero which is the value of gamma and let's run this code and you can see now now we have our messy image which is dominant here because it has the weight 0.9 which is 90 percent of the two and the opencv image have the weight 0.1 which is 10 of the two okay so the opencv image is light and the messy image is a little bit uh you know dominant here you can just give 0.5 and 0.5 so the weight of the two images will be the same and now you will see those two images in the same domination okay so 50 50 now let's say we want to increase this value of opencv to 0.8 and the messy image weight will be 0.2 then the dominant image here will be opencv and in the background kind of thing you will see this messy image so this is how you can add two images with their weight and the scaler and that's it for this video so in this video you have seen some of the basic operations on the images and some of the arithmetic operations on the images which you can do using opencv in this video we will talk about bitwise operations on images using python and opencv so bitwise operations can be very useful when working with masks masks are binary images that indicates the pixel in which an operation is to be performed so let's see how we can perform bitwise operations on images so to start with i have here one image which is image underscore one dot png file and let me show you the image also so this image is half black and half wide now the second image i'm creating using numpy so first of all i have used np.zeros and i'm just creating this image with the same dimension as this image underscore 1 is having so 250 comma 500 is the dimension of this image and the number of channels are three and this code is going to create a black image as you might already know from our previous videos now this code is just creating a white rectangle inside the black image which we got from numpy's zeros array okay so this is the dimension of the rectangle inside your black image and the color of the rectangle will be white because this is 255 comma 255 comma 255 and we are taking thickness as 1 that means your rectangle will be filled with white color now here i'm just showing both the images using i am show method and this code you might already know what this is doing from our previous videos so let me run this code and let's see what happens first of all so when we run this code you will see first image is this one which we have created using the numpy zero so this is img1 and this is the black image and we are just creating a white rectangle inside this numpy zeros image and this is the second image which is half black and half wide now we want to perform some bit wise operations on these two images so let's see how we can perform these bitwise operations on these two images so to perform these bitwise operations we have some methods inside the opencv library so the first method will be bit and so i'm going to just create a variable called bit and with let's say like this and the method inside opencv is cv2 dot bit wise underscore and so this bitwise underscore and takes several arguments as you might see here the source of the first image the source of the second image and the destination which is none by default and the mask which is also optional so we are going to just provide our images here so i'm going to provide the i am g2 here first of all as the first argument and the second image will be img1 and once we perform this bitwise and operation on these two images we are going to show the result in the form of windows i am going to create one more window which is cb2 dot i am show and i'm going to name this window as let's say bit and and the second argument will be our variable bit and which we got from the operation bitwise and on these two images so let's run this code once again and let's see what happens so now as a result we have the third image so let me just open all the images so this is our first image this is the second image and the last one is the result which is the bit and operation on these two images so now you might already know how the logical end works but those of you who might not know how the logical end works let me show you the truth table of logical end so this is the truth table of logical end and if the input a and input v b is zero then we get the result zero okay if input a and input b either of them is zero then also we get zeros right the result one we will only get when both the sources are one so a and b are one then only we will get a one in case of and logic so same and logic will work here so this is the zeros array right so we have created this black region from the zeros so here in these images black is performing as zeros and the white part is performing as 1 so when 0 and 0 the result will be 0 right so from this truth table we have seen when the input is 0 and 0 the result is 0 same here we are seeing so when the image is black and black we get the result black when the input is white and black this means 0 and 1 the result will be once again 0 using the logical end but when the input will be white and white that means 1 and 1 the result will be wide that means the one okay so the only reason region white here is the result of this white and this white and the resulting image you can see here and all the other part is black because the and operation on 0 and 0 is 0 and 0 and 1 is also 0. so this is how bit wise end works let's see how bitwise or and other operation works so i'm going to just comment this code and now we are going to just create the bitwise or operation so for that i'm going to just uh instead of writing bit and i'm going to just write bit r and instead of bitwise and we are going to just write bit wise or here and then we will simply call this uh image using i am show method so we are just calling here bit or now let's run the code once again and let's see the result so you can see the result here so let's see the truth table so in the logical or if only one input is one then the result will be one so either a or b is one or both are one then the result will be one and if both inputs are zero then the result will be zero so same you will be able to see here so when the first source and the second source is zero the result is zero but when the first source and second source is one or white the result is white when the first source and the second source is zero and one or black or white the result is once again one or white here okay so this is how the logical r works on the image now let's uh see how the xor operation work on those images so i'm going to once again comment this code and this time i'm going to just perform the xor operation on these two images and now we are going to run this code once again and you will see this kind of result so once again let's see how the xor logic works so when both the inputs are 0 or both the inputs are 1 then we will get the 0 and if either a or b is 1 then only we will get the result 1. so same you will be able to see here so when both first and second source is 0 the result is 0 when both the first and second source is one you can see here and here the result is once again zero here right but when the input is zero and one result will be one and in this case also the black and white will result in the white image which is the logical xor operation so once again let's close this and now let me show you how the not operation work so i'm going to just comment this code and then i'm going to just use the bitwise not so here bit not let's say we will perform the bit not on the first image and the second image so i'm going to just write bit not on the first image because it only takes one argument bit not is just the opposite of the source so if you get the input 0 then the result will be 1 if you have the input 1 the result will be 0 so the opposite of the input so let's perform this operation on image 1 and image 2 and let's comment this code and we are going to just show these two result windows using the i am show method and also i need to change this name otherwise we will face problems and here also i haven't changed the name of these i am show windows so let's change the name of these windows and let's run the code once again and now you will get these results so we will get the first result so the first bit not one is the not of the first image and bit not two is the not operation on the second image so you can see wherever we have white we got black and wherever we have black we got white so just the opposite of the input here also wherever we have the black region we got the white image here and wherever we have the white pixel we got the black pixel so this is how bitwise knot operation works on the images so these are some of the bitwise operations which you can perform on your images and as i said bitwise operations can be very useful when working with masks which we will see in the later videos in this video we will talk about track bars in opencv now track bars are really useful whenever you want to change some value in your image dynamically at runtime so let's see how we can use trackbars in opencv now to start with i have this simple code which you might know what it does so first of all i have imported cv2 as a cv and then i'm creating an image using the numpy zeros array with these dimensions and then i'm creating a named window with the name image so this might seem new to you because i haven't created a named window in the previous video so the named window you can use to create a window with a name and this time we have given the named window name as image now in this while loop we are just using this i am show method to call this window and then loading this image inside this named window now you might already know what this code does it just wait for the key and if the key is escape key then we will break out of this loop and in the last we are just destroying all the windows which we have created now in order to create a tracked bar you just need to use cv and then call a method called create track bar now the first argument here you need to give is the track bar name because you can create multiple track bars in your image window that's why you need to provide the name which is unique to this track bar so i'm going to just give the name to my track bar as b because what i want to do is i want to change the bgr values of the image using the drag bar so the first track bar will change the b channel values that's why this first argument is the track bar name which is b and the second argument here we will give is the name of the window so that one that is why we have created this named window so that we can provide the name of the window which is image in this case and that is how we know that in which window we need to add the track bar so in the image window which is this one we want to add the b track bar now the third argument here will be the value which is the initial value at which your track bar is set and the next value here will be the count which is the final value you want to set for your track bar now there is this last thing which we want to set here and this is the callback function which will be called whenever your trackbar value changes so here for example i'm going to create a callback function called nothing and this callback function definition or signature i'm going to create here so we can just create a callback function with a name nothing and this function can take this value x and this is the value of the current position of your track bar so we will see what it does little bit later and what we are going to do is we are going to just print the value of x so we will know the current position if this track bar is changed so this is the callback function which will be called whenever your track bar value changes same we will do with the other track bar so we will create the three track bars in the same window with the name b and the next track bar name will be the g and the last track bar name will be r okay so this will be capital r so now let's run this code and let's see what happens when we run this code so i'm going to right click and run this script and you can see here inside this named window with the name image we have this black image which we have created using numpy zeros array and now we have three track bars here with bgr names so these track bar values you can change using this uh scroller and as you can see here let me show you in this terminal whenever you change the value of any bar the corresponding value will be shown here using this callback function and inside this callback function we have the print statement okay so as i said whenever you change this value this callback function is called and it will print the value of the current track bar okay so for this functionality what we want to do is we want to get the current position of the track bar and because we can change the value of bgr channels from 0 to 255 that's why i have given the range between 0 to 255 to the track bars also so that you can change these bgr channel values so now in order to get the current value of your track bar first of all we will just check the value of the b track bar so we will just use cv dot get track bar position which is this method get track bar pos and then we just need to give the name of our track bar so let's say we want to check the position of track bar b then we will just say we want to have this track bar position with the name b and the second argument here will be the name of your window so in which window this uh track bar is present so the our track bar is present inside the image window right so same we will do for the g and r values also now we have the values of b g r channels from the track bar so now we want to set these values to our image so what we can do here is we can just write for example i am g inside these square brackets you can just give this kind of notation and then give the bgr channel value so i'm going to just write b comma g comma r that means we want to set the current b g r values to this image so let's run this code and let's see what happens now so i'm going to run this code and now when i change the blue channel values you can see this image becomes blue colored right let's bring it to 0 once again and now let's change the value of g so you can see this image color is changing to green and then we can try changing the red color and you can see when it goes to 255 the color of the image is red you can change the values of different track bars and the corresponding color will be displayed in this uh window here right so you can see the color is changing you can change any track bar here one more example i want to give here is how to add a switch using a track bar so for that i'm going to use one variable called switch and then here i can add first of all the name of the switch and in the next line we will once again call cv2 dot create track bar with the name switch okay so now the name of our track bar will be switch so now we have added one more track bar to our named window and now here we will get the current position of this uh switch track bar so i'm going to name it as s and the name of the window is switch so we will just give the first argument of this get trackbar position as switch okay and the window name is image itself so now we can add some condition here so let's say if this position of the switch which we have if this position is equal to 0 because we only have 0 and 1 in this last track bar so if this position is equal to 0 what we want to do is we want to set im g and then in the square bracket this colon and we don't want to change any value so we will say that img this square bracket colon is equal to 0 which means that we don't want to do anything or in the other condition which is when your track bar is at position 1 then only we want to change the bgr channel of the image okay so let's run this code and let's see what happens so i'm going to run this code and now you can see the position of this track bar switch is 0 and when i change it to 1 so let's change this position to 1 you can see the value to 1 and when this position is at 0 you can change anything here any track bar nothing happens because this condition is met which means that we don't want to do anything as soon as we change the switch to 1 that means we want to change the bgr values you can see this color is changed inside the image so the 0 is just like off switch so we don't want to change any color and one is like on switch so when it's one the value of rb g channels can be changed now i want to give one more example of track bar to you so that's why i have created one more file which is python opencv trackbar example2 and this time i'm going to use just two track bars here so that's why i'm going to delete some of the code here so using the first track bar let's say i want to just change some values inside our image and i want to print that value on that image so let's say now our range is between 10 to 400 okay so the lower range is 10 and the upper range is 400 and using this track bar i want to print the current value on our image and also i want to have a switch which i can toggle and i want to change the color of the image from the colored value or colored image to the grayscale image so now our switch is between color to the grayscale image now in here what we want to do is we want to just assign this i am show value to the image variable itself and then we want to get the current position of the track bar so we will use this method to get the current position of the track bar and i'm going to name this current position as p o s variable and the name of this track bar let's change this name to something else let's say cp for current position and also here cp for the current position and the name of the named window is image itself so we are not changing it so now we have the current position so first of all we will just create the font and then we will just use the cv dot put text method you already know what this method does it just print the text on your image and then we will provide the parameters first argument is the image the second argument is the value which we get from the track bar so this is the position and because it's a number we need to convert it to the string using str method and then the position at which you want to show this text so let's say it's 50 comma 150 and then next is the font so i'm going to just give the font and then the next value is the font scale which is 4 and the next value is for the color of the text so let's say the color here will be 0 comma 0 comma 255 and this should be cv dot font hershey complex let's change this font also let's say this is just the simplex font okay so this code is going to just print the color current position of the track bar on your image and then inside this condition what we want to do is we want to get the switch value so let's use this s variable and then get the current position of the switch using this switch name from the image window and then if the switch is at zero position then we want to do nothing so we will just pass this situation and in case the value of this switch is 1 then what we want to do is we want to change the image value from color to the grayscale value right so we can just write cv dot cbt color and the first argument is the image which we are loading and the second argument is cv dot color bgr to gray which is to convert this colored image to grayscale image but you can see here we are just creating a black colored image and in our project we also have this image so let's read this image so i'm going to just write the cv dot im read and then give the name of the image which is lena dot jpg so this is our colored image and this way we will be able to see the change of color to gray scale image in a better way let's run this code and let's see what happens and you can see image appears and disappears and there is an error so let's see what is the error so the error here is coming from this line so we need to read this image inside the while loop okay so this is why our error is coming and at the last we want to load this image after this if condition okay so now let's run this code once again and you can see this value is printed on our image which is 10 which is the value of cp and if we change this value it is changing on our image also right and once we change this 0 to 1 then our image is converted from colored image to the grayscale image you can also change the font size here for example let's say it's 6 here and the thickness also if you want to change you can change it using this parameter let's say it's 10 and let's run this code once again you can see the thickness and the size of the font is changed and you can see this value in a better way okay so this is how you can use track bars in opencv in this video we will see how we can perform object detection using hsv color space now we have already seen how to work with bgr or colored images or grayscale images and we have already seen how we can convert from colored images to grayscale images so there are more than 150 color space conversion methods in opencv and one of them is colored image to hsv image now what is hsv color space so hsv stands for hue saturation value so h stands for u as for saturation and v for the value now generally r g b in rjb color space are all correlated to the color luminance that is what we loosely call intensity in other words we cannot separate color information from luminance so hsv or hue saturation value is used to separate image luminance from color information so this makes it easier when we are working on or we need luminance in our images that is why generally we use hsb in the situation where color description plays a very important role now as i said hsv stands for hue saturation and value but what is the meaning of each and every single word in hsv now hsv is also known as the hex corn color model so this color space can be described in this kind of cylindrical cone model where hue is this circular angle which varies from 0 to 360 and hence just by selecting the range of hue you can select any color so you can see different colors are available at different angles so these colors are basically red yellow green cyan blue and magenta so hue is this angle in this cylindrical cone now we have saturation so the saturation is amount of color that is the depth of pigment or the dominance of hue and this value is described from the center towards the outer layer of this cylindrical cone so here you can see at the center this saturation start at zero and it can go up to one at the end of this cylindrical cone and this saturation can be increased from zero to hundred percent similarly the value is basically the brightness of the color so this brightness can be increased from 0 to 1 from the bottom of the cone to the top of the cone so all these three value hue saturation and value can be used to pick any color as we can do with the bgr color space so this is the brief introduction about hsv color space and now let's see how we can use this hsv color space to detect an object in an image so here i have this simple code to load an image using imread method and show it inside a window so by now you might already know how this code works so let's run this code and let's see what does this code do so i have this image which is called smarties.png and here are some circles in different colors so we have blue circles or green or red orange and brown circles here inside this image so let's say we somehow want to detect only the blue circles or balls or green circles or balls how can we just detect only these balls let's say we just want to detect the green balls how can we achieve this using opencv we are going to see this using this hsv object detection and here we have one more window which is the tracking window which is coming from this code which is cv2 dot named window and the name of the window is tracking so this tracking window we are going to use little bit later when we will add the track bars to our image but let's say we want to uh use this image and detect these colored balls so first of all after this image is red what we want to do is we want to convert our colored image into our hsv image and by now you might already guess how to convert an image you can just write hsv is equal to cv2 dot cvt color and then your frame name which is frame in this case and then cv 2 dot whatever color space you want to convert from and whatever color space you want to convert to so you can just write color underscore bgr to hsb so this is the property we are going to use now in the next step we will threshold the hsv image for our range of blue color so we are going to just define l underscore b for lower blue color and then we are going to use the numpy array so np dot array and inside this array we are going to define the lower range of blue color now by experience i know that these hsv value for lower blue color will be 110 comma 50 comma 50 right but you might not have every time the idea what is the lower color range or the upper color range of some color so that is why later in this video we will use the track bar in order to perfectly uh define the lower and upper values for this hsv color space right so right now i'm just uh going with my experience so for the upper value i'm going to define the next variable which is ub is equal to np dot array and then once again i'm going to define these three uh color channels which is 130 comma 255 comma 255 so this will be the upper limit for the blue color for our hsv image now in the next step what we are going to do is we are going to threshold the hsv image to get only the blue color lets say so i am going to just define a variable called mask here and then i am going to use cv2 dot in range method where i will provide first of all my hsv variable or image and then i will provide the upper and lower range for this function so my lower range is this numpy array for the blue color so i'm going to just say l underscore b is my lower range and u underscore b is my upper range now we have already seen how we can use bitwise and or bitwise operations on images so what we are going to do next is we are going to define a variable called res and then we will just call cv2 dot bitwise and to mask the original image so here the first value will be our frame which which is the colored frame right so this is the frame which we have read from this image which is the smarties image so this is the source one source two will be the same so the frame itself will be the source two and what we want to do is we want to provide the mask of the lower blue color and the upper blue color values right so here we can just say mask is equal to whatever mask variable we have created so this is the attribute we can set in order to apply the mask for the lower blue value and the upper blue values so once we have this result uh frame what we can do is we can use this cv2 dot i am show method in order to show the mask let's say so we are going to show the mask and we are going to show the result using res variable so this is going to open three windows and let's see what happens when we run this code so we are going to run this code and this opens three windows here and now you can see the mask first of all so we are just detecting the blue colored uh balls using this mask that's why we have defined the lower boundation of the blue color and the upper boundation of the blue color right so that's why it's only detecting you can see the blue uh ball is here here and here and here also you can see the mask also detects only the blue values here right and then in the result you can see when we have applied this mask and we have masked all the other things other than the blue colored ball you can see only the blue balls here so the same method you can apply to detect any other colored ball from this image now as i said it's not easy to detect these lower and upper boundation for the colors so that's why you can use the track bar for adjusting these lower and upper boundation of any color so for that what we are going to do is first of all we will create a named window and then we are going to create a new window which we will use to adjust the lower and upper boundation of hsv values so now i'm going to just use a cv 2 dot we have already seen how to create a track bar so i'm not going to explain in detail how this works but let's say this track bar name is lure hue for lh okay so this is the lower hue value and then the name of the window which is tracking which is this one so we are going to provide the name of the window and the next argument will be the starting and the ending value so we are going to define the start value 0 and the end value let's say we are going to define the 255 here okay and the last thing we want to give here is the callback function which i have already created which is this function which is just no doing nothing we are going to just provide this callback function as a dummy function so it's not going to do anything so this is the track bar for the lower hue value similarly we are going to define the track bar for lower saturation and lower uh value and upper saturation upper value and upper hue okay so this will be lower saturation this will be lower value and then this will be u h which is upper hue and then this will be u as for upper saturation value and this will be upper value right so hsb lower values and hsv upper values so here we are going to set the initial value for the upper value so let's say everything is set to the maximum so 255 255 and 255 here okay so the lower values are set to zeros and upper values will be set to uh 255. now you already know how to get the values from attack track bar so you can use for example l underscore h for the lower q values is equal to cv 2 dot get track bar position so just use get track bar position method and then first of all give the name of the track bar from which you want to get the position so let's say we want to get the position from the lh track bar and then the name of the window which is tracking in our case so here is the second argument and similarly what we are going to do is we are going to define the other lower values and upper values so and also the name of your track bars so once you have the values of lower hsv and upper hsv you can provide these values here in place of these static values so first element of this array will be lh and then the ls variable and then the lv variables similarly for the upper boundation we will provide these three upper boundation variables and now when we will run our code let's see what happens so we are running our code and you can see these windows these three windows one is the mask other is the result and the third one is the frame and we also have these track bars in order to change the value of lower and upper hsv values so first of all let's set this mask for the blue color so i'm going to just move it to 110 as we have done in the last step and then this will be around 50 and this also will be around 50 okay so let's move it to 50 and upper value here will be around 130 right so you can see once again using this track bar it's easier to adjust these lower and upper boundation and now you can see all the three uh blue colored balls so you can refine this object detection by moving these track bars a little bit uh left or little bit right you can see here now let's uh adjust this value to detect some other balls so let's say we want to detect the green balls so let's see what happens when we just change the saturation values here and you can see now you almost see the green values and the blue color is almost disappearing so you can see now there are only green uh balls which are detected and all the other balls are masked so you just need to play with this track bar for the lower hsv values and the upper hsb values and you will be able to detect the object whatever colored object you want to detect from the image now this is the object detection from the image similarly we can use the same method in order to track an object from a live video so i'm going to just escape to just close all the windows and in order to change this code for the video input what we can do here is we can just add this code so we are going to just add the cap variable which is the capture variable is equal to cv2 dot video capture so we are going to use this one and we are going to capture the video from our default camera which is at the index 0 and then you already know how we can read the values from the camera input so i'm going to just comment this code and instead of reading the image what we are going to do is we are going to write underscore comma frame is equal to cap dot read which is going to read the frames from your default camera and at the end when you are done playing with your images you can just destroy this cap using the release method so you can just write cap dot release just going to release all the cameras you are just capturing right so now this is the three line code you need to use in order to capture the camera input and then track any uh object of any color so i'm going to run this code now and you can see i'm just holding a blue colored object here and i'm moving this object on the left and right and you can see only blue colored object is detected and every other frame value is masked so this is how you can do the object tracking of any color using the hsv color space so you can see the real image which is captured from the camera and then the mask and then the result of the mask and the real image in this blue colored object tracking so this is how you can do object detection and object tracking using hsv color space in this video we will see how we can perform simple thresholding on images using opencv so first of all what is thresholding so thresholding is a very popular segmentation technique used for separating an object from its background the process of thresholding involves comparing each pixel of an image with a predefined threshold value and this type of comparison of each pixel of an image to a threshold value divides all the pixels of the input image into two groups so the first group involves pixels having intensity value lower than the threshold value and the second group involves the pixels having intensity value greater than the threshold value and using the different thresholding techniques which are available in opencv we can give different values to these pixels which are higher than the threshold value and which have the intensity lower than the threshold value so let's see how we can use simple thresholding techniques on an image so to start with i have the simple code which loads an image on a window and this image is called gradient dot png so let me show you how this image looks like so this image looks like this so as you can see in this image we have on the left hand side the black values and when we gradually move from left to right we move towards the white value so on the left hand side the pixel value are closer to zero and on the right hand side the pixel values are closer to 255. so now we are going to just involve some thresholding techniques and we will see how these this image is affected by the thresholding techniques so first of all what we are going to do is we are going to uh define two variables one is underscore because the the result of the thresholding gives two result one is ret value and the second is the thresholded value of an image so i'm going to just say the second value which is given by the thresholding technique is th1 is equal to cv dot threshold and this threshold method takes several values the first is the source so our source is image the second is the threshold value so as we have seen that our image have on the left hand side 0 pixel value and when we move towards the right its uh pixel value uh increases to 255 right so let's say our threshold here is 127 and the maximum value of the threshold is 255 which is the white color on the right hand side and then the fourth value here will be the threshold type so there are several threshold type in simple thresholding we are going to see them one by one so the first thresholding type is cv2 dot thrash binary so first of all let me show you how the result looks like and then i will explain what does this trash binary type does so what we are going to do is we are going to use one more cb2 dot iom show method to show this thresholded value into a new window so we are going to just show this value into a new window and we already have the original image in the image window so let's run this code and let's see what happens so you can see in this binary thresholding we are comparing each and every pixel of this original image to 127 and if the value of the pixel is less than 127 the value is assigned to 0 and if the value of the pixel is greater than 127 the pixel value is assigned 255 that means wide so if the value of the pixel is zero it will look black and if the value of the pixel is 255 it will look wide so this is how binary thresholding works and by the name itself you can understand that this is just a binary thresholding so it's either 0 or 1. now let's see the other type of thresholding technique so now i will just change the name of this variable as th2 and the next type of thresholding is called thresholding binary inverse and as the name suggests the thresholding binary inverse is going to give the inverse result of what you get from the trash binary so i'm going to once again use the i am show method to show the result of this thresholding binary inverse value and let's run the code and let's see what happens so this is the original image and then we have the thresholding one image and the thresholding uh inverse image so this image you got from the first thresholding which is by using thrash binary and the second image you got from this method which is thresh binary inverse and this thresh binary inverse image is just the inverse of what you get using the thresh binary so if the pixel value is lower than 127 which is our threshold the pixel is assigned 255 otherwise if the value is greater than 127 then the pixel value is assigned 0 which is the inverse of what we got in the previous step now let's change this threshold to let's say 50 and here also let's say we change this threshold to 200 and let's see how this result changes when we change the threshold value so i'm going to run this code once again and you can see this is the result of thresh binary and now because our threshold is up to 50 that's why our result is like this so until the pixel value is 50 it's black otherwise if the pixel value is greater than 50 it's going to give you the white pixel value and the thresh binary inverse is going to give you the inverse value of what you get in the thresh binary step so i'm going to once again just close these windows and let's see the next uh thresholding type so i'm going to name my variable as three so the next thresholding type is called thresh trunk so this is this type and let's first of all see what is the result of this thresholding technique and then i'm going to explain what it does so we are going to just show this thresholded image into a new window and run the code and now we have the result so let's move it like this and we have here the original image and the result of the thresh trunk is this three so here what happens is up to the threshold the value of the pixels will not be changed so up to 200 because our threshold is up to 200 so when the pixel value is up to 200 the pixel value will not change and after the threshold which is 200 the pixel value will remain the same which is 200 so from here to here the pixel value will remain 200 let's change this threshold to some other value let's say 127 and then let's uh run this code and you will see that now from black to 127 pixel value the value of this image will not change so original image up to half is the same and after the pixel value 127 the value remains 127 okay so the pixel intensity value will remain 127 until the end so if the value is greater than 127 the value will remain 127 and if the pixel value is lesser than 127 then the pixel value will remain unchanged so this is how the trash trunk works and let's see the other method which is let's say th4 and this is the method which is called thresh 2 0 so we are going to just use thresh to 0 and then we are going to open this th4 into a new window and let's run this code and let's see what happens so now we have this result let's move this to the left and the result of the thresh to zero is this one so in thresh to zero thresholding whenever your pixel value is lesser than threshold the value assigned to pixel will be zero okay so when the pixel value is lesser than the threshold the pixel value is assigned to zero that's why you can see half of the image is black and when the pixel value is greater than 127 which is our threshold value the image or pixel value will remain the same so after 127 all the pixels will remain the same let's see the other technique which is called thresh to zero inverse which you uh already understood i think what it does so this is thresh to zero inverse and we can just change this variable name to th5 and here we can just open it into a new window and i'm going to run this code once again let me move this here and the result here so you can see this thresh to zero inverse is just the opposite of the thresh to zero so if the value of the pixel is greater than the threshold value which is 127 the value will be assigned to zero otherwise if the value of the pixel is lesser than threshold the value of the pixel will remain the same so this is how some of the simple thresholding techniques works in opencv in the last video we have seen how we can perform simple thresholding in opencv using python using various thresholding techniques so we have used trash binary threshold binary inverse thresh trunk thresh to zero trash to zero inverse so these were all the simple thresholding techniques now in these thresholding techniques we were setting a global value of our threshold so in this example for example here the global value of threshold is 50 here 200 here 127. so we were setting in simple thresholding the global value and this means that it is same for all the pixels in the image now in this video we are going to learn how to use adoptive thresholding so adoptive thresholding is the method where the threshold value is calculated for the smaller region so the threshold is not global for every pixel but it's calculated for a smaller region and therefore there will be different threshold value for different regions now why do we need this type of adoptive thresholding so using simple thresholding might not be a good idea in all the conditions so there might be conditions where the image has different lighting conditions in different regions and in those cases where the lighting conditions in the images varies from point to point in those cases we might want to use adoptive thresholding so as i said adoptive thresholding calculates the threshold for a smaller region of images so we get different thresholding values for different regions of the same image and as a result adoptive thresholding gives us better results for images with varying illumination so let me show you the problem with simple thresholding for the image which have different illumination at different regions so i have this image called sudoku dot png which i'm loading using i'm read method and then i'm just showing this image using i'm show method and then let's use the simple thresholding technique which is trash binary for this and we have set the global threshold value of 127 here and then we will see the result after this threshold is applied to the image so i'm going to run this program and let's see what happens so this is our original image and then this is the result which we got so on in the result you can see when we apply a same global threshold value some of the region of this image is black and other region of this image is visible right so because the image have different illumination value at different regions that's why we see half of the image which have the good illumination and we don't see half of the image which doesn't have the better illumination so in that case it's a better idea to use adoptive thresholding so let's see how we can use adoptive thresholding so here what i'm going to do is i'm going to declare a variable called th2 and then we use cv dot adoptive threshold so this is the method which we are going to use for performing adopting thresholding and this takes few arguments so first is the source so our source is the image variable now the second parameter here is the max value so the max value is the nonzero value assigned to the pixels for which the condition is satisfied so in our case the maximum value which we can provide to a pixel is 255 and we cannot go more than that right now the third parameter here is the adoptive method so this adoptive method is going to decide how the thresholding value is calculated and there are two types of adoptive methods which we can use so the first method is called cb2 dot adoptive thrash mean c so what is the meaning of this adoptive thresh mean underscore c so using this method the threshold value is the mean of the neighborhood area and here is the documentation of these two methods so adoptive thresh means c gives us the threshold value using this function and this is going to give us the mean of the block size multiplied by block size neighborhood of x comma y minus c which is the constant and the second adoptive threshold type is this one which is adoptive trash gaussian underscore c and in this adoptive thresholding the threshold value is the weighted sum of neighborhood values where weights are the gaussian window so let's use the first adoptive method which is the adoptive thresh mean underscore c now the next parameter here is the threshold type so the threshold type which we are going to use is the thresh binary which we have also seen in the last video also and then the next value is the block size so block size decides the size of the neighborhood area so here we are going to give the block size 11 and the next parameter here is the value of c so we have seen that we need to uh give the value of c also when we use the adoptive thresh mean c and adopt a thresh gaussian c so this is the value of c which we are going to give and c is just a constant which is subtracted from the mean in the case of uh this adopted thrash mean method or the weighted mean in the case of gaussian adaptive threshold okay so constant we are going to give here is two and now what we are going to do is we are going to just load this image which we got after applying this adaptive thresholding and lets uh just comment the other window so we will just see the original image and the adoptive thresholding result so i'm going to run this code and you can see the original image here and the result of adoptive thresholding which looks much better than the simple thresholding technique so let's uncomment uh this also so i'm going to uncomment this so we will see all the three result at the same time so this is the original image and you can see the simple thresholding gives us this value using the global threshold of 127 and adoptive thresholding gives us this value or this image which is much more readable than the simple thresholding technique image so this is how you can use adoptive thrash mean c method in a same way we are going to use the other adoptive thresholding technique which is called adoptive thresh gaussian c so instead of this we are going to use adoptive thresh gaussian c and then all the parameters we are going to leave as same and let's load the result of this type of thresholding which is stored in three so let's run this code and let's see what happens so we have already seen this image which is the simple thresholding this is the result of the adoptive thresholding mean c and this is the result of adoptive thresholding gaussian underscore c so both of the result looks good because the adoptive thresholding algorithm calculates the thresholding value for different regions so the thresholding value is not global for each and every pixel of the image and we have seen the two adaptive methods which are available in adoptive thresholding so in this way you can use adoptive thresholding in opencv in this video we will talk about a library called matplotlib which you can use with opencv images so first of all what is matplotlib so matplotlib is a plotting library for python which gives you a wide variety of plotting methods and on the official website which is matplotlib.org you can see match plotlib is a python 2d plotting library which produces publication quality figures so it's primarily a 2d plotting library but it's widely used with opencv to display graphs and images and histograms so we will see how we can use matplotlib with opencv it's also written here that for simple plotting the pi plot module provides a matlablike interface so first of all let's see how we can install matplotlib and then we are going to see how to use matplotlib with opencv so to install matplotlib using pip you just need to open your terminal and then just give this command which is pip install matte plot lib and then press enter and in some seconds this matplotlib library will be installed using pip so now you can see matplotlib is installed on my windows operating system and to check it i'm going to just give the python command and here i'm going to import matplotlib so i'm going to just write from mat plot lib import pi plot as plt okay and then press enter and if this import doesn't give you any error that means it's imported successfully and you can start using matplotlib now as we are using pycharm ide let me show you how you can install matplotlib on pycharm so just open your pycharm ide and then here just click on file and then settings and then go to project colon your project name my project name is opencv examples and then click on interpreter and you can see other packages are already there and we just need to install the matplot lib package so just type here in the search matplotlib and you will be able to find matplotlib here in the result so just uh click on matplotlib and then just click on the install package so i'm going to just click on the install package and in some seconds matplotlib library will be installed in your pycharm ide so you can see this message which says package matplotlib installed successfully that means we can close this window and then you will be able to see matplotlib is available in your project interpreter so everything is fine and i'm going to just close this and now i will be able to import matplotlib so i'm going to just write from mat plot lib import pi plot as plt now in order to show the image which you read using the opencv i'm read method you can use this code so just write plt dot i am show so there is also a method inside your pi plot library which is available inside matplotlib and this mess third you can use to show the image which you have read from the opencv imread method so for now just write this kind of code and to show the matplotlib window you just need to write plt dot show so this is going to show this image using the matplotlib library so we are opening this image using the opencv i'm sure window as well as matplotlib window also so let's run this code and let's see what's the result which we are getting so you can see this is the image which is loaded using the matplotlib and this was our original image which is loaded using the opencv library and straight away you can see some difference so this is the original image which is the colored image and in the matplotlib window we also want the same result but it's giving us the different result and the reason behind this is opencv reads the image in the bgr format and the matplotlib reads the image in the rbg format so in order to show this kind of colored window using matplotlib you need to convert your image from bgr to rbg and then only you will be able to see this kind of colored image using matte plotlib so i'm going to just close these windows and now after i'm showing this image using the cb2 i'm show method i'm going to convert this image so i'm going to just write img is equal to cv 2 dot cv t color and then i am going to convert this image from bgr image so i am going to just write cv 2 dot color underscore bgr to rgb okay so our matplotlib library shows the image in the rgb format and the opencv reads the image in bgr format so now we have converted our image from bgr to rgb image and now we are showing this image using the matplotlib and let's run this code and let's see what happens now so now when we run this code you see both the image looks the same right now let's see that advantages of using matplotlib so you can see this is a quite static window but when you see in matplotlib when you hover over this image you can see x and y coordinates of the mouse point and this is helpful you can also save this image in the form of a png file so you can just press this and save this image wherever you want you can also zoom this image if this feature is available there is also configuration subplots options so you can you can just increase these values left bottom wherever you want to place your image you can do that these are some options which are available here you can also reset these options and you can see the coordinates here so because matplotlib is primarily a 2d plotting library so you can see the x coordinates and y coordinates and because this image is about 512 by 512 pixels that's why here it's showing 0 to 512 and here also on the yaxis 0512 so this is how you can load your image using matplotlib and now i'm going to show you one more thing and this is when you write plt dot x ticks here and then when you pass empty array here which is empty square bracket comma pl t dot y ticks and also here you pass mt array this is going to hide the tick value on x and y axis so now when i run this code and you can see now that these x sticks and y takes on x and y axis are gone so let me just comment this out once again and you will be able to see this x and y coordinates here on the image and when you use this code which is to hide the ticks on the x and y axis then you will see the image without these x and y axis ticks so if you remember in the last video we have seen how to use simple thresholding in opencv and we were using six windows to show these six different images using opencv now let's say you want to show all these six uh windows in one matplot lib window how you can do it with the use of matplotlib i'm going to show you so first of all we are going to import matplotlib import pie plot as plt and then what we are going to do is we are going to define the titles and then we are going to define these six different uh titles for six different images so first one is our original image second was the trash binary third was thresh binary inverse fourth was trunk fifth was two zero and six was to zero inverse in the same way we are going to define a variable called images and inside this square bracket we are going to pass first of all our original image and then th one comma th2 comma th3 comma th4 comma th5 okay so these are the six value we want to show and these are the six titles of these images and now we are going to use the for loop so for i in x range so using the python x range we are going to just iterate over these six values so i'm going to just write x range and then the range we are going to provide here is six and then inside this for loop we are going to just call plt and we are going to call a method called subplot okay and this subplot method takes few arguments so first argument is the number of rows which we want to show in our matplotlib plot so because we have six images so we are going to divide these images into two rows and three columns so the first argument here is the number of rows and the second argument here is the number of uh columns and the third argument here will be the index of the image so the index of the image will be i plus 1 and then we are going to write comma plt dot i am show so this is going to show this image and the index of the image so we are going to just write images and then square bracket i so this is going to give you a particular image at index i and then we want to show this image as a gray scale image so anyway when you use thresholding you use the grayscale image so you just need to write a gray here then we are going to show the titles of these images so we are going to just write plt dot title and then this title method takes the title name which we are getting using this titles array and then at the index i this is going to give you uh the title name which we have declared in this title array and at last if you don't want to show the text on the images you can give these two method which is plt dot x and the argument here is the empty list and also plt dot vitix and the argument is the empty list and at the end what we want to do is instead of using this kind of code we just want to show our window so we can just say plt dot show and this is showing us this error unresolved reference yes so this is when you are using uh python 2 but in python 3 this x range is changed to a method called range and that's why it was giving us the error so let's run this script once again and you can see six different results and six different titles so these are all the titles which are shown here and then the result are shown under these titles so using matplotlib you can include multiple images into one window and this is very useful when you want to show multiple image at the same time in the same window so this is how you can use matplotlib library with opencv images and there is a lot of things which you can do with matplotlib so if you want to learn more you can just go to the official website which is matplotlib.org and then you will be able to see more documentation here in this video we are going to discuss about morphological transformations in opencv so we will discuss different morphological operations like erosion dilation opening and closing methods etc but first of all what are morphological transformations so morphological transformations are some simple operations based on the image shape now morphological transformation is normally performed on a binary image and when we perform morphological transformation there are two things which are required first is the original image and second is called a structuring element or a kernel which decides the nature of the operation now there are different type of mark for logical transformations and we are going to see them one by one now to start with i have this simple code which reads the image using opencv imread method and we are just loading or showing this image using matplotlib now if you are unfamiliar with matplotlib and how to use matplotlib to show images in the last video i have explained this topic in details so if you want to see that video about matplotlib you can see it and this is the code i have used in the last video also and i have explained this code in details in the last video so if you are confused what this code is doing just see the last video now there is one important thing to notice here is i am reading this image in a gray scale mode okay so either you can provide here as the second argument of i am read cv2 dot i am read underscore grayscale or you can provide simply 0 here in order to read this image in grayscale so let's run this code and let's see what it does so as expected it's just opening the image in the grayscale mode using matplotlib now as i said normally we perform the morphological transformations on the binary images so that's why we need to provide a mask to our image using the simple thresholding so let's just do that so i'm going to just write underscore comma the mask so i'm going to name my variable as mask here and then i'm going to just write cb2 dot threshold and this threshold take few argument as you might already guess first is the image itself second argument is the value of the threshold so for now i'm going to just provide the threshold of 220 here the maximum value of threshold will be 255 then the next argument here is the type of the threshold so we are going to provide cv 2 dot thrash binary inverse so this is our mask so let's load the mask in the matplotlib window so i'm going to just provide in this titles array one more title which is mask and then we are going to see how this image looks like after the mask okay and here the range i'm going to increase it to 2 because now the array is of two elements and the subplot is also let's say one by two so we want to show two images and i'm going to just run this code and you can see this was the image which was the grayscale image and the second image is the masked image now if you see this image carefully let me just just increase the size of this image and if you see this image carefully after masking there are some black dots here on the balls and let's say we want to remove these dots which are there in between this white area this black dot or this black dot or you can see some black dots are there inside your ball in the bite area and we want to remove these dots from the balls for this we are going to use the dilation transformation so first of all what we are going to do is we are going to just write dilation which will be our variable name and then we are going to use this method called cv2 dot dilate okay so this method uses the source which is mask in our case and then the second thing is the kernel okay so let me explain what the kernel is so a kernel is normally a square or some shape which we want to apply on the image so we are going to define a kernel of numpy once which means we want to apply white square on our balls so you can see when we run our code once again it shows us error because this kernel is undefined so let me define this kernel first of all so i'm going to just say kernel is equal to np dot once and then we are going to define the shape of this kernel let's say this is of 2 comma 2 size and then we will just say np dot u int eight so this is our kernel and kernel in this case is nothing but a two by two square shape and this square shape kernel is going to be applied on our image wherever these black dots are there so now we have defined this kernel so let's see after this kernel is applied on our masked image how it looks like so i'm going to just add one more title here which is dilation and then i'm going to add the image after the dilation is applied on our image and then we are just going to increase the range to three because now we have three images and let's say this plot contains images one by three so one row and three columns right so i'm going to just run this code once again and now you can see all these three images first was the original image second is the masked image and the third one is the image which we got after we applied the dilation let me just increase the size of this image somehow so now you can see that for example here there was a black dot and now it's reduced right the size of this black dot is reduced here also there was a black dot but its size also is reduced but still we can see these black dots here right so how we can remove these black dots completely so there is a third parameter which we can provide to this dilate method and it's called iteration so number of iterations so we can just provide iterations is equal to whatever the number of times we want to perform dilation on our image by default it's 1 and you can provide let's say 2 here and let's see what is the result now so i'm going to just run this code again and now you can see those black dots which we can see here on the masked image are now gradually gone but still i can see some little dots on the images the small dots are already gone right so now what we can do here is we can increase the size of the rectangle so this rectangle is applied to our area which have these spots so we can increase the size of the rectangle and the bigger the rectangle is the better the result will be but there will be a problem which i'm going to show you so let's run this code and you can see now all the black dots from our image is gone so there was a black dot here which you don't see anymore and there was a black dot here here here and here and we don't see these black dots here but you might also observe that the size of this white area is also increased after we applied the dilation on this masked image so now this ball and this ball in the result after the dilation is merging here right so you can see it's merging because the size of our kernel is a big and when we apply dilation the pixel element is one if at least one pixel under the kernel is one that's why the shape of these balls are increasing so let's see how our next morphological transformation works which is called erosion and after that i'm going to explain you how this erosion works and what is erosion so i'm going to just declare a variable called erosion and then i'm going to just call a method called cv2 dot erode so the method name is a road and the first argument here is the source the second argument here is the kernel as we have seen in dilate method and the third argument is the optional argument which is the iterations so for now we just apply one iteration which is by default also one and now we are going to just add this image to our matplotlib window so i'm going to add the title and the image and now i will increase the range of the array to 4 and let's say now we want 2 by 2 metrics of these images right so let's run this code and let's see what happens so now you can see four results here and first was the original image second was the masked image third was the dilation so all the spots in the balls which are black are gone using the dilation but the size was increased and using the erosion you can see the sides of the ball eroded so the basic idea of erosion is just like soil erosion it erodes away the boundary of the foreground object so when this erosion is applied the kernel which we have defined slides through all the image and a pixel in the original image either one or zero will be considered as one only if all the pixels under the kernel is one otherwise it is eroded and this means this value will be set to zero which means this will be a black area so let's increase the number of iterations here so let's say we want to apply the erosion two times on the same image and i'm going to just run this code once again and you can see now these balls are eroded more let's say we want to increase this to five times and then run the code and you can see now these balls are really small because we have applied this erosion multiple number of times so let's say this is one once again and let's make this size of our kernel small 2 by 2 rectangle size right so you can see now our result is better because all the spots from these balls are gone and these balls are not so much eroded now there are two more morphological transformation methods which are called opening and closing so we are going to first of all see how opening works i'm going to define a variable called opening and then i will call cb2 dot morphology x okay and then we will provide the source which is mask the second method is the type of morphological operation which we want to perform so in this we are going to just call cv2 dot and then we can specify which type of morphological operation we want to perform on the image so just write morph and then the type of operation so we want to perform the morph open for the opening right and then the third argument here is the kernel which we have defined and now we are going to just add this opening to our matplotlib window let's add this and then let's do five here and then let's say our matplotlib is going to show these images in a two by three format okay so let's run this code and let's see what happens and let me increase the size of this image now and this is the result of the opening so what is opening in morphological transformations so opening is just another name of erosion followed by dilation so when you perform this opening of a logical operation first of all erosion is performed on the image and then the dilation will be performed on the image so you can see the effect of the erosion followed by the dilation still you see some spots here which can go if you can just increase the size of this block so let's rerun the code let's see what happens so now this image somehow looks better than the older image so opening is the erosion followed by dilation now there is a closing method also which is just the opposite of opening in the closing morphological transformation dilation is performed first on the image and then it is followed by the erosion so let's see if we get the better result when we perform the closing morphological operations and the morphological operation here will be close and run this code and now you can see the result here so in closing as i said first of all the dilation is applied and then the erosion is applied in the opening first of all erosion is applied and then the dilation will be applied now there are different type of morphological operations you can apply using this morphology x so for example i'm going to just use some of them so the main morphological operations other than opening and closing is let's say morphological gradient so i'm going to just say m g for morphological gradient and you just need to change the second argument here so cv2 morph underscore morphological gradient so we are going to just call this morph gradient and it's going to apply the morphological gradient and then the next is the top hat and the black hat so there are different uh morphological techniques you can apply so i'm going to show you one more and then i will leave you with the other techniques so t h for top hat and here also the second argument you just need to change it to top hat right otherwise you can see there are so many number of techniques you can apply on your image so there is gradient close open we have already seen black hat cross dilate uh ellipse erode hit miss wrecked and then top hat which is uh we are going to use right now right and then we can just add these two things to our list of titles and list of images so mg and then we have th for top hat and now we have eight images so range is increased to eight and let's say we just want to show them in two by four uh matrix here in the matplotlib window so you can see this is the result of morphological gradient so morphological gradient is the difference between the dilation and erosion of an image and this is the result of top hat that means it is the difference between the image and the opening of an image so this is how you can perform some of the morphological operations on the images now i will show you one more example i have a image called j dot png so i'm going to just load this image also and because this j dot png is already a binary image i don't need to apply this mask here so instead of this mask i can just directly use our image variable so i'm going to just write this and let's load this image two times because we already have defined this mask variable inside our title list and image list and now i'm going to just run this code so the original image of this j dot png looks like this and after we applied the dilation you can see the dilation increases the area of this j the erosion just erodes away the corners of this j right opening is going to apply the erosion first followed by the dilation and closing is going to first of all perform the dilation followed by the erosion this morphological gradient is going to give you the difference between the dilation and erosion of the image so it's going to give you this kind of result and you can see the top hat result here which is the difference between the input image and the opening of the image so this is how you can use different type of morphological transformations on your images in this video we will discuss about smoothing or blurring images in opencv so smoothing which is also known as blurring is one of the most commonly used operation in image processing the most common use of smoothing operation is to remove noise in the images now when smoothing or blurring images we can use diverse linear filters because linear filters are easy to achieve and are also relatively fast now there are various kinds of filters available in opencv for example homogeneous gaussian median or bilateral filters which we will see one by one so first of all we will see the homogeneous filter so homogeneous filter is the most simple filter and in homogeneous filter each output pixel is the mean of its kernel neighbors now in homogeneous filter all pixels contribute with the equal weight and that's why they are called homogeneous filters now those of you who don't know what the kernel is i have explained about kernel in the last video so you can see the last video and in simple word a kernel is a shape which we can apply or convolve over an image and you can use for example numpy to create this kind of squared kernel so in homogeneous filter the kernel looks like this image which you see on your screen so in homogeneous filter kernel k is equal to 1 by the width of the kernel multiplied by the height of the kernel so let's say we want to use a kernel of 5 by 5 then using this formula we will have k is equal to 1 by 25 and then we will have our kernel matrix of 5 by 5 ones so let's create this kernel first of all and then we will see how to use this kernel for the image filtering using 2d convolution or homogeneous filter so what i have right now here is the simple code which loads this image using matplotlib and this code you might already know because i have explained in detail how matplotlib works and how to read the images using opencv one thing to note here is i'm just converting the image from bgr to rgb because matplotlib reads the images in the rgb format and opencv reads the images in the bgr format so this conversion is necessary so let's define our kernel so i'm going to just say kernel is equal to np dot once and then we are going to take the kernel of five by five so we are going to define this kernel five comma five off once so i am going to just say np dot float 32 here and then we are going to divide this kernel by 25 because our kernel is of 5 by 5 because the formula which we have seen in that formula we have the kernel which was a matrix of ones and then we have the multiplication of one divided by the width and height of the kernel so that's why the multiplication of the width and height is 25 that's why i have taken 25 here so now we have our kernel so we can define our destination image using this kernel and we are going to use cv2 dot there is a method called filter 2d which we are going to use which is used for this homogeneous filter so here the first argument is the image the second argument is the desired depth of the destination image so for now we are going to take it as a minus one the third argument is the kernel so now when we have applied this kernel on our image using 2d filter let's see what the output will look like so i will name this image as 2d convolution and the destination is the final image which we got using filter 2d and let's increase this range by 2 and let's say we want to show this image on matplotlib in 1 by 2 format okay so i'm going to just run this image so this is the result on the left hand side is the original image and on the right hand side is the 2d filter applied image so this is the image which we got by applying the homogeneous filter using filter 2d function so you can see on the corners here there is a little bit noise and after applying this 2d convolution over this image you can see all the corners are now smoothened and overall this image is now smooth and or blurred a little bit so these noises are removed or suppressed by this blurring so this is one way of blurring an image using filter 2d right filter 2d function now as in one dimensional signals images also can be filtered with various low pass filters or high pass filters etc so low pass filter helps in removing the noise or blurring the image etc and high pass filters helps in finding edges in the images now when you want to achieve image blurring we need to convolve over the image with the low pass filter kernel now there are some algorithm as i said there are various kind of algorithm available in opencv so we will also see them one by one so first algorithm is the blur method or it's also called the averaging so what i'm going to do is i'm going to define a variable called blur and then i'm going to call a method called cv2 dot blur okay so this is the method which we will use to apply averaging algorithm for blurring the image and this takes two arguments one is the image and second is the kernel so the kernel we are going to apply is once again five by five and now we are going to just see with the result of this blurring method so we are going to just loaded using the matplotlib so range i'm going to increase it by one once again and let's see this these three images in one by three format on the matplotlib window so this is the result and you can see the original image the result which we got using the filter2d method and the result we got using the blur method which is also called averaging so the result is more or less looks the same to me because we have applied the same kind of kernel to both the functions so this is the result of filter2d function and this is the result of the blur function now there are more functions which are available in opencv so let's see uh them so the next algorithm which we are going to see is the gaussian filter algorithm so the gaussian filter is nothing but using different weight kernel in both x and y direction so in the result pixels located in the middle of the kernel have the higher weight or bigger weight and the weights decreases with distance from the neighborhood center so pixels located on the side have smaller weight and the pixels located on the center have the higher weight so when we take a five by five kernel its result is going to look like this which is shown in the image and now let's see how we can use this gaussian blur in our opencv code so i'm going to remove this semicolon which i somehow added here and let's uh declare a variable called g blur for gaussian blur and then we are going to use cv2 dot gaussian blur so the method name is gaussian blur and the argument here are same as the blur method so first argument is the image itself second argument is our kernel we are going to take the same kernel of five by five and the third argument here is the sigma x value which we are going to take 0 for now let's see the result of the gaussian blur method when it's applied to an image so i'm going to just define one more title which is g blur or gaussian blur or let's take this name which will be more clear and then our result image is g blur and let's increase the range to four and let's say we want to show this image in two by two format so two rows and two columns so i'm going to run this code and for opencv logo the results looks the same you can see for the 2d convolution of filter2d method or blur method using the gaussian blur you can see there is a little bit different between the blur method and gaussian blur method uh results the gaussian blur result is more better in my eyes than the blur method let's try this gaussian blur method with another image so i have this image called half tone underscore gaussian underscore blur and i'm going to run this code now with the new image and you can see the result now so this was the original image which have too much noise here so you can see the pixels here which have too much noise and after applying the gaussian blur you can see this eye image in a much better way and all the noise is removed so the gaussian blur method is designed specifically for removing the high frequency noise from the image like this one now let's see the next method which is called the median filter so median filter is something that replaces each pixel value with the median of its neighboring pixel so this method is great when dealing with something which is called salt and pepper noise now if you don't know what the salt and pepper noise is you can open the wikipedia and under this url or just search for salt and pepper noise wikipedia page and you can see more details about salt and pepper noise so you can see this is an image and there are some pixels which are distorted here so there are some pixels where the white uh dots are there or white noise is there and there are some places where the black noise is there so that's why it's called salt and pepper because we have white pixels which are distorted like salt and we have the black pixels which are which looks like pepper so that's why it's called salt and pepper noise so i have this same image which i'm going to use as a source now so it's called water.png in my case and now let's see how we can use the median blur method so i'm going to just define a new variable called median and then i'm going to use cv2 dot median blur method so this method is called median blur where the first argument is the image and the second argument here is the kernel size now one thing to note here is that the kernel size must be odd here so this must be a three or five or seven and so on except one okay so when you uh just give one it's going to show you the original image and let's say we just give three here as the kernel size or in our case we have the kernel size of five so let's take the five kernel size here so let's just show this result of the median filter in the plot layer window so i'm going to just increase the range 5 and let's say this is 2 by 3 matrix now i'm going to run this code and now you can see the results of all the filtering method and you can see the best results you get using the median filter method so when you have this kind of salt and pepper dots on your images then you can use the median filter now let's see the last filter which is called the bilateral filter so by using all these filters for example homogeneous filter or averaging or the gaussian or the median filter we not only dissolve the noise but we also smooth the edges and sometimes we need to preserve the edges that means we need that all the ages must remain sharper even if the image is blurred so let me show you one example so i have this uh lena dot png image so i'm going to define a variable called bilateral filter and then there is a method called cv2 dot bilateral filter and this bilateral filter takes the first argument which is the image the second argument is the diameter of each pixel neighborhood that is used during the filter so let's take it as nine the third argument is the sigma color and the fourth argument is the sigma space so the sigma color is the filter sigma in the color space and sigma space is the filter sigma in the coordinate space so for this we are going to take this filter sigma color and sigma space as 75 and 75 here and let's see it in the result window so bilateral filter and then the result bilateral filter and this gives me error because this image is called lena.jpg not png so jpg and then we need to increase this range by one to see all the six images and let's run this code and let's see what happens so you can see the result now so let me make it a little bit bigger so you can see them and from here also so now you can see by applying the bilateral filter the edges are preserved in a much better way so here you can see the hat border is blurred but here you can see in the result the border of the head are preserved so the images in which you need to preserve the borders then you can use the bilateral filter so bilateral filter is highly effective in noise removal while keeping the edge sharp so these are some of the methods and algorithms you can use to smoothen or blur your images using opencv in this video we will talk about image gradients in opencv so first of all what is an image gradient so an image gradient is a directional change in the intensity or the color inside the image now the image gradient of an image is one of the fundamental building blocks in image processing for example we use image gradients inside the image to find the edges inside an image now there are several image gradient methods available in opencv and we are going to see three of them first is the laplacian derivatives second is the sobel x method and third one will be the sobel y methods and all these methods which i mentioned are different gradient functions which uses different mathematical operations to produce the required image so the laplacian calculates the laplacian derivatives whereas sobel method is joint gaussian and differentiation operations but don't be overwhelmed with the details you just need to keep in mind that these are just the functions which we use for finding out the gradients of an image to analyze the image so let's use the first method which is called the laplacian gradient now to start with i have this initial code and you might already know what this code is doing so first of all i'm just reading this image messy5 dot jpg in the grayscale mode using the i'm read method and then i'm just loading this image using the matplotlib window so let's first see how the result looks like so this is going to look like this this is just a normal image of messy and let's see how we can apply the laplacian method to find out the laplacian gradient of an image so for that we are going to declare a variable called lap and then there is a function available inside your cv2 library which is called laplacian and this laplacian method takes few argument first argument is the image the second argument here will be the data type which we are going to use which is called cv2 dot cv underscore 64. f so cv2 dot cv underscore 64 f is just a data type and we are using a 64 bit float due to the negative slope induced by transforming the image from white to black so you just need to keep in mind that this is just a data type which is 64 bit float and it supports the negative numbers which we will be dealing with when the laplacian method is run on our image now in the next line what we are going to do is we are going to take the absolute value of our laplacian image transformation and we are going to convert this value back to the unsigned 8bit integer which is suitable for our output so i'm going to just write lab and then using the numpy uint method so np dot u int 8 and as an argument we are going to pass np dot absolute and then inside the absolute method we are going to just pass our image which is going to give us the absolute value of our laplacian image transformation which is going to convert this into the unsigned 8bit integer now let's see the result of this laplacian gradient so i'm going to just add a new title to my title array which is called laplacian and also inside the images list i'm going to add this lap variable which contains this image right after the laplacian gradient is applied here and here the range will be 2 and we are going to see it in 1 by 2 format on the matplot lib window so here you can see the original image which is this one and after the laplacian gradient method is applied on this image you can see all the edges which are detected by this method when we applied this method on this messy5 dot jpg image and an image gradient as i said is the directional change in the intensity or the color in an image so let's close this window and there is one more argument you can provide here which is the kernel size so you can just say k size is equal to 5 this is the kernel size and i'm going to just run this program once again and you can see the kernel size is increased but our result is deteriorated right so let's reduce it to 3 and then once again run this program and the result looks fine and if you apply k size is equal to 1 let's see the result and you can see you get the better result i think so for now i'm going to just use k size is equal to 3 and now let's use the other two uh image gradient methods which are sobel x and sobel y so these methods which uh are called sobel x and sobel y are also called sobel gradient representation so let's just use them and then we will discuss how they are useful so first of all i'm going to declare a variable called sobel x and then i'm going to use the method inside this cv2 library which is called sobel so this is the method which takes again few arguments first is the image second is again this data type which is cv2 dot cv underscore s64 and the third argument here will be the dx so when you write one here this value can be one or zero so when you write one here that means we want to use the sobel x method okay and then the fourth argument here is the d y value okay so this is dx which is for the x direction and this is for the d y which is for the y direction and dx stands for the order of derivative x and the d y stands for order of derivative y now once again we are going to declare the sobel y variable so let's declare the sub y and then cv2 dot sobel and this also takes a few arguments here the difference will be only the third and fourth argument so i'm going to just use the second argument same the third argument will be zero for sobel y and the fourth argument will be one right so this is the order of derivative x if it's 1 this is called the order of derivative which is in the x direction and in the second case it is in the y direction and the fifth argument here can be the k size as we have seen in the laplacian method so if you want you can provide the kernel size also here as the fifth argument but we are going to skip it for now now again we are going to convert these values into the unsigned int as we have done in the case of laplacian so what we are going to do is we are going to once again overwrite this variable sobel x and then we are going to use np dot u int 8 and in the parenthesis we are going to just write np dot absolute and then we are going to just pass the value inside the sobel x variable same we are going to do with the sobel y variable and now let's see the result how the result looks like so i'm going to just uh add these elements inside the title and the image list so let's add sobel x and sub y here and here also so sobel y and now let's increase the range to 4 and let's see it in the form of 2 by 2 matrix on the matplotlib window so i'm going to just run this code and you can see the result here so original image laplacian uh gradient and then sobel x and so blue y so you can see uh when you apply the sobel x gradient method the direction or the change in direction in the intensity is in the x direction and when you apply the sobel y method the change in direction in the intensity is in the y direction so this is like horizontal and this is in the vertical direction i have one more image which will illustrate this sobel x and so y gradient method in a better way i think and this is called sudoku so i'm going to just write sudoku dot png file file and hopefully i didn't do any mistake in the naming yes it works so you can see the laplacian result here and then sobel x and sub y uh result here so in the sobel x you can see more vertical lines so because sobel y is good for the directional change in the vertical direction so you can see more uh change in intensity in the vertical direction and using the subal y you can see the directional change in the intensity in the horizontal direction or the y axis you can also combine the result of sobel x and sub y images and how we can do this let's see so to combine these two result i'm going to just create one more method which is called sobel combined is equal to cv2 dot we are going to use the bitwise or operator in order to merge these two images so we are going to just write bitwise or and then we are going to provide the two sources one is sobel x and the other is the subal y image so this is going to give us the bit wise or result of these two images and then we are going to just add this into the title list so let's say sobel combined and also in the image list so like this and let's just increase the range to five and let's see it in the form of two by 3 on matplotlib so i'm going to just run this once again and you can see the result now so this here is the combination of sobel x and sobel y method and you can see now you can see the directional change in the vertical as well as in the horizontal direction because this is the combination of sobel y and sobel x images so this is how you can use the image gradients inside opencv in this video we will talk about kenny edge detector in opencv so first of all what is kenny edge detector so the kenny edge detector is an edge detection operator that uses multistage algorithm to detect a wide range of edges in images now this kenny edge detector was written and developed by john f kenney in 1986 that's why it's named after his name which is kenny edge detector now the process of kenny edge detection algorithm can be broken down in five different steps the first step is to apply gaussian filter to smooth the image in order to remove the noise the second step is to find the intensity gradients of the image the third step is to apply the nonmaximum suppression to get rid of spurious response to edge detection the fourth step is to apply double threshold to determine the potential edges and the fifth step is to track edges by hysteresis that is to finalize the detection of the edges by suppressing all the other edges that are weak or not connected to strong edges so this seems a little bit complicated but in opencv it's really simple to use so there is a builtin function in opencv which is called kenny and we are going to use this function so to start with i have this sample code which loads this image which is called messy.jpg using the matplotlib library i'm going to just run this to show you the results so this is the image and we want to detect the edges of this image so what we are going to do is we are going to first of all declare a variable called kenny and then there is a method as i already said inside your cv2 library which is called kenny method which takes few arguments so the first argument here is the image source itself the second argument and the third argument as you can see is the first threshold value and the second threshold value so this first threshold value and the second threshold value you need to provide for the hysteresis procedure so there is the last step as i mentioned and in that step hysteresis take place and for that procedure we need to provide the values of the threshold one and the threshold 2 so for now i'm going to provide 100 as the threshold 1 and 200 as the threshold 2 but later you might want to add a track bar in order to see the changes in the edges when you just move the track bar from left to right for the threshold one and the threshold two so this might be a small assignment for you you can just add the track bar and see how the edge detection changes when you change the value of threshold 1 and threshold 2 and i have already explained how you can use track bars with opencv so just watch that video and you will be good to go so now we have the result of kenny edge detection function so we are going to just add it to our list first to the list of titles and then second to the list of images and the range we are going to increase it to uh two and this we are going to just see the images in one by two format so i'm going to just run this python script and see the result so you can see we have the original image here which we have loaded in the grayscale and on the right hand side you can see the result of the kenny edge detection methods so you can see uh all the edges which are available here on this messy5 dot jpg image you can use uh this on the other images also so for example i have the lena dot jpg image let's see the result of that and this is the result of the kenny edge detection method on this lena dot jpg method so this kenny edge detection is really useful because in the last video we have seen how to find out the image gradients and let's see in comparison to those image gradient methods how kenny edge detection method performs so these are all the methods i have explained in the last video laplacian sobel x and sobel y and i have shown you how to combine the result of sobel x and sobel y and additionally i have added this line to the previous code which i have shown you in the last video which is edges is equal to cb2 dot kenny which gives us the result on the same image using the kenny edge detection method and i have added it to the title and the image right so let's run this uh script once again and let's see the differences in the result using all these methods so you can see all the six results this is the original image this is the result of the laplacian method this is the result of sobel x and this is a result of symbol y and this is the combination of sobel x and y and you can see kenny edge detection gives us the result which contains lesser noises so you can see there is a lot of noise present in the other matters you can see here all the noise is present which is removed using kenny edge detection or in the laplacian method also you can see some noises around but in the kenny edge detection method you can see you get the proper edges and more precise edges without any noise so this is the benefit of using kenny edge detection so this is how you can use kenny edge detection in this video we will discuss about image pyramids in opencv so till now normally when we have used images we have used the images of constant size but sometimes we need to work with the images of different resolution so for example if i have an image and i want to search the face inside an image this face can be of different sizes so using image pyramids we just create the images of different resolutions and then we search for the object for example face in all of these images so pyramid or pyramid representation is a type of multiscale signal representation in which a signal or an image is subject to repeated smoothing and sub sampling so a normal pyramid when you create a pyramid of images it will look like this so let's say this is the original image at the bottom then when you down scale an image using a pyramid function it's going to give you this image which have the half resolution than the original image and then when you further go up it's going to give you the one fourth of the original image and then so on so 1 8 or 1 16 of an image now there are two types of image pyramids which are available in opencv first is called gaussian pyramid and second is called laplacian pyramid so first we will discuss about the gaussian pyramid so gaussian pyramid is nothing but repeat filtering and sub sampling of an image now there are two functions available for the gaussian pyramid which is called pair down and pair up so let's uh see them one by one so i have this sample code which is just reading an image and then showing it using the i am show method now in order to uh use this pair down function you can just define a variable let's say l r for lower resolution and then you can use cv2 dot pair down so there are two functions you can see pair down and pair up so first of all we will see pair down and then we are going to pass our image as an argument here so i'm going to just pass our image as an argument and we are already showing the original image and let's show the image after we have reduced the resolution of this image using the pair down method so pair down is going to reduce the resolution of an image so i'm going to just use lr here and let's say this is the pair down one image okay so let's run this code and let's see what happens so you can see this is the original image and this is about you can see one fourth of this original image right so this pair down method is going to reduce the resolution of an image when you apply the same method on the second image so let's say this is lr one and then we create a second variable error lr2 and when we pass lr1 as an argument for this method to create the lr2 method 2 image then let's see what happens so this will be lr1 and let's just say this is going to give us lr2 the resolution of image will reduce further so let's see what happens so this was the original image this was uh the image which we got from the first period down method and then we get this image which we which is further reduced in resolution so this is the image after applying the pair down method second time on the lr1 image okay so you can see the resolution of image is reducing and it's creating a kind of pyramid and that's why it's called the image pyramid now there is a method called pair up also available in opencv so let's see what this pair up method do so as you can expect it's going to increase the resolution of the image so here i'm going to just say h r for higher resolution and then i am going to just say cv2 dot pair up okay and it is going to increase the resolution of an image now let us say we want to increase the resolution of an of this image which is the smallest image which we got using the pair down method right so we are going to apply the pair up on the last image which we got using the pair down method and let's see what happens so when i'm going to use this hr2 here and this we got from pair up method and let's say this is the pair up one and i'm going to just run this code and you you are going to see that we have converted this image which was the smallest image to a higher resolution which resulted in this image but when you see this image carefully so let me just move this to this side and this was the original image so let me just minimize this so this image we have converted to this image using the pair up method so ideally this image should look like this but you have to remember that this pair up image is not going to be equal to this image because once you decrease the resolution using the peer down method you lose the information about that image so when you use pair up to just increase the resolution of this image then you can see the result looks little bit blurred because some of the information is loosed using the pare down method so you have to keep this in mind that when you want to increase the resolution after you have reduced the resolution you're not going to get the same result as you might expect that this image should look like this but they are not equal images so this image is just a higher resolution of this image and it has nothing to do with this image so these are the two methods which are available in gaussian pyramid now if you want to create a pyramid of multiple resolution instead of just using this pair up or pair down method repeatedly what you can do here is i'm going to just remove this and remove this code also so what i'm going to do is i'm going to copy the image into a new variable so i'm going to just say layer is equal to img dot copy there is a method available for copying the image which is a copy and then i'm going to create the gaussian pyramid array okay so i'm going to just create a variable called gp for a gaussian pyramid is equal to 10 in square bracket i'm going to just pass this image here as the first element of this list then what i can do is i can use a for loop instead of just rewriting this pair down method again and again and you might already know how to use for loop in python so for i in range and here we can provide any uh range so let's say we want to create five uh image pyramid okay so five time we want to reduce the resolution so we are going to give six here because range goes uh the number minus 1 so whatever you give here minus 1 right so now what we are going to do is we are going to just use our layer parameter once again and then we are going to just call cv2 dot pair down method so pair it down and then we want to just say layer okay and then we want to append to the gaussian pyramid list okay so we are going to just say gp dot append and we are going to append the result of this pair down to our list which we have created here okay so this is going to uh just append this image to our list of images and then let's uh just show this image using cb2 dot i am show method so cv 2 dot i am show and here we can just say s t r for converting the integer to the string because the first parameter you'd give to i am show is a string parameter that's why i'm converting the integer to the string and the second parameter is the image so let's pass this layer here okay so you have the original image which will be shown using this line of code and then you will see multiple number of images of different resolution using this code so let's run this code and let's see what happens so i'm going to run this code and you can see there are different images resulted using that code which we have written so this was the first image which is zero and then this is the second image and then this is the third fourth fifth and sixth so sixth you can see have a very small resolution so this is how you can use a pair down method multiple times using a for loop now what are laplacian pyramids so laplacian pyramids are formed from the gaussian pyramids there is no exclusive function for creating the laplacian pyramid so as you have seen that in gaussian pyramids there are two methods available pair up and pair down but there is no exclusive function for creating the laplacian pyramid so how we can create a laplacian pyramid if there is no function available for creating them so you can create a laplacian pyramid or a level of laplacian pyramid is formed by the difference between that level in the gaussian pyramid and the extended version of its upper level in the gaussian pyramid so this definition might be confusing to you guys so let me explain you with the code what i mean by this definition so what i'm going to do is first of all i'm going to take the top level layer of the gaussian pyramid so top level layer of the gaussian pyramid is the last image which is generated using this for loop so let's say we have six images or five images using this for loop so what we are going to do is because we have appended each and every image to this list right so we have all the images inside this list so we can just get the last image using the indexing so again i'm going to use the layer variable and then i'm going to just say gp for gaussian pyramid list and then there is the index 5 because last image will be available at the index 5 of this list so we get the last image of that gaussian pyramid and then let's show this image so i'm going to just say cv2 dot i am show and this is the upper level or the last image so i'm going to say upper level gaussian pyramid and then we are going to pass this layer variable here so this is going to show just the last image of this list and let's just comment this code out because we don't want to see all the images and then i'm going to create a new list for laplacian pyramid so i'm going to just say lp for laplacian pyramid and then i'm going to create a list using the layer variable itself as we have done for the gaussian pyramid list also so the first element here is the layer variable itself and now we are going to use the for loop and then i in range and this time what we are going to do is you might already know how to use the range function and if you don't know you can see you can give the stop integer here or you can give a multiple parameters here so you can see there is one more implementation of this range function so you can give the start parameter and the stop range so start is the starting point stop is the stopping point and also you can give the steps so this step means uh in what number you want to reduce okay so let us say we want to start from 5 and then we want to go until 0 and we want to reduce in the step of minus one okay so five four three two one so let's uh print the value of i first of all if you uh might be interested in the result of this range function then let's us just run this code and let's see what happens so this is the images which we get but we are not interested in these images we are interested in the print function output so you can see the output of this print function code is five four three two one as i said uh the lower limit is not reached so if you give zero here then it's going to go until 1 and not 0. if you give 6 here then it's going to go until 5 not 6. so let me repeat the definition of laplacian pyramid once again so laplacian pyramid is formed by the difference between that level in the gaussian pyramid and the extended version of its upper level in the gaussian pyramid so let's first create the extended version of that level so we are going to just create a variable called gaussian extend or extended and then we are going to extend the level of that image which are there in the gaussian pyramid list by using cv 2 dot dot pair up method and here what you need to give is the gaussian pyramid list and then we just need to get the index i from this so this line gives us the extended version of the upper level in gaussian pyramid now let's create the laplacian pyramid so laplacian is equal to cv2 dot subtract because we want to find out the difference between that level in the gaussian pyramid and the extended version of its upper level so i'm going to just say gp for gaussian pyramid and then we are going to just say i minus 1 as the first parameter and the second parameter is the extended version of the gaussian upper limit and now we can use the i am show method to show all these laplacian images so i am going to just say cv2 dot i am show and once again i am going to use str function to convert from a number to string and then in the next parameter i'm going to just pass the laplacian parameter here as an image source so what do you think will this code work so let's see what happens when we are going to run this code so you can see the laplacian pyramid looks just like the edge detection so all the edges are shown here on every uh image this is the first level this is the second level third level fourth fifth level so these images are called the laplacian pyramid now what is the use of creating those laplacian pyramids or the gaussian pyramids so the laplacian and gaussian pyramid helps us to blend the images and the reconstruction of the images so these are the two uh benefits of creating those laplacian and the gaussian pyramids so in the next video we are going to see how we can blend the images or how can we reconstruct the images using the opencv and the image pyramids in the last video we have seen what are image pyramids and i have told you there are two kinds of image pyramids in opencv one is called the gaussian pyramid and the other is called the laplacian pyramid and we have seen in the last video how we can create the gaussian pyramid and the laplacian pyramid now in the last video i have also told you some applications of image pyramids and one of the application of image pyramids is the image blending so let me show you one example so here in this code i have two images one is of apple and other is of orange and i want to blend or merge these two images so let me just run this code first of all so you can see there are two images first is of apple and other is of orange and i have also printed the shape of these two images so you can see the shape is similar 512 by 512 and orange image shape is also 512 by 512 so here what i want to do is i want to blend half of the orange to half of the apple so let's say i want to just blend right hand side of this orange to the left hand half of this apple so how can i achieve this now you might say that i can just cut these two images into half and then i can stack these two images side by side and i will get the half and half of the two images and that's how i can just get the result so let's first of all try this technique first of all we are going to just uh create the half and half of the apple and orange images and we are going to just stack these images side by side so let's say i'm going to create the variable called apple underscore orange and then here there is a method in numpy so i'm going to just say numpy dot h stack so there is this method called h stack and here what i can do is in the form of tuple i can provide the half of my apple image so apple is the image variable name and then what i'm going to do is the half of this image because this image is 512 by 512 so i'm going to just give this kind of expression colon comma and then colon 256 which is the half of the apple image on the left hand side right and then i'm going to just do the same with the orange image so i'm going to just take orange and then colon comma 256 colon so one thing to observe here is i have taken colon before 256 in the apple image and i have taken colon after 256 in the orange image and then i'm going to just show this apple orange image and let's see what result we get when we run our code so these two are the apple and orange image and this is the result of adding the two halves of the orange and the apple image but still you can see this line which is clearly visible and from this line you can say half of this is orange and half of this is an apple so in image blending what we need to do is we need to blend this line also so the orange and the apple image should be merged or blended in a perfect way so for blending this half apple and half orange image what we can do is we can use the image pyramid techniques to blend these two images now in order to blend two images using image pyramids technique we need to follow five steps the first step is to load two images in our case these images are of apple and orange which we are already doing so first step is to load these two images the second step is to find out the gaussian pyramid of our apple and orange image the third step will be from these gaussian pyramids find out the laplacian pyramids okay so we will find out the gaussian pyramid in the second step and then in the third step we are going to find out the laplacian pyramids now in the fourth step we are going to join the left half of the apple and the right half of the orange in each levels of laplacian pyramid and finally in the fifth step what we are going to do is we are going to just join these image pyramids and reconstruct the original image so let's follow these steps one by one and let's see what result we get so as i said first step is already done which is just loading these two images and the second step would be to find out the gaussian pyramid so let me just uh just write this step generate gaussian pyramid for apple first of all and then we are going to find out the gaussian pyramid of the orange so first of all what i'm going to do is i'm going to just copy the apple image so i'm going to just say apple underscore copy is equal to apple dot copy so there is a method called copy which you can use to copy the this image so from this copy what we are going to do is we are going to generate the gaussian pyramid so i am going to once again name my variable as gp lets say underscore apple and then we are going to just pass our image which we have copied in the form of list so i'm going to just say apple copy here so these steps we have already seen in the last video how to create the gaussian pyramid and the laplacian pyramid of an image so i am not going to explain this in detail if you want the detailed explanation you can see the last video next what i'm going to do is i'm going to create a for loop and i'm going to just say for i in our range so i'm going to use the range function and we are going to use the six levels in this example so i'm going to provide the range up to six and then what we are going to do is we are going to just say apple copy or you might have named this variable as apple layer also because we are just creating multiple layer of the apple image for the gaussian pyramid right and then we are going to use the cv2 dot pair down method to create the gaussian pyramid okay this we have already seen in the last video and now as an argument we are going to pass our apple copy a variable here and in the next step what we are going to do is we are going to just append to our gp underscore apple variable which is our gaussian pyramid for the apple image and then we are going to just append this apple copy after we have applied pair down method on the same image so this is just giving us multiple layer of the apple image right the same method we are going to apply for the orange also so i'm going to just copy this code and then i'm going to just paste this code once again and this time this will be for orange and i'm going to just say this is the orange copy and we are going to copy from the orange image and then we are going to just generate the gaussian pyramid for the orange image and this will be passed here and also here and also here and also here and this gaussian pyramid orange will be passed here okay so we have generated the gaussian pyramid for the apple and the orange now we are going to generate the laplacian pyramid for apple and orange so this also we have seen in the last video so i'm going to just comment uh generate laplacian pyramid for apple first of all and to find out the laplacian pyramid for the apple what we are going to do is we are going to once again take our apple copy and then using our gaussian pyramid so lets take gaussian pyramid for the apple and we are going to use the fifth element of this list so what we have learned in the last video how we can find out the laplacian pyramid a level in the laplacian pyramid is formed by the difference between the level in the gaussian pyramid and extended version of its upper level in the gaussian pyramid so this difference we are going to find out in this step so i'm going to just say this is lp for the apple which stands for laplacian pyramid for the apple is equal to in the list we are going to just pass the apple copy and then we are going to use the for loop so fall i in the range so we are going to take the range and in the last video i have shown you how to take the range for the laplacian pyramid so we want to go from 5 until 0 in the steps of minus 1 and then in the next step we are going to create the gaussian extended variables gaussian extended is equal to cv 2 dot pair up this time we are going to use the pair up method and then we are going to pass our gp apple which is gaussian pyramid for apple and then the index here will be i in the next step we are going to create the laplacian variables is equal to cv2 dot subtract so there is a method in cv2 which is called subtract and then we are going to take our gaussian pyramid for the apple so gp apple and the index here will be i minus 1 and the second argument for this subtract method will be our gaussian extended variable so we are going to just pass this gaussian extended variable and in the next step we are going to just append to our laplacian pyramid for the apple so lp underscore apple dot append and we are going to just append this laplacian variable to the laplacian pyramid for the apple same we will do for the orange image also so we are going to generate the laplacian pyramid for the orange orange here and this will be the copy of the orange copy here and here also and then this will be the gp orange right this also will be gp orange this also will be gb orange and here instead of lp apple we are going to just say lp orange and then we are going to just pass this variable here also so now we have finished three steps one is to load both the images second is to find out or generate the gaussian pyramid and the third step is to generate the laplacian pyramid for both the images now the fourth step is to just join the half of these two images so what i'm going to do is now i'm going to just create one more variable which will be apple underscore orange underscore let's say pyramid is equal to and also we are going to create a variable called n and we are going to see uh later how to use this variable and then we are going to use the for loop and then we are going to create two variables one for the first image so i'm going to just say apple and then lap comma orange lab okay so these two variables i'm creating just same as this i in this for loop in zip so there is a method zip which we can use to zip the laplacian pyramid one which is for the apple and for the orange also so i'm going to just say lp for apple comma lp for the orange and inside this for loop first of all we are going to just uh increment the value of n by 1 each time so n plus equals 1 and in this next step we are going to find out the shape of the apple image so the apple image shape gives us three values first is columns so i'm going to just say ceo ls for columns then rows and then the third value is the number of channels and then we are going to just say apple lap dot shape in the next step we are going to just create a variable called laplacian and we are going to just join the two halves of uh these two images which we are getting inside the variable apple lab and orange lab so we are basically doing this step after applying the gaussian pyramid and the laplacian m pyramid on both the images so np dot at stake uh method we are going to apply in this step so i'm going to just write np dot h stack and then as an argument what we are going to do is we in the form of tuple first of all we are going to take our apple lamp variable which is this one and in the square bracket we are going to just write colon comma zero comma int so we are going to just type cast the number of columns in the apple shape so this we got from the shape of the apple uh index and then divided by two so we are going to just uh dividing the columns into half and same we will do for the orange lab so we are going to just say orange lap in the square bracket colon comma int and then once again in the parenthesis we are going to just say calls for the number of columns divided by 2 and then colon as we have done in this step also and at last we are going to just append this laplacian variable to this list which we have created so apple underscore orange underscore pyramid dot append and then we are going to pass the laplacian variable here now the last and the final step is to reconstruct our image so let's reconstruct our image so now what we are going to do is we are going to once again create a variable called apple orange underscore reconstruct is equal to this will be the first index of our apple orange pyramid so i'm going to just say apple orange underscore pyramid and this will be the 0th index and once again we are going to use the for loop so for i in the range so we are going to go from 1 until 6 and the default step is of 1 so we don't need to give the third argument and inside the for loop we are going to just take this variable once again and then we are going to apply the pair up method on this so cv2 dot pair up and as an argument we are going to pass the same variable so we are going to just apply the pair up on this apple orange reconstruct from the zeroth index of the pyramid up to the sixth level and the last step will be to add all the layers so uh apple orange reconstructed once again or reconstruct is equal to cv2 dot add so this is also one method which is called add and here we are adding apple orange pyramid and the reconstructed apple orange image okay so this is this variable which we got by just adding the left and right halves of each level and then we are just reconstructing this image using the pair up method and thus just adding the pyramid level so this should be i think the index i right we cannot just add the list to the image directly okay so this will be at each layer we are just reconstructing and adding it to the image which we got by just adding addition of this half of the images now in the end let's try to just load this reconstructed apple orange image in the i am show window and let's hope it works i haven't checked it yet so i'm not sure it will work or not and you can see it's working in the first go so that's a good thing so you can see the difference so this result we got by just stacking this apple and orange image side by side but this line is clearly visible but when we applied the gaussian pyramid and the laplacian pyramid technique for blending the images then you can see this line is perfectly blended and this line is not any more visible so this is the perfect blending of the orange and the apple image so this is how you can use the laplacian and gaussian pyramids to reconstruct and blend two images together and result is in front of you so you can see how it can blend two images so perfectly so this is how you can blend images using image pyramid technique in this video we are going to understand what contours are and we are going to see how to find contours and how to draw contours so first of all what are contours so contours can be explained as the curve joining all the continuous point along the boundary which are having the same color or intensity now contours can be a useful tool for shape analysis or object detection or object recognition now for better accuracy we generally use binary image for finding the contour so first of all we are going to generate the binary image and then before finding out the contours we are going to apply the threshold or kenny edge detection to find the contours on the image so let's start with an example so here i have a simple code which reads an image and then converts this image into a gray scale image and then i'm just showing both the images using i am show method so let's run this code and let's see what result we get so this is the original image with these colors and after the conversion of this image to the grayscale image this is the result which we are getting and then we are going to find out the threshold or the kenny edge so in this example we are going to just apply the threshold so for applying the threshold on this image we are going to define first of all two variable ret comma thresh is equal to cv to dot threshold so there is a method called threshold which we have already seen how threshold work in detail in the previous videos so the first argument which this threshold method takes is the image so we are going to pass our grayscale image as the source the second argument is the threshold value so because it's a binary image let's set the threshold to 127 which is around half of uh the 255 right the third argument is the maximum value so the maximum value here will be 255 the next argument will be the type and type here will be zero so this is going to give us the trash hold value for this grayscale image and after finding out the threshold of this image we are going to find out the contours so for this we are going to define two variables one is contours and the second is the hierarchy because the method which we are going to use which is cv2 dot find contours this is the method it's going to give us these two values contours and the hierarchy and we are going to see what our contours and hierarchy in details after applying this method on this image so the first argument will be the thresh which we got using this threshold method the second argument will be the contour mode so this is called the contour retrieval mode also and there can be several possibilities here which we can apply but for simplicity and in the most common case we use our etr underscore tree here okay as the mode the third argument here will be the method which we want to apply and this is also called the contour approximation method and here also several possibilities are possible but for now what we are going to use here is this will be cv2 dot approx none so now as you are seeing here this find contour method gives us contours and hierarchy so the contour is a python list of all contours in the image and each individual contour is a numpy array of x comma y coordinates of boundary points of the object and the hierarchy is the optional output vector which is containing the information about image topology and this we are going to see in the later videos so for now we are only concerned about finding out the contours so for this as i said this contains the number of contours right so we can print out these number of contours is equal to and then we are going to just uh convert this number into the string and there is a method called length and then inside this length method we are going to pass our contour variable so this line is going to print out the number of contours which are found inside the image which we are providing so let's run this code and let's see what result we get so we already know that this gives us a grayscale image and the original image but we are interested in this print message and the number of contours which are found is nine inside the source image which we are providing here now we already found out the number of contours now we need to draw these contours on the image itself so how can we achieve this but before this let's see the individual contour also so i'm going to just print out the value of the first contour which will be at index 0. so let's run it once again and let's see what happens so we are running this code once again and you can see after printing out the number of contours it's going to give us the numpy array of the x and y coordinates so if we plot or join all these x and y coordinates we are going to get the boundary of the contour so now we are going to just take these contours and pass it to a method called draw contos which is going to draw or join all these coordinates of those contours so to get this we are going to just say cv2 dot draw contours and then the first argument here will be our original image because we want to draw the contours on our original image so this will be the img and it's the original image and the second argument will be the contours so we are going to just pass the contours which we found inside the image the third argument will be the contours indexes so if we uh just give here 1 then it's going to draw all the nine contours which were found inside the image these all contours so first of all we will give minus one here as an argument and then we will see how to give other arguments as the numbers here also the fourth argument here will be i think the color so we are going to just give the color 0 comma 255 comma 0 let's say and the next argument will be the thickness so we are going to take the thickness three here so using this method what we have achieved is we have drawn the contours on the original image so let's run this code once again and let's see what result we get so you can see this was the grayscale image and this we have used for finding out the contours but the interesting image here is this one and here you can see all the contours are drawn on this image so all the green lines or green uh boundaries are all contours so because we have given 1 it has drawn all the contours on this image and we can also give the contour index so let's say we just want to find out the contour 0 which will be the first contour which is found inside the image we are going to just run this code once again and the first contour which was find out found out is this contours this uh p contour right in a similar way we can go up to eight so zero one and let's rerun this code again you will see that the second contour is this contour so this whole contour from the boundary of this image is the second contour and in a similar way you can go let's say 2 i'm going to run this code once again you will see the next contour here and similarly you can go up to the index 8 because the total number of contours are 9 and we are starting from the index zero that's why we can go up to eight so this value depends on the number of contours okay so because we found out the number of contours are nine so that's why we can go up to 8 and let's run this code and the last contour which was find out and we have drawn this contour here on this blue circle right now if we go beyond this index let's say we give nine here we are going to get the error right so you can go up to eight here and if you want to just draw all the contours then you can just give minus one here and it's going to draw all the contour on the image which you are providing so this is how you can find out the contours and draw contours on the images using find contour and draw contour methods in opencv in this video i'm going to show you how you can create a very basic and simple motion detection and tracking system using python and opencv so let me show you what we are going to achieve at the end of this video so i have this video which is a sample video and you can see some people are walking around inside this video now what i want to do here is i want to show these rectangles around these moving people or persons so this is tracking and when some movement occurs i also want to show this kind of status that status is movement because somebody is moving inside the video so if nobody is moving the status will be blank and if somebody is moving then the status will be movement so this is what we are going to achieve at the end of this video so we are going to try to track each and every person and also we are going to uh track this person with this rectangle and also we will show the status as movement when somebody moves inside the video so let's get started so to start with i have this basic code which just reads a video using video capture class and then if this video is valid then i'm going to just show this frame by frame inside i'm show window and i'm sure you might be knowing all this code because i've shown you step by step how to capture the video or how you can read the video frames using video capture method okay so this is just to uh load this video and show it frame by frame using i am show method so let me run this code first of all to start with so our original video looks like this so some people are moving but we want to track the movement of each and every person and also we want to show a rectangle around them whoever is moving so let's get started so under this video capture code line what i'm going to do is first of all i want to read two frames from the cap instance so i'm going to just copy this code and paste it here so this will be our frame 1 let's say and similarly i'm going to just read the second frame so simply we are just declaring to frame one after another okay and we don't need this uh code anymore so first of all i'm going to declare a variable diff and using a cv2 dot a b s diff method so absolute difference we are going to find out the difference between the first frame and the second frame so this method abs diff is for finding out the absolute difference between the first frame and the second frame now once we have the difference then we are going to convert this difference into a grayscale mode so we are going to just say gray is equal to cv 2 dot convert color so cvt color and the first parameter here will be our difference which we have found between the two frames so i'm going to just pass diff as the first argument and the second argument will be cv to dot we are going to convert this bgr color to the grayscale mode and why we are finding out the grayscale mode of this diff because we are going to find out the contour in the later stages and in the last video we have learned that it's easier to find out the contours in this grayscale mode as compared to the colored mode or the b gr mode so once we have this grayscale mode we are going to just blur our grayscale frame so we are going to just declare a variable called blur and then we are going to apply the gaussian blur on our gray variable so cv2 dot gaussian blur the first parameter here will be gray so let's uh give this grape parameter which we have defined here the second parameter here is the k size or the kernel size so let's say we want to provide the kernel size 5 comma 5 and the third parameter here will be the sigma x value so we are going to just pass 0 here as the sigma x value now we are going to find out the threshold so we are going to just say underscore because we don't need this first variable and then the second variable will be thresh is equal to cv to dot threshold and the first parameter which it takes is the source so we are going to pass our blurred image as the source and then the second parameter here will be the threshold value so we are going to just provide 20 here then the maximum threshold value will be 255 then the type will be uh cv2 dot thresh binary so in the next step what we are going to do is we are going to dilate the threshold image to fill in all the holes this will help us to find out the better contours so there is a method called cv2.dilate so we are going to just declare a variable called dilated and then we are going to apply this uh method so cv2 dot dilate which takes few argument the first argument will be our thresholded version of the image the second argument here will be the kernel so kernel let's say for now we are going to provide none here okay so the kernel size will be uh none and then third parameter will be the number of iterations so let's provide the number of iterations and the number of iterations we are going to provide here will be three so if it doesn't work we can increase or decrease the number of iterations now in the next step what we are going to do is we are going to find out the contour so as you all know that contour or fine contour method is going to give you two result one is the contours and other is the hierarchy so we are going to just say contour and the second result we are going to just say underscore because we are not going to use this uh second result and then we are going to uh just say cv2 dot find contours and we are going to find the contours on this dilated image so we are going to say dilated now the next argument here will be the mode so the mode which we are going to use here will be retter underscore tree so i'm going to just write retr underscore tree which is uh most commonly used and then the next argument here will be the method so the method here will be cv2 dot chain approx simple and once we have our contours we are going to just draw the contours because we already found out the contours so we are going to just say draw contos and the first argument here will be frame one because we want to apply all the contours on the original frame right so we are going to apply all the contours which we have found using all these method on the frame one and then the second argument here will be the contour so you can just give the contours here and the third argument here will be uh the contour id i can just say minus one which is going to apply all the contours and the third and the next argument will be the color so let's say we want to provide the green color so i'm going to just uh say 0 comma 255 comma 0 and the next will be the thickness so let's say we want to provide the thickness of two here so now it's going to draw all the contours which we have found with the difference of frame one and frame two right and then we are going to just display this frame one so we can just say this is our feed and the result after applying the contour will be saved in the frame 1 which we will display now in the next step what we are going to do is we are going to assign the value inside frame 2 into frame 1 so we are going to just say frame 1 is equal to frame 2 and then inside our frame 2 we are going to read a new value so we are going to just say r e t comma frame 2 is equal to cap dot read okay so we are reading the new frame in the variable frame two and before reading the new frame we are assigning the value inside the frame two to the frame one in this way we are reading the two frames and finding out the difference between uh the two frames so let's run this code and let's see if it works or not let's test this so you can see now there are these contours which are drawn around all the moving persons also there are some contours which are drawn around this rope which is also moving right so we have successfully determined the contours and we have already drawn these contours on the frame one but this was not the result we are looking for we want to draw the rectangle around these moving persons and also we want uh some noises to be removed so we not want to draw the contour on the moving rope let us say okay so how to remove these noises and how to draw these rectangles let's see so now in the next step what we are going to do is under or before we are drawing these contours we don't want to draw the contours now we want to draw the rectangles right so what we are going to do is we are going to iterate over all the controls so we are going to just say uh for contour so from contours we are going to find out contour in contours right so this is the list and we are iterating over this list so inside this for loop the first step will be to save all the coordinates of the found contours okay so we are going to define the xcoordinate then the ycoordinate and then we are going to just say width comma height and there is a method called bounding rect which we are going to apply on the contour so we are going to just say is equal to cv 2 dot bounding rect this is the method which we are going to apply which is going to give us the x and y coordinate and the width and height right and we are going to apply this bounding rect method on the contour which we are getting using this contours list now in the next step we are going to find out the area of the contour and we are going to just say if this area is less than certain value then we don't want to do anything we don't want to draw a rectangle or anything we just want to continue otherwise if this contour area is greater than let's say some kind of a person's area then we want to draw a rectangle on it so inside this for loop we are going to just define uh if condition so we can say if cv cv2 dot contour area so there is a method called contour area which is this one where we can pass our contour so we are going to pass our contour and if the area of this contour let's say is less than 700 then we are going to just say continue so this code essentially mean that if the area of the contour is less than 700 then we are going to do nothing we don't want to draw any rectangle otherwise if the area is greater than 700 then we want to draw the rectangle so we are going to just say cv2 dot rectangle we have already learned how to draw a rectangle on an image using the rectangle method the first argument here will be the source which will be frame one the second argument will be the point one so we are going to just say point one will be x comma y the third argument will be point two so we are going to just say x plus w comma y plus h the next argument will be the color so let's say the color will be the same 0 comma 255 comma 0 the next argument will be the thickness let's say we want to give the thickness 2 as we have done with the draw contour we have provided the thickness of two here right now in the next step we are going to just uh print some text on the image if some movement is observed so we can just say cv2 dot put text this also we have seen in the previous videos how to put text on an image so this time the source will be our frame one the second will be the text so we will just say uh status let's say and if there is some moment we are going to just say colon in the curly brackets we're going to just use the format method so this is just formatting the result using the string and we are going to just say movement the next argument here will be the origin so where we want to put this text let's say we want to put this text on 10 comma 20 coordinate and then the next argument will be the font phase so we are going to just say font face will be cv 2 dot font font hershey simplex let's say so we are going to use uh this font and the the next argument will be the font scale so let me just do this on the next line so font scale will be let's say 1 the next will be the color of the font so let's say the color will be 0 comma 0 comma 255 and then the last argument will be the thickness so let's say the thickness will be 3 and this code is going to put the rectangle around your moving persons if the area of that contour is greater than 700 okay so let's run this code and let's see if it works or not so i'm going to just run this code and you can see that status is movement because all the persons here are moving and you can see these rectangles which are drawn around the moving persons and this noise which we were seeing in the previous result is also gone around the movement of uh this rope okay so sometimes uh this uh rectangle is drawn on the movement of the rope also so in this case you can also increase the expected area let's say we just want to find out the contours which are greater than 900 and we can now you can see uh these rectangles are drawn around these moving persons with the area which have the contour area more than 900 so you can remove these kind of noises from the frame using this area so this was a very basic example how you can detect the motion and track your moving object inside your video using python and opencv in this video we are going to see how we can detect simple geometrical shapes using opencv so to start with i have this simple code which reads an image and then show it into uh i am show window so let's run this simple code first of all and let's see what it does so you can see i have this image which i'm loading into a opencv window using i am show method and here we have some shapes so we have a pentagon circle rectangle square triangle and this star shape right and let's say we want to detect using opencv which shape it is based upon the geometrical shape and we want to write the name on top of this shape so how we can achieve this let's see using opencv so as you can see if the first step is to read an image and then in the second line i'm just converting this image into a grayscale mode image so using this code i'm just converting this image into a grayscale mode and in the next step we are going to find out the threshold so i am going to just say underscore comma thrash is equal to cv 2 dot threshold so cv2 dot threshold and we are going to pass our image which is a grayscale image which we have converted as a source and then the next two values are the threshold values and the maximum value of the threshold so for now i am giving the threshold value 240 because i know this will work but if you want to be more flexible you can always use the track bar to find out which threshold will work with your image the second value is the maximum value of the threshold and the next value will be the type so the type here will be cv2 dot thresh binary so we are going to just say cv2 dot thresh binary now in the next step we are going to find out the contours so contours we have already seen in the last videos how to find out the contours and what are contours so for that i am going to define two variables one is contours variable other is the underscore variable because we do not need the second result and then i am going to just say cv2 dot find contours the first argument here will be the thresholded image and then the second argument here will be the mode and third will be the method so let's give these two values so cv2 dot retr 3 and the method will be cv2 dot chain approx none okay so let's give this method so this is the simple procedure to find out the contours inside an image now in the next step i'm going to iterate over all the contours so i'm going to just say for contour in contours so we are going to iterate over all the contours and then we are going to first of all use a method called cb2 dot approx poly dp so i'm going to just declare a variable first of all i'm going to just say approx is equal to cv 2 dot this method which i have mentioned which is called approx polydp so this method approximates a polygonal curves with a specific precision and the first argument which it takes is the curve so our curve here will be the contour which we have found on the shape the second argument here will be epsilon so epsilon is the parameter specifying the approximation accuracy so here what we are going to do is we are going to define epsilon is equal to 0.01 and then we are going to multiply this number by cv 2 dot arc length so there is this method called arc length and what does this arc length method do it calculates a contours parameter or a curve length so here in this arc length parameter we are going to pass once again our contour variable and the second argument here will be if it's closed or the open contour so in our case we know that all the shapes which we want to detect are closed so we are going to just pass through here and the next argument in the approx poly dp method will be once again if it's a closed shape or the open shape so once again we are going to pass through here because all the shapes which we have are closed shapes now once we have this approximation we are just going to draw all the contours first of all so we are going to just say cv2 dot draw contours on which image on our original image so we are going to draw these contours on the original image and then we are going to pass the second argument and this will be our approximation so we can in the square bracket this is uh one other notation of uh just giving the number of contours as an argument to the draw contours method so in the square brackets you can just pass the approx the next parameter here will be the contour index so because we are iterating over all the contours that's why the index will always be zero because there will be only one contour which we are working at a time so this index will be zero the next argument here will be the color so you can give any color here i am going to give 0 comma 0 comma 0 let's say and then the next will be the thickness so thickness i'm going to give here is 5 now the next step is to print out the shape so which shape it is we want to print on the shape which shape it is in simple english let's say so for that we need to find out the coordinates on which we want to uh print this text on the shape so we need to find out the x and y coordinates so we can find this x and y coordinates using uh this approx variable and we can just say approx dot revel so this is a method called ravel and then the first index here will be the x coordinate and see in a same way we are going to just say approx dot ravel and on this method the second argument or the second index at index one will be the y coordinate so on these x and y coordinates we are going to print our uh text now in the next step what we are going to do is so because this approx poly dp is going to approximate the number of polygonal curves so based upon the number of polygonal curves we can just approximate which shape it can be so if this approx length so let's uh just find out the length of this approx and if the length of this approx variable is equal to 3 then we are going to say that it's a triangle because triangle can be made with three points so this length of approx variable if it's equal to three then we are going to say that it's a triangle because if the number of curves here are three then most probably it's going to be a triangle so if we know that this is a triangle then we can easily just print or put text on that image so we are going to just say put text and the first variable here will be the image so we are going to put text on the image the second variable will be the text and we know that this will be a triangle so we are going to just say triangle here and then the next argument here will be the coordinates on which you want to print this text so we already found out the the coordinates at which we want to put this text the next argument here will be the font so we are going to just say cv 2 dot font hershey complex and the next argument here will be the font scale so let's say font scale will be 0.5 and the next argument here will be the color so you can give any color let's uh say we just want to print this text in the black color itself so we are going to just say zero comma zero comma zero then using this logic we can also say that if the length of this approx is equal to 4 then it can either be a square or a rectangle so here if the approx length is 4 then it can be a square or a rectangle but we don't know if it's a square or a rectangle so for now we can just write that it's a rectangle and we are going to decide if it's a rectangle or a square in the next step but let's define the other uh if else conditions also so this was l if similarly if number of approx points are 5 then we are going to say that it's a pentagon so we are going to print out uh the pentagon text on the x and y coordinates and if the number of points are 10 then we are going to just say that it's a star shape so we're going to just say star because in the star the number of points are 10 and then we are going to say that in any other conditions so we are going to just say else and we are going to just remove this condition from here else in any other condition it's going to be a circle okay so if approx length is 3 it's a triangle if approx length is four it's a rectangle or a square a five pentagon if it's ten it's a star if it's uh nothing out of all these options then it's a circle you can also find out uh for example octagon or hexagon here if it's six it's a hexagon if it's eight it's a octagon and so on right now let's once again come to this step and in this step we just know that if the number of points are 4 then it's a rectangle or a square but how can we find if it's a rectangle or a square so let's decide that now so what we are going to do for that is we are going to just say x comma y and then we are going to just say uh w comma h for width and height and there is a method called cv2 dot bounding rect which is going to uh give us the x and y coordinates and the width and height of the rectangle right so we are going to apply that method so cv 2 dot bounding rect on our approximate variable or approx variable which is going to give us the x and y coordinate and within height now based upon the width and height we can find out the aspect ratio so we are going to just say s pact ratio is equal to float first of all we need to type cast uh the width into a float so we're going to just say float w divided by height and this will be the aspect ratio of the rectangle now if this aspect ratio let's print out the aspect ratio also so we know what aspect ratio uh we are getting using the the rectangle or the square and we are going to just say if this aspect ratio is between uh 0.95 and 1.05 then it's going to be a square right because the width and height are almost same okay so we just give some room for some noises that's why we are providing here ideally it should be a one aspect ratio should be one in order to have a square but let us say we are just approximating so we can just say if it's 0.9 if it's greater than 0.95 and if it's uh less than so aspect ratio is less than or equal to 1.05 then it's a square okay in ideal situation you might want to give here one but in images it can be a little bit different so we are just giving this limit so if the aspect ratio falls in this limit then it's going to be square otherwise it's going to be a rectangle right and i'm going to just say that if this is the case then it's going to be uh square otherwise so in the else condition so let's give this else condition here else it's going to be a rectangle so let's uh print rectangle in the put text okay so this is the code which we have written and now finally what we are going to do we are going to just uh show the shapes image including all the contours and the text which we have put on these shapes so let's run this code and let's see if it works or not so you can see now it's going to work like this so all the contours are drawn across these shapes and you can see the text on top of these shapes so circle rectangle pentagon star triangle and squares what you can also do here is you can just change uh this text position using the x and y coordinates so let's say i just want to change this y position to just little bit top of the shape so i just added the minus five offset here in the y axis and now you can see it goes little bit up this text right so now it's much visible this text and you can see rectangle and square text is not going up because we have declared the local x and y here also so we can just say x 1 and a y 1 here and then run this code once again and you can see this rectangle and square text is also moved little bit up so i think the offset of five is okay to show these uh text on top of these shapes so this is how you can detect simple geometric shapes using opencv in this video we will discuss about histograms in opencv so what is a histogram so you can consider histogram as a graph or a plot which gives you an overall idea about the intensity distribution of an image so let me give you some examples and then i will be able to explain you better how histogram works and why they are useful so to start with i have this example which is a very normal example here i'm creating 200 by 200 pixel image using a numpy zeros which essentially mean that we are going to get a 200 by 200 pixel image of black pixels so let me uh just uh just start this uh example and you can see uh this is the final result so all the pixels here in this image are black and the size is 200 by 200 now let's say we want to calculate or find out the histogram of this image so there are several ways of finding out histogram of an image so let's see uh them one by one so first of all we are going to find out the histogram using the matte plot lib uh because uh the plot using matplotlib you can draw easily so let's use that first of all so for that what i'm going to do is i'm going to use plt because i have already imported this matplot library as plt so pltr dot hist there is a function called plt.hist which calculates the histogram of an image and because it's just a grayscale image or it's just a black image so it's easier to find out the histogram so you what you can do here is the first argument here will be your image or your source so i'm going to just say image dot ravel okay so there is a method called ravel the second argument here will be a maximum number of pixel values so i'm going to just say 256 the third argument here will be the range so the range will vary from 0 to 256 okay so this is all you need to find out the histogram using the matplot lib and you just need to show this plot in a matplotlib window so you can just say plt dot show so that's it so let's run this code and let's see what happens so you see this plot using matplotlib and also our original image so as we have created the image of 200 by 200 pixel of black pixels so all the intensity of this graph you can see is zero so you can see here 200 multiplied by 200 is equal to 40 000 so these are the number of pixels so on the yaxis you will see total number of pixels and here the intensity so intensity starts from 0 to 256. so this graph is showing how many number of pixels inside an image which have this uh pixel values so in our example all the pixels inside this image have the pixel value zero that's why this graph is like this so all the 40 000 pixels inside the image have the pixel value 0. so you will get this type of histograms so once again the histogram is a graph or a plot which gives you the overall idea about the intensity distribution of an image now histogram is just another way of understanding the image by looking at the histogram of an image you can get the intuition about the contrast brightness intensity distribution etc now let's uh improve this example which we have so i'm going to just close uh this window and let's say i want to add some white pixel also inside this image so what i'm going to say is i'm going to just cv2 dot rectangle so i'm going to just add the rectangle inside this image and the source here will be the img variable then where i want to introduce this rectangle so i want to introduce this rectangle at this point which will be let's say which starts from 0 comma 100 and the second point here will be let's say 200 comma 200 okay so this will be ah 200 and the next value here will be the color so let us say we want to add the white pixels so this will be 255 which will be the maximum value and then the next argument will be the thickness so i am going to just say 1 which will fill this rectangle inside this image so when i run now this you will see this graph and this image so you can see half of this image contains black pixels and half of this image contains the wide pixels and we already know that the size of this image is 200 by 200 that's why here in the graph you will see 20 000 pixels are black which means that 20 000 pixels have the pixel value 0 and 20 000 pixels have the pixel value 255 that's why you see this here so you can see you can easily find out the pixel intensity of an image easily using histograms now next we are going to add some more pixels into this image and this time what we are going to do is we are going to add the rectangle inside the same image so lets say it goes from 0 comma 50 to 100 comma 100 and the color here we are going to provide the pixel value of 127 let's say okay so which is the half of uh 0 and 255 approximately so i'm going to run this uh example once again and now you will see this kind of image so you can see half of the pixels here are white that means 20 000 pixels have the pixel value of 255 so you can see here now around 15 000 pixels here in the half of this image have the pixel value of zero that's why you can see uh this line here and we have added the rectangle of uh pixel value 127 also so around you can see around 5000 pixels here have the pixel value of 127. so this is how uh the histogram is going to uh work so let's use now the original image so some kind of uh image instead of this black or white image so now what i'm going to do is i'm going to uh just once again declare a variable and then i'm going to just say cv2 dot im read and we are going to read some files so let's say i have this lenna.jpg image so i'm going to just uh read that i hope the extension is correct jp g and we are going to read this image in the grayscale mode so i'm going to just say 0 here and now i'm going to run this example once again and you can see this lena image is loaded in the grayscale mode and here is the histogram of this image so these are all the pixel intensities inside this image so you can see from this uh graph that most number of pixels contained inside this image have the pixel value around 150. now you can also find out the pixel intensity of different colors so till now we have been just using uh the grayscale mode or black or white uh pixels but you can also use the same histogram for the bgr values also so let's see how we can undo that so what we are going to do is let me just remove this code or i am going to just leave it commented and here i am going to just say b comma g comma r and there is a method we have already seen which is called cv dot split which is going to split your image into bgr values so we are going to just give the source which is our image and then if you want to uh show these bgr values you can just show in the i'm sure window so bg r and here also b g and r and when you want to uh show the histogram of bgr values then also you can use matplotlib dot hist method you just need to change this source from image to bgr so b g and r okay so now what we are going to do is we are going to run our code and let's see what happens so it's giving me uh this error because i'm reading this image in the grayscale mode so i'm going to remove this extra parameter from i am read because we want to read this image in the color form and then only we will be able to get the bgr channels right in the gray scope scale mode there are no bgr channels so i'm going to run this script once again and let's see what happens so you can see this histogram of blue channels and green channels and the red channels and these are the images which are loaded in these different uh channels so this is the image which is loaded in the blue channel and this is the green and this is the red channel and you can see uh the histogram of each channel differently using uh matplotlib so let me just close all these windows now there is a method in cv2 also which is called calc hist which is going to give you the histogram of an image so for that what you can do is i'm going to uh just just uh comment all uh the this code because i just want to show how you can use the cv2 calc hist method okay so what you can do is you can use a method so let's say hist and then cv dot calc hist and this method takes a few arguments so the first argument here will be the image so it's the source which you give but the only special thing is you just give this image in the square brackets okay the second argument here is the channel so it is the index of channels for which we calculate the histogram so here in our case because we are going to uh read the image in grayscale mode we can just give the channel 0 here so for one channel you can give 0 here for different channel you can give 0 1 2 value the next argument here is the image mask so to find histogram of full image it is given as a none because our because our image is loaded in the gray scale mode so we can give here none the next value is the hist size so this his size is the representation of bin counts and this is also given in the square bracket so we are going to just say 256 here the next argument is the range so range will vary from 0 to 256 so minimum and the maximum range of the xaxis you can say so 256 and then we can just show this hist or histogram inside the plt so plt dot plot method so dot plot and then we can just give this histogram value here okay so let's run this code and let's see what happens so you can see you get the histogram of this image using the opencv calc hist method and what are the uses of the histogram so a histogram can tell you whether or not your image has been properly exposed so when you take a digital image it's very useful it it can also tell you whether the lighting conditions were flat or harsh when you took that image and using the histogram you can also make the adjustments uh which will work best for your digital images so this uh the usefulness of the histograms we will see in the later videos this was just the basics about the histograms in opencv in this video we will discuss about template matching in opencv so first of all what is template matching so template matching is a method of searching and finding the location of a template image inside a larger image in opencv there is a method called match template for achieving this purpose so let's get started and let's see an example about it so i have this simple code which just loads this image and let's see uh what this image looks like so this is the image and this is the messy image and what i want to do is i want to match the face template which i have which looks like this which is the smaller template which is also available inside this image so this will act like a template for us and we will try to find this template inside this larger image so let's get started and let's see how we can search this template inside this larger image so first of all what we need to do is obviously we need to load this image and also load our template so before loading our template image i'm going to just convert my original image which is the larger image into the grayscale image so i have declared this variable gray underscore image and then i'm going to just say cv 2 dot cvt color which is going to convert my image img and let's convert this image into cv2 dot color underscore bgr to gray now let's load our face image which is called messy underscore face dot jpg so i'm going to just change this name merge c underscore face dot jpg and this will be our face image or you can also say this is a template and i'm going to also load this image as a grayscale image so i'm going to just pass the second argument in the read method as 0 which is going to load this messy image as a grayscale image now in the next line we will simply use this method which is called match template and we are going to save it into some variable so we can just say res is equal to cv2 dot match template which is this method which takes few argument first is our image so i'm going to pass our grayscale image here the second argument here will be the template which we are trying to search inside this image so this will be our template the third is the method so the method can be a several method there are several methods available for the template matching so i want to show you these method for the template matching so you can see a type of template matching operations and there is separate formula involved in order to match that template inside that image so so for now we are going to use this method which is a tm underscore c co f underscore normed dot tm underscore c co f norm which is this method now let's try to print this result and let's see what is the content inside this result so i'm going to just print the content inside this result which we got so i'm going to run this code and this image is loaded but for now we are interested in this array matrix which you are seeing here so you can see when you observe these values carefully you will see uh all are relatively uh smaller values so you can see uh 0.2 0.2 almost every value is around until 0.3 so the maximum value i can see here is 0.3 so let me just show this image once again and the the template also so what this result contains is these all values and there will be one value which contains the number for example 0.8 or the brightest point okay so if here this matrix contains a value which have the value 1 it is the brightest point and it will be there inside this image after applying this match template method which will be around this point at this point at which uh this template matches so top left corner of this template so at the point at which this left top corner of this image will match inside this large image there will be a brightest point there and that brightest point will be reflected inside this image in the form of this decimal number and all the other values will be slightly uh darker darker values okay so that's how this matrix from this matrix we will come to know the the top left corner of the template inside this larger image so now how can we filter out that value which is the brightest point inside this matrix so all the points you can see looks like under 0.3 but there are some points here you can see three dots and there are thousands and thousands of values will be available here all the values are not printed okay so what we are going to do is we are going to try to find out the brightest point so this we can find out with the numpy method uh there is a method called where using which we can find out uh or filter out those values which are greater than certain number so i'm going to uh first of all declare a variable called threshold is equal to i'm going to declare the value of threshold initially as 0.8 which will be relatively brighter point inside the matrix which we are getting using this result uh variable right and then there is a method called uh where numpy where so i'm going to declare once again l o c variable and then p dot where method and here we are going to pass our result which we got and we are going to filter out using this expression so this will be a boolean expression so i'm going to just say give me all those values which are greater than or equal to the threshold inside this result matrix okay so this where method is going to uh just evaluate this expression each and every value will be evaluated and if this value inside the matrix is greater than 0.8 which is our threshold then it's going to uh give those values to us so let's print out those values after the filtering out of most of the values and let's uh just print this loc variable also so i'm going to run this code once again and you can see here this is the matrix which we got so you can see this is the array which we got so still we can increase this threshold in order to find out only one point so there are several points available here so let's say i'm going to increase this value to 0.9 and let's run this code again and you will see only two points 85 and 220 so this is what we were expecting so we wanted to find out uh this point which will be the brightest point uh inside this result matrix so once we got the brightest point uh which will be around here which will be the top left corner as i said of this template and it will be located somewhere here in the original image then we can draw the rectangle around this original image same as the size of this template so this will be the easier task because we already know the width and height of this template we already know how to get the width and height of this template and same size rectangle we just want to draw on this original image so let's see how we can do this so there is already a method so i'm going to just declare two variables width and height and you already know uh the method so template dot shape is going to give you the shape of your image right so i'm going to just say template dot shape and then inside the square brackets we are going to just give two colons and minus one this means that we want to get the column and the rows value in the reverse order so width and height that's why uh i have given this minus one index here now in the next step what we are going to do is uh we are going to uh just draw all the rectangles uh where the template is is matched so uh by seeing this template image and the original image we know that there is only one messy face inside this image but let's say there are several number of uh matched templates inside our original image for that we need to iterate over the result which we got after applying the filter on the result so for that we are going to just iterate over that result in our case as we know that there is only one point so we don't even need to iterate over it but if there are multiple number of meshed templates then this for loop will be uh handy so for uh pt in your loc variable so we are going to just say zip which is going to iterate over this loc variable so s tricks l o c and then we are going to find out the width and height here also so we are just reversing the x axis and yaxis right so we are going to just say colon cool minus 1 here and then once again inside this for loop so cv 2 dot rectangle method and the first argument here will be our original image because we want to draw the rectangle on the original image the second argument will be the first point of the rectangle so the first point will be this one pt which we are getting using the loc uh variable so as you all know that the first point here will be the top left corner of the rectangle and the second point here will be the bottom right corner so how can we get the bottom right corner we will get the bottom right corner using this pt uh variable and then on the zeroth index we are going to just add the width comma on the first index so pt uh square bracket first we are going to add the height okay so essentially we have just found out the width and height of our template and we are getting the second point using this addition on the first point width and height so it's going to give us this bottom right uh corner of this template or this point so this is how we are getting uh our two points to draw the rectangle now the third and fourth variable will be a simple which are the color so you can just say 0 comma 0 comma 255 which will be the green color and the width let's say 2 here so we want to give the width 2 here so let's run this code and let's see what happens so i am going to run this code and you can see this red rectangle is drawn on the face of the messy and you can here also see this rectangle will match our template image so whatever image is inside this rectangle will be exactly same as our template and once again you can see the result let me explain this code once again so if this point this threshold will be uh 0.08 let's say in the case of 0.09 threshold we are only getting two values uh this 85 and 220 right that's why we are seeing the clear rectangle here when we are giving the threshold 0.8 here let's see what happens so i'm going to run this code once again you can see there will be this rectangle but it will be much thicker why it's much thicker because we are getting several number of values one two three four five six seven eight nine so we are getting the nine points on the xaxis and the yaxis so this for loop will iterate nine times and this rectangle will be drawn nine times on the image and that's why this rectangle is much thicker let's uh just change this value to 0.9 once again and you will see this rectangle is you know the single rectangle that's why it's much thinner right now when you give this value let's say we give the value 0.3 so most of the point as you can see here have the value 0.3 and when we run this code you will see so many rectangles here so that's why this thresholding is essential for us to find out the brightest point or the value which have the maximum value right so that's why we were filtering out this these points and finding out the values more than 0.9 threshold and about the method so let's try different methods so let's try to give different methods here these two method behave little bit differently so uh we can start with this uh tm c c o r r normed and uh we can apply it here and it's going to give us uh this kind of uh result you can see we are getting uh several uh points here after filtering so let's try to increase this value to 0.95 and let's rerun this code and let's see what happens so now you are getting four values you can also filter that out let's say 0.99 now let's see what happens so now you are getting only two values okay so you need to uh try to change this value to the maximum point so try to change this value and you will get uh this kind of rectangle only one rectangle so every method is going to give you different result and that's why you need to try all the result not all the result will give you the perfect rectangle or template matching so you need to try different methods on your images so this is how you can do template matching in opencv in this video we will understand the concept behind half transform so first of all what is half transform so half transform is a popular technique to detect any shape if you can represent that shape in a mathematical form half transform can detect the shape even if it is broken or distorted a little bit now this explanation might seem a little bit confusing so let me explain it by an example so let's say you have an image of this road and you want to detect these lane lines in this road image so the first step in order to detect these lane lines in this road is to find the edge pixels using kenny edge detection or any other edge detection method now after you found out the edges using any edge detection method you want a geometrical representation of that edge and in order to find out the geometrical representation for example you want to find out the slope of this edge or its intercept you can use half transform to represent these pixels or edges in the mathematical or geometrical form so after you find out the edges using any edge detector you just have the sequence of pixels so you can loop through all the pixels and somehow figure out the slope and intercepts but it's a very difficult task so we want some mechanism that gives more weightage to pixels that are already in line and this is what we can achieve using half transform so let's begin and let's start with the lines so a line in the image can be represented by two coordinate systems first is using the cartesian coordinate system and using this equation you can represent a line which is y is equal to m x plus c and you can also represent this line using polar coordinate system using this equation which is x cos theta plus y sine theta is equal to r or rho sometimes so let's start with uh this equation first which is a cartesian coordinate system equation which is y is equal to m x plus c so uh when you represent a line in x and y coordinates which is also called the x y space this equation looks like this so y is equal to mx plus c where m is the slope of the line and c is the intercept of this line so if you know the values of m and c you can represent this line in the x and y coordinates now in half transform you can represent this line in other form also and this is called the m c space or the half space so using this equation when you take m on this axis and c on this vertical axis then this is called the m c space so earlier we have represented this line in the x y space and now we are saying that we want to represent this using the m c coordinate where m is on the horizontal line and c is on the vertical line so when you represent this simple line in the mc space or the half space it can be represented as a point so this line can be represented as a point so we all know that a line is a collection of points and managing the collection of points is tougher than managing a single point so if you want to manage a collection of point and if you were to manage a single point which will you prefer and then obvious answer will be to manage the single point and this is what this mc space is doing it's representing a line in the form of a point in mc space or the half space and the opposite of this concept is also possible so if you can represent a point using these coordinate in the x y space then it can be represented as a line in the m c space okay and the formula now will turn into uh this equation which is c is equal to minus x a m plus y a right so you can represent a point and if you have the x and y coordinate in the mc space you can represent this as a line and this will be the equation where x will be the slope now and y will be the intercept earlier m was the slope and c was the intercept but when you just transform or just represent this point into m c space then your x becomes or minus x becomes the slope and y becomes the intercept so how does these concepts are going to help us so the half transform is all about doing what we have learned converting points in the x y space to the lines in the m c space or the half space so for example you can see four points one two three four which are joined by a line right so you can represent these four points and you can join all these four points and it's our representation of a line and here slope is equal to m and intercept is equal to c in the x y space the same line you can represent in the m c space uh using these four lines okay so every point is a line in the mc space and you see the intersection point here which is on the mc coordinate so you have taken an edge detected image and for every point that is a nonblack point you draw lines in the mc space and obviously when you draw these lines these lines will intersect with each other and these intersections mark are the parameter of a line okay so in the mc space you can represent each and every point as a line and they will intersect on a single point and now this intersection point can be used to uh draw a line so this was the representation of points in a line using mc space using a cartesian coordinate system now let's apply these same concepts which we have learned using the cartesian coordinate system uh into a polar coordinate system so as we all know that in the polar coordinate system we can represent a line using this equation also which is r is equal to x multiplied by cos theta plus y multiplied by sine theta or in other form you can also represent this equation like this where y is equal to minus cos theta by sine theta multiplied by x plus r divided by sine theta so this is your x y space where line can be represented like this and we are going to transform or represent this line using this equation into the r theta space or the half space okay so this line using this equation can also be represented as a point in r theta or the half space like this so let's take an example about this so as i said the equation was r is equal to x multiplied by cos theta plus y multiplied by sine theta where this theta is the angle of the line and r is the distance from the origin to the line so let's say we want to represent a point which is from x y space into a half space into r theta space so we give the values of x 0 and y 0 which will be the first point we can represent this point in the form of line in the half space or the r theta space in this formation which looks like a sine curve using this equation so this is for the one point representation in xy space to a line representation in the half space so let's say you have a multiple points so we take three points then uh it's going to look like this so let's say x 0 is equal to 8 and y 0 is equal to 6 x 1 is equal to 4 y 1 is equal to 9 and x 2 is equal to 12 and y 2 is equal to 3 so we have three points in the xy space they can be represented in the half space using three lines and as we have seen in the cartesian coordinate system these points can be represented in uh these lines in the half space in the polar coordinate system also using these curved line and this intersection is going to represent a line in the half space so which representation we are going to use in order to use the half transform so this equation is not able to represent the vertical lines that's why generally we use this equation or a polar coordinate system in order to use a half transform so the half transform algorithm involves these four important steps in the first step edge detection is done using canning edge detector or any edge detection method and the second step mapping of the edge points to the half space is done and all these edge points are stored in an accumulator and the third step interpretation of accumulator to yield lines of infilite length is done and this interpretation can be done by thresholding or any other constraint the fourth step involves the conversion of infinite line to finite lines now opencv implements two kind of half line transforms the first is the standard half transform which is done using huff lines method the second type is the probabilistic half line transform which is done by half lines p method so this is the half lines method and this is the half lines p method in the last video we have seen a brief theory introduction about half line transform so i have told you that opencv implements two kind of half line transforms one is a standard half line transform using half lines method and the second is the probabilistic half line transform using half lines capital p method so we are going to use the half line method in this video and see how we can use this half line method to detect the lines inside an image using half transform now i also told you that there are four steps associated with half transform so the first step was the edge detection step using any edge detection method preferably kenny edge detection the second step is the mapping of edge points to the half space and store these edge point to an accumulator the third step was the interpretation of accumulator to yield lines of infinite length and the fourth step was the conversion of these lines to the finite lines so let's say we have this image of this sudoku.png and you can see all these lines here which we want to detect so this is the line and this is the line so all these lines we want to detect using the half line transform so i have already written this code so i'm going to go step by step to explain how this code works so in the first step you just need to import the normal cv2 and the numpy as np then here i'm just reading this image using i'm read method in the next step i'm converting this image into a grayscale image and storing it into this variable which is gray because for kenny edge detection it's preferred to have gray scale images rather than your normal colored images now in the next step we are applying the kenny edge detection method on this gray scale image so here this cv2 dot kenny method takes these arguments first argument is the image second and third argument is the first threshold and the second threshold so i'm giving the first threshold as 50 and the second threshold here as 150 and the fourth argument here i'm giving aperture size is equal to three now in the next step i'm using this half lines method this is the normal hub transform method which is implemented in opencv now this half line method takes few argument the first argument is the image so we are just just passing this edge detected image to the first argument of this half lines method the second argument here is the row value this row value is the distance resolution of the accumulator in pixels normally it's taken as 1. the third value is the theta value which is the angle resolution of accumulator in radians so for that we are just using numpy so np dot pi divided by 180 so this is also typical in this method and the next argument here is the accumulator threshold parameter so what does this mean it's a threshold so only those lines are returned that get enough vote that means that those lines will be returned which have threshold greater than this value so starting value i have taken here as 200 as threshold so now this half lines method is going to return the output vector of lines now i have explained you how polar coordinate works for the half transform in the last video so these lines will be in the polar coordinates so each line is represented by two or three element vectors either rho and theta or rho theta and volts so as you can see this is the output vector of lines so i'm going to iterate over each and every uh line vector and what it gives is the first element of this line is going to give you these two values rho comma theta it's going to give you rho comma theta or rho comma theta comma vote right so right now i'm using just two parameters here row comma theta so rho is the distance from the coordinate 0 comma 0 which is the top left corner of the image and the theta is the line rotation angle in radians so all this rho and theta i have explained you in the last video and we have seen how we can represent these row and theta values in the half space so first of all what we are going to do is once we get the row and theta value is we are going to uh just get the cos theta value and the sine theta value because we want to convert these polar coordinates into the normal cartesian coordinates for the line method because this line method as you uh can imagine takes uh these coordinates right which are the cartesian coordinates so this is the point one parameter and this is the point two parameter so x1 y1 and x2 y2 so first of all we are just getting the cos theta value and theta here is this theta so cos theta we are just assigning to a and the sine theta value we are just assigning to b and we are just uh multiplying this a to the row so this will give us the x0 value and the y zero value when you multiply b uh by rho so this row is this row value so this x0 and y0 is going to give you the origin which is zero comma zero or top left corner of the image but we want the lines not the top left corner of the image so how we can get these x 1 and y 1 coordinate and x 2 and y 2 coordinate this is given in this equation so once you get your x 0 and y 0 value you can get the value of x 1 and y 1 coordinate using this equation so you just need to type cast everything into integer so this equation x 1 value stores the rounded off value of rho as i have shown here so this r represent rho so rho multiplied by cos theta cos theta we have already uh taken in the a variable so we are essentially here multiplying the rho multiplied by cos theta minus thousand multiplied by sine theta sine theta value is the value of the b right so x0 plus 1000 multiplied by minus b here okay y1 we get using this equation so y1 is equal to int in the bracket y0 plus 1000 multiplied by a which is essentially this equation which is rho multiplied by sine theta plus thousand multiplied by cos theta so these two values are going to give you the first coordinates and similarly we are going to get the x2 and y2 coordinate using these two equations so here everything is same just this minus is nu right so in this equation you just need to replace a plus by minus and you get the x2 value same you have to do in the case of y2 so in this equation if you just replace this plus by minus you will get the y to value and we have already seen how to use the cb2 dot line method it takes a few argument as you can see here first is the image so image is our original image second is the x1 and y1 coordinate which is the first point comma the second point so as you already know that a line is a collection of point so you need at least two point to create a line right so this is the coordinates of the first point and this is the coordinates of the second point the next argument here is the color so color i have taken simply zero comma zero comma 255 and the last parameter here is the thickness of the line which i have taken to here and the next line of code you already know i think so after this line we come out of the loop and we are just plotting all the lines using this loop on the original image and once we get all these lines on the original image we are just showing it using i am show method and at the last we are just destroying our window once we are done with the image so let's uh run this uh code and let's see what happens so i'm going to run this code and you can see all these lines are plotted here let's see uh the kenny edge detected uh image also so i'm going to just uh after the kenny edge detection i'm going to once again add this i'm show method to show the kenny edge detected image also so you can see here this is the kenny edge detected image all the edges are detected and based upon all these lines which are detected here these lines are drawn but the problem here is these lines are of infinite length so there is no end to this line this these lines just go from the start or the corner of the image to the other corner of the image so you can see they start from here and go to the next corner they don't just uh stop here so in this half transform you uh see that even for the line with two argument it takes a lot of computation and we don't even get the correct result so this problem can be solved using the other method which is implemented using this half line p method which is the probabilistic half line transform which i'm going to show you in the next video so how we can get the better result using half line p method we are going to see in the next video in the last video we have seen how to use standard half transform using half lines method in opencv now in this video we are going to see how to use probabilistic half line transform using a method called half lines capital p method in opencv so let's go to our editor and this was the code we have written last time and we have used half lines method for detecting lines inside this image which was the sudoku image so let's run this uh example really fast to see what was the result which we got last time so this was the result which we got last time and the problem with this result is you can see these lines just go from one end to the other end and in this kind of half transform uh you will be able to see that even for the lines which have two argument it takes a lot of computation so in opencv there is also a method called uh half lines capital p which stands for probabilistic half lines transform and this probabilistic half line transform is an optimization of the normal half transform which we have seen in the last video so let me close this example and let's open the example which we are going to see in this video and you can see in this uh example we have used this huff lines capital p method so when we use this half lines capital p method it doesn't take all the points into consideration instead it takes only the random subset of the points which is sufficient for the line detection so let's go through this code from the top uh to the end so as you can see i have imported these two uh packages cv2 and numpy as np and then i'm reading this image so doku using i'm read method and then i'm converting this image to the grayscale image using cvt color method in cv2 now the next step is to find out the edges of the images this we have also seen in the last video so until here everything is same so once we got the edge detected image using kenny edge detection instead of using the half lines method we are now using this half lines capital p method and it takes few arguments the first argument is your edge detected image the second argument is the row which is the distance resolution of the accumulator in pixels the third argument is the theta value which we have taken np dot pi divided by 180 which is the angle resolution of the accumulator in radians the next value is the threshold so right now we have taken this threshold as 100 and this threshold is the accumulator threshold parameter which means that only those lines are returned that get enough vote that means greater than the threshold value the extra two argument here are a little bit different from the half lines method so you can see all these arguments are almost same these four arguments but there are two extra arguments here or parameter here which we need to provide so the first parameter here is the min line length and this we have taken hundred so this min line length is the minimum length of the line which means that line segments shorter than this length which is 100 in our case will be rejected the next argument is the maximum line gap and it is the maximum allowed gap between the line segments to treat them as a single line so these are the two extra arguments we have taken and this half lines capital p method is going to return again the output vector of the lines but the difference between this return value from half line p method and the half lines method is you can see here this line at index 0 is going to directly give you the values of x1 y1 and x2 y2 which are the two points which we will be able to join and we will be able to draw the line using cv2 dot line method in the last video i have shown you that you have to do so much calculation in order to find this x1 y1 and x2 y2 and this probabilistic half line transform method is going to do our job easy and it's going to directly give us these four values so you don't need to do anything you just need to pass these x 1 y y1 and x2y2 value to the cb2.line method so cv2.line method is going to take the first argument which is the image and then the second argument is the 0.1 coordinate which is x1 and y1 which we got from the line variable at index 0. and the third parameter here is the 0.2 which are the coordinate of the 0.2 which is here x2 and y2 the next argument here is the color which we have taken right now 0 comma 2 55 comma 0 and the last parameter here is the thickness of the line so we have taken two here and the next three line are going to just show this image first of all all these lines which we found out are drawn on the image this image which is the original image and then we are just showing this image after drawing all the lines which we got using half lines p method on the original image and then we are just loading this image using this uh i'm sure method and then after we are done we are just destroying all the windows so let's see what result we get after this script is run so i'm going to run this script and this is the kenny edge detected image and this is the image you got when you apply this half lines p method on your kenny edge detected image so you can see these lines are no longer going to the end to end these are more uh you know accurately detecting all the lines which are there in this doku uh image you can see some lines are broken here so that's why these lines are not even you know drawn because they are not even detected by kenny edge detection so this one or this one are not detected by kenny edge detection so that's why these lines are not drawn so let me show you these results side by side so this was the result which we got after applying the hof line transform method which is half lines on our kenny edge detected image and you can see all these lines here and this is the result which we got after applying half lines p method which is the probabilistic half line transform so these two methods are available in opencv to detect these lines in an image now let's go back to our script and here instead of this image which is the sudoku.png image i have one more image which is called road dot jpg and this is the image which contains a road and inside this road we have some lane lines so you can see this result now here which is the road and these are the lane lines which are detected using this half lines p method so in case of lane line detection you can use this half line p method but you need to decide your roi or region of interest because you can see some lines are detected here here here and here so you just need to uh you know define your line of interest region and you will be able to detect all the lines or lane lines on the road so maybe in the next video we are going to see how we can detect these lane lines on the road accurately without these noises which we are seeing here on the other part of the image so we just need to detect these lane lines and nothing more and we will uh do the same on our video so on the video in which these lane lines are there and we just need to continuously detect these lane lines so in the case of let's say selfdriving car you need to detect these lane lines we are going to see how to detect these lane lines in the last videos we have learned some important concepts in opencv now in this video and the next few coming videos i'm going to create a simple project which uses most of these concepts which we have learned in the previous videos so what we are going to do is we are going to create a very simple lane detection system so first of all we will start with a still image you can see there is an image which contains this road and this road contains lanes so what we want to achieve is we want to detect these lanes on which our vehicle is traveling so first of all we will do this with this image and gradually we will move towards the video frames so first of all we will see how to detect these lanes in this image and then we will see how to detect these lanes in the moving video so let's get started so i have created this new project in my pycharm ide you can use any other editor of your choice and first of all obviously you just need to install opencv python package and matplotlib package once you have done that i will create a new file here so i'm going to just right click here and create a new file and i'm going to name this file as detector dot py file so here we are going to import a few uh packages for example matplotlib so matplotlib dot pi plot as plt so let's say as plt also we are going to import the cv2 package and we are going to import numpy so an import numpy as np in the next uh section what we are going to do is we are going to simply uh load an image so i am going to create an image variable so image is equal to cv2 dot im read and we are going to read our image which is the road image so road dot jpg now in the next line we are going to convert this image into the rgb format because we are going to load this image using matplotlib so i am going to just write once again image so i am going to overwrite this image variable with the converted image so cv 2 dot cvt color and the source is our image so this is the variable and then cv2 dot color from bgr to rgb right so this is what we want to use now in the next line what we want to do is we want to load our image using plt dot i am show method and at last we are going to just say plt dot show so this is how we are going to just load our image so i'm going to right click on this file and then run this script and you can see this road jpg image is loaded now on this plot you also see these values and one things to observe here is horizontally these values goes from 0 to 1200 something and vertically normally in the graphs you will see that values increases from the bottom to top but in matplotlib this value goes from top to bottom right so 0 is at the top and then the maximum value will be at the bottom so this is a one thing to note because we are going to define our region of interest and that will be based upon these values now in the next step we want to define our region of interest so once again let me just run this code once again and one thing to notice here is this lane in which our vehicle is traveling is parallel so there are two parallel lines and eventually they are going to merge here right so all the lanes on which the vehicle travels have the same pattern so this lane and this lane are parallel to each other and they're going to merge at some point so it's not merging but it seems to be merging at some point so we can define our region of interest from this point to this point and from this point to this point so this region of interest will be the triangle so this region of interest we are going to define for our vehicle will mask any other uh obstruction for example this is also one lane line for us it's not important because this is the other side of the lane so here uh the vehicle will come in the opposite direction so this is our region of interest so it will mask out this lane line or any other lines or distortions which we have in this picture we are going to just mask them and we are going to just concentrate on this triangle so let's do this first so first of all we are going to find out the shape of the image so i'm going to just print and then we are going to just say image dot shape and also we are going to uh just define the height and width of the image so i'm going to just say okay so let's print this value and let's see what happens so what's at 0 and what's at 1 so you can see it prints 704 as our height and 1279 as the width so this is what i'm just taking from this image shape method so it's going to return this kind of tuple so at 0th index there will be height and at the first index there will be the width and as i said it starts from 0 to 704 from top to bottom and horizontally it goes from 0 to 1279 from the left hand side to the right hand side right so once we have the width and height we can define our region of interest so we are going to uh define a variable called region of interest vertices and here we are going to provide some values so we are going to provide three points which will be the three points of our region of interest so as i said that our region of interest we want is this point which is the left bottom corner this point which is the right bottom corner and somewhere in the middle of this image so here so in the image because the vertical height starts from zero so i'm going to just say zero comma height and the second point will be the half of the width and half of the height which will be the center of the image so i'm going to just say width divided by 2 comma height divided by 2 and this will be inside these uh parentheses and the third point will be the next corner so this will be width and then the height so let's try to see these points in our matpot plotlib window so the first point here is 0 comma 704 which is this point the second point is somewhere here which is the half of the height and half of the width and the third point will be here which is width comma height which is 700 comma 1279 which is this one right so this will be our region of interest now we are going to define one function to mask every other thing other than our region of interest so i will just define this function def region of interest and this is going to take two parameter first will be the image and second will be the vertices so vertices and inside this function let me just minimize this terminal also so you can see the function so inside this function in the first step we are going to define a blank matrix that matches the image height and the width so this will be the easy step we are going to define a variable called mask and we are going to use np dot zeros like method which uh is going to take one parameter which will be our image matrix now in the next step we are going to just uh retrieve the number of color channels from the image this will be the easy step also so channel count and then we are going to just say image dot shape and at the second index we are going to find out the channel because we have seen that image dot shape is going to give you uh three values height width and the channel count so this channel count is coming from this index now in the next step what we are going to do is we are going to create a match color with the same color channel counts so i'm going to just say match underscore mass underscore color this will be our variable name and then we are going to just take 255 comma and then multiply it by the channel count so let's multiply it by the channel count so this is going to create a match color with the same color channel counts now in the next step we are going to fill inside the polygon using the fill poly method because we have our region of interest and we want to mask every other thing other than our region of interest so we are going to just say cv2 dot fill poly which is going to take few arguments first will be our mask second will be the vertices which we are providing using the second argument and the third argument will be our match mask color variable so we are going to pass this variable as the third argument and in the next step we are going to just return the image only where the mask pixel matches so i'm going to just say masked underscore image is equal to cv 2 dot bit wise and so we're going to just apply bitwise and using this bitwise and method and the first argument here will be the image and the second argument is the mask which we got using this zeros like method right and in the last step we are going to just return this so i'm going to just write return this masked image and that's it so we are going to just uh apply our region of interest on the image using this method and then we are going to just get our image which contains region of interest and any other thing will be masked so now it's time to use this method so we are going to just use this method using this variable i'm going to just define a variable called let's say cropped image or masked image whatever you want to write here so let's say cropped underscore image and then we are going to just use this function which is a region of interest function which takes this argument so because we have already read our image in the image variable we are going to pass this as the first argument and the vertices is simply our region of interest variables so this region of interest variable we are going to pass using numpy dot array method and let's uh split this line so we will be able to see what i am doing inside this np.aria method so first of all the first argument will be our region of interest variable which is this one region of interest vertices so in the square bracket we are going to just pass region of interest vertices and the second argument here will be np dot int 32 so np dot int 32 and now we are going to just uh show this image using our matte plot lib window so let's run this code and let's see what happens when it runs and there is a problem here so let's see what the problem is so you can see uh this problem is coming from this line and most probably this region of interest has some problem so you can see we have passed this first element as the tuple second element as the tuple and the third element also we need to pass as a tuple and that's why it's giving us the problem so i have just fixed it and let's see what happens when we run this code again and you can see our image is now masked with our region of interest so we have defined our region of interest uh from this point to this point to this point so now we have only this region of interest so we will be able to easily find out this lane line and this lane line inside our region of interest and any other distraction will be marked now right so this is the first step which we have achieved which is masking our image and just applying our region of interest on the image in the next step we are going to uh see how we can apply the edge detection and find out the lane lines on the image in the last video we have started our simple project of detecting lane lines on the road using opencv and we came to the point where we were able to define our region of interest and our result was looking like this so let me run this project so we have defined this region of interest and now the only thing which remains here is to detect these lane lines so we will once again go to the next step and the next step will be to find out the edges and then we are going to apply half line transform to draw the lines so first thing first what i'm going to do is i'm going to just move this region of interest function which we have created in the last video on the top of this script so we can see uh this other code clearly this code which we have written so we have this region of interest function which we have created then we have just created this region of interest variable and then we just used our region of interest function using this region of interest vertizes variable so the next step as i said is to find out the edges and for that we need to first convert our image into a grayscale image so i'm going to just say a gray image and then we all know how to find out the gray scale image out of an image so we just need to write cvt color and the source is our cropped image so we are going to pass our cropped image and then we are going to just convert it into a grayscale image using cv2 dot color underscore rg b2 gray so let's do this so once we got our grayscale image we can apply kenny edge detection on this image so i'm going to just write kenny image and then i'm going to just say cv2 dot kenny which is the function which we want to use which takes few parameter first parameter will be our gray scale image the second parameter will be the first threshold and the second threshold so generally uh we are going to take here 100 as the first threshold and 200 as the second threshold now in the next step we are going to just uh display this image on our matplotlib window and let's see what happens once we apply the skinny edge detection method on the image so now you can see this result which detects all the edges and here you can see the lean line edges are detected but there is one more thing here which is the edges of our region of interest are also detected so how to solve this how to remove these edges because these edges doesn't interest us the interesting uh edges here in this image for us are these edges which are of the lanes road lanes right so to solve this problem we can apply the scanning edge detection before we find out the region of interest so i'm going to just copy this code and paste it just before we apply this region of interest method which we have created in the last video so now in our kenny edge detection uh we will pass the gray scale image but here instead of this uh cropped image which we were getting in the last step from this variable we directly are going to pass our image which we have read using the i am read method right so let me just remove all these line breaks so you will be able to see the code at once so here you can see i have directly passed now this image variable to the cvt color method so we get the grayscale image of the original image and then we apply the kenny edge detection on the original image and then we are applying the region of interest method which we have created in the last video now because we are applying uh this uh region of interest method on the grayscale image or the edge detected image therefore we don't uh need this channel here so we can comment out this code which was kind counting out the number of channels and for the grayscale image and the kenny has detected the image we just take this match mask color as 255 because it's only one color right we don't need any color channels here because we are just passing the grayscale image which has only one color so that's why we don't need any channel because there will be only one channel and that's why i have commented this code and the value of the match mask color will be 255 now once you do that let's try to run this code and let's see what happens once again we need to load the cropped image not the kenny image so just replace this variable here in the i am show method and let's run this code once again so you can see now uh there is some mistake here because we were expecting the edge detected image and we are getting this image so let's see what's the problem is so the problem i see here is because we have applied this region of interest on the original image which we don't want now we want to apply this region of interest on the kenny edge detected image so we have to pass as the first variable of the region of interest method the skinny uh edge detected image not the original image right so once again for you you can see this code region of interest method and all this code at a one glance let's run the code and let's see what happens so now we get the better result so we have these edges which are detected by the kenny edge detection for only the lane lines inside our region of interest and now it will be easier to draw the lines on these edges which we have detected so the next step will be to draw the lines on these edges using the half line transform so we have in the previous videos have already seen how to use the half line transform so i'm not going to go into the details so let's uh just directly jump into uh using that half line transform so what we are going to do is in the next line after we have got our cropped image we are going to just define a variable called lines and we are going to use this half line transform probabilistic half line transform method so here cb2 dot half line transform and this will be this method which takes few argument first argument will be the image so i'm going to pass this cropped image here the second argument here will be the value of row so let's provide this row value variable value which will be 6 in our case then in the next parameter we have to pass the value of theta and theta will be equal to np dot pi which is the method inside the numpy library so np dot pi divided by 60 so i'm going to pass here divide by 60 then the next parameter here will be the threshold so the threshold value we are going to provide here will be 160. the next parameter here will be lines which is equal to none by default so i'm going to provide this variable lines is equal to and then uh we are going to pass the empty uh numpy array so i'm going to just say uh numpy and p dot array and then we are going to just pass the blank square bracket here the next two parameters will be the min line length so let's provide this uh min line length and let's say we want 40 as the minimum line length and the max line gap so let's provide that also max line gap and this will be let's say initial value for that will be 25 so now after applying this half line transform you know that it's going to return the line vector of all the lines which are detected inside our image which we have provided as the source here so if you don't know what are these parameters which i'm using here you can see my uh last videos about probabilistic half line transform and you will be able to know what they actually mean now once we got our line vectors then we can draw the lines easily and for that we are going to define our next function which is to draw the lines so i'm going to just define this function with the name draw the lines for example and it's going to take few parameters so let's pass these parameters first parameter will be the image or the original image the second parameter will be the line vectors which we have found out and that's it so there are these two uh parameters we are going to pass here now uh inside this function what we are going to do is we are going to first of all uh copy our image so i'm going to just say i'm g is equal to np dot copy and then we are going to just make a copy of the image variable which we are providing and then or you can write here copy image whatever i'm just just reassigning this copied image to the same variable but you can define a new variable here for the copied image also now in the next line we are going to create a blank image that matches the original image size so the dimension should be equal so for this we can just say line image and then we are going to just say np dot zeros inside these parentheses we are going to provide the shape of our image right so you can provide the shape of our image using the image variable so first of all it's going to take the height and then the width and then the number of channels so because we know that this is a colored image which we are working with so we are going to just say img.shape and we all know that the 0th index parameter here will be the height the second parameter img dot shape the value at the first index will be the width and the number of channels for the colored image are always three so we are going to provide the third parameter as three here so this is in the form of tuple i'm providing and the next parameter here will be the data type or d type so let's uh provide that d type is equal to numpy dot u in okay so u int eight not one in eight u into eight so this will be uh the second parameter so once we have this image which is exactly same as the size of our original image we are going to loop around these line vectors and then we are going to draw the lines right so let's loop around these uh line vectors and uh draw all these lines which were found so for that we are going to use the for loop and then we are going to say line in lines and these this lines variable is coming from this lines variable so we are going to use this draw lines function and we are going to pass this lines vector as the second parameter here so this is how uh this line variable is coming here so now inside this for loop we are going to uh just define one more for loop because this line is going to give us uh four parameters which is uh the coordinates of the first point in the line and the coordinates of the second point in the line so we are going to just once again say for x1 which is the first coordinate of the first point and the y one and then similarly x2 and the y2 so this will be the line coordinate in the line uh which we got from the line vector and then inside this for loop we are going to draw a line and drawing line is really easy by using cv2.line method which takes a few parameters as you already know the image and then the second parameter is the coordinate of the first point which we already have using this iteration which is x1 comma y1 and third parameter here will be the coordinates of the second point x2 comma y2 and then you can provide the color and thickness so let's uh provide this color so the color here i'm going to take let's say 0 comma 255 comma 0 you can take any color here and the thickness so the thickness here i'm going to take is uh let's provide this parameter thickness is equal to 3 okay so this is the thickness of the line which we want to draw and here i think this blank image should uh be uh given because we want to draw the line on the blank image and then merge it with the original image so here we have to provide this line image or you can say this is the blank image which is more appropriate in this case so we want to draw the line on the blank image which is of the same size of the original image and now once we draw these lines on the blank image we can merge this blank image and the original image which will give us the line which are drawn on the original image so outside this for loop we are going to merge the image with the lines into the original image so our original image is the image itself so image variable is the our original image and then we are going to just say cv to dot add weighted this function also we have seen in the last videos and this is the function which we use to merge two images with some weights so the first parameter here will be image now the second parameter here will be the value of alpha so which we are going to give here 8 this is like a weight to an image which we want to provide and then the third parameter here will be the second image so we want to merge the blank image with the original image the fourth parameter is the value of beta so this value we are going to take as one and the last value will be of uh gamma so gamma we are going to take as zero 0.0 here okay so this add weighted also we have seen in the last video how to use it so i'm not going into the details and at last once we have the lines on the image then we are going to simply return it so let's uh return this uh image img so once we have this function we are going to call this function after applying the half line transform method which is the probabilistic half line transform so here in the next line we are going to just define a variable called image with lines let's say is equal to our method which is draw the lines method the first argument is the original image so we are going to pass the original image the second argument is the line vector which we got from this method right so the original image and the line vector variable which we got here at last we are going to just see what is the result which we got after applying this draw the lines method on the original image so let's run this code and let's see what happens so now you can see let me just maximize it you can see this line is drawn on our image so this is the first line and this is the second line so we got the result which we wanted if you want to change the thickness or the color of this line on the image you can just change it using this draw lines method so this is the line and thickness parameter so for example i want to change this to 255 here some different color and the thickness let's say four and i'm going to run this code and now you can see this yellow color here right so you can change the thickness and the color using uh this method so let's say for now we want the red color so we are going to go with this red color on the lane lines so this is what we wanted to achieve we wanted to draw the lane lines on these lanes and we have achieved this in the next video we are going to see how we can apply the same concept on a live video or on a video of this road for example so for example this car is running on the road and we want to continuously draw these lines on the lane lines how we can achieve this using opencv we are going to see in the next video in the last two videos we have seen how we can detect the lane lines on the road using opencv now till now we have only worked with this image and in this video we are going to try to apply what we have written not on an image but with the video frames but you have already learned in the previous videos that a video frame is like an image so a video contains many number of frames so if we apply the same technique on each frame we will be able to detect these lane lines on the video frames also so let's apply that concept on our script what we have till now so right now i have added this test dot mp4 video inside my project so let me show you how it looks like so our video looks like this so we are going to apply all these concepts which we have applied on an image on this video so let's get started so i hope you have this code which we have written in the last two videos the only thing we need to do here is we need to read the video instead of an image and then apply those concepts on the frame instead of an image so we till now have two functions region of interest draw the line and we have this code so this all code we are going to enclose inside the function so that it will be easier to apply all this code on the video frames now as you already know that this will not be used because we are reading the videos so we don't need to read the image obviously so we are going to comment these two lines out so we don't need to convert bgr to rgb because we are going to use this native cv2 library not the matplotlib library for which we have converted this bgr to rgb image so now we are going to define a function so let's define this function and i'm going to name this function as process and it's going to take an argument which will be the image argument and all this code which is under this which we have written in the last video we are going to enclose this code inside this process function we don't need these two lines because we are not going to use matplotlib for processing this video so i'm going to remove these two lines and i'm going to just give a space here for this code so it can be enclosed inside this function now at last or at the end of this function we are going to just return this image with lines so we are going to return this image with lines using this process function that means on every frame we are going to draw the lines and return it using this process function next we are going to read the video using the video capture functions so i am going to declare a variable cap is equal to cv2 dot video capture and then we are going to just pass one argument which will be our video file which is test dot mp4 in our case so test dot mpu4 and then once we have this video we are going to check if the video frame is available using the while loop so let's use this while loop and we are going to check if cap dot is open is valid or not so is opened and this function is going to return the boolean value so if this video frame is still available is going to return true and whatever we write inside the while loop is going to be executed now in the next line we are going to just read every frame so we all know from our previous videos that this cab dot read returns two uh result or two variables one is ret and the other is the frame and we are going to just say cap dot read and then we are going to apply our process function on this frame so we are going to once again take this frame variable and we are going to overwrite this frame with the lines on the frames so this we are going to get from our process functions let's call the process function and pass the frame variable inside it okay so this frame is going to go to the process function it's going to process everything and then the final result which we get is going to be saved once again into the frame variable with the actual lines on the frame in the next line we are going to just show our result using cb2 dot i am show method and we are going to just pass the frame variable here in the next line we are going to just write the code for the quitting from this loop so we are going to just say if cv2 dot weight key is going to be 1 and then we are going to apply the end operator and then write 0xff for the cross platform functionality and then we are going to just say is equal to ord so whenever somebody presses the q key then we are going to exit from this loop and then in the next line we are going to just say break so break out of the loop the last two line in the last two line outside this loop we are going to just uh call the release function on the cap variable and we are going to destroy all the windows in the cv2 so we are going to just say destroy all windows that's it hopefully this is going to work so let's uh just run this script once again and let's see what happens and here we got the error and it's coming from this line which is cv2 dot i am show we forgot to give the first argument here which will be the name of this window we're going to just say frame here and let's run this script once again and let's see what happens and you can see on this video on this lane line our lines are drawn right so this is the result which we were expecting we can improve this result by adjusting few uh variables so we are going to first of all press q to quit and let's uh change some of these values here in the half line transform so we are going to just say that the max line gap is going to be 100 we are going to reduce the threshold value to 50 and draw value to 2 okay inside this uh hof lines p method and let's run this code so let's see what result we get this is also okay let's improve it little bit more in the kenny edge detection we can reduce this threshold value here to 120 the second threshold value and let's run this code once again and now we get the better result so the problem might be the edge detection so we have reduced our second threshold and now we get the better result you can see on this middle lane the lines are drawn clearly so this is how you can write a simple script to detect lane lines on the road i hope you have enjoyed this video and i will see you in the next video we have already seen how to use half line transform to detect lines in opencv in this video we are going to see how we can use half circle transform to detect circles in an image now as you can see here i have this small example which loads an image and shows it into the i am show window so let me run this code and let me show you how this image looks like so you can see there are so many smarties here inside this image and all the smarties are circle form right they are not the perfect circles but they are in the form of circles and we want to detect all these circles forms inside the image we can use half circle transform for that so let's see how we can use this half circle transform to detect the circles in the image so a circle is represented mathematically by this equation which you see on your screen so here x center and the y center are the coordinates of the center and r here is the radius of the circle so if you know these three parameters then you can draw a circle so the coordinates of the circle and the radius of the circle we need to detect so now let's see how half circle method is applied using opencv so you might observe here that i have created a copy of this original image which i have read using this i am read method in the next step i'm going to just convert this image into a grayscale image so i'm going to just write gray is equal to cv dot cvt color which is going to take two parameters first is the source and second is the method so we are going to convert the color bgr to gray now in the next step because our half circle method works better with the blurred images so we are going to uh create this blurred image using median blur so i'm going to just say gray so we are going to overwrite this gray variable with cv2 dot median blur which is going to take a few arguments first is the image itself so we are going to pass gray here and the second is the k size or the kernel size so we are going to initially provide the kernel size of 5 here now we are going to apply our half circles method so i'm going to declare this circle's variable and then i'm going to just call this method which is called cv dot half circles method so this is the method and you can see it takes few parameters so we are going to give these parameters one by one first is the obvious one which is the image so we are going to provide the gray uh scale image here which is already blurred so the second parameter here is the method which we want to use currently the only implemented method is half gradient method so the choice is very simple here we are going to just provide this cv dot huff gradient method the third parameter here will be a dp dp is the inverse ratio of accumulator resolution to the image resolution so for example when dp is equal to 1 the accumulator has the same resolution as the input image and if the dp is equal to 2 then accumulator has the half as big as width and the height so we are going to take this dp value as one the next parameter here will be min dist it is the minimum distance between the center of the detected circles okay so here we are going to give initially the value of 20 and later we will adjust this value if the circles are very near to each other the next parameter which we are going to give here is the value of parameter 1 and parameter 2 or param 1 or param 2. the param 1 is the first method specific parameter in case of half gradient it is higher threshold of the two passed to the kenny edge detector param2 is the second method specific to the method which we have provided here which is the half gradient method it is the accumulator threshold for the circle centers at the detection stage so we are going to provide the value of uh the param1 and param two here so let's start with the param one value and we are going to provide param one value is equal to 50 and param 2 value is equal to let's say 30 so those param1 and param 2 parameters are specific to this method which we are using the next parameter which we are going to pass here is the min radius and the min radius is the minimum circle radius and we are going to just start with the zero so we are going to say that anything which is greater than zero we are going to just draw it and then we are going to provide the max radius if this max radius is greater than or equal to zero it uses the maximum image dimension if it's only greater than zero it returns center without finding the radius so this also we are going to start with 0 let me just break this function so you can see all the parameters here so this half circle method is going to give us the circle vector which we can iterate upon but first of all we need to convert those circle parameters which we got using this circles variable that is x and y coordinate and the radius into an integer so to do that we are going to just declare a parameter called detected circles and then we are going to use numpy to convert them into an integer so i'm going to just say np dot u in 16 and then in the parenthesis i'm going to just use np dot around and we are going to pass our circles parameter which we got using the half circles method now in the next step we are going to iterate over those detected circles so we are going to just say uh for and because this circle vector is going to give us x y and the radius we can directly uh just extract those values so we're going to just say x comma y comma r and then in our detected circles and those circles will be at this index so 0 comma colon and then inside this for loop we are going to first of all draw the circle and also we are going to draw the center so to draw the circle we already know that we have this circle method available which takes view parameter first is the uh image so we are going to pass the copy of this image here so let's pass this copy which is output the second argument here will be the center which are the coordinate of the center which we already got in the form of x comma y so we will give them uh in the form of tuple the third argument is the radius so radius is extracted in the r parameter here so we're going to pass the radius here and then the next parameter will be the color so let's start with let's say green color and then the thickness so we are going to give the thickness of let's say 3 here similarly when we use the same circle method and we want to draw the center then we know that this is the center so these are the coordinates of the center and if the radius is very small let's say 2 then it's going to just draw a small point right so that's why i have given very small value for example 2 here so it's going to just draw a very small circle which will look like a dot on the circle that's why this value is very small and we are going to just say that this will be also three and let's change the color of this dot let's say this will be this color okay so we are just drawing those circle on the copy of the image which is called the output so let's run this code and let's see what happens when we run this code and you can see this dot is first of all drawn on each circle which is detected which is in the form of yellow and also in the form of green all the circles are drawn so you can see this circle is drawn so every uh circular shape is uh you know enclosed by the detected circle we also strangely detected this circle uh somehow because uh open c we think that this is also a circle i have one more image which is shapes.jpg so we are going to just uh see that also so i'm going to just say shapes dot jp g let me show you this image first of all so it looks like this so it has only one circle and some other shapes right so we are going to just run this code once again and you can see it just detect this circle and it just draws a small dot on the center and all the other shapes are undetected so this is how you can detect the circles inside an image using half circle transform in this video we are going to discuss about the basics of face detection using har feature based cascade classifiers so object detection using har feature based cascade classifiers is an effective object detection method proposed by paul viola and michael jones in their paper now haar feature based cascade classifier is a machine learning based approach where a cascade function is trained for a lot of positive and negative images now what are these positive and negative images so first a classifier is trained with few hundred sample views of particular object that is a face or a car or any other object that is called a positive example so whatever you want to detect if you train your classifier with those kind of values so for example if you want to detect face then you need to train your classifier with the number of images which contain faces so these are called the positive images which contains the object which you want to detect similarly we want the classifier to train with the negative images that means the images which doesn't contain the object which you want to detect so in our case for example we want to detect the face then the image which doesn't contain the face then it is called the negative image and if the image contains face or number of faces then it's called the positive image and after a classifier is trained it can be applied to a region of interest in an input image and the classifier outputs a 1 if the region is likely to show the object or zero otherwise so let's see how we can use har cascade detection in opencv so opencv comes with a trainer as well as a detector so if you want to train your classifier for any object for example a watch or a car or a train or anything then you can use this classifier also on opencv's github page you can find some trained classifier xml files so let me show you these classifiers on the opencvs github page so here is the opencv repository and inside this repository you can see this data folder and then go to har cascades i will just share the link with you in the description so you can directly navigate to this website and this location and you can see plenty of trained classifiers are available inside this repository so for our example we want to detect the face so we are going to use this trained classifier which is called heart cascade underscore frontal face underscore default dot xml file so you just need to open this file and then download it you can just click on the raw icon here this button and once this raw file is open you can just right click and save it on your computer so you can just say save page as and then you can just save this inside your opencv project so i have already saved this file inside my opencv project you can see uh this file here which is a xml file which i have downloaded using this repository so as you can see here i have this code which is the minimal code to load an image and show it using opencv window now in the next step what i'm going to do is before this reading we are going to just define our classifiers so because it's a face classifier i'm going to name my variable as face cascade and then in opencv there is a method called so i'm going to just call this method and there is a method called cascade classifier so this is this method called cascade classifier where you can provide your classifier name which is the xml file so just provide your trained classifier file name in our case it's hard cascade underscore frontal face underscore default dot xml so once we have our classifier we read the image and then because this classifier will work with the grayscale images we are going to convert our image into a grayscale image and it will be really easy to convert our image to a grayscale image now once we have our grayscale image the next step is to detect the faces inside this image so for that we are going to declare this variable let us say faces and then we are going to use this result which we got using this cascade classifier and then we can call a method called detect multi scale so we are going to just call this method which takes few argument first is the image so we are going to provide our grayscale image here and the second argument we are going to use here will be the scale factor so the scale factor parameter specifies how much the image size is reduced at each image scale so to start with we are going to provide a 1.1 value here and then the next parameter which we are going to provide here will be the min neighbors parameter so min neighbors parameter is going to specify how many neighbors each candidate rectangle should have to retain it so we are going to provide this value 4 here to start with and if it doesn't give us the proper result we are going to change it and the last step here will be to iterate over all the faces which we have detected and then draw a rectangle on them so this face variable will be the vector of rectangle where each rectangle contains the detected object in our case this will be the detected face so the rectangle may be partially outside the original image if it's on the corner so the if the object or the face is on the corner then this rectangle may be a little bit outside the original image so we are going to iterate over this faces object and here we are going to get the parameter x comma y comma w comma h which means the values of x and y and the width and height of the rectangle of the object in our case this is the faces right so we got all the four parameters for drawing the rectangle and then we can just call cv2 dot rectangle method to draw the rectangles the first parameter here will be the image the second parameter will be the point 1 which will be x comma y which we got using this faces vector and then we need to give the second point which will be x plus w comma y plus height okay and then the next two parameters are the color and the thickness so we are going to give the color 255 comma 0 comma 0 here and the thickness to start with we are going to give a 3 here that's it so it's this simple to detect faces inside the images using har cascade classifiers so now i'm going to run this code and let's see what happens so you can see this is the face so this is how you can detect the face or a multiple number of faces inside an image let's try to detect the face inside a video so i'm going to just close uh this window and now we are going to try to detect the face inside a video so this will be nothing different than this approach we just need to apply this approach on each and every single frame so instead of this code we are going to use the video capture method to capture the video so you can see i have this test.mp4 video here so we are going to define a cap variable is equal to cv2 dot video capture and then in the parenthesis we are going to provide the test dot mp4 file here or if you have the camera you can provide zero here as the parameter and then all this code we are going to just enclose inside a while loop so we are going to just say that while cap dot is opened so if cap dot is opened is going to give us a true value then we are going to read the frame so underscore let us say the parameter name will be img in this case also normally we take the variable name frame here because we are reading each and every frame and then i'm going to just say cap dot read okay cap dot read this means we are reading every frame and let's uh enclose this code also inside this while loop so i'm going to just provide a little space here so basically we are getting every frame and then applying the same procedure on each and every frame and at last outside our while loop we are going to release our cap so we are going to just say cap dot release and here instead of using this cv2 dot weight key we are going to provide a condition if cv2 dot weight key and in the parenthesis we are going to provide 1 and 0 x f f is equal to our d and we are going to listen for the key uh queue so if somebody presses the key q then we are going to break out of this while loop so let's run this uh script and let's see what happens when we run the same script on a video so this is the video and this is uh in this video so you can see in this video the face is detected in real time in the real live video so this is how you can use haar based cascade classifiers to detect faces or any other object inside an image in the last video we have seen the basics of face detection using haar feature based cascade classifiers in this video we are going to extend our code to detect eyes using the same har cascade classifier so for that first of all you need to download the pretrained class classifier for the eyes from the same source which i have shown you last time also which is the github repository of opencv again i'm going to give you this link in the description so you can directly come to this page and this time we are going to download this xml file with the name har cascade i underscore tree underscore i glass dot xml file so this is the pretrained classifier for detecting eyes so you can just click on draw and then save it as this same file name in your project okay so i have already downloaded this xml file you can see here our cascade i underscore tree underscore iglast.xml file and now we are ready to write our code so this is the code which we have written last time so if you don't know how this code works you can see the last video i'm going to just extend this code to detect eyes so first thing first we need to create the cascade classifier for the eyes so instead of face cascade we are going to name it as i cascade and this file name will be the file which we have downloaded which is i underscore tree underscore i class dot xml file so once we have our classifier then in the last video we have already seen how to detect faces so our region of interest will be the face this time because the eyes will not be present outside the face right so eyes will always be present inside the face so our region of interest will be the face and face we have already detected last time so this face will be now our region of interest so go inside this for loop where we are iterating over this face variable and then we are going to create our roi so i am going to create this variable which is called roi underscore gray and this will be the original grayscale uh image which we have created here but we just want the face out of this image so we can just index the space using y colon y plus h comma x colon x plus w which is the width so this line is going to give us the grayscale region of interest but we also want the colored image also so we are going to just say roi color which will be the colored roi and here instead of gray we are going to take the direct image which will be before we have converted this bgr image to the grayscale image so we have the colored roi and the grayscale roi once we have this we will follow the same concept which we have applied for detecting the faces so so we are going to use this detect multiscale method so i'm going to just write eyes is equal to because we already have our i cascade which is a classifier so we are going to use this variable and then use this method called detect multiscale and then we are going to simply uh pass our roi gray which we got using the faces now we are going to iterate over those eyes so inside this for loop we are going to create one more for loop to iterate over all the eyes which are found on the face so far and then this will be e x comma e y comma e w comma e h for x y coordinate and the width and height now we will just say in eyes and then we are going to just draw this rectangle which is also very simple cb2 dot rectangle and then we are going to pass our image first of all which will be our colored roi image which is this one so here we will pass this roi color and then the first point in the rectangle which will be ex and ey so i'm going to just say ex comma e y and the second point will be e x plus e w which is x plus width so we are going to just write this e x plus e w comma e y plus e h which is the y coordinate and the height the next parameter will be the color so let's uh provide the color let's say this will be 0 comma 255 comma 0 and then the next parameter will be the width so let's say the width we want here is five so that's it so hopefully this code is going to work out of the box we don't need to do anything else we just need to define our classifier and then we just need to use this detect multiscale method to detect the eyes and then we just need to draw the rectangle on all the eyes which are detected so let's run this code and let's see what happens so we are going to see you can see eyes are detected but there is some problem because something is wrong so i'm going to just quit this script and see what's going wrong here so you can see this should be x e x comma e y and then our problem will be solved hopefully so i'm going to run this code once again and you can see the eyes are properly detected so this is how you can detect eyes in the face using opencv and har cascade classifiers in this video we are going to try to understand how we can find out the corners inside an image using a method called harris corner detection now first of all what are corners so corners are the region in the image with large variation in intensity in all the direction now this harris corner detector was first introduced by chris harris and mike stephens in their paper in 1988 now detecting corners using harris corner detector contains three main steps so the first step is to determine which windows produces very large variation in intensity when we move in the x direction and the y direction now what are windows here so windows in this case means that let's say we want to just find out this corner here so windows will be your small box here and then you check for the intensity when you move in the vertical direction and also in the horizontal direction so you check for the change or large variation in the intensity when you move in the x direction and when you move in the y direction in the second step with each such window which we found a score r is computed so this r value which is computed is going to give us the estimate or give us an idea about where this corner is located depending upon the value of r and in the third step after applying a threshold to this score the important corners are selected and marked so let me explain you all these steps one by one what do i mean by detecting the windows and calculating the value of ours let's see step by step so as i said in the first step we determine which windows produces very large variation in the intensity in the x direction and in the y direction so let's say a window or a center is located at the position x comma y and let's say the intensity of the pixel at this location is i x comma y so if this window is slightly shifted to a new location and let's say this displacement is u comma v then the intensity of the pixel at this location will be x plus u and y plus v because our displacement is u comma v so we are just adding it uh to the x value and the y value and hence the difference between the shifted intensity and the original intensity will be the difference in the intensities of the windows shift so for a corner this difference will be very large and that's how we detect the corners using this harris corner detection method now as you can see here this value will be given in the e u comma v format so we have to maximize this function for the corner detection and this we can achieve by applying a taylor expansion to this equation which is given here and by using some mathematical steps so i'm not going to go deep into the mathematical steps but after applying the taylor expansion you will get this kind of approximate value where m is equal to this value and here in this equation i x and i y are the image derivatives in the x and y direction respectively so this can be easily found out using the cv dot sobel method in opencv now comes the second step and in this step we find out or calculate the score for r so this r is equal to this value and the m we have already uh seen how we can get this m value in the first step right so in this equation d e t m is equal to lambda 1 multiplied by lambda 2 and trace m is equal to lambda 1 plus lambda 2 where lambda 1 and lambda 2 are the eigenvalues of m so again if you want to go into the details you can refer to some book or you can go to the wikipedia page to learn more about this equation so once we got the value of r then based upon the value of r we can make some decision and this we can do in the third step so if the value of r is very small that means the value of lambda 1 and lambda 2 are also very small and we can conclude that the region is a flat region and not the corner if the value of r is less than 0 that means lambda 1 is very large in comparison to lambda 2 or vice versa and that means it's an edge and not the corner and if the value of r is large which happens when lambda 1 and lambda 2 are large and this means that this region is a corner so if the value of r is very large that means the region is a corner and that's how harris corner detector detects if it's a corner or a edge or a flat area so this was the theory about harris corner detector let's see how we can use this harris corner detection concept inside opencv using our python code so i have this script already written here so just import cv2 and numpy and then we are reading this image called crossboard underscore image.png using i'm read method and after we read this image i'm just showing the original image so we have the original image and the output at the end to compare now in the next step i'm converting this image into a grayscale image to get the better results and because the cv2 dot corner harris method takes the grayscale image in the float 32 format that's why we need to convert our image into float 32 format so that's why we are using numpy.float32 to convert this image into uh floating point values because our corner harris method which we are going to use in the next step is going to take this kind of value and not the value which comes directly from the conversion of this image to the grayscale image so this step is necessary for the harris corner method and in the next uh step we are just applying the cv 2 dot corner harris method which takes few arguments first is our image in the floating point so this we have passed and the second parameter here is called the block size so here i have given the value 2 here so block size means the window in the first step so we have seen we have to define the window right so for example we define this block size 2 that means neighborhood size is equal to 2 that means for each pixel value block size multiplied by block size that means 2 by 2 neighborhood is considered the next parameter here is called the k size and it's the aperture parameter for the sobel operation and then we have the next parameter here and this next parameter is called the k which is the harris detector free parameter in the equation so after applying this harris corner method to our image we get this destination image and to get the better result we need to dilate this result so we apply cv2.dilate method on our uh image which we get using the harris corner so this image are marked through the dilated corners and then in the next step we are reverting back to the original image with optimal threshold value and we are just just marking all our corners with this color so basically we want to mark all the corners with the red color here and in the next step we are just showing our result in the i am show window and at last we are destroying all the windows so let's run this code and let's see what happens when we run this code and we will see the results so you can see this is the original image which have so many corners and all the corners are detected and it's marked with this red color here so this is how you can find out and mark all the corners using harris corner detection in opencv in the last video we have seen how we can use harris corner detector in order to find out the corners inside an image in this video i'm going to show you how you can use sheet tomasi corner detector method to detect the corners inside an image so in late 1994 j she and c thomasi made a small modification in the harris corner detector method in their paper which was called good features to track so this shi thomasi method is similar to harris corner detector apart from the way the score of r is calculated which we have seen in the last video so this sheet omasi method gives us better result in comparison to harris corner detector and also when you use this c to marsy method we can find the top and corners which means we can provide the number of corners we want and this might be useful in cases where we don't want to detect all the corners inside an image so let's see in the code how we can implement this sheet tomasi corner detector in opencv so here i have already written all the code so let me explain you all the lines of the code one by one so as you can see here i'm just importing the libraries in the first two line and in the next line i'm just reading the image using i'm read method and then i'm converting this image into a grayscale image using this cbt color method so i'm converting this image from bgr to grayscale image now as i said the paper which was published by she and tomasi was named good features to track that's why in opencv this method is also called good features to track so here in this line we are just using this method cv dot good features to track which takes few arguments so first argument here is our input image which is a grayscale image which we are providing as the first parameter the second parameter is the maximum number of corners so here we can limit the number of corners we want to detect so for example i have given 25 here that means we just want to detect 25 corners and if there are more than 25 corners which are present in the image they will not be shown so this value means maximum number of corners to return and if there are more corners than the corners found then the strongest corners will be returned right now the third parameter here is called the quality level so this is the parameter characterizing the minimal expected quality of the image corner the next parameter here is the min distance which is the minimum possible euclidean distance between the returned corners so i have taken 10 here as the minimum distance and 0.01 as the quality level now once all the corners are detected using this good features to track method we convert those corners into the integer values and here int 0 is a mere alias for int 64. and once all the corners are detected we iterate over all the corners and then we find out the value of x and y using this i and then it's easier to just draw the circles over these values using the cb dot circle method so this cb dot circle method takes few arguments first is the input image so the second parameter is the center the third parameter is the radius of the circle which we want to provide the fourth parameter is the color we want to provide and the fifth parameter is the thickness and if it's minus one that means we want to fill the color inside that circle and at last once all the circles are drawn on the corner which are detected then we are just showing this using i'm sure method so let's see how this works in the case of sheet omasi method on an image so i'm going to run this code so you can see all the corners inside this image are detected and because we have just provided this number 25 here so maximum number of corners which will be detected here will be 25 and the rest of them will not be shown so if we increase the value of the maximum number of corners let us increase it to 100 let us say so i am going to just increase it to the value 100 you will see more number of circles are drawn on an image now let's compare the result of the harris corner detector and shea tomasi corner detector so on the left hand side you can see the original image and this middle image shows the harris corner detector method and you can see all the corners are detected using harris corner detector and using the sheath omasi corner detector it gives us better result and we can control the number of corners we want to detect and you can see all the important corners are detected using sheet omasi corner detector in a better way so that's how she tomasi corner detector works in this video we are going to see how to use background subtraction method in opencv so first of all what is background subtraction so background subtraction is common and widely used technique for generating the foreground mask which is also known as the binary image containing the pixels belonging to the moving object of a scene when these images are captured using a static camera and as the name suggests background subtraction calculates the foreground mask performing the subtraction between the current frame and the background model containing the static part of the scene so for example the background subtraction method can be used in the case of visitor counter where you have a static camera capturing the number of visitors entering or leaving the room or you have a traffic camera which wants to count the various telematic data from the moving car or moving car data which is captured by that traffic camera now there are several algorithms which were introduced for the purpose of this background subtraction and opencv has implemented few of them which we are going to see one by one so as you can see here i have this example which is a very simple example of just taking a video and then we are extracting each and every frame of that video and showing it into a window so using i am show method i am just showing each and every frame of that video so this you already know from the previous videos how to capture the video frames from a video file or the live camera so when i run this code you will see that there are few uh persons which are moving here and we want to uh detect all those moving uh persons which are moving in the image so for that we are going to use a few methods which are available in opencv so let's first write some code and i will explain you what this code is going to do so i'm going to define a variable after this line of code and i'm going to define a variable name f g b g for foreground background and then i'm going to just call cv dot b g s e g m so b g s e g m and then i'm going to call a method called create background subtraction m o g method so this create background subtraction mog method is a gaussian mixture based background and foreground segmentation algorithm so using this line what we are doing is we are just creating a background object of the function using this method create background subtraction emoji now uh this method has some optional parameters like history number of gaussian mixtures and threshold but all of them are set by default so you don't need to set anything specifically unless you want to change some of the optional parameters so i'm going to leave everything as default and i'm not going to give any argument here for this method and then after i captured each and every frame inside this while loop what i'm going to do is i'm going to create a new variable called fgmask for foreground mask so i'm going to just write fgmask is equal to and for getting the foreground mask we are going to just call a method called apply on this fgbg or the background subtractor image so we are going to just take fgbg and then we are going to call a method called apply here and it takes one argument which is the frame which we are capturing okay so we have applied this method and then we are just getting the foreground mask using the apply method on this background subtractor variable and that's it so this is your foreground mask so when i just use one more i'm sure window and this is for the fg frame let's say so fg uh mask frame let's say okay so fg mask frame and we are going to just pass this argument here so let's see what result we get after we apply create background subtractor mog method so you can see this normal image here and also you will see uh these moving persons in the foreground mask right so you have subtracted the background for from the foreground and you can easily detect the moving persons here inside this image using this mask you will also observe that there is a very uh little noise not much when you use this kind of subtraction using create background subtractor mog method there is one more method which is called background subtractor mog2 which is also gaussian mixture based background for and program segmentation algorithm so let's use that method also so this method is directly available under cv2 so you just need to write cv dot create background subtractor mog2 okay and everything will remain the same so it's going to return you the background subtractor variable which you can use uh with this apply method to get the foreground a mask okay so let's see how this method performs so you can see the result which is quite different from the first method which we have used so in the previous case we have to create the background subtractor object and here in this method you have an option of detecting the shadows so there is an optional parameter which you can give into this method which is this create background substructure mog2 which is called detect shadows so by default this detect shadows is true that's why you see the shadows there if you just write the text shadows is equal to false then it's going not going to detect the shadows so i'm going to just run the code once again and you can see now shadows are less visible right so let's run the default case once again so let's say we just write true here and you will see the shadows in the gray color right so these shadows in the gray color and when we just make it false so the text shadows false you will not see that gray color okay so shadows are displayed in the gray color so if you don't see any gray color then shadows are not detected this is a noise which is detected but not the shadow okay so this is uh the difference between the first background subtractor method and the background subtractor mog2 method there is one more method which is called the background subtractor gmg so this algorithm which we are going to use so let's use this uh method first of all which is called background subtractor g m g which is available under cv dot bg segment as the first method so just write b g s e g m dot create background subtractor gmg method so this create background subtractor gmg method algorithm combines statistical background image estimation and p pre pixel biasing segmentation let's see how this method performs when we just use this gmg method and when you will see here there is nothing on this uh foreground mask frame so to get the better result you need to apply morphological opening to the result to remove the noises so we are going to do just that so i'm going to just overwrite this fgmask frame using a method called cv dot morphology x this also we have seen in the previous videos right so the first uh parameter here will be fg mask parameter the second parameter here will be uh the op so cv2 dot morph open we are going to use the morph open uh method and then the third parameter will be the kernel so we need to define the kernel also so for defining the kernel let's define the kernel outside this while loop so i'm going to just write kernel which takes few argument first is the shape so we are going to say we want the more eclipse shapes so i'm going to just write more eclipse and then the kernel size will be let's say three comma three okay so we are going to apply this kernel using this morphology x method and when we are going to run this code you can see uh these kind of results which are not as good as you have seen in the first method now let me show you the last background subtraction method which is called the background subtractor knn method so this method is available under cv2 directly so we are going to just comment this kernel code because for this method we don't need to define any kernel so we can just write cv dot create background subtractor and then at last you just need to write k and n in capital okay and it also takes few optional parameters like history and other parameters but these are optional parameters so for now we are not going to set any uh parameter and let's see the result which we get using this k n method so i'm going to run this code and you can see this knn method result it also shows the shadows in the form of gray pixels so whatever gray pixels you see here in this image are the shadows in this method also there is an optional parameter which is called detect shadows which is set by default to true so when you make it false the shadows will not be detected so you can see no gray pixels are visible now when you make it true then the gray pixels will be uh visible and those gray pixels indicates the shadows right so these are the few methods which you can use for the background subtraction in opencv in this video we will talk about an object tracking method which is called mean shift so first of all what is object tracking so in simple words object tracking is the process of locating a moving object over time using a camera and what is mean shift the idea behind mean shift is really simple consider you have a set of points it can be a pixel density like histogram back projection and you are given a window which is a very small window which can be a circle or rectangle or a square and you have to move that window to an area of maximum pixel density or maximum number of points so in the image you can see this illustration very easily so essentially the working of mean shift algorithm can be summarized in following points so in the first step we pass the initial location of our target object and the histogram back projected image to a mean shift function and then in the second step as the object moves the histogram back projected image also changes and in the third step the mean shift function moves the window to the new location with the maximum probability density so we will see all these steps with the help of an example so here i have the simple code where i'm loading a video which is called slow traffic small dot mp4 and i'm just iterating over each and every frame of that video so this code till now you already know how it works so i'm going to just run this code and let's say i just want to track this window of the white car okay so let me just run this video once again so i want to track this window of the white car or window in general of each and every car let's say okay so how can i track this window using mean shift algorithm let's see so as i said the first step is the passing of the initial location of our target so this can be you can say a disadvantage of mean shift that you have to provide the initial location of your target in our case that target is the car window so what i have done is i have just calculated the initial position of the white car window and that we are going to see in the next step so first of all we are going to take the first frame of our video so the first frame of our video can be uh retrieved by uh this code so ret comma frame is equal to cap we have this cap function and we will read the first frame using the read method and this is going to give you the first frame of the video so this is our first frame now once we have our first frame we are going to define the initial location of the car window in our case we want to track first of all the white car window right so i'm going to define four variables first two are x comma y and the next two are width and height so and because i have already calculated the initial position of the window i'm going to hard code this uh position of the window so 300 uh 200 comma 100 comma 50 okay so this is the hardcoded value which i have already calculated which is the initial position of the car window now we can say that this x y and width and height is our track window so we are going to define a variable called track underscore window let's say and then we are going to just pass all these four variables x comma y comma with comma height okay so let's pass all these four variables and in the next step we are going to define our region of interest so let's define this region of interest with the variable called roi and we already have our first frame so we are going to take our first frame and then we are going to pass that window so y colon y plus height comma x colon x plus width so this is our window or the position of the window so as i said in the first step we will pass the initial location of our target object and the histogram back projected image of the mean shift function so histogram back projection in simple words creates an image of the same size but of a single channel as of our input image in our case this will be our frame where each pixel corresponds to the probability of that pixel belonging to our object that is the output image will have our object of interest or region of it in more white color compared to the remaining part of that image so this is a back projection so for calculating the histogram back projection there are some steps which are involved so we are going to uh follow all these steps to calculate the histogram back projection but first of all let's uh just see the region of interest because we already have a region of interest so i'm going to just write cv dot i'm sure and our region of interest so let's say how our region of interest looks like so i have this video and this image which is our region of interest right so this is the initial position i'm going to pass to our mean shift function right so now in the next step what we are going to do is we are going to uh define the histogram back projection so we already have our roi so in the next step we are going to just convert this roi to the hsb color space so i'm going to just write hsv underscore roi hsb we have already learned in the previous video so i'm going not going to go into the details of hsv color space i'm going to just try it cv dot cb t color which is going to convert this uh image into the hsv color space so our input image will be the roi and the next parameter will be cv dot color underscore bgr to hsv okay so we are converting this image to the hsp color space and then we are going to calculate the mask so let's say we define a variable called mask and for the mask we are going to just write cv dot in range so this also we have learned in the hsv tutorial so if you want to learn more about all these functions you can just go to that video so first parameter we are going to pass is our hsv image and the second parameter and the third parameter will be the lower and the upper bound so the lower limit will be 0 dot comma 60 dot comma 32 dot okay so let's pass this and the upper limit so let's define the upper limit also so the third parameter will be the upper limit in the form of the tuple but we need to use the numpy for that right so numpy dot array and inside that we just passed this tuple value which will be 180 dot 255.2255 okay so 180 dot comma 255 dot comma 255. so why we use the in range function because for the histogram only hue is considered from hsv right so the first uh channel right and also to avoid the false value due to low light or low light value we use the in range function okay so these low light values are discarded using the inner range function and then in the next step we are going to calculate our histogram value so i'm going to define this variable called roi hist this also we have learned in the previous videos i'm not going to go into the details so i'm going to just use the function called calc hist which takes the first parameter which will be the image so i'm going to just pass our hsv roi so just pass hsb underscore roi the second value here will be the channels so we are just using only hue channel the first channel in the hsb space so we are going to just write 0 here ask now the next parameter will be the mass so we have already calculated the mass so we are going to just pass this mask parameter here the next parameter will be the hist size so as we have already learned in the previous videos that this hist size uh starts from 0 to 179 so essentially 180 values and then we just need to pass the ranges so as i said it starts from zero to one and now in the next step we are going to just normalize these uh values using the normalized functions so this normalize function takes few values first is the source so the source is our roi hist variable the next value is the destination so let's say we have the same destination we just want to overwrite this roi hist value the next parameter here will be the value of alpha so alpha will start from 0 and the value of beta will be 255 so we want to normalize these values between 0 to 255 okay and then the next value will be the norm type so the norm type we are going to take is cv dot norm min max okay so we are going to just take this one norm min max so all these steps which we have written here is going to give us the histogram back projected image now once we have this histogram back projected image we are going to use this histogram back projected image uh which is also going to change with the moving object so now in the next step we are going to go inside our while loop and read each and every frame one by one and first of all what we are going to do is we are going to calculate the hsv value of the frame as we have done with the first frame also right so we are going to just uh take the frame and then calculate the hsb roi value let's say this time we are going to name it as hsb and we are going to pass frame as the source instead of this roi value now in the next step we are going to use a function called calculate back project so let's define the variable called dest for destination and then cv dot calc back project which is the function for calculating the back projection and this function takes few argument first is the number of images so we only have our hsb image so we are going to pass in the form of the list the second argument will be the channels so as i said we just want to use the hue values here so only one channel so we are going to just write uh 0 so because channel starts from 0 1 2 so that's why i have written 0 here the third parameter is the hist value so in this case our hist value is the roi hist which we have calculated the next parameter is the ranges so we will start from 0 to 180 as we are talking about the hsv color space and the next value will be the scale so let's say scale for now we take one as the scale so this is going to give you the back projected image and then in the next step we are going to apply the mean shift to get the new location so i'm going to just write ret comma track window so i'm going to just say track window which is this variable which we have already defined and then we are going to just use cv dot mean shift which is going to take few arguments first is the image which is the destination image which we got from the back project function calc back project and next argument will be our track image which is the track window so we have to define this term criteria so i'm going to just write term c rit for criteria and then we have to define this outside this while loop so i'm going to go here and we are going to set up the termination criteria either 10 iterations or move by at least one point okay so we are going to define that criteria so here in the curly brackets we are going to just say cv dot term criteria esp or cv dot term criteria count so because we want to either provide the termination criteria for either 10 iterations so we just give 10 or we want to uh terminate by moving at least one point this is the criteria for the mean shift and we are providing these two criteria so once we have our track window for the car we can draw a rectangle with the help of this track window and this will be visible on our video so we are going to draw that uh window we are going to just say x comma y comma w comma h for x y width and height and this will be our track window and then we are going to just draw a rectangle so i'm going to just say we have the final image and then we are going to just write cv dot rectangle which is going to take the frame and then the point for the first point of the rectangle and the second point of the rectangle which are the coordinates of that point so the first point coordinates will be x comma y and the second point coordinate will be x plus width comma y plus height okay and then the next value will be the color let's say we want to use 255 here and the thickness so thickness we want to take three here for example now we can just uh show this final image using i'm sure method so till now we were just uh showing our original frame so we can just say let's say we want to show the final image here also if you want to see the back projected image you can just uh use this destination so we can print this destination uh image also and see how does this back projected image looks like so let's run this code and you can see this car window is dragged right so as the car moves this window also moves once this car goes out of the scope it tracks the other window so this is how the mean shift algorithm works in opencv now as i said this mean shift has few disadvantages or limitations the first limitation is the size of the target window does not change so as we have seen once this car is coming near to us the size of this window is not changing it remains always same so this is one problem the second problem is we have to give the initial position of our region of interest for example if initial position of the region of interest is not known then it will be really hard to apply mean shift method so there are these two mean limitations of this mean shift algorithm and we are going to try to solve these uh limitations in the next video when we learn cam shift which stands for continuously adoptive mean shift in the last video we have learned how to use mean shift algorithm to find and track objects in the video in this video we are going to learn camshaft algorithm to track the object in the video so if you have seen the last video we have written this code so we are going to use all this code which we have written in the mean shift uh video tutorial and first of all let me just run this mean shift code which we have written in the last video and we have discussed about this problem of this rectangle which always remains the same even if the object is coming closer to the camera so we need to adopt the window size with the size and rotation of the target so once again the solution came from opencv labs and this time they introduced an algorithm which is called cam shift which stands for continuously adoptive mean shift so this camshaft algorithm applies mean shift first and then once the mean shift converges it updates the size of the window in addition it also calculates the orientation of the best fitting eclipse to it now let's talk about the implementation part of the cam shift so as i said all the code which we have written in the last video will remain the same except one thing which is we have used this mean shift algorithm in the last video and in this video we are going to use the cam shift shift so just write cv dot cam shift and all the parameters also will remain the same which is destination track window and the termination criteria so let's run this code once again and let's see what result came out of this algorithm so you can see this rectangle is changing its size according to the target now this result which we have seen can be better because the camshaft function returns a rotated rectangle that is our result and also the box parameters which are used to be passed as the search window in the next iteration so here when we see the result inside the ret variable so let's print the result inside the ret variable i'm going to just print it using the print function now let's run this code and let's see what this ret variable prints on the terminal so let me just press escape so what is this result so here you will see the value of x and y and also you will see these three values which are your width height and the value of rotation so in camshaft you can also rotate your rectangle according to your object size so now we are going to use all these parameters which are there inside this ret variable and we are going to try to draw the rectangle which might be uh rotating so there will be a different approach other than this rectangle we are going to use that approach to print those points which we got using the ret variable so let's draw that rectangle so here we are going to define a variable called pts and there is a function called cv dot box points so we are going to use that function here which is box points and it takes a few arguments we just need to uh pass our ret variable here so we are going to just pass our ret so let's see what values this is going to give us so i'm going to just print this pts value so i'm going to just print the value of pts now let's run this uh code once again you won't see anything and you will see these values right so it's going to give these floating point values which we need to convert it into the integers and the error was due to this because this is no longer defined right so for that we are going to just convert these points pts into the integer values so i'm going to overwrite this variable pts and then there is a function in numpy which is called int 0 and here when you pass this pts variable it's going to convert those point into the integers and now we can just draw our rectangle but remember this is a rotating rectangle so we cannot use this normal rectangle function so we need to use the other function for drawing those points so i'm going to define this final image variable once again and then i'm going to use cv dot poly lines so there is this function called polylines which can you can use to draw those lines which you get using this points variable so we are going to just uh pass the frame first of all so we need to pass the frame as a first parameter the second parameter will be our pts value and then the third parameter will be the closed or not closed so when we pass true here then this rectangle will be closed right then we need to pass the color so you can pass any color here let's say it zero comma 255 comma zero and then you can also pass the thickness so let's say we just need to give the thickness of two here okay so this is our final image and now we are going to run this example once again and let's see what happens so you can see this rectangle is drawn and it can rotate also with the object so this is how camshaft algorithm works with opencv i hope you've enjoyed this video and i will see you in the next video
