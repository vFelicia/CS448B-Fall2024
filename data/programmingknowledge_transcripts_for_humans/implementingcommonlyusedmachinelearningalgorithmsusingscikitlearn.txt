With timestamps:

00:00 - first we need to understand what machine
00:02 - learning means with the use case or with
00:04 - an example
00:05 - we need to know the different types of
00:06 - machine learning and also how machine
00:08 - learning can be used in different
00:09 - industries in the world
00:11 - for this you can check out the intro to
00:13 - ml video by the programming knowledge
00:15 - channel and also read multiple blogs
00:16 - online which explain ml better
00:19 - the next thing is python programming
00:21 - skills as we all know
00:22 - python is the widely used programming
00:25 - language
00:26 - for machine learning so we must have the
00:28 - python program basics
00:29 - right to start learning machine learning
00:33 - again the programming knowledge channel
00:35 - has passion tutorial videos which you
00:37 - can check out
00:38 - to get your basics right and you can
00:40 - also follow different tutorials and blog
00:41 - posts
00:43 - the next thing is basic mathematical
00:45 - skills
00:47 - machine learning has a lot of
00:49 - mathematics involved behind the scenes
00:50 - and
00:51 - to understand machine learning properly
00:52 - one must
00:54 - have these basics in their bags first is
00:57 - linear algebra basics
00:59 - we should know how to solve a linear
01:00 - equation in one variable or two
01:02 - variables
01:03 - and for probability basics we just need
01:05 - to do the simple very simple basis of
01:07 - probability
01:08 - uh the basic formula and how probability
01:10 - works after this
01:13 - we can branch out to do different types
01:15 - based on what our end goal is so
01:17 - the first is academic machine learning
01:20 - if you want to produce novel research
01:22 - write research papers read research
01:24 - papers
01:25 - build machine learning tools from
01:26 - scratch and understand the deep
01:28 - underlying concepts then
01:30 - you can refer to these prerequisites
01:32 - before jumping into the academic machine
01:34 - learning part of it
01:36 - so let's start the first thing is strong
01:38 - mathematical concepts
01:40 - as you'll be producing novel research
01:41 - new research or try to read
01:43 - complicated research papers and also
01:45 - build tools from scratch
01:46 - you should have a strong mathematical
01:49 - foundation
01:51 - in algebra we must know how logarithms
01:54 - work how matrices work
01:56 - how to perform matrix multiplication and
01:58 - things like that
02:00 - in calculus we should know what does the
02:02 - concept of derivative
02:03 - what is the chain rule partial
02:05 - derivatives and gradients
02:07 - in statistics we should know the simple
02:09 - statistics basics which is mean median
02:11 - mode
02:12 - then how outliers work the ability to
02:15 - read an histogram
02:16 - and some algorithms like conditional
02:18 - probability
02:20 - next is intermediate python programming
02:23 - since we'll be writing some tools from
02:25 - scratch and also write the code for the
02:27 - new
02:28 - type of algorithms which we read or try
02:31 - to implement
02:31 - we should have some intermediate python
02:33 - programming skills and use some
02:35 - complicated
02:36 - structures which can help us write
02:38 - better code for this
02:39 - we can learn list comprehensions lambda
02:42 - functions and also learn some
02:43 - third-party libraries
02:44 - which make our jobs easier and the last
02:47 - thing is strong programming science
02:49 - computer science fundamentals
02:51 - as a computer science graduate or not
02:54 - being one
02:55 - we should have some simple data
02:56 - structures and algorithms in our back
02:58 - just to make sure that the concepts
03:00 - which we implement
03:01 - have been implemented in the most
03:03 - efficient way possible
03:06 - this is academic machine learning this
03:08 - helps you
03:09 - get into academia faster and also will
03:11 - help you have a better learning curve
03:14 - than before the next is industrial
03:16 - machine learning now if you want to
03:19 - work in industry or in a company or be a
03:21 - software engineer with a machine
03:23 - learning
03:23 - specialization if you want to create new
03:26 - products
03:27 - business value and apply existing
03:29 - off-the-shelf tools to solve business
03:30 - problems then
03:31 - you can refer to this as the
03:33 - prerequisite
03:35 - so in a company or being a software
03:37 - developer with a machine learning
03:38 - specialization
03:39 - the first thing which you need to know
03:41 - is intermediate or advanced python
03:42 - programming so
03:44 - as you'll be writing a lot of code and
03:45 - we'll be busy
03:47 - focusing more on writing code than doing
03:49 - research
03:50 - the companies will expect you to know a
03:52 - lot of python
03:54 - to use the tools which are off the shelf
03:57 - again things like list comprehension
03:59 - lambda functions and also
04:00 - understanding how third-party libraries
04:01 - work with machine learning
04:04 - the second thing is similar to the
04:06 - academic machine learning part of it
04:07 - which is strong computer science
04:08 - fundamentals because
04:09 - these fundamentals will be useful no
04:12 - matter which
04:14 - branch are you in you are in and
04:17 - next we come to the most important part
04:19 - data analysis
04:21 - so before trying to implement machining
04:23 - algorithms in a company
04:25 - one must need to know what the data is
04:27 - uh
04:28 - what the data is trying to represent and
04:30 - also understand
04:32 - how the data works so people should have
04:34 - a proficiency with frameworks and tools
04:36 - such as pandas matplotlib and cbon
04:40 - in the upcoming videos we'll talk about
04:41 - how these frameworks work and also
04:44 - look at them with the code lastly
04:47 - people who would like to work in a
04:49 - company as a software engineer or as a
04:51 - data scientist they should have minimum
04:53 - linux skills they should have
04:55 - they should know how to work on the
04:56 - terminal how to work on the command line
04:59 - how to use that to the benefit and
05:02 - also make sure that they have enough
05:06 - to write most of the simple code on the
05:08 - terminal
05:10 - now these are some prerequisites which i
05:13 - feel can give you a bit of a better
05:15 - learning curve when you're trying to
05:16 - jump into machine learning you can also
05:18 - explore both the
05:20 - branches you can try academic machine
05:22 - learning first and you can jump onto
05:24 - industrial
05:24 - or vice versa now
05:27 - let's talk about some resources all the
05:30 - resources to learn these prerequisites
05:31 - will be in the description below and
05:33 - also this notion
05:35 - document which you see here will also be
05:36 - in the description below
05:38 - if you have any questions please use the
05:40 - comment section below to ask them
05:42 - in this video we'll start with what is
05:44 - linear regression
05:45 - the intuition behind the algorithm and
05:47 - understanding all the elements of it
05:49 - let's have an overview of linear
05:50 - regression it is one of the most basic
05:53 - machine learning algorithm
05:55 - and easy to implement the algorithm has
05:57 - already been used in statistics
05:59 - and is a common process using many
06:01 - applications of statistics in the real
06:04 - world
06:06 - so what is linear regression
06:09 - by definition it is used for finding a
06:12 - linear relationship between the target
06:14 - and one or more predictors the idea
06:16 - behind
06:17 - linear regression is to fit the
06:19 - observations
06:20 - of two variables into a linear
06:22 - relationship between them
06:25 - in simple terms the task is to draw the
06:28 - line that is best fitting
06:29 - or closest to the points where the x y
06:32 - coordinates are observations of the two
06:34 - variables
06:34 - which are expected to depend linearly on
06:37 - each other
06:38 - in more simpler terms given two
06:40 - variables x and y
06:42 - the model can predict values of y given
06:44 - future observations of x
06:47 - this idea is used to predict variables
06:50 - in countless situations
06:51 - example the outcome of political
06:54 - elections
06:55 - or the behavior of the stock market or
06:57 - the performance of a professional
06:58 - athlete
07:00 - there are two types of linear regression
07:02 - simple and multiple
07:04 - in this video we'll only cover simple
07:06 - linear regression
07:10 - so we had talked about drawing a line
07:12 - that is closest to the points which are
07:14 - our variables
07:15 - this line can be modeled based on a
07:18 - linear equation shown
07:19 - on the slide here x and y
07:23 - are our variables which will be present
07:24 - in the data set
07:26 - the motive of the linear regression
07:28 - algorithm is to find the best values for
07:30 - a0 and a1
07:32 - which we call the parameters before
07:35 - moving on to the algorithm
07:36 - let's have a look at two important
07:38 - concepts you must know before
07:40 - understanding linear regression
07:44 - cost function the cost function helps us
07:46 - to figure out
07:47 - the best possible values for a0 and a1
07:50 - which would provide the best fit line
07:53 - for the data points
07:55 - the difference between the predicted
07:56 - values and the ground truth measures
07:59 - the error difference we square the error
08:01 - difference and sum over all the data
08:03 - points and divide that value
08:05 - by the total number of data points
08:08 - this provides the average squared error
08:11 - over all the data points
08:12 - therefore this cost function is also
08:14 - known as the mean square
08:15 - error by cost
08:18 - i mean the cost of incorrectly
08:20 - predicting a data point or how far
08:22 - the line is from the point
08:26 - mathematically we find the mean distance
08:28 - between all the points
08:29 - and we want to minimize that distance so
08:31 - that the line fits the data perfectly
08:34 - to minimize the cost function we use a
08:36 - technique called gradient descent
08:39 - the next important concept needed to
08:41 - know linear regression
08:43 - is gradient descent gradient descent is
08:45 - a method of updating
08:46 - a0 and a1 to reduce the cost function
08:50 - the idea is that we start with some
08:53 - values of a0
08:54 - and a1 and then we change these values
08:56 - iteratively to reduce the cost
08:58 - gradient descent helps us how to change
09:00 - the values
09:03 - to update a0 and a1 we take gradients
09:06 - from the cost function
09:07 - to find these gradients we take partial
09:09 - derivatives
09:10 - with respect to a 0 and a1 now to
09:13 - understand how partial derivatives work
09:15 - you would require some calculus but if
09:17 - you don't it is all right
09:18 - you can take it as it is the partial
09:21 - derivatives are the gradients and they
09:23 - are used to update the values of a0 and
09:25 - a1
09:26 - alpha is the learning rate here which is
09:28 - a hyper parameter that you must
09:30 - specify a small learning rate could get
09:34 - you closer to the minima
09:35 - but takes more time to reach the minima
09:38 - a larger earning rate converges sooner
09:40 - but there is a chance that you could
09:41 - overshoot the minima
09:43 - this can be depicted on the slide
09:47 - to implement the algorithm we have two
09:49 - choices we can use
09:50 - the circuit learn library to import the
09:52 - linear regression model and use it
09:54 - directly
09:55 - or we can write our own regression based
09:57 - model based on the equations above
09:59 - in this video we'll implement linear
10:01 - regression using the scikit learn
10:02 - library
10:03 - to learn more about what is linear
10:05 - regression you can check out the link to
10:07 - the video in the description
10:09 - the entire code and the data set can be
10:11 - downloaded using the link in description
10:14 - which will direct you to this github
10:16 - page after this download the data
10:18 - directory
10:19 - and store that in your projects folder
10:24 - let's start with the implementation i'm
10:27 - using a jupyter notebook here
10:28 - but you can implement the same in a
10:30 - single python file as well
10:33 - first we start with importing all the
10:35 - libraries and the dependencies that are
10:37 - required
10:39 - we need the pandas library to manipulate
10:41 - the data set
10:48 - next we nee we import the matplotlib
10:50 - library to visualize our data and the
10:52 - results
10:57 - we use the pi plot here
11:00 - and lastly we need the linear regression
11:03 - model from the circuit learn library
11:05 - which is the main dependency so from
11:09 - sklearn dot linear
11:12 - model we import the linear regression
11:16 - class
11:22 - now we start with reading our data into
11:25 - the code using pandas
11:27 - make sure that the data directory is in
11:29 - the projects folder
11:33 - we use the read csv function here
11:35 - because our data is in the csv format
11:39 - we move inside a data directory and
11:42 - use the advertising data set
11:50 - let's just check if the spelling is
11:51 - right
11:54 - and yes that should be good
11:57 - now to see what the data looks like we
12:00 - use the head function which is data dot
12:02 - head
12:06 - as you can see here the column unnamed 0
12:09 - is redundant and hence
12:11 - we need to remove it to remove a column
12:14 - we can use the drop function
12:15 - in pandas
12:19 - we have to remove the unnamed column
12:23 - name 0 and we specify the axis equal to
12:26 - 1.
12:28 - here axis is equal to 1 to remove the
12:30 - entire column and the axis is equal to 0
12:33 - to remove only an index as you can see
12:36 - in the output
12:37 - the unnamed zero column is being has
12:39 - been removed
12:42 - all right now our data is clean and it
12:44 - is ready for linear regression
12:47 - for simple linear regression let's
12:49 - consider only the effect of
12:51 - tv ads on sales
12:55 - before jumping right into the modeling
12:57 - let's look at what the data looks like
13:01 - we use matplotlib a popular python
13:03 - plotting library to make a scatter plot
13:10 - let's set
13:13 - a size of the plot which can be 16
13:17 - comma 8. then we generate
13:21 - a scatter plot using the scatter
13:24 - function
13:25 - in which we have the tv ads
13:29 - and the sales
13:33 - let's color the
13:37 - scatter plot with a black dot with black
13:40 - dots
13:44 - and as you can see there is a clear
13:47 - relationship between the amount
13:49 - spent on tv ads and the sales
13:54 - let's see how we can generate a linear
13:56 - approximation of this data
13:59 - first we convert these values into
14:01 - vectors and then store them into two
14:03 - variables
14:04 - so x is equal to data of the tv ads
14:10 - their values and we convert them into
14:13 - vectors using the dcf function
14:15 - which is minus 1 and 1 then we do
14:18 - do the same for the sales which is
14:21 - date of sales and their values
14:24 - which are converted into vectors
14:30 - now after this we use the fit function
14:33 - of the linear regression class
14:35 - to fit a line on the x and y values
14:39 - let's name the variable reg which is
14:42 - linear
14:43 - regression object
14:46 - and then we call the fit function on x
14:48 - and y
14:54 - the minimization of the cost function
14:56 - using gradient design works behind the
14:58 - scenes here
14:59 - behind the fit function to learn more
15:01 - about the cost function and how gradient
15:03 - descent works
15:04 - you can check out the introduction to
15:05 - linearization video in the description
15:06 - below
15:08 - now we have fit a straight line to the
15:11 - data set and let's visualize this using
15:12 - a scatter plot again
15:16 - now since the code for visualizing the
15:19 - best fit line is long i'm going to copy
15:20 - paste it but the entire code will be
15:22 - available
15:23 - in the github repo now here
15:27 - first we predict all the values on the x
15:31 - data set and then we use those
15:33 - predictions to make a line
15:34 - on the scatter plot here the dots will
15:38 - be in black
15:39 - and the line will be in blue the x
15:42 - label will be the money spent on the tv
15:44 - ads and the y label will be
15:46 - the sales
15:50 - from the graph it seems that a simple
15:52 - linear regression
15:53 - model can explain the general impact of
15:55 - amounts spent on tv ads and sales
15:59 - this is how we implement linear
16:01 - regression in scikit learn live using
16:03 - the scikit-learn library
16:05 - we'll talk about what is logistic
16:06 - regression classification techniques are
16:09 - an essential part of machine learning
16:10 - and determining applications
16:12 - approximately 70 percent of problems in
16:14 - data science are classification problems
16:17 - there are a lot of possible
16:18 - classification problems that are
16:20 - available
16:20 - but the logic regression is common and
16:22 - is a useful regression method for
16:24 - solving the binary classification
16:26 - problem
16:27 - logistic regression can be used for
16:29 - various classification problems such as
16:31 - spam detection
16:32 - diabetes prediction if a customer will
16:34 - purchase a product or not
16:36 - whether the user will click on a given
16:38 - ad link or not
16:41 - logistic regression is one of the most
16:43 - simple and commonly used machine
16:44 - learning algorithm
16:45 - for two class classification it is a
16:47 - statistical method to predict binary
16:49 - classes
16:50 - it its basic fundamental concepts are
16:53 - also used in deep learning
16:56 - it is a special case of linear
16:57 - regression where the target variable is
16:59 - categorical in nature
17:01 - it uses a log of odds as a dependent
17:05 - variable
17:06 - logistic regression predicts the
17:07 - probability of a binary event
17:10 - utilizing a logic function
17:14 - as we can see here we need to categorize
17:16 - the data in two different categories
17:18 - and our job is to define the line which
17:20 - does that
17:22 - now why is it called logistic regression
17:24 - if it's a classification
17:25 - mechanism contrary to popular belief
17:29 - logistic regression is a regression
17:31 - model the model builds
17:33 - a regression model to predict the
17:34 - probability that a given data entry
17:36 - belongs to the category numbered as one
17:38 - just like linear regression assumes that
17:41 - the data follows a linear function
17:43 - logistic regression models the data
17:46 - using the sigmoid function
17:47 - linear regression gives you continuous
17:49 - output
17:51 - but loyalty regression provides a
17:52 - constant output
17:54 - an example of continuous output would be
17:56 - house price prediction or stock price
17:57 - prediction
17:58 - an example of discrete output is
18:00 - predicting whether a patient has cancer
18:02 - or not
18:02 - or predicting whether a customer will
18:04 - click on an add or not
18:07 - now let's modify the linear regression
18:09 - equation we had seen in the previous
18:11 - video
18:11 - for logistic regression we apply
18:14 - something called as a sigmoid function
18:15 - on the linear linear regression equation
18:18 - let's see what the sigmoid function is
18:22 - the sigmoid function also called the
18:24 - logistic function
18:26 - gives an s-shaped curve that can take
18:28 - any real valued number
18:30 - and map it into a value between 0 and 1.
18:35 - if the curve goes to positive infinity y
18:38 - predicate predicted
18:40 - will be 1 and if the curve goes to
18:41 - negative infinity
18:43 - y predicted will become zero if the
18:46 - output of the sigmoid function
18:47 - is more than zero point five we can
18:49 - classify the outcome as
18:51 - yes or a one and if it is less than zero
18:53 - point five
18:54 - we can classify it as zero or a no for
18:57 - example
18:57 - if the output is 0.75 we can say in
19:00 - terms of probability as
19:02 - there is 75 percent chance that patient
19:04 - will suffer from cancer
19:08 - just like we have a cost function in
19:09 - linear regression we need one
19:11 - for logistic regression as well which
19:13 - has to be reduced to obtain the best fit
19:15 - line
19:16 - but the cost function used in linear
19:18 - regression will not work here
19:20 - if you try to use the linear regression
19:22 - cost function
19:23 - in a logistic regression problem you
19:25 - would end up with a non-convex function
19:27 - a weirdly shaped graph with no easy way
19:29 - to find minimum global point
19:31 - hence we have a different cost function
19:33 - for linear regression
19:37 - for logistic regression the cost
19:38 - function is defined as
19:40 - minus log h of x if y equal to 1 and
19:43 - minus log 1 minus h of x
19:45 - if y equal to 0. this is the cost the
19:48 - algorithm pays
19:50 - if it predicts a value h theta of x
19:52 - while the actual cost label turns out to
19:54 - be y
19:56 - by using this function we will grant the
19:58 - convexity to the function
19:59 - the gradient descent algorithm has to
20:01 - process
20:03 - there is also a mathematical proof of
20:05 - how we get this cost function which is
20:07 - outside the scope of this video
20:09 - the final cost function can be seen at
20:11 - the bottom of the slide
20:15 - now we have the hypothesis function and
20:17 - the cost function and we are almost done
20:19 - it is now time to find the best values
20:21 - for our parameters in the cost function
20:24 - or in other words to minimize the cost
20:26 - function by running the gradient
20:27 - decision algorithm
20:29 - the procedure is identical to what we
20:32 - did for linear regression to
20:34 - understand more about gradient descent
20:36 - please find the link in the description
20:38 - which will explain in regression and
20:39 - also gradient descent
20:42 - to minimize the cost function we have to
20:44 - run the gradient descent function on
20:46 - each parameter
20:47 - and that is how logistic regression
20:49 - works
20:50 - at the end we get the best parameters
20:52 - that can work with the hypothesis
20:54 - function to predict whether
20:56 - a data point belongs to one class or the
20:58 - other
21:00 - now for the implementation we can either
21:03 - use the circuitron library to import the
21:05 - logitech regression model
21:06 - and use it directly or we can also write
21:08 - our own model based on the equations
21:10 - above
21:11 - logistic regression is amongst the most
21:13 - commonly known core machine learning
21:15 - algorithm out there
21:16 - with its cousin linear regression it has
21:19 - many applications in businesses
21:21 - one of which is the pricing optimization
21:23 - in this video
21:24 - we will learn how to code logistic
21:26 - regression in python
21:28 - using the scikit learn library to solve
21:30 - a bit pricing problem
21:33 - let's have some recap logistic
21:35 - regression is a predictive linear model
21:37 - that aims to explain the relationship
21:39 - between
21:40 - a dependent binary variable and one or
21:42 - more independent variables
21:44 - the output of logical regression is a
21:47 - number between 0 and 1
21:48 - which you can think of as being the
21:50 - probability that a given class is true
21:53 - or not the output is between 0 and 1
21:56 - because
21:57 - the output is transformed by a function
21:59 - which is usually
22:00 - the sigmoid function let's start
22:04 - implementing logic regression in python
22:06 - with a very simple example
22:08 - note that the intent of this video is
22:11 - only to implement
22:12 - a very basic logistic regression model
22:15 - using
22:15 - circuit learn without using a trained
22:18 - test split on the data set
22:20 - and with minimum data visualization
22:23 - so let's start first we import all the
22:26 - dependencies that are required
22:30 - we need matplotlib
22:34 - for visualization so we need the pi plot
22:38 - as plt next is
22:42 - numpy to store our data
22:45 - and finally we need the sklearn
22:50 - logistic regression model which we can
22:53 - use
22:56 - to fit our data
23:01 - yeah so the next
23:04 - is that we have to define a data set
23:07 - let's generate a data set
23:09 - that will be using to learn how to apply
23:11 - logistic regression to a pricing problem
23:15 - the bid price is contained in our x
23:17 - variable while the result
23:18 - a binary lost or one category is encoded
23:21 - as
23:22 - one or zero in our y variable
23:26 - here i have defined my own data set but
23:28 - for complicated or more advanced
23:30 - uh examples you can also import a data
23:32 - set from kegel and use that
23:39 - let's go ahead and visualize this data
23:41 - using matplotlib to gain a better
23:42 - understanding of what we're dealing with
23:45 - let's have a scatter plot of x and
23:49 - y and
23:52 - let's actually give it a title of
23:56 - pricing bins
24:00 - and the x label is going to be
24:04 - price and the
24:07 - y label is the binary output 1
24:12 - or loss so status
24:17 - 1 is a 1
24:20 - and 0 is a lost
24:26 - so here each point above represents a
24:30 - build that we participated in
24:32 - on the x-axis you can see the price that
24:34 - was offered and on the y-axis
24:36 - you see the result if we won the bid or
24:40 - not
24:42 - our goal is to use logistic regression
24:45 - to come up with a model that generates
24:46 - the probability of winning
24:48 - or losing a bed at a particular place
24:51 - in python logistic regression is made
24:54 - simple
24:55 - thanks to the circuit learn module for
24:57 - the task at hand we'll be using the
24:59 - logic regression class
25:00 - by the sql linear model
25:04 - so log reg let's start let that be the
25:08 - name of the variable
25:09 - and logistic
25:12 - regression class
25:15 - where the regularization strength
25:18 - c is equal to 1.0
25:22 - and the solver uh let
25:26 - that be lb
25:30 - fgs which is an optimization just like
25:32 - we did in descent
25:34 - and for multi-class we specify ovr
25:37 - because we're using a binary
25:38 - classification problem here so
25:40 - multi-class is equal to
25:44 - ovr for binary classification
25:49 - the next step is to fit the logical
25:51 - equation model by running the fit
25:53 - function of a class
25:54 - and before we do that we transform our x
25:57 - array into a 2d array as is required by
25:59 - the sql model
26:01 - this is because we only have one feature
26:02 - which is the price and if we had more
26:04 - than one feature our array would already
26:06 - be 2d
26:08 - so let's reshape our data
26:12 - as one comma minus one comma one
26:17 - and finally we can fit our model so log
26:20 - reg
26:20 - dot fit capital x and y
26:26 - now we have a model and now let's
26:30 - predict some data if we wanted to run
26:32 - the prediction on a specific price you
26:34 - can also do that
26:35 - as shown so let's print
26:39 - a prediction
26:42 - uh let's say
26:46 - we need to find whether we've lost or
26:48 - won if the price
26:50 - is 110 so as you can see on the graph
26:53 - above
26:54 - if the price is 110 we should be winning
26:58 - so let's try that
27:01 - as you can see when the price is around
27:03 - 110 which is between 100
27:05 - 200 we win the bed and
27:08 - if the price is
27:12 - around 275 we should lose the bet
27:16 - so let's try that again
27:20 - which 275 we should lose the bet
27:24 - as you can see we have lost the bed this
27:27 - is a very basic implementation of
27:29 - florida's regression using the
27:30 - scikit-learn
27:31 - library to understand how the algorithm
27:33 - works on a data set
27:34 - as we have a basic understanding now we
27:36 - can start working with the kegel data
27:38 - set
27:38 - and also study more about data analytics
27:40 - and data visualization
27:42 - this video is introduction to support
27:44 - vector machines
27:45 - for an in-depth understanding please
27:47 - refer to the links in the description
27:52 - support vector machines are perhaps one
27:54 - of the most popular and talked about
27:55 - machine learning algorithms
27:57 - they were extremely popular around the
27:59 - time they were developed in the 1990s
28:01 - and continued to be the go-to method for
28:04 - a high performing algorithm
28:05 - with a little tuning support vector
28:08 - machine
28:09 - is a supervised machine learning
28:11 - algorithm which can be used for
28:12 - classification challenges
28:14 - in addition to performing linear
28:16 - classification svms can efficiently
28:18 - perform on non-linear classification
28:21 - as well so what are support vector
28:25 - machines
28:25 - it is a discriminative classifier
28:28 - formally defined by a separating
28:30 - hyperplane
28:31 - in other words given label training data
28:34 - the algorithm outputs an optimal
28:36 - hyperplane which categorizes new
28:38 - examples
28:41 - in simple terms an svm model is a
28:44 - representation
28:45 - of the examples as points in space
28:47 - mapped
28:48 - so that the examples of the sub examples
28:50 - of the separate categories are divided
28:52 - by a clear gap
28:53 - that is as wide as possible let's
28:56 - visualize this
28:58 - in this graph we can see that the two
29:00 - classes are separated by the largest cap
29:02 - possible
29:03 - the space between the red line and the
29:05 - closest point to the red line is called
29:07 - a margin
29:09 - so for one dimensional data the support
29:12 - vector classifier
29:13 - is a point for two dimensional data the
29:15 - support vector classifier is aligned as
29:17 - seen in the previous slide
29:19 - for three dimensional data the support
29:21 - vector is a plane
29:23 - and for four dimensional or more the
29:25 - support vector classifier
29:26 - is a hyperplane so let's talk about the
29:30 - hyperplane now
29:31 - a hyperplane in an n-dimensional
29:33 - euclidean space
29:35 - is a flat n minus one-dimensional subset
29:38 - of that space
29:39 - that divides the space into two
29:40 - disconnected parts
29:42 - so a line is a hyperplane or even a 2d
29:45 - plane for a 3d data
29:47 - is a hyperplane
29:50 - svm algorithms use a set of mathematical
29:53 - functions that are defined as the kernel
29:56 - sometimes it is not possible to find a
29:58 - hyperplane
29:59 - or a linear decision boundary for some
30:01 - classification problems
30:02 - if we project the data into higher
30:04 - dimension from the original space
30:06 - we may get a hyperplane in the projected
30:08 - dimension that helps to classify the
30:10 - data
30:12 - let's see what we mean here as shown in
30:15 - the figure
30:16 - it is impossible to find a line to
30:18 - separate the two classes
30:19 - green and blue in the input space but
30:23 - after projecting the data into higher
30:24 - dimension
30:26 - we were able to classify the data using
30:29 - the
30:30 - hyper plane hence kernel helps to find
30:34 - a hyper plane in the higher dimension
30:36 - space without increasing
30:37 - the computation cost much usually the
30:40 - computational cost will increase if the
30:42 - dimension of the data increases
30:45 - the mathematics behind how kernels work
30:47 - is out of scope for this video
30:51 - the svm model needs to be solved using
30:54 - optimization procedure
30:55 - you can use a numerical optimization
30:58 - procedure to search for the coefficients
31:00 - of the hyperplane
31:01 - the most popular method for fitting an
31:03 - svm is the sequential minimal
31:06 - optimization smo method that is very
31:09 - efficient
31:10 - it bakes the problem into sub problems
31:12 - that can be solved analytically by
31:14 - calculating
31:15 - rather than numerically by searching or
31:17 - optimizing
31:19 - in the next video we'll implement the
31:21 - support vector machine we have two
31:23 - choices here
31:24 - we can either use the circuit learn
31:25 - library to import large
31:27 - the svm model and use it directly or we
31:30 - can
31:31 - write our own model based on the
31:32 - equations above a support vector machine
31:35 - is a type of supervised machine learning
31:36 - classification algorithm
31:38 - svms were introduced initially in the
31:41 - 1960s
31:42 - and were later refined in 1990s however
31:45 - it is only now that they are becoming
31:47 - extremely popular
31:48 - owing to their ability to achieve
31:50 - brilliant results
31:52 - svms are implemented in a unique way
31:54 - when compared to other machine learning
31:56 - algorithms
31:57 - in this video we'll implement support
31:59 - vector machines with the help of the
32:01 - scikit-learn library
32:04 - for the implementation our task is to
32:06 - predict whether a bank currency note is
32:08 - authentic or not
32:10 - based on four attributes of the note
32:13 - those attributes are
32:14 - the skewness of the wavelet performed
32:16 - image
32:17 - the variance of the image entropy of the
32:19 - image and the kurtosis of the image
32:22 - this is a binary classification problem
32:24 - and we will use the svm algorithm to
32:26 - solve this problem
32:29 - the detailed information about the data
32:32 - and a link to download the data set can
32:33 - be found in the description
32:35 - download the data set and store it
32:37 - locally on a computer where you intend
32:39 - to write the implementation
32:42 - so let's start the implementation by
32:45 - importing all the necessary libraries
32:49 - first we need to import pandas as we
32:52 - need to store
32:53 - our data and the data frame
32:57 - and the next is numpy
33:03 - followed by a matplotlib
33:14 - and then we need the circuit learn
33:22 - modules
33:32 - here we'll be using the train test split
33:35 - module
33:36 - which we have had not in the previous
33:38 - videos for linear knowledge equation
33:39 - implementations
33:41 - but for svm we'll actually have a train
33:43 - test split and see how
33:45 - the algorithm works on the test data
33:49 - next we need to import the
33:53 - svm class which is svc
33:57 - support vector classifier and at last we
34:00 - need to evaluate
34:01 - our algorithm so we'll need some matrix
34:04 - for that
34:10 - and let's use the classification report
34:13 - module
34:15 - and yeah i think we're done
34:19 - so now let's import the data
34:23 - okay i think we have some problem yeah
34:25 - it's a spelling mistake
34:30 - dot metrics
34:36 - i have a lot of spelling mistakes here
34:38 - okay now
34:40 - let's import the data into a program to
34:42 - read the data from the csv file
34:44 - the simplest way is to use the read csv
34:47 - method of the pandas library
34:50 - the following code which i'm going to
34:51 - write is going to read the bank currency
34:53 - node data into a pandas data frame
34:56 - let's have bank data equal to pd dot
35:00 - read csv
35:05 - and the name of the csv file is bill
35:08 - authentication which you can find in the
35:10 - description
35:11 - of the video
35:14 - okay now there are virtually limitless
35:17 - ways to analyze data sets with a variety
35:19 - of python libraries
35:21 - for the sake of simplicity we will only
35:23 - check the dimensions of the data
35:25 - and see the first few rows to see the
35:28 - rows
35:28 - and columns of the data execute the
35:30 - following command
35:34 - and in the output you'll see 137 to
35:38 - comma 5
35:39 - this means that the bank node data set
35:41 - has 1372 rows and five columns
35:48 - now to get a feel of how our dataset
35:50 - actually looks
35:51 - let's actually see the first five rows
35:53 - of the data set using the head command
36:02 - and here you can see the first firozor
36:05 - data set
36:06 - and you can also see the attributes of
36:08 - the dataset are numeric
36:09 - the label is also numeric that is class
36:12 - one or zero
36:18 - let's pre-process the data before
36:20 - training the model
36:21 - data pre-processing involves two steps
36:23 - first dividing the data into attributes
36:26 - and labels
36:27 - and second dividing the data into
36:29 - training and testing sets
36:31 - to divide the data into attributes and
36:33 - labels execute the following
36:34 - code let x equal to the attributes so
36:38 - bank
36:39 - data dot drop
36:47 - one and y equal to
36:50 - bank data class which is the
36:55 - label in the first line of the script in
36:58 - the cell
36:59 - all the columns of the bank data data
37:01 - frame are being stored in
37:03 - x except the class column which is the
37:05 - label column
37:06 - the drop method drops this column in the
37:09 - second line
37:10 - only the class column is being stored in
37:11 - the y variable
37:13 - at this point of time x variable
37:15 - contains attributes while the y variable
37:17 - contains
37:18 - corresponding labels
37:21 - once the data set is divided into
37:23 - attributes and labels
37:25 - the final pre-processing step is to
37:27 - divide the data into training and test
37:29 - sets
37:30 - luckily the model selection library
37:33 - of the cyclic learn library contains the
37:35 - trained test split method
37:36 - that allows us to seamlessly divide the
37:38 - data into training and test sets
37:41 - let's write the code for that x strain
37:44 - comma
37:49 - x test comma y
37:52 - train comma by test
37:55 - is equal to train test
38:00 - split
38:03 - x comma y and we want the test size to
38:06 - be twenty percent
38:14 - uh yeah
38:17 - we have divided the data into training
38:19 - and testing sites
38:20 - now is the time to train our scm on the
38:23 - training data
38:25 - circuit learn contains the svm library
38:27 - which contains built-in classes for
38:29 - different svm algorithms
38:31 - since we are going to perform a
38:32 - classification task we will use the
38:34 - support vector classifier class
38:36 - which is written as svc in the circuit
38:38 - learns svm library
38:40 - this class takes one parameter which is
38:42 - the kernel type
38:43 - this is very important in the case of a
38:45 - simple svm we simply set this parameter
38:48 - as linear
38:49 - since simple scms can only classify
38:51 - linearly separable data
38:53 - let's write the code for that so svc
38:57 - classifier is equal to svc
39:02 - and the kernel is equal to
39:05 - linear
39:09 - the fit method of the fvc fsbc
39:13 - class is called to train the algorithm
39:15 - on the training data
39:16 - which is passed as a parameter to fit
39:18 - the mod fit the men the fit method
39:21 - svc classy fire dot
39:26 - fit
39:28 - x train comma y train
39:35 - to make predictions the predict method
39:37 - of the svc class is used
39:39 - so why prediction is equal to sv
39:44 - classifier dot predict
39:49 - x test
39:52 - and let's actually print vibrate to see
39:54 - our predictions
39:56 - and as you can see the algorithm has
39:59 - been run on the xtest
40:01 - data and all the predictions have been
40:03 - saved in the vibrate variable and we can
40:05 - see the predictions for each of the
40:07 - row for x text now
40:10 - to evaluate the algorithm confusion
40:12 - matrix
40:13 - precision recall and f1 measures are the
40:15 - most commonly used metrics
40:17 - circuit learns matrix library contains
40:19 - the classification report which can be
40:21 - readily used to find out the values for
40:23 - these important metrics
40:25 - so let's actually print the
40:26 - classification report
40:33 - for y test
40:36 - and why spread
40:42 - so here as you can see the most
40:44 - important metric which we can see
40:46 - is the output of the accuracy of
40:48 - algorithm which is 99
40:50 - this is a very basic implementation of
40:53 - svm using the cyclic learn library
40:55 - and now you can go ahead and implement
40:57 - the algorithm on different data sets
40:59 - from kegel etc this video is an
41:01 - introduction to random forest
41:03 - for in-depth understanding
41:06 - please refer to the links in the
41:08 - description a big part of machine
41:10 - learning is classification
41:12 - that is we want to know what class an
41:15 - observation belongs to
41:17 - the ability to precisely classify
41:19 - observations is extremely valuable
41:22 - for various business applications like
41:25 - predicting whether a particular user
41:26 - will buy a product or not
41:28 - or whether a loan has to be given to a
41:31 - person or not
41:32 - in this video we'll talk about the
41:33 - random forest classifier
41:37 - random forest is a flexible easy to use
41:39 - machine learning algorithm that produces
41:41 - even without hyper parameter tuning a
41:43 - great result most of the time
41:46 - it is also one of the most used
41:48 - algorithms because
41:49 - of its simplicity and diversity it can
41:52 - be used for both
41:53 - classification and regression
41:57 - so before learning about random forest
42:00 - one must know how decision trees work
42:03 - so make sure that you know what are
42:05 - decision trees
42:06 - before watching this video you can find
42:08 - a very good explanation
42:10 - of decision trees in the link in the
42:11 - description
42:13 - so let's start what are random forests
42:16 - random forest is a supervised learning
42:18 - algorithm
42:20 - the forest it builds is an ensemble or a
42:23 - group of decision trees
42:24 - usually trained with the bagging method
42:27 - we'll talk about
42:28 - bagging a little late in the video the
42:30 - general idea of bagging method is that a
42:32 - combination of learning models
42:34 - increases the overall result now let's
42:37 - see what this means
42:38 - in simple layman terms random forests
42:42 - build multiple decision trees and merges
42:44 - them together to get a more accurate and
42:46 - stable protection
42:49 - each individual tree in the random
42:52 - forest spits out a class prediction
42:54 - and the class with the most votes
42:57 - becomes our model's prediction
42:59 - in the figure six decision trees predict
43:02 - one and one predicts a zero
43:04 - hence the final prediction of the
43:06 - classifier is one
43:09 - now the fundamental concept behind
43:11 - random forest
43:12 - is a simple but a powerful one the
43:15 - wisdom of crowds
43:16 - now what does that mean a large number
43:19 - of relatively uncorrelated modules or
43:21 - decision trees
43:22 - operating as a committee will outperform
43:25 - any of the individual constituent models
43:27 - for individual decision trees
43:29 - this is the most fundamental concept in
43:31 - random forest
43:32 - the low correlation between models is
43:34 - the key the reason for this wonderful
43:36 - effect is that the trees protect each
43:38 - other
43:39 - from the individual errors while some
43:41 - trees may be wrong
43:42 - many other trees will be right so as to
43:45 - group so as
43:46 - a group of trees are able to move in the
43:48 - correct direction
43:51 - now let's see how the random foils
43:52 - algorithm works
43:54 - first create a bootstrap bootstrapped
43:57 - dataset
43:57 - by randomly selecting a samples from the
44:00 - original data set
44:01 - we can pick sample from or we can pick
44:03 - the same sample more than once
44:06 - then create a decision tree using the
44:08 - bootstrap data set
44:10 - but only use a random subset of columns
44:13 - in each step
44:15 - now go back to step 1 and repeat make a
44:18 - new bootstrap data set
44:20 - and build a tree considering a subset of
44:22 - variables at each step
44:24 - this results in a wide variety of trees
44:27 - now the variety is what makes random
44:30 - forests
44:31 - more effective than individual decision
44:32 - trees
44:35 - now how do we measure or ensure
44:39 - that the trees divers these trees
44:41 - diversify each other
44:43 - bagging helps us here bootstrapping the
44:45 - data and using the aggregate to make a
44:47 - decision is known as bagging
44:49 - random forest takes advantage of this by
44:52 - allowing each individual tree
44:53 - to randomly sample from a data set with
44:56 - replacement resulting
44:57 - in different trees next
45:00 - is feature randomness each tree in a
45:03 - random forest
45:04 - can pick one can pick only from a random
45:07 - subset of features
45:08 - in a normal decision tree when it is
45:10 - time to split a node
45:12 - we consider every possible feature and
45:14 - pick the one that produces the
45:15 - separation between the observation in
45:17 - the left node versus those in the right
45:19 - node
45:20 - in contrast each tree is a random
45:24 - forest can pick only from a random
45:26 - subset of features
45:27 - this forces even more variation amongst
45:30 - the trees in the model and ultimately
45:32 - results in lower correlation across
45:34 - trees and more diversification
45:36 - so since we are selecting random
45:39 - features
45:40 - in multiple decision trees they ensure
45:42 - that
45:43 - this will diversify all of the decision
45:45 - trees when combined together
45:48 - now how do we implement this for the
45:50 - implementation we have two choices
45:52 - we can either use the circuit learn
45:54 - library to import the random forest
45:56 - model and use it directly
45:57 - or we can write our own model from
45:59 - scratch so in this video we will
46:01 - implement random forest using the cyclic
46:03 - learn library
46:05 - let's start with defining a problem now
46:08 - the task here is to predict whether a
46:10 - bank currency note is authentic or not
46:13 - based on four attributes which are the
46:16 - variance of the image
46:17 - wavelet transformed image skewness
46:20 - entropy and kurtosis of the image this
46:24 - is a binary classification problem
46:26 - and we will use a random forest
46:27 - classifier to solve this problem
46:31 - now for the data set you can download
46:33 - and learn more about the data set from
46:35 - the link in the description
46:38 - let's start with importing the required
46:40 - libraries
46:41 - unlike my previous implementations where
46:43 - i import all the libraries at once
46:45 - this time i'll import the library only
46:47 - when it is required
46:50 - so initially we need numpy and pandas
46:52 - library to handle the data so let's
46:54 - start with that let's import
46:56 - pandas as pd and import num
47:01 - pi as np
47:04 - and let that run and we're good to go
47:07 - now let's import the data set into our
47:10 - code
47:10 - so data set equal to
47:15 - pandas dot read csv is the name of the
47:19 - function and the name of the csv file is
47:22 - bill authentication
47:24 - dot csv and i think i'm right here let
47:28 - me just check
47:31 - yes awesome now let's get a high level
47:34 - view of the data set
47:35 - and let's do that by executing the
47:37 - following command so data set
47:39 - dot head and as we can as you know we
47:43 - can see the first five
47:44 - uh rows of the data set and you can see
47:46 - the variance the skewness sculptosis
47:49 - entropy and the class
47:52 - now here when you see the first five
47:55 - rows
47:56 - you can see that the values in the data
47:58 - set are not very well scaled
48:00 - so we will have to scale the data before
48:02 - training it
48:04 - so to do all of that let's divide the
48:06 - data set into x and y variables
48:08 - which are the attributes and the labels
48:10 - so x
48:11 - is going to be data set dot
48:14 - lock to separate
48:17 - the values and let's do 0 to 4
48:22 - so that it's 0 to 3 and fourth is the
48:25 - class
48:25 - from zero indexing and we need the
48:29 - values
48:30 - and let y be equal to data set dot
48:34 - i lock and we just need
48:37 - the last index which is four
48:40 - so as these follow zero indexing
48:44 - we need the first four columns zero to
48:47 - three which is variance q and squared as
48:49 - an entropy in the x
48:51 - variable and the last way last column
48:54 - the fourth column
48:55 - class in the y variable
49:00 - okay after this let's divide the data
49:02 - set into a train and test split
49:05 - so for that we'll need the train test
49:07 - split module from sklearn
49:10 - so sklearn dot model
49:14 - selection we import
49:18 - train test and split
49:22 - now let's have our variables ready
49:25 - so like strain x
49:30 - test uh y train
49:33 - and y test
49:37 - and let's start give a function call
49:40 - here
49:42 - so x and y
49:46 - let the test size be twenty percent so
49:48 - tesla is equal to zero point two
49:50 - and we don't need a random state so
49:52 - let's put that to
49:54 - zero uh i think we're good to go let me
49:57 - just
49:58 - check back again and
50:02 - if we have an error so
50:06 - s size it shouldn't be test should be
50:08 - test size
50:09 - sorry about that yeah and we're good to
50:12 - go
50:13 - so next let's apply some feature scaling
50:15 - on our data so that the data can be
50:17 - uh really scaled and proper when we
50:20 - actually train it
50:21 - so for that we need something from the
50:23 - pre-parsing
50:24 - module of sql so sk learn
50:28 - dot pre processing
50:32 - import standard
50:36 - scalar scalar
50:39 - then let's scale both x train and x test
50:43 - so
50:47 - let's call the class first and make an
50:50 - object so we have sc as the object
50:52 - and now let's scale xtrain first
50:56 - sc dot fit transform
51:06 - x train and
51:09 - x test is equal to sc.fit
51:13 - transform x test
51:19 - and now we have scaled a data set we can
51:22 - train a random forest to
51:24 - solve this classification problem let's
51:27 - do that with the random forest
51:28 - classifier
51:29 - so we can get that from the sk learn
51:31 - ensemble module
51:34 - so on some so
51:37 - since random forest is an ensemble of
51:40 - many decision trees
51:41 - uh the ensemble module will contain the
51:43 - random forest classifier
51:46 - and um for rest
51:50 - classifier and then
51:54 - let's
51:58 - make this ready make the class ready so
52:00 - let's say
52:02 - class if fire is equal to
52:07 - random for this classifier and
52:10 - first we need uh the n estimators so the
52:13 - end
52:13 - decision trees let's say we have 20 of
52:18 - those
52:20 - then a random state of zero again
52:26 - after which we are going to fit our
52:28 - classifier so
52:30 - classifier dot fit
52:33 - x train comma y train
52:39 - and let's actually
52:42 - save our predictions so
52:46 - classifier dot
52:49 - predict x test
52:53 - awesome and let's see what the errors
52:56 - are
52:58 - so i think i have some
53:01 - uh error here so from sk
53:04 - learn hot
53:12 - random forest classifier
53:15 - cannot be
53:22 - let me just see what the error is
53:33 - oh i see it there has to be a capital f
53:36 - sorry about that
53:40 - and yeah we're good to go now the random
53:42 - forest classifier takes in n estimators
53:44 - as a parameter
53:46 - uh the parameter defines the number of
53:48 - trees in a random form list and we are
53:49 - using 20 t's here
53:52 - so for for classification problems the
53:54 - matrix used to evaluate
53:55 - algorithms are accuracy confusion matrix
53:58 - precision recall and f1 values luckily
54:02 - the circuit turn library provides all
54:04 - these metrics out of the box
54:06 - so let's actually use this matrix to see
54:08 - how
54:09 - good our model performed on this data
54:11 - set
54:12 - so from sk learn dot metrics module
54:17 - we're going to import a few things so
54:19 - classy vacation report is going to be
54:21 - the first thing confusion
54:24 - matrix is next and we need the accuracy
54:28 - to see how good
54:29 - our model is and let's just print all of
54:32 - them now
54:34 - so confusion matrix is first
54:40 - and why test why bread
54:44 - next is classification report so
54:48 - classification report
54:52 - again y test and y predicted
54:57 - and the last is the accuracy score
55:01 - for the same parameters
55:06 - and let's see how this works and again
55:10 - we have some errors here
55:11 - so let's solve them cannot import
55:15 - confusion
55:17 - so you have to stop misspelling things
55:20 - wrong
55:22 - yes so as you can see at the last
55:26 - uh print accuracy score our accuracy is
55:29 - 98
55:30 - so that is good enough and we can see
55:33 - the other metrics
55:34 - that are used for classification
55:35 - problems so this was a very simple
55:38 - implementation of random forest with
55:39 - minimum minimum data processing
55:42 - now what you can do is practice more on
55:44 - kegel
55:45 - on a real-life data set which deals with
55:47 - more data processing which can help you
55:49 - understand
55:50 - how a data scientist works in this
55:53 - machine learning playlist
55:55 - until now we have only talked about
55:57 - supervised learning algorithms
55:58 - where we knew what the output of the
56:00 - target variable was
56:02 - in this video we'll explore an
56:03 - unsupervised learning algorithm
56:05 - called k-means clustering
56:08 - k-means clustering is one of the
56:10 - simplest and popular supervised
56:11 - algorithms
56:15 - and there are plethora of real-world
56:17 - applications of k-means clustering which
56:18 - we will talk about in this video
56:20 - and in the next video we will see the
56:22 - implementation of kmes
56:24 - and how easy it is when compared to
56:26 - algorithms like svms
56:27 - and etc
56:30 - now before we jump into the algorithm
56:33 - itself
56:34 - or even supervised learning we must
56:36 - understand what clustering means
56:38 - so clustering is the process of dividing
56:41 - the entire data
56:42 - into groups or also known as clusters
56:45 - based on the patterns in the data
56:47 - let's try to understand that with a
56:49 - simple example
56:52 - a bank wants to give credit card offers
56:54 - to its customers
56:56 - currently they look at the details of
56:58 - each customer and based on this
56:59 - information
57:00 - decide which offer should be given to
57:02 - which customer
57:04 - now the bank can potentially have
57:06 - millions of customers right
57:08 - does it make sense to look at all the
57:10 - details of each customer separately and
57:12 - then make a decision
57:13 - certainly not it is a manual process and
57:16 - will take a huge amount of time
57:18 - so what can the bank do one option is to
57:21 - segment its customers into different
57:23 - groups
57:24 - for instance the bank can group the
57:25 - customers based on their incomes
57:28 - the groups that are shown here are known
57:30 - as clusters
57:31 - and the process of creating these groups
57:33 - is known as clustering
57:35 - awesome now let's talk about super
57:38 - unsupervised learning
57:40 - unsupervised learning is a type of
57:41 - machine learning algorithm
57:43 - used to draw inferences from data sets
57:45 - consisting
57:46 - of input data without labeled responses
57:50 - so to understand all of this let's see
57:52 - how a superfile supervised algorithm
57:54 - works first
57:57 - we have a label data set with the output
57:59 - or target variable
58:00 - in this particular example the task is
58:02 - to predict whether a loan will be
58:03 - approved or not
58:05 - as we have all the data labeled with
58:07 - appropriate targets
58:08 - we call it as supervised learning
58:12 - in clustering we do not have a target to
58:14 - predict
58:15 - we look at the data and try to club
58:17 - similar observations and form different
58:19 - groups
58:20 - hence it is an unsupervised learning
58:22 - algorithm
58:24 - so let's see where this helps us in the
58:27 - real world
58:28 - so starting with customer segmentation
58:30 - as we discussed before about the bank
58:32 - making clusters based on the income for
58:34 - the credit cards
58:36 - next thing is document clustering this
58:38 - is another common application
58:40 - let's say you have multiple documents
58:42 - and you need to cluster similar
58:43 - documents together
58:44 - clustering helps us group these
58:46 - documents such that similar documents
58:48 - are in the same clusters
58:50 - the next is image segmentation we can
58:53 - also use
58:54 - clustering to perform image segmentation
58:56 - here we try to club similar pixels in
58:59 - the image together
59:00 - we can apply clustering to create these
59:03 - clusters having similar pixels in the
59:05 - same group
59:07 - the next is recommendation engines let's
59:09 - say you
59:10 - want to watch or you want to recommend
59:12 - songs to your friends
59:14 - you can look at the songs like by that
59:16 - person and then use clustering to find
59:18 - similar songs and then finally recommend
59:20 - those songs to the person
59:23 - so let's we let's talk about k-means
59:27 - clustering now
59:28 - we have finally arrived the main part of
59:30 - the video now with regards to generating
59:33 - clusters
59:34 - our aim here is to minimize the distance
59:36 - between the points within a cluster
59:39 - there is an algorithm that tries to
59:41 - minimize the distance of the points in a
59:42 - cluster
59:43 - with the centroid this is called the
59:45 - k-means clustering technique
59:49 - the main objective of the k-means
59:50 - algorithm is to minimize the sum of
59:52 - distances
59:53 - distances between the points and the
59:56 - respective cluster centroid
59:58 - let's see how the algorithm works in
60:00 - action
60:02 - so we have this eight points we want to
60:06 - apply
60:07 - we want to apply k means on to create
60:09 - clusters
60:10 - so let's see how we can do that
60:13 - the first step in k means is to pick the
60:15 - number of clusters k
60:18 - next we randomly select the centroid for
60:21 - each cluster
60:22 - let's say we have two clusters so the k
60:24 - is equal to 2 here
60:26 - we then randomly select the centroid
60:31 - step three once we have initialized the
60:34 - centroid we
60:35 - assign each point to the closest cluster
60:37 - centroid
60:38 - here you can see that the points which
60:40 - are closer to the red point
60:42 - are assigned to the right cluster
60:43 - whereas the points which are closer to
60:45 - the green point are assigned to the
60:46 - green cluster
60:49 - now once we have assigned all of the
60:51 - points to
60:52 - either clusters the next step is to
60:53 - compute the centroids of newly formed
60:56 - clusters here the red and green crosses
60:59 - are the new centroids
61:01 - now we repeat steps three and four
61:04 - so essentially there are three ways to
61:07 - stop k-means clustering
61:09 - first is the centroid of newly formed
61:11 - clusters do not change
61:12 - so if the centroids don't change that
61:14 - means we have reached the end
61:16 - and that is the best way we can actually
61:17 - cluster our data
61:19 - second the points remain in the same
61:21 - cluster so if the points remain in the
61:23 - same cluster that means that there is no
61:25 - a further possible way or a possible way
61:28 - to improve our clustering
61:29 - algorithm and the last is the maximum
61:32 - number of iterations that i reach
61:34 - so the number of iterations is
61:37 - a subjective so it depends from person
61:40 - to person
61:41 - so we can actually focus more on the
61:43 - first two points and not in the last
61:44 - point as much
61:47 - so coming to the implementation we have
61:49 - two choices we can either use the cyclic
61:51 - learn library
61:52 - and import the k-means model and use it
61:54 - directly or we can write our own model
61:56 - from scratch
61:57 - so writing our own model from scratch
61:59 - using numpy and python is very easy for
62:01 - k-means
62:02 - but to see how the algorithm works
62:06 - very fastly will implement k-means using
62:09 - the cyclic run library
62:10 - in this video we will use circuit learn
62:12 - to implement the k-means clustering
62:14 - algorithm
62:15 - let's get started first we import all
62:19 - the required libraries
62:20 - so we need matlab
62:24 - sorry matplotlib
62:27 - dot pi plot as plt
62:31 - we need numpy to handle the data
62:35 - and we need the cluster
62:41 - from scikit learn
62:51 - let's wait for it to run the star here
62:54 - indicates that it's currently running
62:56 - and we have
62:57 - run it properly now let's prepare the
62:59 - data
63:00 - let's create a numpy array of 10 rows
63:03 - and two columns
63:04 - so it's better to actually show you how
63:06 - k means is implemented using our own
63:08 - pre-made data set and not a real-time
63:10 - data set because it gets really
63:11 - confusing
63:12 - so we start with a simple handcraft data
63:16 - set and then we move to complicated
63:17 - trailer dataset
63:19 - we create a numpy array of data points
63:20 - because the circuit learn library can
63:22 - work with numpy array type data inputs
63:24 - without requiring any pre-processing so
63:26 - we can directly focus
63:27 - on implementing the algorithm and not
63:29 - worry about pre-crossing in the initial
63:30 - stages of implementation
63:32 - so this is the numpy array
63:38 - now let's visualize the data the written
63:40 - code simply
63:41 - or the code which we're going to write
63:43 - simply plots all the values in the first
63:45 - column of x array against all the values
63:47 - in the second column
63:48 - so let's see what the code looks like so
63:51 - we make a scatter plot
63:56 - and we start from zero
64:02 - and we want y values now
64:08 - and let's give it a label as well while
64:10 - we're at it
64:17 - and this is how our data looks so from
64:19 - the naked eye
64:20 - we have to form two clusters of the
64:22 - above data points
64:24 - we will probably make one cluster of
64:25 - five points on the
64:27 - bottom left and one cluster of five
64:29 - points on the top right
64:30 - let's see if our k-means clustering
64:32 - algorithm does the same or not
64:36 - okay so let's create the clusters now to
64:38 - create a k-means cluster
64:40 - with two clusters simply type the
64:42 - following script
64:43 - so k means equal to the class k
64:46 - means and
64:50 - number of clusters which we want is
64:52 - equal to 2
64:54 - and let's fit the algorithm now
65:00 - to our data set x and you've done that
65:05 - and yes it is just two lines of code to
65:07 - actually run the algorithm
65:09 - in the first line we create a k-means
65:12 - object
65:12 - and pass it to the value 2 as the number
65:15 - of clusters
65:16 - next we simply have to call the fit
65:19 - method on key means
65:20 - and pass the data that we want to
65:22 - cluster
65:23 - which in this case is the x array that
65:25 - we created earlier
65:27 - now let's see what the central values
65:29 - the algorithm generated
65:31 - for the final clusters let's print them
65:43 - and yep those are centroids or the
65:46 - centers
65:47 - the output will be a 2d array of the
65:49 - shape 2 cross 2
65:51 - to see the labels for data point let's
65:53 - execute the following
65:54 - so let's print
65:58 - k means labels so let's do it here again
66:02 - so
66:10 - and those are our two clusters so it is
66:13 - point by point so
66:14 - a cluster zero and cluster one so the
66:17 - output is a one dimensional array of ten
66:19 - elements corresponding to the cluster
66:20 - assigned to our ten data points
66:22 - here the first five points have been
66:24 - clustered together and the last five
66:26 - points have been clustered
66:27 - here zero and one are merely used to
66:29 - represent the cluster ids and have no
66:31 - mathematical significance towards
66:33 - towards each other if there were three
66:35 - clusters
66:36 - the third cluster would have been
66:37 - represented by the digit two
66:41 - let's plot the data point again on the
66:43 - graph and visualize how the data has
66:45 - been clustered
66:46 - this time we will plot the data along
66:48 - with the assigned label so that we can
66:50 - distinguish between the clusters
66:53 - so let's write the code for that we'll
66:55 - make a scatter plot again to see how
66:57 - this works
66:58 - with our data points in x
67:03 - 0 and x colon comma 1
67:09 - and c is going to be k means
67:13 - labels underscore and let
67:16 - the c map be a rainbow so let's see how
67:19 - that works
67:25 - here we are plotting the first column of
67:27 - the x array and needs a second column
67:30 - however in this case we are also passing
67:32 - k means labels as the value for the c
67:34 - parameter
67:35 - that corresponds to the labels
67:38 - the c map rainbow parameter is parse for
67:41 - choosing the color type
67:42 - for different data points so that is how
67:44 - we get the differentiated
67:46 - bluish violet color or the purple color
67:48 - and the red car
67:51 - as expected the first five points in the
67:53 - bottom left have been clustered together
67:55 - displayed with blue while the remaining
67:57 - points in the top right have been
67:59 - clustered together with red so here we
68:00 - have two different opposite
68:02 - scenarios so the bottom has been done
68:04 - with red and the top right has been done
68:06 - with blue
68:08 - let's execute the k-means algorithm with
68:10 - three clusters and see
68:12 - the output graph so let's implement it
68:14 - again
68:15 - k means equal to k means
68:18 - class and now the clusters
68:22 - is equal to three let's
68:25 - fit our data set on this algorithm
68:30 - and
68:33 - plot this again so
68:37 - scatter
68:41 - x again colon and 0
68:49 - c is equal to k means dot
68:53 - labels again and the c
68:56 - map is going to be rainbow
69:01 - and yeah you can see that again the
69:04 - points are close to each other have been
69:05 - clustered together
69:07 - now let's plot the points along with the
69:10 - centroid coordinates of each cluster to
69:12 - see how the centered position
69:14 - affects clustering so here we're going
69:17 - to also point out the
69:18 - centroid of all the clusters which which
69:20 - we can see here so
69:22 - we have we have three clusters here so
69:23 - we'll be plotting the three centroids
69:25 - along with the clusters
69:28 - let's write the code for that we always
69:31 - use scatter plot for kms clustering
69:33 - because it's easier to
69:34 - see the scatter plot when we have to
69:36 - differentiate between the clusters
69:38 - colon and zero
69:42 - and again one
69:47 - the c is k means dot labels again
69:51 - the c map is equal to rainbow
69:57 - now we need to plot the centroids here
70:00 - so let's try that
70:03 - cluster centers
70:07 - if i'm right with that and
70:11 - we need only till the zeroth point and
70:16 - we have to plot that with the y
70:19 - axis so cluster underscore centers
70:22 - underscore
70:24 - colon one and let the color of these
70:27 - points be
70:28 - black
70:31 - awesome let's see how this looks so
70:34 - in the case of three clusters the two
70:36 - points in the middle which are displayed
70:38 - in red
70:38 - have distance closer to the centroid in
70:40 - the middle displayed in back
70:42 - between the two reds as compared to the
70:45 - centroids on the bottom left or top
70:46 - right
70:48 - however if there were two clusters there
70:50 - wouldn't have been
70:51 - a centroid in the center hence the red
70:54 - points
70:55 - would have been clustered together with
70:57 - the bottom left or top right clusters
71:00 - so that was a simple implementation of
71:02 - k-means clustering with our very own
71:04 - handmade data set now you can go ahead
71:07 - and try implementing the algorithm on
71:08 - regular data sets
71:09 - the k nearest neighbors algorithm is a
71:12 - simple
71:13 - easy to implement supervised machine
71:14 - learning algorithm that can be used to
71:16 - solve both classification and regression
71:18 - problems
71:19 - now a supervised machine learning
71:21 - algorithm
71:24 - is one that relies on labeled input data
71:26 - to learn a function that produces an
71:28 - appropriate output when given a new
71:30 - unlabeled data
71:33 - the k n algorithm assumes that similar
71:35 - things exist
71:36 - in close proximity in other words
71:39 - similar things are near to each other
71:42 - we can relate this definition to
71:43 - something like birds of a feather flock
71:46 - together
71:48 - now notice in the image that most of the
71:51 - time
71:51 - similar data points are close to each
71:53 - other the k n algorithm
71:55 - hinges on this assumption being true
71:57 - enough for the algorithm to be useful
72:02 - knn captures the idea of similarity
72:05 - sometimes called distance
72:06 - proximity or closeness with some
72:08 - mathematics we might have learned in our
72:10 - childhood
72:11 - calculating the distance between points
72:13 - on a graph
72:14 - there are many ways to calculate
72:16 - distance and one might
72:17 - one way might be preferable depending on
72:19 - the problem that we are trying to solve
72:22 - however we are going to use something
72:24 - called as the euclidean distance which
72:26 - is a popular and a familiar choice
72:29 - let's see how the knn algorithm works in
72:32 - action
72:33 - first we load the data set next we
72:35 - initialize the number of
72:36 - neighbors which we want which is k in
72:38 - our case
72:39 - now for each example in our data set we
72:42 - calculate the distance
72:43 - between the query example and the
72:45 - current example of the data
72:46 - the distance here being the nuclear
72:48 - distance
72:50 - next we add the distance and the index
72:52 - of the example
72:53 - to an ordered collection for example a
72:55 - dictionary
72:57 - now sort the ordered collection of
72:58 - distances and indices from smallest to
73:00 - largest
73:01 - in ascending order by the distances now
73:04 - let's pick the first k entries from the
73:06 - solid collections
73:08 - get the labels of the selected k entries
73:12 - now if you want to find the mean then
73:14 - that is the regression problem and if
73:16 - you find the mode it's a classification
73:18 - knl algorithm
73:22 - now let's talk about choosing the right
73:24 - value of k
73:26 - to select the k that's right for your
73:28 - data
73:29 - we run the k n algorithm several times
73:31 - with different values of k
73:33 - and choose the k that reduces the number
73:35 - of errors we encounter
73:37 - while maintaining the algorithm's
73:38 - ability to accurately make predictions
73:41 - when given
73:42 - data it hasn't seen before
73:47 - k n has the following advantages
73:50 - the algorithm is simple and easy to
73:52 - implement and we'll see that in the next
73:54 - video
73:55 - there is no need to build a model tune
73:57 - some hyper parameters
73:59 - or even make additional assumptions it
74:01 - is a very simple and straightforward
74:02 - algorithm
74:03 - the algorithm is also versatile it can
74:05 - be used for classification regression
74:07 - and search as well
74:10 - one of the major disadvantages of the
74:12 - algorithm is that it gets significantly
74:14 - slower as the number of examples
74:16 - or variables increase
74:20 - coming to this let's talk about the
74:21 - applications of k n
74:24 - k n can be useful in solving problems
74:27 - that have solutions that depend on
74:28 - identifying similar objects
74:30 - right the nearest neighbors or the
74:32 - nearest similar objects
74:33 - an example of using this would be in
74:36 - recommender systems which is an example
74:38 - application of k n search
74:41 - now uh at a large scale this would look
74:44 - like recommending products on amazon or
74:46 - articles on medium movies on netflix
74:48 - although we can be certain that these
74:50 - companies
74:52 - they all use more efficient means of
74:54 - making recommendations
74:55 - due to enormous volume of data and when
74:57 - you have an enormous volume of data that
74:59 - is when k n
75:01 - starts to suffer so that was
75:04 - a very brief introduction of how the k
75:06 - nearest algorithm works
75:08 - in this video we will implement k
75:10 - nearest neighbors using the cyclic loan
75:11 - library
75:13 - so the k nearest neighbors algorithm is
75:15 - a type of supervised machine learning
75:16 - algorithm
75:18 - k n is extremely easy to implement in
75:20 - its most basic form and yet performs
75:22 - quite
75:23 - complex classification tasks it is a
75:26 - non-parametric learning algorithm which
75:28 - means that it does not assume anything
75:29 - about the underlying data
75:32 - this is an extremely useful feature
75:33 - since most of the real-world data
75:35 - doesn't really follow any theoretical
75:37 - assumptions for example linear
75:39 - scalability or uniform distribution
75:41 - let's start implementing the k n
75:43 - algorithm using cyclic learn now
75:46 - we are going to use the famous iris data
75:48 - set for rkn example
75:50 - the data set consists of four attributes
75:52 - sample width sample length
75:54 - petal width and petal length these are
75:56 - the attributes of specific types of iris
75:58 - plant
75:59 - the task is to predict the classes to
76:02 - which these plants belong
76:04 - there are three classes in the data set
76:06 - i recetosa
76:07 - iris versicolor and iris virginica
76:12 - let's start the implementation by
76:14 - importing some libraries
76:17 - so we need to import numpy
76:20 - as np import matplot
76:27 - lib.pyplot spld
76:32 - and import pandas as pd for data
76:36 - handling
76:39 - now let's import the data set into a
76:41 - notebook and
76:43 - then into a pandas data frame so here we
76:46 - have the url from which we can
76:48 - access the data the url will also be in
76:50 - the description
76:51 - so we assign some names to the columns
76:54 - of a data set
76:55 - and read the data set into the pandas
76:57 - data frame
77:02 - to see what the dataset actually looks
77:03 - like let's execute the following script
77:05 - dataset.head and we can see the first
77:08 - five rows of our dataset now
77:11 - the next step is to split a dataset into
77:13 - its attributes and labels
77:15 - to do so let's write the following
77:17 - script so x equal to data set dot
77:19 - along dot
77:22 - values y is equal to the same thing
77:28 - but
77:32 - minus 1 it's going to be 4 because the
77:35 - last
77:36 - column and
77:39 - let's fix this and we are good
77:45 - the x variable here contains the first
77:47 - four columns of the data set
77:49 - or the attributes while the y contain
77:51 - the labels
77:53 - to avoid overfitting we will divide a
77:55 - dataset into training and test splits
77:57 - which gives us which gives us a better
77:59 - idea as to how
78:01 - our algorithm perform during the testing
78:03 - phase this way our algorithm is tested
78:06 - on unseen data
78:07 - as it would be in a production
78:08 - application
78:10 - to create training and testing splits
78:12 - let's execute the following script
78:14 - so let's import the test train model
78:18 - from sk learn
78:30 - and let's define some variables
78:34 - so x test y
78:38 - train and y
78:41 - test is equal to test
78:44 - train split that's a very big function
78:47 - name
78:49 - x y and the test size
78:52 - is 0.2 or 20
79:00 - let's wait for it to run so that we can
79:01 - see some
79:04 - so test train split cannot be imported
79:06 - let's see what the
79:09 - error is
79:26 - now so it's strained this split
79:33 - that makes much more sense i get it now
79:37 - and we're good so now the above script
79:40 - splits the data set into 80
79:42 - train data and 20 test data this means
79:45 - that
79:45 - out of total 150 records the training
79:47 - set will contain 120 records and the
79:49 - test set contains 30 of those records
79:52 - now before making any actual predictions
79:54 - it is always a good practice
79:56 - to scale the features so that all of
79:58 - them can be uniformly evaluated
80:01 - the gradient is an algorithm which is
80:03 - used in neural network training and
80:05 - other machining algorithms
80:06 - also converges faster with normalized
80:08 - features
80:09 - so let's write the script for
80:10 - normalization now sk learn
80:18 - pre-processing
80:21 - import standard
80:25 - scalar
80:29 - and the scale r is equal to
80:32 - an object so standard scalar
80:38 - and let's
80:42 - split it now and get a fit
80:47 - x underscore train
80:51 - and let's x train is equal to
80:55 - scalar dot transform
81:00 - extreme x underscore test
81:03 - is equal to
81:07 - scalar transform for test
81:12 - and again i think i've made a mistake
81:14 - with the spellings
81:17 - pre-processing
81:21 - standard scalar i'm sorry
81:25 - yes now let's fit the canon algorithm to
81:29 - the desired
81:30 - dataset it is extremely straightforward
81:33 - to train the k n algorithm and
81:34 - especially makes prediction out of it
81:36 - when using the cyclical library so let's
81:40 - import
81:40 - our model from circuit learn so sk learn
81:43 - dot
81:45 - neighbors
81:50 - classif file
81:53 - and let me just make sure that the
81:55 - spelling is right now
81:57 - so that we don't have any more errors so
81:59 - k neighbors classifier
82:02 - and the classifier is equal to
82:05 - we have the same name class
82:09 - and let's say we want
82:13 - five neighbors
82:19 - let's fit the classifier now
82:34 - so the first step here was to import the
82:37 - k n n classifier class from the sql
82:39 - neighbors library in the second line
82:42 - we initialize the class with one
82:44 - parameter that is the n neighbors
82:46 - this is basically the value for the k
82:48 - there is no ideal value for k
82:50 - and is selected after testing and
82:52 - evaluation however to start out phi
82:54 - seems to the most commonly used k n
82:56 - algorithm
82:58 - the final step is to make predictions on
83:00 - our test data
83:02 - so why spread is equal to
83:05 - dot predict
83:09 - for x test
83:12 - now let's evaluate the algorithm and see
83:15 - how good it performs
83:16 - for evaluating an algorithm confusion
83:19 - matrix precision recall and f1 score are
83:21 - the most commonly used metrics
83:23 - all of these can be found in the scalar
83:27 - metrics
83:28 - module so from sklearn
83:31 - dot matrix
83:34 - import classification
83:39 - report
83:44 - confusion matrix
83:52 - and let's print them now
84:00 - for y test
84:05 - and y predicted
84:08 - and also print the confusion matrix to
84:11 - see the matrix
84:17 - and the same parameters y test and y
84:21 - and here we can see all the metrics
84:24 - which we need
84:24 - to evaluate our algorithm and how it
84:27 - performs
84:28 - so this was a very basic implementation
84:30 - of k
84:31 - nearest neighbors and after this we can
84:34 - actually go
84:34 - on kegel and download a real-life data
84:37 - set and actually
84:38 - perform and see how k n performs on a
84:41 - real-life data set
84:43 - thank you

Cleaned transcript:

first we need to understand what machine learning means with the use case or with an example we need to know the different types of machine learning and also how machine learning can be used in different industries in the world for this you can check out the intro to ml video by the programming knowledge channel and also read multiple blogs online which explain ml better the next thing is python programming skills as we all know python is the widely used programming language for machine learning so we must have the python program basics right to start learning machine learning again the programming knowledge channel has passion tutorial videos which you can check out to get your basics right and you can also follow different tutorials and blog posts the next thing is basic mathematical skills machine learning has a lot of mathematics involved behind the scenes and to understand machine learning properly one must have these basics in their bags first is linear algebra basics we should know how to solve a linear equation in one variable or two variables and for probability basics we just need to do the simple very simple basis of probability uh the basic formula and how probability works after this we can branch out to do different types based on what our end goal is so the first is academic machine learning if you want to produce novel research write research papers read research papers build machine learning tools from scratch and understand the deep underlying concepts then you can refer to these prerequisites before jumping into the academic machine learning part of it so let's start the first thing is strong mathematical concepts as you'll be producing novel research new research or try to read complicated research papers and also build tools from scratch you should have a strong mathematical foundation in algebra we must know how logarithms work how matrices work how to perform matrix multiplication and things like that in calculus we should know what does the concept of derivative what is the chain rule partial derivatives and gradients in statistics we should know the simple statistics basics which is mean median mode then how outliers work the ability to read an histogram and some algorithms like conditional probability next is intermediate python programming since we'll be writing some tools from scratch and also write the code for the new type of algorithms which we read or try to implement we should have some intermediate python programming skills and use some complicated structures which can help us write better code for this we can learn list comprehensions lambda functions and also learn some thirdparty libraries which make our jobs easier and the last thing is strong programming science computer science fundamentals as a computer science graduate or not being one we should have some simple data structures and algorithms in our back just to make sure that the concepts which we implement have been implemented in the most efficient way possible this is academic machine learning this helps you get into academia faster and also will help you have a better learning curve than before the next is industrial machine learning now if you want to work in industry or in a company or be a software engineer with a machine learning specialization if you want to create new products business value and apply existing offtheshelf tools to solve business problems then you can refer to this as the prerequisite so in a company or being a software developer with a machine learning specialization the first thing which you need to know is intermediate or advanced python programming so as you'll be writing a lot of code and we'll be busy focusing more on writing code than doing research the companies will expect you to know a lot of python to use the tools which are off the shelf again things like list comprehension lambda functions and also understanding how thirdparty libraries work with machine learning the second thing is similar to the academic machine learning part of it which is strong computer science fundamentals because these fundamentals will be useful no matter which branch are you in you are in and next we come to the most important part data analysis so before trying to implement machining algorithms in a company one must need to know what the data is uh what the data is trying to represent and also understand how the data works so people should have a proficiency with frameworks and tools such as pandas matplotlib and cbon in the upcoming videos we'll talk about how these frameworks work and also look at them with the code lastly people who would like to work in a company as a software engineer or as a data scientist they should have minimum linux skills they should have they should know how to work on the terminal how to work on the command line how to use that to the benefit and also make sure that they have enough to write most of the simple code on the terminal now these are some prerequisites which i feel can give you a bit of a better learning curve when you're trying to jump into machine learning you can also explore both the branches you can try academic machine learning first and you can jump onto industrial or vice versa now let's talk about some resources all the resources to learn these prerequisites will be in the description below and also this notion document which you see here will also be in the description below if you have any questions please use the comment section below to ask them in this video we'll start with what is linear regression the intuition behind the algorithm and understanding all the elements of it let's have an overview of linear regression it is one of the most basic machine learning algorithm and easy to implement the algorithm has already been used in statistics and is a common process using many applications of statistics in the real world so what is linear regression by definition it is used for finding a linear relationship between the target and one or more predictors the idea behind linear regression is to fit the observations of two variables into a linear relationship between them in simple terms the task is to draw the line that is best fitting or closest to the points where the x y coordinates are observations of the two variables which are expected to depend linearly on each other in more simpler terms given two variables x and y the model can predict values of y given future observations of x this idea is used to predict variables in countless situations example the outcome of political elections or the behavior of the stock market or the performance of a professional athlete there are two types of linear regression simple and multiple in this video we'll only cover simple linear regression so we had talked about drawing a line that is closest to the points which are our variables this line can be modeled based on a linear equation shown on the slide here x and y are our variables which will be present in the data set the motive of the linear regression algorithm is to find the best values for a0 and a1 which we call the parameters before moving on to the algorithm let's have a look at two important concepts you must know before understanding linear regression cost function the cost function helps us to figure out the best possible values for a0 and a1 which would provide the best fit line for the data points the difference between the predicted values and the ground truth measures the error difference we square the error difference and sum over all the data points and divide that value by the total number of data points this provides the average squared error over all the data points therefore this cost function is also known as the mean square error by cost i mean the cost of incorrectly predicting a data point or how far the line is from the point mathematically we find the mean distance between all the points and we want to minimize that distance so that the line fits the data perfectly to minimize the cost function we use a technique called gradient descent the next important concept needed to know linear regression is gradient descent gradient descent is a method of updating a0 and a1 to reduce the cost function the idea is that we start with some values of a0 and a1 and then we change these values iteratively to reduce the cost gradient descent helps us how to change the values to update a0 and a1 we take gradients from the cost function to find these gradients we take partial derivatives with respect to a 0 and a1 now to understand how partial derivatives work you would require some calculus but if you don't it is all right you can take it as it is the partial derivatives are the gradients and they are used to update the values of a0 and a1 alpha is the learning rate here which is a hyper parameter that you must specify a small learning rate could get you closer to the minima but takes more time to reach the minima a larger earning rate converges sooner but there is a chance that you could overshoot the minima this can be depicted on the slide to implement the algorithm we have two choices we can use the circuit learn library to import the linear regression model and use it directly or we can write our own regression based model based on the equations above in this video we'll implement linear regression using the scikit learn library to learn more about what is linear regression you can check out the link to the video in the description the entire code and the data set can be downloaded using the link in description which will direct you to this github page after this download the data directory and store that in your projects folder let's start with the implementation i'm using a jupyter notebook here but you can implement the same in a single python file as well first we start with importing all the libraries and the dependencies that are required we need the pandas library to manipulate the data set next we nee we import the matplotlib library to visualize our data and the results we use the pi plot here and lastly we need the linear regression model from the circuit learn library which is the main dependency so from sklearn dot linear model we import the linear regression class now we start with reading our data into the code using pandas make sure that the data directory is in the projects folder we use the read csv function here because our data is in the csv format we move inside a data directory and use the advertising data set let's just check if the spelling is right and yes that should be good now to see what the data looks like we use the head function which is data dot head as you can see here the column unnamed 0 is redundant and hence we need to remove it to remove a column we can use the drop function in pandas we have to remove the unnamed column name 0 and we specify the axis equal to 1. here axis is equal to 1 to remove the entire column and the axis is equal to 0 to remove only an index as you can see in the output the unnamed zero column is being has been removed all right now our data is clean and it is ready for linear regression for simple linear regression let's consider only the effect of tv ads on sales before jumping right into the modeling let's look at what the data looks like we use matplotlib a popular python plotting library to make a scatter plot let's set a size of the plot which can be 16 comma 8. then we generate a scatter plot using the scatter function in which we have the tv ads and the sales let's color the scatter plot with a black dot with black dots and as you can see there is a clear relationship between the amount spent on tv ads and the sales let's see how we can generate a linear approximation of this data first we convert these values into vectors and then store them into two variables so x is equal to data of the tv ads their values and we convert them into vectors using the dcf function which is minus 1 and 1 then we do do the same for the sales which is date of sales and their values which are converted into vectors now after this we use the fit function of the linear regression class to fit a line on the x and y values let's name the variable reg which is linear regression object and then we call the fit function on x and y the minimization of the cost function using gradient design works behind the scenes here behind the fit function to learn more about the cost function and how gradient descent works you can check out the introduction to linearization video in the description below now we have fit a straight line to the data set and let's visualize this using a scatter plot again now since the code for visualizing the best fit line is long i'm going to copy paste it but the entire code will be available in the github repo now here first we predict all the values on the x data set and then we use those predictions to make a line on the scatter plot here the dots will be in black and the line will be in blue the x label will be the money spent on the tv ads and the y label will be the sales from the graph it seems that a simple linear regression model can explain the general impact of amounts spent on tv ads and sales this is how we implement linear regression in scikit learn live using the scikitlearn library we'll talk about what is logistic regression classification techniques are an essential part of machine learning and determining applications approximately 70 percent of problems in data science are classification problems there are a lot of possible classification problems that are available but the logic regression is common and is a useful regression method for solving the binary classification problem logistic regression can be used for various classification problems such as spam detection diabetes prediction if a customer will purchase a product or not whether the user will click on a given ad link or not logistic regression is one of the most simple and commonly used machine learning algorithm for two class classification it is a statistical method to predict binary classes it its basic fundamental concepts are also used in deep learning it is a special case of linear regression where the target variable is categorical in nature it uses a log of odds as a dependent variable logistic regression predicts the probability of a binary event utilizing a logic function as we can see here we need to categorize the data in two different categories and our job is to define the line which does that now why is it called logistic regression if it's a classification mechanism contrary to popular belief logistic regression is a regression model the model builds a regression model to predict the probability that a given data entry belongs to the category numbered as one just like linear regression assumes that the data follows a linear function logistic regression models the data using the sigmoid function linear regression gives you continuous output but loyalty regression provides a constant output an example of continuous output would be house price prediction or stock price prediction an example of discrete output is predicting whether a patient has cancer or not or predicting whether a customer will click on an add or not now let's modify the linear regression equation we had seen in the previous video for logistic regression we apply something called as a sigmoid function on the linear linear regression equation let's see what the sigmoid function is the sigmoid function also called the logistic function gives an sshaped curve that can take any real valued number and map it into a value between 0 and 1. if the curve goes to positive infinity y predicate predicted will be 1 and if the curve goes to negative infinity y predicted will become zero if the output of the sigmoid function is more than zero point five we can classify the outcome as yes or a one and if it is less than zero point five we can classify it as zero or a no for example if the output is 0.75 we can say in terms of probability as there is 75 percent chance that patient will suffer from cancer just like we have a cost function in linear regression we need one for logistic regression as well which has to be reduced to obtain the best fit line but the cost function used in linear regression will not work here if you try to use the linear regression cost function in a logistic regression problem you would end up with a nonconvex function a weirdly shaped graph with no easy way to find minimum global point hence we have a different cost function for linear regression for logistic regression the cost function is defined as minus log h of x if y equal to 1 and minus log 1 minus h of x if y equal to 0. this is the cost the algorithm pays if it predicts a value h theta of x while the actual cost label turns out to be y by using this function we will grant the convexity to the function the gradient descent algorithm has to process there is also a mathematical proof of how we get this cost function which is outside the scope of this video the final cost function can be seen at the bottom of the slide now we have the hypothesis function and the cost function and we are almost done it is now time to find the best values for our parameters in the cost function or in other words to minimize the cost function by running the gradient decision algorithm the procedure is identical to what we did for linear regression to understand more about gradient descent please find the link in the description which will explain in regression and also gradient descent to minimize the cost function we have to run the gradient descent function on each parameter and that is how logistic regression works at the end we get the best parameters that can work with the hypothesis function to predict whether a data point belongs to one class or the other now for the implementation we can either use the circuitron library to import the logitech regression model and use it directly or we can also write our own model based on the equations above logistic regression is amongst the most commonly known core machine learning algorithm out there with its cousin linear regression it has many applications in businesses one of which is the pricing optimization in this video we will learn how to code logistic regression in python using the scikit learn library to solve a bit pricing problem let's have some recap logistic regression is a predictive linear model that aims to explain the relationship between a dependent binary variable and one or more independent variables the output of logical regression is a number between 0 and 1 which you can think of as being the probability that a given class is true or not the output is between 0 and 1 because the output is transformed by a function which is usually the sigmoid function let's start implementing logic regression in python with a very simple example note that the intent of this video is only to implement a very basic logistic regression model using circuit learn without using a trained test split on the data set and with minimum data visualization so let's start first we import all the dependencies that are required we need matplotlib for visualization so we need the pi plot as plt next is numpy to store our data and finally we need the sklearn logistic regression model which we can use to fit our data yeah so the next is that we have to define a data set let's generate a data set that will be using to learn how to apply logistic regression to a pricing problem the bid price is contained in our x variable while the result a binary lost or one category is encoded as one or zero in our y variable here i have defined my own data set but for complicated or more advanced uh examples you can also import a data set from kegel and use that let's go ahead and visualize this data using matplotlib to gain a better understanding of what we're dealing with let's have a scatter plot of x and y and let's actually give it a title of pricing bins and the x label is going to be price and the y label is the binary output 1 or loss so status 1 is a 1 and 0 is a lost so here each point above represents a build that we participated in on the xaxis you can see the price that was offered and on the yaxis you see the result if we won the bid or not our goal is to use logistic regression to come up with a model that generates the probability of winning or losing a bed at a particular place in python logistic regression is made simple thanks to the circuit learn module for the task at hand we'll be using the logic regression class by the sql linear model so log reg let's start let that be the name of the variable and logistic regression class where the regularization strength c is equal to 1.0 and the solver uh let that be lb fgs which is an optimization just like we did in descent and for multiclass we specify ovr because we're using a binary classification problem here so multiclass is equal to ovr for binary classification the next step is to fit the logical equation model by running the fit function of a class and before we do that we transform our x array into a 2d array as is required by the sql model this is because we only have one feature which is the price and if we had more than one feature our array would already be 2d so let's reshape our data as one comma minus one comma one and finally we can fit our model so log reg dot fit capital x and y now we have a model and now let's predict some data if we wanted to run the prediction on a specific price you can also do that as shown so let's print a prediction uh let's say we need to find whether we've lost or won if the price is 110 so as you can see on the graph above if the price is 110 we should be winning so let's try that as you can see when the price is around 110 which is between 100 200 we win the bed and if the price is around 275 we should lose the bet so let's try that again which 275 we should lose the bet as you can see we have lost the bed this is a very basic implementation of florida's regression using the scikitlearn library to understand how the algorithm works on a data set as we have a basic understanding now we can start working with the kegel data set and also study more about data analytics and data visualization this video is introduction to support vector machines for an indepth understanding please refer to the links in the description support vector machines are perhaps one of the most popular and talked about machine learning algorithms they were extremely popular around the time they were developed in the 1990s and continued to be the goto method for a high performing algorithm with a little tuning support vector machine is a supervised machine learning algorithm which can be used for classification challenges in addition to performing linear classification svms can efficiently perform on nonlinear classification as well so what are support vector machines it is a discriminative classifier formally defined by a separating hyperplane in other words given label training data the algorithm outputs an optimal hyperplane which categorizes new examples in simple terms an svm model is a representation of the examples as points in space mapped so that the examples of the sub examples of the separate categories are divided by a clear gap that is as wide as possible let's visualize this in this graph we can see that the two classes are separated by the largest cap possible the space between the red line and the closest point to the red line is called a margin so for one dimensional data the support vector classifier is a point for two dimensional data the support vector classifier is aligned as seen in the previous slide for three dimensional data the support vector is a plane and for four dimensional or more the support vector classifier is a hyperplane so let's talk about the hyperplane now a hyperplane in an ndimensional euclidean space is a flat n minus onedimensional subset of that space that divides the space into two disconnected parts so a line is a hyperplane or even a 2d plane for a 3d data is a hyperplane svm algorithms use a set of mathematical functions that are defined as the kernel sometimes it is not possible to find a hyperplane or a linear decision boundary for some classification problems if we project the data into higher dimension from the original space we may get a hyperplane in the projected dimension that helps to classify the data let's see what we mean here as shown in the figure it is impossible to find a line to separate the two classes green and blue in the input space but after projecting the data into higher dimension we were able to classify the data using the hyper plane hence kernel helps to find a hyper plane in the higher dimension space without increasing the computation cost much usually the computational cost will increase if the dimension of the data increases the mathematics behind how kernels work is out of scope for this video the svm model needs to be solved using optimization procedure you can use a numerical optimization procedure to search for the coefficients of the hyperplane the most popular method for fitting an svm is the sequential minimal optimization smo method that is very efficient it bakes the problem into sub problems that can be solved analytically by calculating rather than numerically by searching or optimizing in the next video we'll implement the support vector machine we have two choices here we can either use the circuit learn library to import large the svm model and use it directly or we can write our own model based on the equations above a support vector machine is a type of supervised machine learning classification algorithm svms were introduced initially in the 1960s and were later refined in 1990s however it is only now that they are becoming extremely popular owing to their ability to achieve brilliant results svms are implemented in a unique way when compared to other machine learning algorithms in this video we'll implement support vector machines with the help of the scikitlearn library for the implementation our task is to predict whether a bank currency note is authentic or not based on four attributes of the note those attributes are the skewness of the wavelet performed image the variance of the image entropy of the image and the kurtosis of the image this is a binary classification problem and we will use the svm algorithm to solve this problem the detailed information about the data and a link to download the data set can be found in the description download the data set and store it locally on a computer where you intend to write the implementation so let's start the implementation by importing all the necessary libraries first we need to import pandas as we need to store our data and the data frame and the next is numpy followed by a matplotlib and then we need the circuit learn modules here we'll be using the train test split module which we have had not in the previous videos for linear knowledge equation implementations but for svm we'll actually have a train test split and see how the algorithm works on the test data next we need to import the svm class which is svc support vector classifier and at last we need to evaluate our algorithm so we'll need some matrix for that and let's use the classification report module and yeah i think we're done so now let's import the data okay i think we have some problem yeah it's a spelling mistake dot metrics i have a lot of spelling mistakes here okay now let's import the data into a program to read the data from the csv file the simplest way is to use the read csv method of the pandas library the following code which i'm going to write is going to read the bank currency node data into a pandas data frame let's have bank data equal to pd dot read csv and the name of the csv file is bill authentication which you can find in the description of the video okay now there are virtually limitless ways to analyze data sets with a variety of python libraries for the sake of simplicity we will only check the dimensions of the data and see the first few rows to see the rows and columns of the data execute the following command and in the output you'll see 137 to comma 5 this means that the bank node data set has 1372 rows and five columns now to get a feel of how our dataset actually looks let's actually see the first five rows of the data set using the head command and here you can see the first firozor data set and you can also see the attributes of the dataset are numeric the label is also numeric that is class one or zero let's preprocess the data before training the model data preprocessing involves two steps first dividing the data into attributes and labels and second dividing the data into training and testing sets to divide the data into attributes and labels execute the following code let x equal to the attributes so bank data dot drop one and y equal to bank data class which is the label in the first line of the script in the cell all the columns of the bank data data frame are being stored in x except the class column which is the label column the drop method drops this column in the second line only the class column is being stored in the y variable at this point of time x variable contains attributes while the y variable contains corresponding labels once the data set is divided into attributes and labels the final preprocessing step is to divide the data into training and test sets luckily the model selection library of the cyclic learn library contains the trained test split method that allows us to seamlessly divide the data into training and test sets let's write the code for that x strain comma x test comma y train comma by test is equal to train test split x comma y and we want the test size to be twenty percent uh yeah we have divided the data into training and testing sites now is the time to train our scm on the training data circuit learn contains the svm library which contains builtin classes for different svm algorithms since we are going to perform a classification task we will use the support vector classifier class which is written as svc in the circuit learns svm library this class takes one parameter which is the kernel type this is very important in the case of a simple svm we simply set this parameter as linear since simple scms can only classify linearly separable data let's write the code for that so svc classifier is equal to svc and the kernel is equal to linear the fit method of the fvc fsbc class is called to train the algorithm on the training data which is passed as a parameter to fit the mod fit the men the fit method svc classy fire dot fit x train comma y train to make predictions the predict method of the svc class is used so why prediction is equal to sv classifier dot predict x test and let's actually print vibrate to see our predictions and as you can see the algorithm has been run on the xtest data and all the predictions have been saved in the vibrate variable and we can see the predictions for each of the row for x text now to evaluate the algorithm confusion matrix precision recall and f1 measures are the most commonly used metrics circuit learns matrix library contains the classification report which can be readily used to find out the values for these important metrics so let's actually print the classification report for y test and why spread so here as you can see the most important metric which we can see is the output of the accuracy of algorithm which is 99 this is a very basic implementation of svm using the cyclic learn library and now you can go ahead and implement the algorithm on different data sets from kegel etc this video is an introduction to random forest for indepth understanding please refer to the links in the description a big part of machine learning is classification that is we want to know what class an observation belongs to the ability to precisely classify observations is extremely valuable for various business applications like predicting whether a particular user will buy a product or not or whether a loan has to be given to a person or not in this video we'll talk about the random forest classifier random forest is a flexible easy to use machine learning algorithm that produces even without hyper parameter tuning a great result most of the time it is also one of the most used algorithms because of its simplicity and diversity it can be used for both classification and regression so before learning about random forest one must know how decision trees work so make sure that you know what are decision trees before watching this video you can find a very good explanation of decision trees in the link in the description so let's start what are random forests random forest is a supervised learning algorithm the forest it builds is an ensemble or a group of decision trees usually trained with the bagging method we'll talk about bagging a little late in the video the general idea of bagging method is that a combination of learning models increases the overall result now let's see what this means in simple layman terms random forests build multiple decision trees and merges them together to get a more accurate and stable protection each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model's prediction in the figure six decision trees predict one and one predicts a zero hence the final prediction of the classifier is one now the fundamental concept behind random forest is a simple but a powerful one the wisdom of crowds now what does that mean a large number of relatively uncorrelated modules or decision trees operating as a committee will outperform any of the individual constituent models for individual decision trees this is the most fundamental concept in random forest the low correlation between models is the key the reason for this wonderful effect is that the trees protect each other from the individual errors while some trees may be wrong many other trees will be right so as to group so as a group of trees are able to move in the correct direction now let's see how the random foils algorithm works first create a bootstrap bootstrapped dataset by randomly selecting a samples from the original data set we can pick sample from or we can pick the same sample more than once then create a decision tree using the bootstrap data set but only use a random subset of columns in each step now go back to step 1 and repeat make a new bootstrap data set and build a tree considering a subset of variables at each step this results in a wide variety of trees now the variety is what makes random forests more effective than individual decision trees now how do we measure or ensure that the trees divers these trees diversify each other bagging helps us here bootstrapping the data and using the aggregate to make a decision is known as bagging random forest takes advantage of this by allowing each individual tree to randomly sample from a data set with replacement resulting in different trees next is feature randomness each tree in a random forest can pick one can pick only from a random subset of features in a normal decision tree when it is time to split a node we consider every possible feature and pick the one that produces the separation between the observation in the left node versus those in the right node in contrast each tree is a random forest can pick only from a random subset of features this forces even more variation amongst the trees in the model and ultimately results in lower correlation across trees and more diversification so since we are selecting random features in multiple decision trees they ensure that this will diversify all of the decision trees when combined together now how do we implement this for the implementation we have two choices we can either use the circuit learn library to import the random forest model and use it directly or we can write our own model from scratch so in this video we will implement random forest using the cyclic learn library let's start with defining a problem now the task here is to predict whether a bank currency note is authentic or not based on four attributes which are the variance of the image wavelet transformed image skewness entropy and kurtosis of the image this is a binary classification problem and we will use a random forest classifier to solve this problem now for the data set you can download and learn more about the data set from the link in the description let's start with importing the required libraries unlike my previous implementations where i import all the libraries at once this time i'll import the library only when it is required so initially we need numpy and pandas library to handle the data so let's start with that let's import pandas as pd and import num pi as np and let that run and we're good to go now let's import the data set into our code so data set equal to pandas dot read csv is the name of the function and the name of the csv file is bill authentication dot csv and i think i'm right here let me just check yes awesome now let's get a high level view of the data set and let's do that by executing the following command so data set dot head and as we can as you know we can see the first five uh rows of the data set and you can see the variance the skewness sculptosis entropy and the class now here when you see the first five rows you can see that the values in the data set are not very well scaled so we will have to scale the data before training it so to do all of that let's divide the data set into x and y variables which are the attributes and the labels so x is going to be data set dot lock to separate the values and let's do 0 to 4 so that it's 0 to 3 and fourth is the class from zero indexing and we need the values and let y be equal to data set dot i lock and we just need the last index which is four so as these follow zero indexing we need the first four columns zero to three which is variance q and squared as an entropy in the x variable and the last way last column the fourth column class in the y variable okay after this let's divide the data set into a train and test split so for that we'll need the train test split module from sklearn so sklearn dot model selection we import train test and split now let's have our variables ready so like strain x test uh y train and y test and let's start give a function call here so x and y let the test size be twenty percent so tesla is equal to zero point two and we don't need a random state so let's put that to zero uh i think we're good to go let me just check back again and if we have an error so s size it shouldn't be test should be test size sorry about that yeah and we're good to go so next let's apply some feature scaling on our data so that the data can be uh really scaled and proper when we actually train it so for that we need something from the preparsing module of sql so sk learn dot pre processing import standard scalar scalar then let's scale both x train and x test so let's call the class first and make an object so we have sc as the object and now let's scale xtrain first sc dot fit transform x train and x test is equal to sc.fit transform x test and now we have scaled a data set we can train a random forest to solve this classification problem let's do that with the random forest classifier so we can get that from the sk learn ensemble module so on some so since random forest is an ensemble of many decision trees uh the ensemble module will contain the random forest classifier and um for rest classifier and then let's make this ready make the class ready so let's say class if fire is equal to random for this classifier and first we need uh the n estimators so the end decision trees let's say we have 20 of those then a random state of zero again after which we are going to fit our classifier so classifier dot fit x train comma y train and let's actually save our predictions so classifier dot predict x test awesome and let's see what the errors are so i think i have some uh error here so from sk learn hot random forest classifier cannot be let me just see what the error is oh i see it there has to be a capital f sorry about that and yeah we're good to go now the random forest classifier takes in n estimators as a parameter uh the parameter defines the number of trees in a random form list and we are using 20 t's here so for for classification problems the matrix used to evaluate algorithms are accuracy confusion matrix precision recall and f1 values luckily the circuit turn library provides all these metrics out of the box so let's actually use this matrix to see how good our model performed on this data set so from sk learn dot metrics module we're going to import a few things so classy vacation report is going to be the first thing confusion matrix is next and we need the accuracy to see how good our model is and let's just print all of them now so confusion matrix is first and why test why bread next is classification report so classification report again y test and y predicted and the last is the accuracy score for the same parameters and let's see how this works and again we have some errors here so let's solve them cannot import confusion so you have to stop misspelling things wrong yes so as you can see at the last uh print accuracy score our accuracy is 98 so that is good enough and we can see the other metrics that are used for classification problems so this was a very simple implementation of random forest with minimum minimum data processing now what you can do is practice more on kegel on a reallife data set which deals with more data processing which can help you understand how a data scientist works in this machine learning playlist until now we have only talked about supervised learning algorithms where we knew what the output of the target variable was in this video we'll explore an unsupervised learning algorithm called kmeans clustering kmeans clustering is one of the simplest and popular supervised algorithms and there are plethora of realworld applications of kmeans clustering which we will talk about in this video and in the next video we will see the implementation of kmes and how easy it is when compared to algorithms like svms and etc now before we jump into the algorithm itself or even supervised learning we must understand what clustering means so clustering is the process of dividing the entire data into groups or also known as clusters based on the patterns in the data let's try to understand that with a simple example a bank wants to give credit card offers to its customers currently they look at the details of each customer and based on this information decide which offer should be given to which customer now the bank can potentially have millions of customers right does it make sense to look at all the details of each customer separately and then make a decision certainly not it is a manual process and will take a huge amount of time so what can the bank do one option is to segment its customers into different groups for instance the bank can group the customers based on their incomes the groups that are shown here are known as clusters and the process of creating these groups is known as clustering awesome now let's talk about super unsupervised learning unsupervised learning is a type of machine learning algorithm used to draw inferences from data sets consisting of input data without labeled responses so to understand all of this let's see how a superfile supervised algorithm works first we have a label data set with the output or target variable in this particular example the task is to predict whether a loan will be approved or not as we have all the data labeled with appropriate targets we call it as supervised learning in clustering we do not have a target to predict we look at the data and try to club similar observations and form different groups hence it is an unsupervised learning algorithm so let's see where this helps us in the real world so starting with customer segmentation as we discussed before about the bank making clusters based on the income for the credit cards next thing is document clustering this is another common application let's say you have multiple documents and you need to cluster similar documents together clustering helps us group these documents such that similar documents are in the same clusters the next is image segmentation we can also use clustering to perform image segmentation here we try to club similar pixels in the image together we can apply clustering to create these clusters having similar pixels in the same group the next is recommendation engines let's say you want to watch or you want to recommend songs to your friends you can look at the songs like by that person and then use clustering to find similar songs and then finally recommend those songs to the person so let's we let's talk about kmeans clustering now we have finally arrived the main part of the video now with regards to generating clusters our aim here is to minimize the distance between the points within a cluster there is an algorithm that tries to minimize the distance of the points in a cluster with the centroid this is called the kmeans clustering technique the main objective of the kmeans algorithm is to minimize the sum of distances distances between the points and the respective cluster centroid let's see how the algorithm works in action so we have this eight points we want to apply we want to apply k means on to create clusters so let's see how we can do that the first step in k means is to pick the number of clusters k next we randomly select the centroid for each cluster let's say we have two clusters so the k is equal to 2 here we then randomly select the centroid step three once we have initialized the centroid we assign each point to the closest cluster centroid here you can see that the points which are closer to the red point are assigned to the right cluster whereas the points which are closer to the green point are assigned to the green cluster now once we have assigned all of the points to either clusters the next step is to compute the centroids of newly formed clusters here the red and green crosses are the new centroids now we repeat steps three and four so essentially there are three ways to stop kmeans clustering first is the centroid of newly formed clusters do not change so if the centroids don't change that means we have reached the end and that is the best way we can actually cluster our data second the points remain in the same cluster so if the points remain in the same cluster that means that there is no a further possible way or a possible way to improve our clustering algorithm and the last is the maximum number of iterations that i reach so the number of iterations is a subjective so it depends from person to person so we can actually focus more on the first two points and not in the last point as much so coming to the implementation we have two choices we can either use the cyclic learn library and import the kmeans model and use it directly or we can write our own model from scratch so writing our own model from scratch using numpy and python is very easy for kmeans but to see how the algorithm works very fastly will implement kmeans using the cyclic run library in this video we will use circuit learn to implement the kmeans clustering algorithm let's get started first we import all the required libraries so we need matlab sorry matplotlib dot pi plot as plt we need numpy to handle the data and we need the cluster from scikit learn let's wait for it to run the star here indicates that it's currently running and we have run it properly now let's prepare the data let's create a numpy array of 10 rows and two columns so it's better to actually show you how k means is implemented using our own premade data set and not a realtime data set because it gets really confusing so we start with a simple handcraft data set and then we move to complicated trailer dataset we create a numpy array of data points because the circuit learn library can work with numpy array type data inputs without requiring any preprocessing so we can directly focus on implementing the algorithm and not worry about precrossing in the initial stages of implementation so this is the numpy array now let's visualize the data the written code simply or the code which we're going to write simply plots all the values in the first column of x array against all the values in the second column so let's see what the code looks like so we make a scatter plot and we start from zero and we want y values now and let's give it a label as well while we're at it and this is how our data looks so from the naked eye we have to form two clusters of the above data points we will probably make one cluster of five points on the bottom left and one cluster of five points on the top right let's see if our kmeans clustering algorithm does the same or not okay so let's create the clusters now to create a kmeans cluster with two clusters simply type the following script so k means equal to the class k means and number of clusters which we want is equal to 2 and let's fit the algorithm now to our data set x and you've done that and yes it is just two lines of code to actually run the algorithm in the first line we create a kmeans object and pass it to the value 2 as the number of clusters next we simply have to call the fit method on key means and pass the data that we want to cluster which in this case is the x array that we created earlier now let's see what the central values the algorithm generated for the final clusters let's print them and yep those are centroids or the centers the output will be a 2d array of the shape 2 cross 2 to see the labels for data point let's execute the following so let's print k means labels so let's do it here again so and those are our two clusters so it is point by point so a cluster zero and cluster one so the output is a one dimensional array of ten elements corresponding to the cluster assigned to our ten data points here the first five points have been clustered together and the last five points have been clustered here zero and one are merely used to represent the cluster ids and have no mathematical significance towards towards each other if there were three clusters the third cluster would have been represented by the digit two let's plot the data point again on the graph and visualize how the data has been clustered this time we will plot the data along with the assigned label so that we can distinguish between the clusters so let's write the code for that we'll make a scatter plot again to see how this works with our data points in x 0 and x colon comma 1 and c is going to be k means labels underscore and let the c map be a rainbow so let's see how that works here we are plotting the first column of the x array and needs a second column however in this case we are also passing k means labels as the value for the c parameter that corresponds to the labels the c map rainbow parameter is parse for choosing the color type for different data points so that is how we get the differentiated bluish violet color or the purple color and the red car as expected the first five points in the bottom left have been clustered together displayed with blue while the remaining points in the top right have been clustered together with red so here we have two different opposite scenarios so the bottom has been done with red and the top right has been done with blue let's execute the kmeans algorithm with three clusters and see the output graph so let's implement it again k means equal to k means class and now the clusters is equal to three let's fit our data set on this algorithm and plot this again so scatter x again colon and 0 c is equal to k means dot labels again and the c map is going to be rainbow and yeah you can see that again the points are close to each other have been clustered together now let's plot the points along with the centroid coordinates of each cluster to see how the centered position affects clustering so here we're going to also point out the centroid of all the clusters which which we can see here so we have we have three clusters here so we'll be plotting the three centroids along with the clusters let's write the code for that we always use scatter plot for kms clustering because it's easier to see the scatter plot when we have to differentiate between the clusters colon and zero and again one the c is k means dot labels again the c map is equal to rainbow now we need to plot the centroids here so let's try that cluster centers if i'm right with that and we need only till the zeroth point and we have to plot that with the y axis so cluster underscore centers underscore colon one and let the color of these points be black awesome let's see how this looks so in the case of three clusters the two points in the middle which are displayed in red have distance closer to the centroid in the middle displayed in back between the two reds as compared to the centroids on the bottom left or top right however if there were two clusters there wouldn't have been a centroid in the center hence the red points would have been clustered together with the bottom left or top right clusters so that was a simple implementation of kmeans clustering with our very own handmade data set now you can go ahead and try implementing the algorithm on regular data sets the k nearest neighbors algorithm is a simple easy to implement supervised machine learning algorithm that can be used to solve both classification and regression problems now a supervised machine learning algorithm is one that relies on labeled input data to learn a function that produces an appropriate output when given a new unlabeled data the k n algorithm assumes that similar things exist in close proximity in other words similar things are near to each other we can relate this definition to something like birds of a feather flock together now notice in the image that most of the time similar data points are close to each other the k n algorithm hinges on this assumption being true enough for the algorithm to be useful knn captures the idea of similarity sometimes called distance proximity or closeness with some mathematics we might have learned in our childhood calculating the distance between points on a graph there are many ways to calculate distance and one might one way might be preferable depending on the problem that we are trying to solve however we are going to use something called as the euclidean distance which is a popular and a familiar choice let's see how the knn algorithm works in action first we load the data set next we initialize the number of neighbors which we want which is k in our case now for each example in our data set we calculate the distance between the query example and the current example of the data the distance here being the nuclear distance next we add the distance and the index of the example to an ordered collection for example a dictionary now sort the ordered collection of distances and indices from smallest to largest in ascending order by the distances now let's pick the first k entries from the solid collections get the labels of the selected k entries now if you want to find the mean then that is the regression problem and if you find the mode it's a classification knl algorithm now let's talk about choosing the right value of k to select the k that's right for your data we run the k n algorithm several times with different values of k and choose the k that reduces the number of errors we encounter while maintaining the algorithm's ability to accurately make predictions when given data it hasn't seen before k n has the following advantages the algorithm is simple and easy to implement and we'll see that in the next video there is no need to build a model tune some hyper parameters or even make additional assumptions it is a very simple and straightforward algorithm the algorithm is also versatile it can be used for classification regression and search as well one of the major disadvantages of the algorithm is that it gets significantly slower as the number of examples or variables increase coming to this let's talk about the applications of k n k n can be useful in solving problems that have solutions that depend on identifying similar objects right the nearest neighbors or the nearest similar objects an example of using this would be in recommender systems which is an example application of k n search now uh at a large scale this would look like recommending products on amazon or articles on medium movies on netflix although we can be certain that these companies they all use more efficient means of making recommendations due to enormous volume of data and when you have an enormous volume of data that is when k n starts to suffer so that was a very brief introduction of how the k nearest algorithm works in this video we will implement k nearest neighbors using the cyclic loan library so the k nearest neighbors algorithm is a type of supervised machine learning algorithm k n is extremely easy to implement in its most basic form and yet performs quite complex classification tasks it is a nonparametric learning algorithm which means that it does not assume anything about the underlying data this is an extremely useful feature since most of the realworld data doesn't really follow any theoretical assumptions for example linear scalability or uniform distribution let's start implementing the k n algorithm using cyclic learn now we are going to use the famous iris data set for rkn example the data set consists of four attributes sample width sample length petal width and petal length these are the attributes of specific types of iris plant the task is to predict the classes to which these plants belong there are three classes in the data set i recetosa iris versicolor and iris virginica let's start the implementation by importing some libraries so we need to import numpy as np import matplot lib.pyplot spld and import pandas as pd for data handling now let's import the data set into a notebook and then into a pandas data frame so here we have the url from which we can access the data the url will also be in the description so we assign some names to the columns of a data set and read the data set into the pandas data frame to see what the dataset actually looks like let's execute the following script dataset.head and we can see the first five rows of our dataset now the next step is to split a dataset into its attributes and labels to do so let's write the following script so x equal to data set dot along dot values y is equal to the same thing but minus 1 it's going to be 4 because the last column and let's fix this and we are good the x variable here contains the first four columns of the data set or the attributes while the y contain the labels to avoid overfitting we will divide a dataset into training and test splits which gives us which gives us a better idea as to how our algorithm perform during the testing phase this way our algorithm is tested on unseen data as it would be in a production application to create training and testing splits let's execute the following script so let's import the test train model from sk learn and let's define some variables so x test y train and y test is equal to test train split that's a very big function name x y and the test size is 0.2 or 20 let's wait for it to run so that we can see some so test train split cannot be imported let's see what the error is now so it's strained this split that makes much more sense i get it now and we're good so now the above script splits the data set into 80 train data and 20 test data this means that out of total 150 records the training set will contain 120 records and the test set contains 30 of those records now before making any actual predictions it is always a good practice to scale the features so that all of them can be uniformly evaluated the gradient is an algorithm which is used in neural network training and other machining algorithms also converges faster with normalized features so let's write the script for normalization now sk learn preprocessing import standard scalar and the scale r is equal to an object so standard scalar and let's split it now and get a fit x underscore train and let's x train is equal to scalar dot transform extreme x underscore test is equal to scalar transform for test and again i think i've made a mistake with the spellings preprocessing standard scalar i'm sorry yes now let's fit the canon algorithm to the desired dataset it is extremely straightforward to train the k n algorithm and especially makes prediction out of it when using the cyclical library so let's import our model from circuit learn so sk learn dot neighbors classif file and let me just make sure that the spelling is right now so that we don't have any more errors so k neighbors classifier and the classifier is equal to we have the same name class and let's say we want five neighbors let's fit the classifier now so the first step here was to import the k n n classifier class from the sql neighbors library in the second line we initialize the class with one parameter that is the n neighbors this is basically the value for the k there is no ideal value for k and is selected after testing and evaluation however to start out phi seems to the most commonly used k n algorithm the final step is to make predictions on our test data so why spread is equal to dot predict for x test now let's evaluate the algorithm and see how good it performs for evaluating an algorithm confusion matrix precision recall and f1 score are the most commonly used metrics all of these can be found in the scalar metrics module so from sklearn dot matrix import classification report confusion matrix and let's print them now for y test and y predicted and also print the confusion matrix to see the matrix and the same parameters y test and y and here we can see all the metrics which we need to evaluate our algorithm and how it performs so this was a very basic implementation of k nearest neighbors and after this we can actually go on kegel and download a reallife data set and actually perform and see how k n performs on a reallife data set thank you
