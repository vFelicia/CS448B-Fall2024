With timestamps:

00:00 - in this video we are going to do a new
00:01 - machine learning project that
00:03 - is real time facial recognition so let
00:06 - me give you a brief overview that
00:08 - what is the output of the project then
00:09 - we will move to the roadmap that what we
00:12 - are going to do step by step
00:13 - okay so the output of the project is
00:15 - like this we will open
00:16 - with laptops camera camera or webcam and
00:19 - a colored box uh will surround our face
00:23 - and our name will do that in the box so
00:25 - basically our laptop
00:27 - will recognize our face that who we are
00:30 - and what is our name
00:31 - so let me just give you an example of
00:33 - that
00:35 - yeah so i have just run my test sample
00:37 - and this is my face as you can see
00:48 - so basically our camera is recognizing
00:51 - that what is my name and what is the
00:52 - name associated with that with that face
00:55 - okay so this is a real-time face
00:57 - recognition which our system is doing
00:59 - so let us see that how we will proceed
01:01 - to this project
01:03 - so let me give you a roadblock just that
01:05 - what we are going to do step by step
01:07 - so basically first we are gonna we are
01:10 - gonna feed
01:10 - our video that like we should open with
01:13 - camera and we should record that
01:16 - what face is in front of the camera
01:17 - right so
01:19 - i have made a several a separate snippet
01:22 - of video record
01:24 - it is just to check that uh whether the
01:26 - camera is working or not
01:28 - so by this code we can check that uh our
01:30 - camera is working or not
01:32 - um don't be stressed out i will explain
01:35 - each time of the code step by step i'm
01:36 - just
01:37 - in the roadmap right now okay so this is
01:40 - about just checking
01:41 - this this codes check that whether our
01:44 - camera is working or not
01:45 - okay so this is a real test
01:49 - and after this okay so after this
01:52 - we are gonna we are gonna uh collect our
01:55 - face data
01:56 - so basically in this snippet uh
02:00 - this the function of this code is like
02:01 - this this code
02:03 - opens up a camera and uh ask ask
02:06 - ourselves write our name
02:08 - okay so if i play this sample so
02:11 - it is asking my name so i will enter my
02:14 - name here
02:14 - and after that when i enter the camera
02:17 - will open and it will start recording my
02:19 - face
02:20 - it will start uh memorizing my face
02:23 - and will put each and every data in an
02:26 - array
02:27 - okay i will explain this uh entire thing
02:30 - step by step i'm just like giving an
02:32 - overview
02:33 - overview right now okay so it this code
02:36 - recognizes
02:37 - this code basically collects our facial
02:40 - data
02:41 - and stores it in an array and that area
02:44 - is used for recognition afterwards
02:46 - so after collecting our facial data we
02:49 - will
02:50 - run this code this is basically facial
02:52 - recognition
02:53 - okay so after collecting our data
02:56 - we will store that data in another area
02:59 - and we will start another live streaming
03:02 - i mean
03:02 - we will we'll start with camera again
03:05 - and it will match
03:06 - that this current phase matches with the
03:09 - previous array
03:10 - and it maps down the name of the person
03:13 - with that figure
03:14 - okay so basically first step is data
03:18 - data collection over faces and then
03:20 - after that we will do
03:21 - a face recognition and we will use
03:23 - basically k means algorithm in this
03:26 - and a classifier that is hard cascade
03:28 - classifier
03:29 - okay so this is a brief overview
03:32 - overview of the project that what we are
03:33 - going to do
03:34 - and now i'm gonna explain you okay so
03:37 - before go
03:38 - deeping uh so because before diving into
03:40 - the project
03:42 - let us first see that what is hard
03:44 - casket classifier okay after explaining
03:47 - this hardcasket classifier i will
03:48 - explain you each and every line of the
03:50 - code that what we have
03:51 - we have done step by step okay
03:56 - because of what our casket classifier do
03:58 - is it is a collection of features
04:00 - like edge features light features and
04:02 - many or other features
04:04 - so basic purpose of hard cassette
04:06 - classifier is it detects
04:08 - each and every feature of our face okay
04:10 - so this is a box it moves in every
04:13 - direction
04:14 - in the in the entire image and it
04:17 - just collects the data that where is our
04:19 - eyes where is our nose
04:20 - i'll show you the animation that how it
04:22 - works uh before that list
04:25 - you can just see on this website that
04:26 - how uh what is the basic documentation
04:29 - of our casket classifier
04:30 - um after this i will show you the
04:32 - animation of this
04:34 - okay so i've done the basic of this so
04:37 - if i enter my image here and the hard
04:40 - casket classifier will
04:41 - basically uh classify that where is our
04:44 - eyes and where is
04:45 - our face located in that that entire
04:48 - image okay
04:49 - so as you can see that here is my face
04:51 - and these are my eyes
04:52 - so this simple code which uses hard
04:55 - casket classifier has just recognized
04:57 - that
04:58 - where are my eyes and where is my face
05:00 - in the entire image
05:02 - okay so now let us see that
05:05 - this hard casket animation
05:10 - okay so as you can see that this hard
05:13 - casket classifier is just moving in the
05:15 - entire image
05:16 - and it is just classifying that where is
05:18 - our face
05:19 - and where are the other components of
05:21 - the face so as you can see that
05:24 - uh the edge features the line features
05:26 - and the other features of the
05:28 - casket classifier just checking that
05:31 - where are the main features of our face
05:34 - okay so it is traversing the whole image
05:36 - by increasing the size so just see the
05:39 - animation
05:45 - i have increased the size of the the
05:47 - speed of the video to 5.5 x
05:49 - so you can see clearly
05:53 - okay so this is how hard casket
05:55 - classifier works okay
05:58 - so i hope that you will get and you have
06:00 - got a
06:01 - basic idea of that how and hard casted
06:03 - transfer works
06:04 - so in this video in this project we are
06:08 - we will be using that hard casket
06:09 - classifier to detect our facial
06:12 - components and
06:13 - after detecting that storing that face
06:16 - into an array mapping that with the name
06:18 - of the person and just recognizing that
06:21 - again
06:21 - by our k means algorithm okay so
06:25 - before that before starting data
06:27 - collection let us check that
06:28 - whether the camera is working or not
06:30 - okay so
06:32 - by this code we will just check that
06:35 - whether our camera is working
06:36 - yes or no okay so let me just explain
06:39 - you this code
06:40 - so this code starting with like we will
06:42 - import cv2
06:44 - like we will we are importing opencv
06:46 - library
06:47 - and by this cap is equal to cv2 let me
06:50 - just
06:51 - zoom in zoom in okay so second line that
06:54 - cap is
06:54 - cv dot video capture it means that just
06:58 - open our webcam okay so this line opens
07:01 - our webcam
07:02 - and this loop just tells that
07:06 - if a webcam is reading our image
07:10 - then just uh continue and if it is false
07:13 - then
07:14 - move out of the loop okay so
07:17 - it shows like a video frame it
07:20 - okay so let me just open the camera
07:24 - i'm playing this and
07:40 - so as you can see that our camera has
07:42 - been opened
07:43 - and after that this camera name just the
07:46 - uh the
07:47 - name of uh camera is frame as okay as
07:50 - you can see as you have just seen before
07:52 - and after this i have entered this
07:56 - logic that if i press q q as a key
08:00 - if i press the key q then it
08:03 - breaks the camera and closes the window
08:05 - and the camera will not
08:07 - not be closed by the simple closing
08:10 - function
08:11 - it will be only closed if you enter q
08:14 - okay so this is a logic which i have
08:15 - entered and i will explain to the
08:17 - purpose of that
08:18 - why i have entered this okay and this is
08:21 - a key pressed
08:22 - this line simply means that uh
08:25 - if our currently pressed key matches
08:28 - with this key
08:29 - or not this is that's it and i will
08:32 - explain this in the meaning of this line
08:34 - later in this project when we are doing
08:35 - data collection
08:37 - okay i will explain the logic behind
08:39 - this that why i have
08:40 - bit wise why i have used with bitwise
08:44 - and
08:44 - here with zero xfs okay so
08:48 - for right now you have to just
08:49 - understand that we have just opened up a
08:51 - camera and just checked that
08:53 - if return is equal to false then we will
08:55 - just continue and
08:56 - if our camera is reading then we will
08:59 - just move
09:00 - move forward and just show the camera
09:04 - that and just check that whether the
09:06 - camera is open or not and we'll just
09:08 - uh close that camera by pressing the key
09:10 - q
09:11 - okay and after that we will release our
09:14 - camera
09:14 - tab dot release if when we break out of
09:17 - the loop
09:18 - then we just release a camera by this
09:21 - line
09:22 - this means closing the camera and by
09:25 - this
09:25 - it means that destroying all windows
09:27 - means uh
09:30 - disabling all the cookies that have been
09:32 - formed by opening the camera
09:33 - and deleting all the data which has been
09:36 - captioned
09:37 - in the back memory okay so this is
09:39 - basically a video read
09:41 - we are just checking that our camera is
09:43 - reading the video or not
09:45 - okay so i hope that you have got a clear
09:48 - idea that what
09:49 - is this okay so after that
09:52 - we'll do facial data collection we have
09:55 - covered this video read part like we are
09:57 - just thinking that how video is being
09:59 - read
09:59 - from your webcam of the laptop or
10:02 - computer
10:03 - and how it has been processed and how it
10:05 - is the video is being stopping
10:07 - so in today's video we're gonna see that
10:11 - how the laptop camera or webcam is
10:14 - reading our face
10:15 - data and storing it in an array to
10:18 - like detect further okay so
10:21 - this file phase data consists that how
10:24 - our phase data is being stored
10:26 - in uh the memory of computer and we will
10:29 - process it
10:30 - later when we are doing phase detection
10:32 - using k n algorithm
10:34 - okay so so let us get started about it
10:38 - so i will explain you line by line that
10:40 - what each line is doing
10:42 - so so let's get started so
10:46 - first we import two libraries like
10:48 - opencv and numpy library
10:50 - opencv to like read the camera feed or
10:52 - open the camera
10:53 - and numpy has basic array operations and
10:56 - some more operations okay
10:58 - so by this line we are just reading we
11:00 - are just opening our webcam and
11:02 - um in this variable cap our webcam has
11:05 - been storing
11:06 - the data is being stored in this gap
11:08 - okay so after that
11:10 - we are using like in previous video i've
11:12 - shown you a hard cascade classifier that
11:15 - that is classifying the facial
11:17 - components of the face okay
11:19 - so we have just imported this hard
11:22 - casket classifier in this variable
11:24 - and like this is the location of hard
11:27 - gasket classifier
11:28 - so before starting the project uh just
11:31 - see here
11:32 - it is in my d folder d drive and
11:35 - this folder so this hard cascade
11:38 - classifier file
11:39 - uh we will just download this from the
11:41 - internet it is easily
11:43 - available on github so just download it
11:45 - or i will give the link in the
11:47 - description okay we you have to store
11:50 - this hard casket file
11:52 - in this directory only so like
11:56 - just wait a second so in my data
11:59 - in my d drive i have a folder called
12:02 - face recognition project and in this
12:04 - uh folder i have this file hard casket
12:07 - classifier
12:08 - file okay this is hard casket frontal
12:11 - face
12:12 - so you have to just download this file
12:15 - and
12:16 - this is the mainly classifier we are
12:17 - going to use here so
12:20 - we have just imported this classifier
12:22 - here in this
12:23 - variable so let's move forward
12:26 - the skip variable just means that uh it
12:29 - i will explain this later when we are
12:31 - just
12:31 - using the script variable here for now
12:35 - just ignore it and in this space data
12:38 - area
12:38 - this is basically a list in which we
12:40 - will be storing each face
12:42 - data okay so don't be confused you will
12:46 - get a clear understanding later
12:48 - when we are proceeding towards it and
12:50 - this dataset path
12:52 - is based dataset so this means that
12:55 - the faces which we are storing has to be
12:58 - downloaded or
13:00 - has to be stored in the computer's
13:01 - memory okay so like if i
13:03 - open my camera and store my face so my
13:06 - computer has to download that face and
13:09 - store
13:09 - that file in a particular folder right
13:12 - so
13:13 - i have just created this folder here so
13:17 - so as you can see that in my d drive i
13:19 - have created a folder phase data set by
13:21 - this line
13:22 - okay and this phase data set contains
13:24 - all the
13:25 - files that has been deleted through my
13:27 - camera so if i open it
13:29 - i have just
13:33 - i have just recorded my face and given
13:35 - my name to this
13:36 - like anything and
13:39 - this uh every data of my camera feed is
13:43 - stored in this data folder in this
13:45 - folder okay
13:46 - so let's move forward
13:49 - so um so like when we open our camera
13:52 - then we will enter the name there as i
13:55 - just
13:56 - shown you in the previous video when we
13:58 - open the camera we enter our name
14:00 - and after that video starts recording
14:02 - and our face
14:04 - is starting getting recognized and
14:06 - feeding the camera
14:07 - okay so uh we are just giving the input
14:11 - that
14:11 - when we open the camera just enter our
14:14 - name
14:15 - and index and in the console
14:18 - uh this dialog box will appear in where
14:21 - we have to enter our name
14:23 - don't worry i will just when i will just
14:24 - play this video like when i play this
14:27 - code then you will see that but this
14:29 - line is
14:29 - actually doing here in the console okay
14:33 - okay so let us just now uh play it once
14:37 - so when we play this file then this
14:39 - comes in this console
14:41 - you can see that here it is coming like
14:43 - enter your name
14:44 - like enter the name of person so when i
14:46 - write my name here like
14:47 - this and press enter then the camera
14:49 - will open okay
14:51 - so as of now i'm just stopping this
14:53 - program
14:54 - we will do this later okay so i've just
14:57 - shown you that
14:58 - but how it is being appeared in the
15:00 - console
15:01 - okay so let us move further so now we
15:04 - are opening up a camera and
15:06 - we will store our data feed now okay so
15:09 - in
15:10 - in a while loop which is always true we
15:12 - are just doing this code so let me just
15:14 - explain
15:14 - that what we are doing here okay so
15:18 - this red comma frame is equal to it
15:20 - means that red is basically a boolean
15:22 - variable
15:23 - which just check that camera is over or
15:25 - not and frame basically captures the cam
15:28 - is a variable where we are defining our
15:30 - camera okay
15:32 - so and here we are just checking that
15:35 - if the camera is not open then just
15:37 - continue and
15:39 - don't do this iteration and this lines
15:42 - means that
15:43 - we are just converting our uh
15:46 - like initially we will be having a
15:48 - colorful image like
15:49 - we all are colorful right so initially
15:52 - our images
15:53 - are rgb image and in python we just like
15:57 - rgb like we like v red bgr instead of
16:00 - rgb every time
16:01 - so in this line we are converting our
16:06 - rgb image to a grayscale image okay
16:09 - and to convert that we use this type of
16:12 - syntax like cv2 dot cvt color
16:16 - and frame which is storing the camera
16:19 - feed
16:20 - and cv2 dot color bgr to gray
16:24 - bj to gray means we are converting bgr
16:26 - like r
16:27 - uh it means rgb we are converting rgb
16:31 - to grayscale so this is a syntax
16:35 - so our gray so initially we are having
16:38 - frame
16:39 - in this frame we are we are having a
16:41 - colorful image
16:42 - and now in this gray frame we will have
16:45 - we will be having a grayscale image
16:47 - okay so grayscale we have converted into
16:50 - grayscale limit because
16:51 - in gain and gotham it is much easier to
16:54 - like
16:55 - understand a grayscale image and do the
16:57 - algorithm
16:58 - like do the functionalities in grayscale
17:00 - image it is much easier than a colorful
17:02 - image
17:03 - and it doesn't affect the output output
17:05 - is much better in greyscale
17:07 - okay so when we are processing grayscale
17:09 - okay
17:10 - so let us move forward
17:14 - and this line like faces is equal to
17:16 - face cascade dot detect multiscale
17:19 - okay so i will mx being the parameter
17:22 - that is used here
17:23 - so this is a gray frame that is we just
17:25 - created
17:26 - here and this is a scaling factor this
17:29 - is
17:30 - the number of neighbors okay so
17:33 - let me just give a comment here like
17:39 - frame name parameter frame name
17:42 - scaling factor
17:48 - and number of neighbors
17:51 - that we defined by k and just we write
17:57 - number of
18:01 - number of neighbors okay so uh number of
18:03 - neighbors is basically used in k n and
18:05 - gotham
18:06 - if you don't know that k n in custom for
18:09 - now
18:09 - just skip this line i will explain the
18:12 - each and e functionality of scaling
18:13 - factor
18:14 - k and uh the whole functionality of this
18:17 - line later when
18:18 - we i will be explaining the k algorithm
18:21 - because you will get a clear
18:22 - understanding then
18:23 - okay so so uh just let me give you a
18:27 - quick overview
18:28 - that what is the scaling factor so
18:30 - initially if our image size
18:31 - if we scale down our image to one then
18:35 - if we are doing 1.3 then it simply means
18:37 - that we are just
18:39 - shrinking our image to 30 percent don't
18:42 - worry i will just explain to this
18:43 - step later when we are doing gaining
18:45 - algorithm so for now let us move further
18:48 - so in this line we are just checking
18:49 - that if face is length
18:51 - equal equal to zero like if this
18:54 - um if this face is
18:57 - equally equal to zero then this continue
18:59 - we don't have any phase here
19:01 - okay so as of now just understand that
19:05 - this line like this function just
19:07 - returns you the coordinates of the face
19:09 - captured
19:09 - like if i open the camera and the camera
19:12 - will capture my face
19:13 - then this function will return the
19:15 - coordinates of my face
19:17 - like top left top right bottom left
19:20 - bottom right okay
19:21 - so all the coordinates will be returned
19:23 - by this function
19:24 - to this faces variable okay so this
19:28 - so this lines this function is only
19:29 - doing this this is just
19:31 - telling the coordinates of faces to us
19:33 - okay
19:34 - okay so let me just explain you that
19:36 - what actually this function is returning
19:38 - us
19:38 - and what type of coordinates are they
19:40 - okay so just
19:42 - let me open my notepad
19:46 - okay so this function like this function
19:49 - so let me just select a brush here
19:54 - okay so just function like space cascade
19:56 - dot detect multi scale
19:58 - which is uh this variable faces
20:02 - just means that this basis is a list and
20:06 - this list is storing the cordless of the
20:08 - face
20:09 - so if i draw here like if i draw pieces
20:12 - like like if this is the person here
20:17 - like if this is person one like person
20:19 - one and if this is the person two
20:21 - [Music]
20:23 - okay so these both have faces okay
20:27 - so these are the top coordinates like x
20:30 - y are the top coordinates of the faces
20:32 - and this is the width this is the height
20:36 - okay so a box it returns like this
20:40 - detect multiscale returns a block to you
20:43 - like it returns a box in which we have
20:46 - the top left and
20:49 - we have the top left coordinates of x y
20:52 - and
20:53 - the height is h and the width
20:56 - is w okay so this function just returns
21:01 - x this function returns x
21:04 - y w and z so the w
21:07 - and h x y w and h
21:11 - okay so these are the coordinates and by
21:12 - this w and h we can just calculate it
21:15 - what are what can be these coordinates
21:17 - like
21:17 - this can be like x plus h and y plus w
21:22 - right like you can just calculate it by
21:25 - adding w
21:25 - h in x and y so we can get this these
21:28 - and this coordinate also
21:30 - so this function is just returning as
21:32 - the coordinates so that
21:34 - we can just uh make a box into the field
21:38 - around the face okay that said about
21:41 - this
21:42 - let us move forward okay so now let us
21:44 - see that
21:45 - the further code so this line like faces
21:49 - is sorted faces key lambda x2 into x3
21:53 - so what is it is doing so it is just
21:56 - arranging your faces like if you have
21:58 - multiple faces in the video field
22:00 - like if i show my face my sister's face
22:02 - my brother's face
22:04 - my parents face my every my friend's
22:06 - face so
22:07 - multiple number of faces will come right
22:09 - and we have to sort that
22:10 - faces in increasing or decreasing order
22:12 - as per our need
22:14 - okay so what we are doing here is we are
22:17 - passing the function name sorted and we
22:19 - are just using
22:20 - using a laptop function okay so in this
22:23 - uh this time mean that we are passing
22:26 - our faces list here
22:27 - like in this faces list
22:31 - we have uh x y h and w coordinates
22:35 - uh numbers okay and we pass this list
22:38 - here
22:39 - and we are just calculating so this
22:42 - means that
22:45 - okay so like faces lists like if we have
22:48 - faces
22:49 - then faces contains a list of
22:52 - x y w and
22:56 - h okay so it is zero index this is one
23:00 - index
23:00 - two index is three index okay so the
23:03 - area of image is what
23:04 - area of any image is weight into height
23:08 - okay we went into height and
23:12 - weight into height basically here is
23:14 - faces
23:16 - [Music]
23:19 - interfaces three okay this is width into
23:22 - height and this is basically area
23:24 - and the image which have larger area
23:27 - will obviously come first if we are
23:29 - sorting it in a decreasing order
23:31 - and if the area is less then it will
23:34 - come first in
23:35 - the sorted order if we are sorting it in
23:38 - ascending order right
23:39 - so this so in this here so
23:42 - lambda x is just representing our faces
23:46 - and we are just using the lambda
23:48 - function here if you don't
23:50 - like what is lambda function so let's
23:51 - like just get a quick overview
23:54 - by seeing the documents documentations
23:56 - okay so
23:58 - x is representing this spaces and
24:01 - x2 into x3 basically
24:12 - arranging this in a descending order
24:14 - like we are sorting it in a descending
24:16 - order
24:17 - right so and these like if we are having
24:20 - faces like this first phases this second
24:23 - phase is this
24:24 - third phase is like small face then big
24:27 - face
24:27 - and small place and these all faces will
24:29 - be sorted in a descending order like
24:31 - first become
24:32 - big and small and then small this one
24:35 - okay so in this way we sort our faces
24:39 - and every data will be stored in this
24:44 - in this variable okay so i hope that you
24:47 - get a clear understanding of what we are
24:49 - actually doing here
24:50 - and now just forget this line
24:53 - as of now so now we are just we will
24:56 - just hydrate
24:57 - every phases and you'll just store every
25:00 - face in our data
25:03 - so now we are we are just i'm trending
25:05 - in our faces a
25:07 - so let us see that we are starting a for
25:10 - loop here
25:11 - and for loop is going to end okay
25:14 - and this x y w h is are the coordinates
25:18 - of the face so face and faces means that
25:22 - it is like this is phase one this is
25:25 - phase two this is stage three this is
25:26 - phase four
25:27 - like these are the faces and the
25:30 - coordinates of each and every face
25:32 - will come in this x y w and h variables
25:35 - okay as we iterate over every faces then
25:38 - the
25:38 - uh coordinates of every phase will come
25:41 - in
25:42 - x y w and h okay and this offset
25:45 - simply means that we are providing a
25:47 - padding so if you have done
25:48 - html before before you might know that
25:51 - what is
25:52 - padding which is what is margin so
25:54 - padding is basically uh
26:01 - so if we are having our face here this
26:03 - is our face then this
26:05 - gap is basically padding okay so this
26:08 - gap is padding
26:10 - sorry about the handwriting uh actually
26:12 - pen is quite bold here
26:14 - okay so if you are having a face here
26:16 - like
26:18 - and this is covered in a box then this
26:21 - box
26:22 - is having a padding of five okay
26:25 - so this length is basically five or five
26:28 - carry
26:29 - okay so we have just given the padding
26:31 - of five to like
26:32 - uh show a clear user interface okay
26:37 - so as before we are just having this uh
26:40 - this box as outer box which is which we
26:43 - are having the coordinates x y and
26:46 - everything
26:47 - so now here we are just extracting the
26:49 - face section
26:50 - so this line is basically extracting the
26:53 - section of a face
26:54 - so it is subtracting the offset
26:57 - so basically it is subtracting the
26:59 - padding and
27:01 - it is just uh calculating a particular
27:04 - section of a face
27:05 - initially there was padding and now we
27:07 - are just scattering the particular
27:08 - section of the face
27:09 - okay and after that like you will see
27:12 - you can understand this easily like
27:14 - weird is subtracting offset
27:17 - from length from top and bottom
27:20 - and subtracting from left and right okay
27:23 - we are just subtracting our padding and
27:25 - just getting the section of face by this
27:27 - line okay just
27:28 - go through it once and by this face
27:31 - selection we are just resizing my face
27:33 - to 100 across 100 image
27:36 - okay so i hope that you understand it
27:39 - and now this skip so this tip just means
27:43 - that
27:43 - when we start recording our face then
27:46 - after every second i will skip after
27:50 - every second or
27:50 - after every iteration this tip will
27:53 - increment
27:54 - okay like we have recorded one first
27:56 - phase second phase third phase
27:58 - like a particular uh time frame of the
28:01 - face
28:02 - is being recorded by this script by the
28:05 - script
28:06 - okay so after every 10th phase like
28:09 - when i started the recording when i
28:10 - started recording my face
28:12 - so i recorded at first second second
28:14 - second
28:15 - third second fourth second i'm recording
28:18 - my face at every second
28:19 - okay so when i hit the 10 second of
28:22 - camera
28:23 - i will just do a skip operation okay
28:27 - and after every 10th phase i will be
28:31 - recording this 10th face
28:32 - into my array okay i'll be recording
28:35 - each tenth hydrated phase in my array
28:39 - so that i don't have many faces in my
28:41 - area and i have a good
28:45 - like recognize later so as you can see
28:49 - that
28:50 - like after each 10th 10th iteration
28:54 - i am just appending my face into the
28:56 - face
28:57 - and printing the length of face that's
29:00 - it like understanding these lines this
29:02 - line simply means that
29:03 - we are just creating a rectangular box
29:06 - towards our frame
29:07 - big frame is basically a section of
29:09 - frame a section of face which we are
29:11 - having
29:12 - and this these are the coordinates like
29:15 - top left
29:16 - top left quadrants and these are the
29:18 - bottom right coordinates
29:20 - we pass this these parameters at the
29:22 - color of the box which we have to
29:24 - display
29:24 - okay so this is rgb red green and blue
29:29 - so basically we have given green to its
29:31 - full number and
29:32 - red and blue to zero zero so basically a
29:35 - green color box will
29:37 - uh will appear on our face okay
29:40 - so like let me show you that so so when
29:44 - i
29:44 - when i will open my camera so what will
29:46 - happen like
29:48 - a camera will open like this it will be
29:51 - a frame
29:52 - and my face will be here like i will be
29:54 - standing or sitting here
29:56 - and a green color box will appear around
29:59 - my face like
30:00 - around a section of face a green color
30:02 - box will appear
30:04 - okay and by this like
30:07 - this is basically our gp i have given 0
30:10 - to
30:11 - r 0 to blue and 255 degree 255 is the
30:14 - maximum number that's why a green color
30:16 - box will appear
30:18 - if i give like 255
30:21 - zero then a red color box will appear
30:24 - around my face red color box will appear
30:27 - so this is simply uh this line is simply
30:29 - creating a rectangular box
30:31 - which will appear around this frame
30:34 - frame is basically
30:35 - uh this frame okay
30:38 - so and by this this will just show the
30:42 - frame
30:43 - like it is understandable and this uh
30:46 - this is
30:47 - like that if we have if a webcam
30:50 - is open then if we place a key
30:53 - if we press q then the webcam will just
30:55 - switch off
30:56 - like i will explain the function of this
30:59 - later so let us move
31:00 - forward about this as of now and
31:04 - so now after when we have stored our
31:06 - faces then after that
31:07 - we will we are we will be having a list
31:10 - of faces like we are having a list of
31:11 - faces
31:12 - in which faces our faces data are stored
31:15 - okay
31:16 - so now to implement the kb's algorithm
31:19 - here we have to implement basically kn
31:22 - so it implements only on numpy array so
31:25 - a numpy array works best in
31:27 - some scenarios and the compilation power
31:29 - is pretty much higher then list
31:32 - okay so we will convert our list so face
31:34 - data is basically a list now
31:37 - we will convert this list to a numpy
31:39 - array by this function
31:41 - by np dot array and
31:44 - phase data here we are basically
31:47 - converting our face
31:49 - this is a list here as of now we will we
31:52 - are converting this to a numpy array
31:54 - okay so to have a faster computation
31:57 - and to better fit in algorithm
32:00 - after converting into a numpy array we
32:02 - will just reshape it
32:04 - okay so reshaping is a basic like basic
32:07 - concept
32:08 - you will know it and if you don't know
32:10 - like what is reshaping
32:11 - i would say that just look at the
32:13 - documentation like
32:14 - what it is doing and after this
32:18 - reshaping we are just
32:20 - printing our face not shape phase data
32:22 - shape like what is the
32:24 - shape like what is the shape of the face
32:26 - as of now
32:27 - and after that we are just saving our
32:29 - data in
32:30 - our folder which we have just made
32:34 - like just let me explain you this so
32:36 - this data set path is basically
32:38 - the d drive path which i have just uh
32:41 - stated
32:42 - earlier so so basically this
32:45 - this is a data set path and now
32:48 - we are just storing like current data
32:52 - into this folder okay so to store our
32:55 - data in this folder what we are doing is
32:57 - like we are just saving our current data
32:59 - data path file name and phase data
33:01 - okay so file name is basically this uh
33:05 - when we are entering our name the name
33:06 - is storing it this file name
33:08 - okay so now just see that what we have
33:12 - done
33:12 - we have saved our data set plus file
33:15 - name
33:15 - and phase data and so if we print this
33:20 - it will be like data set like let me
33:22 - just show you if we print this what will
33:24 - happen here
33:25 - so now when we print it like it will
33:27 - come like this if the
33:29 - folder name is my data if data is my
33:31 - folder name then
33:32 - and my file name is if i write my my
33:35 - name
33:36 - like our breadth
33:39 - and extension is basically dot npy
33:44 - because we store in this so this will be
33:47 - our
33:48 - location or source where we are storing
33:51 - the current data
33:52 - okay so this this dot np
33:55 - will contain all the faces all the my
33:58 - face data
33:59 - which is being stored in the camera feed
34:01 - and we will use this data to recognize
34:03 - and pass this data
34:08 - okay and now but this this is this means
34:11 - that we are releasing our camera which
34:13 - we have just opened your webcam and this
34:16 - light is just destroying all the windows
34:17 - which have
34:18 - been created by the camera like
34:20 - basically the frames which have been
34:21 - created
34:22 - here okay so this is the basically
34:24 - everything about
34:26 - base data reading and data collection so
34:29 - now
34:29 - we will see that how we can recognize
34:31 - the data by using
34:32 - gain canon algorithm so this is the
34:35 - basically the code
34:36 - of the face recognition and here i have
34:40 - implemented a canon algorithm that what
34:42 - the algorithm is doing
34:44 - okay so i will explain line by line that
34:46 - every like what
34:48 - every line of functions is doing i'll
34:50 - explain everything and
34:51 - uh if you have any confusion like if you
34:53 - don't want to make
34:55 - alien algorithm from scratch like here
34:57 - in this video i made the
34:59 - this algorithm from scratch you can also
35:01 - use inbuilt library like
35:03 - alien inbuilt library comes it just
35:06 - works in simple one line and here i've
35:09 - used like
35:11 - very lines here to implement it from
35:12 - scratch okay so you can use
35:15 - like direct library function also and
35:17 - you can also like implement it from
35:19 - scratch
35:20 - okay so don't worry about that we will
35:22 - just discuss everything here
35:24 - okay so before starting like a face
35:27 - recognition
35:28 - let me just give you a quick overview
35:31 - that what is
35:32 - kanan and gotham doing okay so now let's
35:34 - see this
35:36 - okay so now let me give you an example
35:38 - of like what is
35:39 - care and in a very very simplest way and
35:42 - i will show you that how
35:43 - our gain can be implemented in this
35:45 - project also
35:46 - okay so let us see that so if we are
35:48 - having uh suppose
35:49 - we are having suppose i have given a
35:53 - small example
35:54 - before like we are having a class one
35:57 - here
35:58 - let me just okay so
36:02 - it is a it is my class one
36:05 - and okay
36:10 - and i am just just let me change my
36:12 - color
36:13 - and this is my class two
36:20 - last two
36:27 - and in between this class 1 and class 2
36:30 - i have
36:31 - a person here like suppose this is me
36:35 - now i have to like go to class 1 or
36:37 - class 2
36:38 - and we have to find that this one belong
36:41 - to which class like
36:42 - this one belongs to this class or this
36:44 - class we don't know as of now
36:46 - so what we do is we will find distance
36:49 - of current point with all the points
36:52 - and find the nearest point to this green
36:55 - point
36:55 - okay we find distance to every point and
36:58 - we'll check that
36:59 - what is the average nearest distance we
37:01 - have so suppose
37:03 - if so what we do is
37:06 - we find distance of this green person to
37:10 - every person of the class and we find
37:12 - the top a person's like suppose
37:15 - we find distance to every every person
37:17 - in the class
37:18 - and after that finding we just
37:20 - categorize the top five
37:22 - person like if we write k is equal to
37:23 - five then these five people
37:25 - are the nearest people to this like if
37:28 - k is equal to five then this three
37:31 - people like this this this
37:33 - this three people are plus one and these
37:35 - two people of class two are the nearest
37:36 - people
37:37 - to this green okay so
37:41 - a is equal to five simply means that
37:43 - these five peoples are the nearest
37:45 - k people to this green color
37:49 - this green person okay now
37:52 - there are three persons of class one and
37:55 - two persons of class two
37:57 - this simply means that the we will just
37:59 - check that
38:00 - at which class we have the most number
38:03 - of
38:04 - uh nearest neighbors okay so this class
38:07 - one has the most number of nearest
38:09 - neighbors to this green
38:11 - green person okay so as class one has
38:13 - three neighbors
38:14 - class two has two nearest tables so
38:16 - obviously we will categorize our
38:18 - this green person to class one because
38:21 - these
38:22 - are more nearest neighbors as compared
38:24 - to plus two
38:25 - okay so here class means class one wins
38:29 - so basically this point belongs to class
38:32 - one
38:32 - is it it is a basic overview like what
38:34 - is gaining and i will explain
38:36 - this in this further video like by
38:38 - giving more detailed examples
38:40 - i explain you the complete like how to
38:42 - take facial data
38:44 - and store into a numpy array and also
38:47 - showing the basic
38:48 - quick overview that what is scaling
38:50 - algorithm and in today's video we're
38:52 - going to see that
38:54 - how can a gotham can be implemented here
38:56 - and
38:57 - why should be implemented here okay so
38:59 - like
39:01 - like in this project we are like
39:02 - recognize recognizing our face
39:05 - with this canon earth kane and gotham
39:07 - but this is not the only way
39:09 - there are many other algorithms like
39:12 - uh convolution neural networks and like
39:15 - many other things by which we can like
39:19 - recognize our face so this is the one
39:21 - this is one of the method came in
39:23 - by which we recognize so let me give you
39:26 - example that how can can
39:28 - be implemented here so just see this
39:31 - so suppose that it is my graph
39:34 - and suppose i have stored the data of
39:38 - five people like uh four people i have
39:41 - state i have uh recorded the data of
39:44 - four people
39:44 - four of my friends and one of one it's
39:47 - me one is me
39:48 - and uh other three are my friends okay
39:52 - and total is i have four people okay
39:56 - one eight one is me and three are
39:57 - friends so
39:59 - just let's see that suppose this is my
40:02 - this is me
40:03 - uh this is these these are my face
40:06 - records like i just
40:08 - i just told you that i am recording my
40:10 - faces
40:11 - after every 10th iteration in and
40:15 - it will depend like when the number of
40:19 - the number of minutes or seconds or
40:21 - hours i will open my camera
40:23 - if my face will be get recorded and and
40:26 - due to that i have i will be having
40:28 - number of faces like if i open my camera
40:30 - for 10 minutes then i will be having a
40:32 - lot of faces recorded
40:34 - if i open my camera for one minute then
40:36 - i will be having only
40:38 - six faces recorded because the my faces
40:41 - are recording after every 10 seconds
40:43 - okay it is just a overview example so
40:46 - suppose
40:46 - i am i am i am having like my
40:50 - six faces and every friend of mine is
40:52 - having six faces
40:54 - okay so this is person one and these are
40:57 - the faces of person one
40:59 - and just let me change the color
41:03 - this is like this is me and
41:06 - suppose this is my friend face
41:10 - okay this is my friend's face
41:13 - and like like friend one and this is me
41:18 - and suppose i'm having my one more
41:22 - friend stay stored
41:26 - okay so this is my friend's three face
41:29 - like
41:29 - a second friend face and
41:42 - this is these are the faces of my friend
41:43 - three okay
41:45 - and now i am just like these are the
41:48 - these are stored in my data
41:50 - these all faces are stored in my data
41:52 - and now i open my camera feed to
41:54 - recognize
41:55 - and use gain in algorithm now so suppose
41:59 - i open my camera and i open up my camera
42:02 - like my camera face has been opened and
42:04 - a random person comes in the camera like
42:07 - it is none of us so it will not match
42:09 - with any of them
42:11 - and it will just simply cannot recognize
42:13 - okay so
42:14 - if i open my camera and i show my face
42:17 - to it like
42:18 - i show my face to it and
42:21 - it will recognize my face as like this
42:24 - so
42:25 - how it is happening so if i open my
42:26 - camera my face will come here
42:29 - like this is my camera and my face will
42:31 - come here
42:32 - so now my face will calculate the
42:34 - distance
42:35 - from every facial data okay so i have
42:38 - just tell you that i have used hard
42:40 - casket classifier
42:41 - so my hard casket class where i am just
42:43 - calculating my
42:45 - difference between faces okay so i will
42:49 - calculate the difference
42:50 - my of my face with every other face is
42:52 - stored okay
42:54 - with every other face is stored i will
42:56 - calculate the difference
42:58 - between my face and other faces okay
43:01 - like every faces okay and
43:04 - i will just calculate like if i take k
43:06 - as
43:07 - 3 then i will just
43:11 - get the three nearest neighbors
43:14 - three nearest neighbors and
43:18 - obviously i will be having the least
43:20 - faced facial difference with me only
43:23 - if i am wearing in the front of the
43:24 - camera then i will be having the least
43:26 - facial difference with me only
43:28 - okay so by this algorithm we can just
43:31 - state that
43:33 - um i am having like
43:36 - this space means this is me and
43:39 - if if it recognize that this phase means
43:42 - like my name is
43:44 - so it will just recognize it and show my
43:47 - name
43:47 - on the box like don't be confused i will
43:50 - explain everything
43:52 - step by step after like moving into the
43:55 - code
43:55 - so let us get started about that
43:58 - okay so let us get further like
44:00 - explaining that what is going on here
44:02 - so in this face recognition file what we
44:05 - have done is we have just imported
44:07 - imported our numpy
44:08 - library opencv and os library
44:12 - okay so this is the code of canon
44:15 - algorithm
44:16 - and if you don't know like mathematics
44:19 - of canon wortham
44:20 - then i pray for you that like just read
44:23 - the documentation or just
44:24 - read any video just see any video and or
44:27 - see any
44:28 - code and just try to implement it my own
44:31 - by your own
44:31 - okay if you try to implement it by own
44:34 - you will get a
44:35 - clear understanding that what this is
44:38 - doing in actual
44:39 - i have explained you like like what are
44:41 - the logics which have been done
44:43 - and these logics are simply coded here
44:46 - okay
44:47 - like sorting on the basis of this and
44:49 - frequency of each label
44:50 - finding the max frequency i'm just
44:52 - explaining everything about this you
44:54 - have to just understand like what code
44:56 - is doing
44:56 - so i and this is basically a distance
45:00 - like uh this point distance from this
45:03 - is basically euclidean distance which is
45:08 - x 1 minus x 2 whole square plus y 1
45:11 - minus y 2 whole square
45:13 - so this is basically euclidean distance
45:15 - you might know this
45:16 - as everyone has started this in school
45:18 - so
45:19 - here the euclidean distance is being
45:22 - calculated
45:23 - and here is the basic logic of caden so
45:26 - i
45:27 - like give the task to you
45:31 - i have to find that what this game
45:33 - actually doing so i've explained the
45:34 - logic
45:35 - just go through it once and understand
45:38 - that which
45:38 - and what is this canon code is doing if
45:41 - you don't understand this right in the
45:42 - comment box like i will explain
45:44 - everything about that
45:45 - and make another video in if you don't
45:47 - understand but this is very logical
45:48 - thing
45:49 - i'll explain the logic just go through
45:51 - this code once and understand that what
45:53 - it is doing
45:54 - okay this is quite simple so like like
45:57 - uh so now let's move to our main project
45:59 - that how to recognize that using our
46:01 - inversion so i have just changed my ide
46:05 - because
46:05 - uh previously i was having spider id
46:08 - that come
46:09 - that comes from anaconda and now i have
46:11 - id of
46:12 - python so it doesn't affect our code
46:16 - and so let's start it so like in the
46:19 - previous
46:20 - video we have seen that like uh all the
46:23 - things like
46:23 - based data collection video camera on
46:26 - like how to how to do that
46:28 - base recognition okay all of that and
46:31 - now like
46:31 - we have to do like the last practical
46:34 - part of it like we have to recognize our
46:36 - face after
46:37 - after loading and loading the data of
46:39 - the faces
46:40 - right like i will open my webcam and i
46:42 - will upload my data
46:43 - like i will start the webcam and my
46:46 - camera feed will constantly
46:48 - and consecutively uh load my data
46:51 - okay and after that like my facial data
46:54 - would be stored in
46:55 - a numpy array and in prediction i will
46:57 - use that numpy array
46:59 - for in my canon algorithm to predict my
47:02 - new face
47:02 - okay so but okay so basic overview that
47:06 - like in your family just record the
47:08 - faces of four or five people
47:10 - and after that in the data collection
47:12 - record that
47:13 - and while the data recognition just one
47:16 - person coming the camera or
47:18 - multiple persons and a label will become
47:21 - at
47:21 - around your face and a box will come i
47:24 - will show you that later
47:25 - okay so till then just see here
47:28 - so this is our face organization code
47:31 - okay so like we have done
47:32 - uh this place data collection
47:36 - and like everything we have done till
47:40 - in the later in the earlier classes and
47:42 - this is a hard casket classifier
47:44 - which we have used for detecting the
47:46 - facial components
47:47 - and now like let us see our face
47:50 - recognition
47:51 - file so basically like we have
47:54 - like wait now like we have discussed
47:56 - kane and gotham in the other class
47:58 - like how our face is detected in the
48:01 - bunch of many phases
48:03 - okay so like this is my k n algorithm
48:06 - i have not used a library function
48:08 - rather than i have coded it from scratch
48:10 - okay like i will show you later like i
48:12 - also told you to study the scaling code
48:14 - by yourself
48:15 - so basically it is nothing just
48:17 - calculating the euclidean distance
48:19 - and just calculating that which face is
48:21 - the nearest phase
48:22 - to the point of convergence like two of
48:25 - a new phase
48:26 - how many previous places are near to us
48:28 - okay
48:29 - so like like that thing is done here
48:32 - like
48:33 - i have also given comments and just you
48:35 - can read that
48:36 - okay and like i will share this code i
48:38 - will share the repository so that you
48:40 - can
48:41 - uh read the code and understand it and
48:43 - run it in your computer
48:45 - okay and as of now let me just
48:49 - unhide the scale code okay so now let me
48:53 - just unhide it
48:57 - okay so i have just minimized my canon
48:59 - code so let us see now like what further
49:01 - things are done in this
49:02 - in this part so like initially like
49:05 - while we
49:05 - while i have to recognize my face so
49:08 - before that like basic steps has to be
49:10 - done
49:10 - as we have done in previous classes like
49:12 - okay so like
49:14 - we have to first open the camera like
49:17 - when we open the camera then only we can
49:19 - like put our face in front of it right
49:21 - so first we open our camera by cb2 dot
49:24 - video capture zero by this we are
49:27 - opening our webcam
49:29 - like this opencv function video capture
49:32 - and by this you open a webcam and like
49:34 - with c
49:35 - and i have stored my webcam in this cap
49:38 - variable
49:38 - so cap will contain my webcam okay my
49:41 - video field
49:42 - basically and after that like i have
49:45 - called my classifier that is hardcaster
49:47 - classifier
49:48 - and the function to call our classifier
49:50 - is cascade classifier
49:52 - okay and there are multiple classifier
49:54 - like hard casper class frontal phase
49:56 - classifier is only used for
49:58 - like uh directing your faces
50:02 - and there are multiple classifiers like
50:04 - which are which are used for detection
50:05 - the number rate of the car
50:07 - or a person is the gender of a person or
50:10 - many things
50:10 - so there are like a lot of classifiers
50:12 - which have already been made
50:14 - so we have used hardcast classifier to
50:16 - do to classify a particular part of the
50:18 - faces
50:19 - okay and after that after that the
50:21 - series data set paths
50:23 - so initial like in previous video we
50:25 - have seen that whenever we are
50:27 - we are capturing our face then we are
50:29 - storing that face in a particular data
50:31 - file
50:32 - okay so uh just see here face data
50:35 - collection
50:36 - here we have stored our face um i think
50:39 - here
50:42 - just see here like we have stored our
50:44 - face in this
50:46 - data set path okay and this data set
50:49 - path is basically this folder
50:52 - now like when i show you this folder so
50:55 - just see here
50:56 - i think this is a folder okay
51:02 - just see this okay so this is a folder
51:05 - that is like
51:07 - my like this is my folder basically
51:09 - which contains my all the files python
51:10 - files and this is my face data set
51:12 - folder
51:13 - in which all the faces like whatever the
51:16 - name i have given in the data collection
51:18 - is being written here in the dot npy
51:20 - format
51:21 - so if i open this like face data
51:25 - then you can see that uh when i was
51:27 - printing it
51:28 - then i was printing in such a way like
51:31 - dot format data set path plus file name
51:33 - plus dot npy okay so you can see that
51:37 - like
51:38 - it is my data set path is space data and
51:40 - data set
51:41 - name and uh file name is this like
51:48 - this is my name and this is the file
51:50 - name and dot np by say extension
51:53 - okay so like this my store data and now
51:55 - i have to use this data
51:57 - so like in basic cognition i have
52:01 - uh i will be using by this facial data
52:04 - for the recognition part
52:06 - okay and this is the normal areas by
52:10 - which we are just
52:12 - calculating our facial data like with
52:14 - where we are storing our data
52:15 - and label are the name of the data like
52:18 - what is the label
52:19 - like currently and class i like i have
52:21 - made two new variables here
52:23 - class id and names so class id will be
52:25 - basically labels for every given file
52:27 - okay and names are like we are mapping a
52:30 - particular name with a particular
52:32 - data file okay and now let us
52:35 - see like how to do data preparation okay
52:38 - so like just see this line just see this
52:41 - loop
52:42 - for fx in os dot
52:46 - os dot list directory data set path so
52:49 - basically
52:50 - os is a uh another library in python
52:53 - which is used to for interaction with
52:55 - our operating system
52:57 - os basically means operating system okay
53:00 - so here you can see that like i have
53:01 - also imported this os file
53:03 - os library basically and now i'm using
53:06 - that
53:07 - so by this os dot list direct
53:10 - it means list directory and data set
53:13 - path so
53:14 - this this line is basically looping in
53:17 - by this folder okay this is my folder
53:20 - face data set
53:21 - and here my uh
53:25 - like this loop is i this line is
53:28 - iterating over this
53:30 - uh folder okay so every file like every
53:33 - file which is present here will be in
53:35 - the
53:35 - effects like i will be iterating over
53:37 - every file
53:38 - and like in this line fx dot with end
53:41 - width
53:42 - it is just finding that which file ends
53:44 - with this
53:45 - extension dot npy so basically if i made
53:49 - some new file layer like if i made a new
53:51 - folder here
53:52 - suppose new folder and suppose i make
53:58 - a word doc here okay so like i made a
54:01 - new folder and a word document dot word
54:03 - doc
54:04 - so basically if these folders are
54:05 - present in my file
54:07 - so i have to just find the dot npy files
54:11 - okay so with
54:14 - if fx dot ends with dot mpy
54:17 - i am finding just only dot and py files
54:20 - where my data are stored
54:22 - okay where the different faces data are
54:24 - stored i'm just finding those files
54:26 - okay and like just see names
54:30 - or names bracket class id is equal to fx
54:33 - colon minus four so i hope that you
54:36 - remember the slicing operator we have
54:37 - discussed in
54:38 - previous class also like it is a slicing
54:40 - operator so basically
54:44 - you see this we are we are taking all
54:46 - the num we are taking all the characters
54:49 - uh except the last four okay so last
54:53 - quarter basically this
54:54 - dot npy are the last four indices
54:57 - right and everything before
55:00 - dot npy is our name so basically
55:04 - this names class id contains our name
55:07 - basically
55:07 - okay and here we are just loading our
55:10 - data and
55:11 - appending this particular name and data
55:14 - item
55:14 - in my face data array okay so phase data
55:18 - is this so we are just appending every
55:21 - data
55:22 - data in this phase data area okay
55:25 - and now you see that now we are making a
55:29 - particular target variable
55:31 - like like suppose we have five persons
55:33 - person number one two sibo five and
55:35 - these five person are the family members
55:37 - okay
55:38 - and now like we have to make a
55:39 - particular area a particular scale down
55:42 - array
55:42 - with that five percent okay so just see
55:44 - that like we have to meet different
55:46 - target errors for chart so initially my
55:49 - class id is zero
55:50 - so let's just see this line so basically
55:52 - like we have target variables for
55:53 - different phases like we have stored our
55:55 - facial data and now we are just
55:57 - simplifying that data into target
55:58 - variables
55:59 - okay so target are nothing just a simple
56:01 - array okay
56:03 - so basically what we are doing is like
56:05 - initially like we are having classes
56:06 - like
56:06 - initial classes 0 and after that like we
56:09 - are will be implement
56:10 - incrementing our class id with every
56:13 - iteration
56:14 - so a particular class id is associated
56:16 - with every phase data
56:18 - okay initially my class id is zero and
56:21 - now you see this
56:22 - like i have got my uh data item
56:25 - and this i'm just taking the shape of
56:28 - the data item
56:29 - like the length of my data items data
56:31 - items are basically the number of times
56:33 - our phase has been stored
56:34 - okay so like i'm just calculating the
56:37 - length of the data item
56:38 - and like converting that into
56:41 - an array of once so i'll be forming an
56:45 - area of once
56:46 - with the length of the data okay
56:50 - the size will be length of the rate item
56:52 - and i am multiplying it by class id
56:54 - so my class id is 0 so when 0 is
56:57 - multiplied with then
56:58 - one uh ones it will become an area of
57:01 - zeros
57:02 - okay so this will be my target number
57:04 - one
57:05 - after that like i will increase my class
57:07 - id i'll increment it
57:09 - i'll store the target in the labels area
57:11 - and i will just
57:12 - move to the next iteration after that i
57:15 - will go at my second phase
57:16 - okay and what i will do is like i will
57:19 - get another data item
57:21 - i will tell you the size of the data
57:23 - item because i'm having a different
57:24 - person now
57:26 - and i will convert i will make a np
57:29 - area of once and the size of that area
57:32 - will be
57:32 - data item size okay and i will multiply
57:36 - that by class id
57:37 - so whenever an area of once is
57:39 - multiplied with class id
57:41 - which is one so it will become the same
57:44 - area again
57:44 - so that will be my target number two and
57:46 - similarly when i class id are being
57:49 - incremented
57:50 - so basically like when class id is equal
57:52 - to when two is multiplied by area of
57:54 - once
57:55 - then it will become an area of two okay
57:57 - the basic difference is that the size of
58:00 - this
58:00 - these all the target arrays are
58:02 - different okay
58:04 - based on the particular faces the size
58:05 - of errors are different and the number
58:07 - on there is our difference
58:09 - okay so that's how like we are just
58:11 - modifying the facial data
58:13 - into target variables and storing all
58:15 - the data in this
58:16 - labels so labels is also an area we are
58:20 - storing all the different
58:21 - phase data and labels now and now we are
58:24 - just printing that data
58:26 - like whatever the face data we have what
58:28 - the face labels we have
58:30 - all that simple stuff and just combining
58:33 - that
58:33 - into np like np or concatenate paste
58:36 - data and phase labels
58:38 - space data is the phase data and phase
58:40 - labels are the labels of the faces
58:42 - associated with that okay and now just
58:45 - see like
58:46 - like we have printed that stuff and now
58:49 - it is my
58:50 - font this line doesn't care as of now
58:52 - and now just see that like
58:55 - like this this line
59:04 - okay so now let's see that like uh
59:11 - so now like we have like stored our
59:13 - facial data like we have extracted our
59:15 - data from the data set and we have
59:16 - stored
59:18 - okay now like we have to implement our
59:21 - like we have to call our gain
59:22 - function and we have to make a box
59:25 - around our face
59:26 - so what will happen is that when you
59:28 - store your face data
59:30 - okay and like one part is storing our
59:32 - face data in face data dot py file
59:35 - and now like when we are recognizing it
59:37 - like open your camera like play this
59:39 - file
59:39 - your camera will be open and then you
59:41 - will show your face in front of the
59:42 - camera
59:43 - a box will become around your face
59:46 - and your name will be written above that
59:48 - box okay suppose
59:51 - if i show you this
60:07 - we just just see this suppose like we
60:09 - are having a phase like this
60:11 - and like what what will happen is like
60:13 - when i open my webcam
60:15 - suppose this is my webcam okay
60:18 - this this whole screen is my webcam and
60:20 - this is my face
60:21 - like i'm having neck also and like so
60:24 - [Music]
60:26 - okay like rest of the body and a face
60:30 - so basically whatever got some will do
60:32 - is like when you open up the camera
60:34 - then a colored box will be covered on
60:36 - our face
60:37 - which will be like this which will cover
60:40 - only our face part
60:41 - okay like
60:45 - okay so a squared box will be formed
60:47 - around our face
60:48 - and our name will become here at the top
60:51 - okay so our like our
60:55 - algorithm will do that so like this is
60:57 - our webcam and this is
60:59 - me or anything or you and a particular
61:02 - green color box will be covered on this
61:03 - and this is due to this hard casket
61:05 - classifier which is
61:07 - like uh defining our face basically and
61:10 - our name will be written over that let
61:12 - me just show you that code
61:14 - so basically like
61:18 - like we are just doing the same thing
61:20 - again which we have seen earlier also
61:23 - like we will start a loop an infinite
61:25 - loop which will be recording our camera
61:27 - so we will start on the camera by cap
61:29 - dot read
61:30 - and this red and frame will basically
61:33 - be having our camera variables like
61:35 - webcam variables
61:36 - return is basically like telling us that
61:38 - whether the camera is on or off
61:40 - okay and frame is basically our main
61:42 - camera opening which we will be using
61:44 - okay so first like we have just
61:46 - converted our image to a grayscale image
61:49 - okay like whatever the video we are
61:51 - having we have just converted that into
61:52 - a grayscale video
61:54 - and after that like we are done same
61:56 - thing like we have just
61:58 - detect multiscale and these are the
61:59 - scaling factor and the number of
62:01 - neighbors
62:02 - like we have seen all this part in
62:04 - earlier video also
62:05 - okay just get over here so this spaces
62:09 - will basically contain
62:11 - our coordinates of the faces like x
62:14 - y w and h so basically
62:17 - uh if i change the plane so this this
62:20 - this corner like top left corner will
62:22 - contain the
62:23 - will having the point number x and y
62:26 - sorry i don't have the mouse so i'm
62:27 - talking like this
62:28 - so x and y are the coordinates and this
62:31 - is
62:32 - our width and this is our height
62:35 - basically
62:36 - okay so basically our
62:39 - this function will give us the x y with
62:42 - the height
62:43 - so that that all will be stored in in
62:45 - the face area
62:46 - okay and now like we are just starting a
62:48 - loop
62:49 - okay so like in the new loop we are
62:52 - having a face
62:53 - and with a phase we are having the
62:54 - coordinates and these coordinates are
62:56 - stored
62:57 - are got from the space okay so this
63:00 - space is basically the one
63:02 - phase on which we are iterating
63:06 - and everything this is same just we are
63:08 - just cutting the part of the
63:10 - camera frame and just converting that to
63:13 - 100 cross 100 frame size
63:15 - and now just say this out is equal to k
63:17 - n
63:19 - we are calling our function this c just
63:22 - concentrate here
63:23 - we are calling our function k in and now
63:25 - like train
63:26 - train set and face section flatten
63:29 - so face section dot plating so basically
63:32 - training set is our training set which
63:34 - we
63:34 - have which we are carrying over from the
63:36 - past time and phase section dot platinum
63:38 - means
63:39 - like we are defined as earlier also like
63:41 - we are having a face section
63:43 - just here we are having a face section
63:45 - basically
63:46 - the camera frame which is just uh uh
63:49 - calibrating our face
63:50 - so we're just flattening that to convert
63:52 - that into a area
63:54 - okay of one column in multiple rows we
63:56 - are just starting that because
63:58 - in canon our parameter should be a
64:00 - flattened array
64:02 - only a area of one call okay so with
64:04 - train set we have that we have
64:07 - our new face just we are just parting
64:09 - passing the two parameters
64:11 - and this will give us the output output
64:14 - will be that phase
64:15 - like training set we have trained and
64:18 - space section
64:19 - is the section we got here so it will
64:21 - basically
64:22 - guess the output that which frame we are
64:25 - having currently
64:26 - so trains that we got from data phase
64:28 - data
64:29 - okay all the phase data we have so it
64:32 - will just iterate over every data and
64:34 - just
64:34 - apply our k n algorithm with that okay
64:38 - which is having a training set and a
64:40 - test set okay
64:41 - on the base of that it will give us the
64:43 - prediction which is this
64:45 - and it will tell us that which point it
64:47 - belongs to
64:48 - okay it's like if we have this
64:58 - basically like this is a stuff like if
65:00 - you are having a face in between so it
65:02 - will just tell us that
65:04 - which face does our face belong to from
65:06 - this rest of the five phases
65:08 - basically okay and now like we have done
65:11 - that like we got the output face
65:13 - from here and now we we have to make a
65:15 - box on the face
65:17 - and a text on the face basically
65:21 - okay so c v2 dot rectangle dot rectangle
65:23 - is a function which is used to make a
65:25 - shape
65:26 - by opencv so what we are doing is just
65:28 - we are passing a frame
65:30 - the coordinates the top left coordinates
65:33 - and the bottom right coordinates okay
65:36 - top left
65:37 - xy and bottom right are exclusively y
65:39 - plus h we are passing the coordinates
65:42 - okay and it will make a face round it
65:44 - will make a
65:45 - rectangle in these coordinates and this
65:47 - is the color
65:48 - and this is the width width of the box
65:52 - color is 255 255 this is basically bgr
65:56 - color
65:56 - okay blue green red okay like if i
66:00 - do zero here and if i do
66:03 - so bgr means b g and
66:06 - r so if my b and g are 0
66:09 - and my r is 255 it will make a
66:12 - box of a red color
66:16 - okay and if i make this like uh if i
66:19 - make this too much
66:20 - here so it will make a green color box
66:23 - okay
66:24 - this is just a color color format and
66:27 - now like
66:28 - to put a text like here like we write a
66:30 - function co2 co2.put text
66:32 - and it will just uh put a text here and
66:35 - this is the font of the text
66:37 - and this is the and this is the
66:38 - coordinates where i have to put my text
66:41 - and this is my color of the text like
66:44 - pgr format
66:45 - b is 255 so my text will be of blue
66:47 - color
66:48 - okay and this is the width okay and
66:50 - similarly like that we are just
66:52 - putting everything and now like we are
66:54 - just uh displaying our face
66:56 - okay like we are just showing our camera
66:58 - feed and
67:00 - all of that okay and after that
67:03 - everything just
67:04 - make a function like if i press a key q
67:06 - so my function will be entered
67:08 - and i can close my webcam and my webcam
67:11 - will not be closed
67:14 - by anything except pressing q okay
67:17 - and now like after all everything we
67:19 - just will just destroy the window and
67:21 - close the program that's all okay like
67:24 - this is a
67:25 - project of face cognition and i hope
67:27 - that like this is a very beginner level
67:29 - project
67:30 - in machine learning okay it's like i
67:32 - hope that this will this will be clear
67:34 - to all of you
67:35 - and just write like just give the
67:39 - comment below that how
67:40 - like like uh how much you like this
67:43 - playlist of
67:44 - this project okay and
67:47 - okay so let's end this video now okay
67:50 - bye

Cleaned transcript:

in this video we are going to do a new machine learning project that is real time facial recognition so let me give you a brief overview that what is the output of the project then we will move to the roadmap that what we are going to do step by step okay so the output of the project is like this we will open with laptops camera camera or webcam and a colored box uh will surround our face and our name will do that in the box so basically our laptop will recognize our face that who we are and what is our name so let me just give you an example of that yeah so i have just run my test sample and this is my face as you can see so basically our camera is recognizing that what is my name and what is the name associated with that with that face okay so this is a realtime face recognition which our system is doing so let us see that how we will proceed to this project so let me give you a roadblock just that what we are going to do step by step so basically first we are gonna we are gonna feed our video that like we should open with camera and we should record that what face is in front of the camera right so i have made a several a separate snippet of video record it is just to check that uh whether the camera is working or not so by this code we can check that uh our camera is working or not um don't be stressed out i will explain each time of the code step by step i'm just in the roadmap right now okay so this is about just checking this this codes check that whether our camera is working or not okay so this is a real test and after this okay so after this we are gonna we are gonna uh collect our face data so basically in this snippet uh this the function of this code is like this this code opens up a camera and uh ask ask ourselves write our name okay so if i play this sample so it is asking my name so i will enter my name here and after that when i enter the camera will open and it will start recording my face it will start uh memorizing my face and will put each and every data in an array okay i will explain this uh entire thing step by step i'm just like giving an overview overview right now okay so it this code recognizes this code basically collects our facial data and stores it in an array and that area is used for recognition afterwards so after collecting our facial data we will run this code this is basically facial recognition okay so after collecting our data we will store that data in another area and we will start another live streaming i mean we will we'll start with camera again and it will match that this current phase matches with the previous array and it maps down the name of the person with that figure okay so basically first step is data data collection over faces and then after that we will do a face recognition and we will use basically k means algorithm in this and a classifier that is hard cascade classifier okay so this is a brief overview overview of the project that what we are going to do and now i'm gonna explain you okay so before go deeping uh so because before diving into the project let us first see that what is hard casket classifier okay after explaining this hardcasket classifier i will explain you each and every line of the code that what we have we have done step by step okay because of what our casket classifier do is it is a collection of features like edge features light features and many or other features so basic purpose of hard cassette classifier is it detects each and every feature of our face okay so this is a box it moves in every direction in the in the entire image and it just collects the data that where is our eyes where is our nose i'll show you the animation that how it works uh before that list you can just see on this website that how uh what is the basic documentation of our casket classifier um after this i will show you the animation of this okay so i've done the basic of this so if i enter my image here and the hard casket classifier will basically uh classify that where is our eyes and where is our face located in that that entire image okay so as you can see that here is my face and these are my eyes so this simple code which uses hard casket classifier has just recognized that where are my eyes and where is my face in the entire image okay so now let us see that this hard casket animation okay so as you can see that this hard casket classifier is just moving in the entire image and it is just classifying that where is our face and where are the other components of the face so as you can see that uh the edge features the line features and the other features of the casket classifier just checking that where are the main features of our face okay so it is traversing the whole image by increasing the size so just see the animation i have increased the size of the the speed of the video to 5.5 x so you can see clearly okay so this is how hard casket classifier works okay so i hope that you will get and you have got a basic idea of that how and hard casted transfer works so in this video in this project we are we will be using that hard casket classifier to detect our facial components and after detecting that storing that face into an array mapping that with the name of the person and just recognizing that again by our k means algorithm okay so before that before starting data collection let us check that whether the camera is working or not okay so by this code we will just check that whether our camera is working yes or no okay so let me just explain you this code so this code starting with like we will import cv2 like we will we are importing opencv library and by this cap is equal to cv2 let me just zoom in zoom in okay so second line that cap is cv dot video capture it means that just open our webcam okay so this line opens our webcam and this loop just tells that if a webcam is reading our image then just uh continue and if it is false then move out of the loop okay so it shows like a video frame it okay so let me just open the camera i'm playing this and so as you can see that our camera has been opened and after that this camera name just the uh the name of uh camera is frame as okay as you can see as you have just seen before and after this i have entered this logic that if i press q q as a key if i press the key q then it breaks the camera and closes the window and the camera will not not be closed by the simple closing function it will be only closed if you enter q okay so this is a logic which i have entered and i will explain to the purpose of that why i have entered this okay and this is a key pressed this line simply means that uh if our currently pressed key matches with this key or not this is that's it and i will explain this in the meaning of this line later in this project when we are doing data collection okay i will explain the logic behind this that why i have bit wise why i have used with bitwise and here with zero xfs okay so for right now you have to just understand that we have just opened up a camera and just checked that if return is equal to false then we will just continue and if our camera is reading then we will just move move forward and just show the camera that and just check that whether the camera is open or not and we'll just uh close that camera by pressing the key q okay and after that we will release our camera tab dot release if when we break out of the loop then we just release a camera by this line this means closing the camera and by this it means that destroying all windows means uh disabling all the cookies that have been formed by opening the camera and deleting all the data which has been captioned in the back memory okay so this is basically a video read we are just checking that our camera is reading the video or not okay so i hope that you have got a clear idea that what is this okay so after that we'll do facial data collection we have covered this video read part like we are just thinking that how video is being read from your webcam of the laptop or computer and how it has been processed and how it is the video is being stopping so in today's video we're gonna see that how the laptop camera or webcam is reading our face data and storing it in an array to like detect further okay so this file phase data consists that how our phase data is being stored in uh the memory of computer and we will process it later when we are doing phase detection using k n algorithm okay so so let us get started about it so i will explain you line by line that what each line is doing so so let's get started so first we import two libraries like opencv and numpy library opencv to like read the camera feed or open the camera and numpy has basic array operations and some more operations okay so by this line we are just reading we are just opening our webcam and um in this variable cap our webcam has been storing the data is being stored in this gap okay so after that we are using like in previous video i've shown you a hard cascade classifier that that is classifying the facial components of the face okay so we have just imported this hard casket classifier in this variable and like this is the location of hard gasket classifier so before starting the project uh just see here it is in my d folder d drive and this folder so this hard cascade classifier file uh we will just download this from the internet it is easily available on github so just download it or i will give the link in the description okay we you have to store this hard casket file in this directory only so like just wait a second so in my data in my d drive i have a folder called face recognition project and in this uh folder i have this file hard casket classifier file okay this is hard casket frontal face so you have to just download this file and this is the mainly classifier we are going to use here so we have just imported this classifier here in this variable so let's move forward the skip variable just means that uh it i will explain this later when we are just using the script variable here for now just ignore it and in this space data area this is basically a list in which we will be storing each face data okay so don't be confused you will get a clear understanding later when we are proceeding towards it and this dataset path is based dataset so this means that the faces which we are storing has to be downloaded or has to be stored in the computer's memory okay so like if i open my camera and store my face so my computer has to download that face and store that file in a particular folder right so i have just created this folder here so so as you can see that in my d drive i have created a folder phase data set by this line okay and this phase data set contains all the files that has been deleted through my camera so if i open it i have just i have just recorded my face and given my name to this like anything and this uh every data of my camera feed is stored in this data folder in this folder okay so let's move forward so um so like when we open our camera then we will enter the name there as i just shown you in the previous video when we open the camera we enter our name and after that video starts recording and our face is starting getting recognized and feeding the camera okay so uh we are just giving the input that when we open the camera just enter our name and index and in the console uh this dialog box will appear in where we have to enter our name don't worry i will just when i will just play this video like when i play this code then you will see that but this line is actually doing here in the console okay okay so let us just now uh play it once so when we play this file then this comes in this console you can see that here it is coming like enter your name like enter the name of person so when i write my name here like this and press enter then the camera will open okay so as of now i'm just stopping this program we will do this later okay so i've just shown you that but how it is being appeared in the console okay so let us move further so now we are opening up a camera and we will store our data feed now okay so in in a while loop which is always true we are just doing this code so let me just explain that what we are doing here okay so this red comma frame is equal to it means that red is basically a boolean variable which just check that camera is over or not and frame basically captures the cam is a variable where we are defining our camera okay so and here we are just checking that if the camera is not open then just continue and don't do this iteration and this lines means that we are just converting our uh like initially we will be having a colorful image like we all are colorful right so initially our images are rgb image and in python we just like rgb like we like v red bgr instead of rgb every time so in this line we are converting our rgb image to a grayscale image okay and to convert that we use this type of syntax like cv2 dot cvt color and frame which is storing the camera feed and cv2 dot color bgr to gray bj to gray means we are converting bgr like r uh it means rgb we are converting rgb to grayscale so this is a syntax so our gray so initially we are having frame in this frame we are we are having a colorful image and now in this gray frame we will have we will be having a grayscale image okay so grayscale we have converted into grayscale limit because in gain and gotham it is much easier to like understand a grayscale image and do the algorithm like do the functionalities in grayscale image it is much easier than a colorful image and it doesn't affect the output output is much better in greyscale okay so when we are processing grayscale okay so let us move forward and this line like faces is equal to face cascade dot detect multiscale okay so i will mx being the parameter that is used here so this is a gray frame that is we just created here and this is a scaling factor this is the number of neighbors okay so let me just give a comment here like frame name parameter frame name scaling factor and number of neighbors that we defined by k and just we write number of number of neighbors okay so uh number of neighbors is basically used in k n and gotham if you don't know that k n in custom for now just skip this line i will explain the each and e functionality of scaling factor k and uh the whole functionality of this line later when we i will be explaining the k algorithm because you will get a clear understanding then okay so so uh just let me give you a quick overview that what is the scaling factor so initially if our image size if we scale down our image to one then if we are doing 1.3 then it simply means that we are just shrinking our image to 30 percent don't worry i will just explain to this step later when we are doing gaining algorithm so for now let us move further so in this line we are just checking that if face is length equal equal to zero like if this um if this face is equally equal to zero then this continue we don't have any phase here okay so as of now just understand that this line like this function just returns you the coordinates of the face captured like if i open the camera and the camera will capture my face then this function will return the coordinates of my face like top left top right bottom left bottom right okay so all the coordinates will be returned by this function to this faces variable okay so this so this lines this function is only doing this this is just telling the coordinates of faces to us okay okay so let me just explain you that what actually this function is returning us and what type of coordinates are they okay so just let me open my notepad okay so this function like this function so let me just select a brush here okay so just function like space cascade dot detect multi scale which is uh this variable faces just means that this basis is a list and this list is storing the cordless of the face so if i draw here like if i draw pieces like like if this is the person here like if this is person one like person one and if this is the person two okay so these both have faces okay so these are the top coordinates like x y are the top coordinates of the faces and this is the width this is the height okay so a box it returns like this detect multiscale returns a block to you like it returns a box in which we have the top left and we have the top left coordinates of x y and the height is h and the width is w okay so this function just returns x this function returns x y w and z so the w and h x y w and h okay so these are the coordinates and by this w and h we can just calculate it what are what can be these coordinates like this can be like x plus h and y plus w right like you can just calculate it by adding w h in x and y so we can get this these and this coordinate also so this function is just returning as the coordinates so that we can just uh make a box into the field around the face okay that said about this let us move forward okay so now let us see that the further code so this line like faces is sorted faces key lambda x2 into x3 so what is it is doing so it is just arranging your faces like if you have multiple faces in the video field like if i show my face my sister's face my brother's face my parents face my every my friend's face so multiple number of faces will come right and we have to sort that faces in increasing or decreasing order as per our need okay so what we are doing here is we are passing the function name sorted and we are just using using a laptop function okay so in this uh this time mean that we are passing our faces list here like in this faces list we have uh x y h and w coordinates uh numbers okay and we pass this list here and we are just calculating so this means that okay so like faces lists like if we have faces then faces contains a list of x y w and h okay so it is zero index this is one index two index is three index okay so the area of image is what area of any image is weight into height okay we went into height and weight into height basically here is faces interfaces three okay this is width into height and this is basically area and the image which have larger area will obviously come first if we are sorting it in a decreasing order and if the area is less then it will come first in the sorted order if we are sorting it in ascending order right so this so in this here so lambda x is just representing our faces and we are just using the lambda function here if you don't like what is lambda function so let's like just get a quick overview by seeing the documents documentations okay so x is representing this spaces and x2 into x3 basically arranging this in a descending order like we are sorting it in a descending order right so and these like if we are having faces like this first phases this second phase is this third phase is like small face then big face and small place and these all faces will be sorted in a descending order like first become big and small and then small this one okay so in this way we sort our faces and every data will be stored in this in this variable okay so i hope that you get a clear understanding of what we are actually doing here and now just forget this line as of now so now we are just we will just hydrate every phases and you'll just store every face in our data so now we are we are just i'm trending in our faces a so let us see that we are starting a for loop here and for loop is going to end okay and this x y w h is are the coordinates of the face so face and faces means that it is like this is phase one this is phase two this is stage three this is phase four like these are the faces and the coordinates of each and every face will come in this x y w and h variables okay as we iterate over every faces then the uh coordinates of every phase will come in x y w and h okay and this offset simply means that we are providing a padding so if you have done html before before you might know that what is padding which is what is margin so padding is basically uh so if we are having our face here this is our face then this gap is basically padding okay so this gap is padding sorry about the handwriting uh actually pen is quite bold here okay so if you are having a face here like and this is covered in a box then this box is having a padding of five okay so this length is basically five or five carry okay so we have just given the padding of five to like uh show a clear user interface okay so as before we are just having this uh this box as outer box which is which we are having the coordinates x y and everything so now here we are just extracting the face section so this line is basically extracting the section of a face so it is subtracting the offset so basically it is subtracting the padding and it is just uh calculating a particular section of a face initially there was padding and now we are just scattering the particular section of the face okay and after that like you will see you can understand this easily like weird is subtracting offset from length from top and bottom and subtracting from left and right okay we are just subtracting our padding and just getting the section of face by this line okay just go through it once and by this face selection we are just resizing my face to 100 across 100 image okay so i hope that you understand it and now this skip so this tip just means that when we start recording our face then after every second i will skip after every second or after every iteration this tip will increment okay like we have recorded one first phase second phase third phase like a particular uh time frame of the face is being recorded by this script by the script okay so after every 10th phase like when i started the recording when i started recording my face so i recorded at first second second second third second fourth second i'm recording my face at every second okay so when i hit the 10 second of camera i will just do a skip operation okay and after every 10th phase i will be recording this 10th face into my array okay i'll be recording each tenth hydrated phase in my array so that i don't have many faces in my area and i have a good like recognize later so as you can see that like after each 10th 10th iteration i am just appending my face into the face and printing the length of face that's it like understanding these lines this line simply means that we are just creating a rectangular box towards our frame big frame is basically a section of frame a section of face which we are having and this these are the coordinates like top left top left quadrants and these are the bottom right coordinates we pass this these parameters at the color of the box which we have to display okay so this is rgb red green and blue so basically we have given green to its full number and red and blue to zero zero so basically a green color box will uh will appear on our face okay so like let me show you that so so when i when i will open my camera so what will happen like a camera will open like this it will be a frame and my face will be here like i will be standing or sitting here and a green color box will appear around my face like around a section of face a green color box will appear okay and by this like this is basically our gp i have given 0 to r 0 to blue and 255 degree 255 is the maximum number that's why a green color box will appear if i give like 255 zero then a red color box will appear around my face red color box will appear so this is simply uh this line is simply creating a rectangular box which will appear around this frame frame is basically uh this frame okay so and by this this will just show the frame like it is understandable and this uh this is like that if we have if a webcam is open then if we place a key if we press q then the webcam will just switch off like i will explain the function of this later so let us move forward about this as of now and so now after when we have stored our faces then after that we will we are we will be having a list of faces like we are having a list of faces in which faces our faces data are stored okay so now to implement the kb's algorithm here we have to implement basically kn so it implements only on numpy array so a numpy array works best in some scenarios and the compilation power is pretty much higher then list okay so we will convert our list so face data is basically a list now we will convert this list to a numpy array by this function by np dot array and phase data here we are basically converting our face this is a list here as of now we will we are converting this to a numpy array okay so to have a faster computation and to better fit in algorithm after converting into a numpy array we will just reshape it okay so reshaping is a basic like basic concept you will know it and if you don't know like what is reshaping i would say that just look at the documentation like what it is doing and after this reshaping we are just printing our face not shape phase data shape like what is the shape like what is the shape of the face as of now and after that we are just saving our data in our folder which we have just made like just let me explain you this so this data set path is basically the d drive path which i have just uh stated earlier so so basically this this is a data set path and now we are just storing like current data into this folder okay so to store our data in this folder what we are doing is like we are just saving our current data data path file name and phase data okay so file name is basically this uh when we are entering our name the name is storing it this file name okay so now just see that what we have done we have saved our data set plus file name and phase data and so if we print this it will be like data set like let me just show you if we print this what will happen here so now when we print it like it will come like this if the folder name is my data if data is my folder name then and my file name is if i write my my name like our breadth and extension is basically dot npy because we store in this so this will be our location or source where we are storing the current data okay so this this dot np will contain all the faces all the my face data which is being stored in the camera feed and we will use this data to recognize and pass this data okay and now but this this is this means that we are releasing our camera which we have just opened your webcam and this light is just destroying all the windows which have been created by the camera like basically the frames which have been created here okay so this is the basically everything about base data reading and data collection so now we will see that how we can recognize the data by using gain canon algorithm so this is the basically the code of the face recognition and here i have implemented a canon algorithm that what the algorithm is doing okay so i will explain line by line that every like what every line of functions is doing i'll explain everything and uh if you have any confusion like if you don't want to make alien algorithm from scratch like here in this video i made the this algorithm from scratch you can also use inbuilt library like alien inbuilt library comes it just works in simple one line and here i've used like very lines here to implement it from scratch okay so you can use like direct library function also and you can also like implement it from scratch okay so don't worry about that we will just discuss everything here okay so before starting like a face recognition let me just give you a quick overview that what is kanan and gotham doing okay so now let's see this okay so now let me give you an example of like what is care and in a very very simplest way and i will show you that how our gain can be implemented in this project also okay so let us see that so if we are having uh suppose we are having suppose i have given a small example before like we are having a class one here let me just okay so it is a it is my class one and okay and i am just just let me change my color and this is my class two last two and in between this class 1 and class 2 i have a person here like suppose this is me now i have to like go to class 1 or class 2 and we have to find that this one belong to which class like this one belongs to this class or this class we don't know as of now so what we do is we will find distance of current point with all the points and find the nearest point to this green point okay we find distance to every point and we'll check that what is the average nearest distance we have so suppose if so what we do is we find distance of this green person to every person of the class and we find the top a person's like suppose we find distance to every every person in the class and after that finding we just categorize the top five person like if we write k is equal to five then these five people are the nearest people to this like if k is equal to five then this three people like this this this this three people are plus one and these two people of class two are the nearest people to this green okay so a is equal to five simply means that these five peoples are the nearest k people to this green color this green person okay now there are three persons of class one and two persons of class two this simply means that the we will just check that at which class we have the most number of uh nearest neighbors okay so this class one has the most number of nearest neighbors to this green green person okay so as class one has three neighbors class two has two nearest tables so obviously we will categorize our this green person to class one because these are more nearest neighbors as compared to plus two okay so here class means class one wins so basically this point belongs to class one is it it is a basic overview like what is gaining and i will explain this in this further video like by giving more detailed examples i explain you the complete like how to take facial data and store into a numpy array and also showing the basic quick overview that what is scaling algorithm and in today's video we're going to see that how can a gotham can be implemented here and why should be implemented here okay so like like in this project we are like recognize recognizing our face with this canon earth kane and gotham but this is not the only way there are many other algorithms like uh convolution neural networks and like many other things by which we can like recognize our face so this is the one this is one of the method came in by which we recognize so let me give you example that how can can be implemented here so just see this so suppose that it is my graph and suppose i have stored the data of five people like uh four people i have state i have uh recorded the data of four people four of my friends and one of one it's me one is me and uh other three are my friends okay and total is i have four people okay one eight one is me and three are friends so just let's see that suppose this is my this is me uh this is these these are my face records like i just i just told you that i am recording my faces after every 10th iteration in and it will depend like when the number of the number of minutes or seconds or hours i will open my camera if my face will be get recorded and and due to that i have i will be having number of faces like if i open my camera for 10 minutes then i will be having a lot of faces recorded if i open my camera for one minute then i will be having only six faces recorded because the my faces are recording after every 10 seconds okay it is just a overview example so suppose i am i am i am having like my six faces and every friend of mine is having six faces okay so this is person one and these are the faces of person one and just let me change the color this is like this is me and suppose this is my friend face okay this is my friend's face and like like friend one and this is me and suppose i'm having my one more friend stay stored okay so this is my friend's three face like a second friend face and this is these are the faces of my friend three okay and now i am just like these are the these are stored in my data these all faces are stored in my data and now i open my camera feed to recognize and use gain in algorithm now so suppose i open my camera and i open up my camera like my camera face has been opened and a random person comes in the camera like it is none of us so it will not match with any of them and it will just simply cannot recognize okay so if i open my camera and i show my face to it like i show my face to it and it will recognize my face as like this so how it is happening so if i open my camera my face will come here like this is my camera and my face will come here so now my face will calculate the distance from every facial data okay so i have just tell you that i have used hard casket classifier so my hard casket class where i am just calculating my difference between faces okay so i will calculate the difference my of my face with every other face is stored okay with every other face is stored i will calculate the difference between my face and other faces okay like every faces okay and i will just calculate like if i take k as 3 then i will just get the three nearest neighbors three nearest neighbors and obviously i will be having the least faced facial difference with me only if i am wearing in the front of the camera then i will be having the least facial difference with me only okay so by this algorithm we can just state that um i am having like this space means this is me and if if it recognize that this phase means like my name is so it will just recognize it and show my name on the box like don't be confused i will explain everything step by step after like moving into the code so let us get started about that okay so let us get further like explaining that what is going on here so in this face recognition file what we have done is we have just imported imported our numpy library opencv and os library okay so this is the code of canon algorithm and if you don't know like mathematics of canon wortham then i pray for you that like just read the documentation or just read any video just see any video and or see any code and just try to implement it my own by your own okay if you try to implement it by own you will get a clear understanding that what this is doing in actual i have explained you like like what are the logics which have been done and these logics are simply coded here okay like sorting on the basis of this and frequency of each label finding the max frequency i'm just explaining everything about this you have to just understand like what code is doing so i and this is basically a distance like uh this point distance from this is basically euclidean distance which is x 1 minus x 2 whole square plus y 1 minus y 2 whole square so this is basically euclidean distance you might know this as everyone has started this in school so here the euclidean distance is being calculated and here is the basic logic of caden so i like give the task to you i have to find that what this game actually doing so i've explained the logic just go through it once and understand that which and what is this canon code is doing if you don't understand this right in the comment box like i will explain everything about that and make another video in if you don't understand but this is very logical thing i'll explain the logic just go through this code once and understand that what it is doing okay this is quite simple so like like uh so now let's move to our main project that how to recognize that using our inversion so i have just changed my ide because uh previously i was having spider id that come that comes from anaconda and now i have id of python so it doesn't affect our code and so let's start it so like in the previous video we have seen that like uh all the things like based data collection video camera on like how to how to do that base recognition okay all of that and now like we have to do like the last practical part of it like we have to recognize our face after after loading and loading the data of the faces right like i will open my webcam and i will upload my data like i will start the webcam and my camera feed will constantly and consecutively uh load my data okay and after that like my facial data would be stored in a numpy array and in prediction i will use that numpy array for in my canon algorithm to predict my new face okay so but okay so basic overview that like in your family just record the faces of four or five people and after that in the data collection record that and while the data recognition just one person coming the camera or multiple persons and a label will become at around your face and a box will come i will show you that later okay so till then just see here so this is our face organization code okay so like we have done uh this place data collection and like everything we have done till in the later in the earlier classes and this is a hard casket classifier which we have used for detecting the facial components and now like let us see our face recognition file so basically like we have like wait now like we have discussed kane and gotham in the other class like how our face is detected in the bunch of many phases okay so like this is my k n algorithm i have not used a library function rather than i have coded it from scratch okay like i will show you later like i also told you to study the scaling code by yourself so basically it is nothing just calculating the euclidean distance and just calculating that which face is the nearest phase to the point of convergence like two of a new phase how many previous places are near to us okay so like like that thing is done here like i have also given comments and just you can read that okay and like i will share this code i will share the repository so that you can uh read the code and understand it and run it in your computer okay and as of now let me just unhide the scale code okay so now let me just unhide it okay so i have just minimized my canon code so let us see now like what further things are done in this in this part so like initially like while we while i have to recognize my face so before that like basic steps has to be done as we have done in previous classes like okay so like we have to first open the camera like when we open the camera then only we can like put our face in front of it right so first we open our camera by cb2 dot video capture zero by this we are opening our webcam like this opencv function video capture and by this you open a webcam and like with c and i have stored my webcam in this cap variable so cap will contain my webcam okay my video field basically and after that like i have called my classifier that is hardcaster classifier and the function to call our classifier is cascade classifier okay and there are multiple classifier like hard casper class frontal phase classifier is only used for like uh directing your faces and there are multiple classifiers like which are which are used for detection the number rate of the car or a person is the gender of a person or many things so there are like a lot of classifiers which have already been made so we have used hardcast classifier to do to classify a particular part of the faces okay and after that after that the series data set paths so initial like in previous video we have seen that whenever we are we are capturing our face then we are storing that face in a particular data file okay so uh just see here face data collection here we have stored our face um i think here just see here like we have stored our face in this data set path okay and this data set path is basically this folder now like when i show you this folder so just see here i think this is a folder okay just see this okay so this is a folder that is like my like this is my folder basically which contains my all the files python files and this is my face data set folder in which all the faces like whatever the name i have given in the data collection is being written here in the dot npy format so if i open this like face data then you can see that uh when i was printing it then i was printing in such a way like dot format data set path plus file name plus dot npy okay so you can see that like it is my data set path is space data and data set name and uh file name is this like this is my name and this is the file name and dot np by say extension okay so like this my store data and now i have to use this data so like in basic cognition i have uh i will be using by this facial data for the recognition part okay and this is the normal areas by which we are just calculating our facial data like with where we are storing our data and label are the name of the data like what is the label like currently and class i like i have made two new variables here class id and names so class id will be basically labels for every given file okay and names are like we are mapping a particular name with a particular data file okay and now let us see like how to do data preparation okay so like just see this line just see this loop for fx in os dot os dot list directory data set path so basically os is a uh another library in python which is used to for interaction with our operating system os basically means operating system okay so here you can see that like i have also imported this os file os library basically and now i'm using that so by this os dot list direct it means list directory and data set path so this this line is basically looping in by this folder okay this is my folder face data set and here my uh like this loop is i this line is iterating over this uh folder okay so every file like every file which is present here will be in the effects like i will be iterating over every file and like in this line fx dot with end width it is just finding that which file ends with this extension dot npy so basically if i made some new file layer like if i made a new folder here suppose new folder and suppose i make a word doc here okay so like i made a new folder and a word document dot word doc so basically if these folders are present in my file so i have to just find the dot npy files okay so with if fx dot ends with dot mpy i am finding just only dot and py files where my data are stored okay where the different faces data are stored i'm just finding those files okay and like just see names or names bracket class id is equal to fx colon minus four so i hope that you remember the slicing operator we have discussed in previous class also like it is a slicing operator so basically you see this we are we are taking all the num we are taking all the characters uh except the last four okay so last quarter basically this dot npy are the last four indices right and everything before dot npy is our name so basically this names class id contains our name basically okay and here we are just loading our data and appending this particular name and data item in my face data array okay so phase data is this so we are just appending every data data in this phase data area okay and now you see that now we are making a particular target variable like like suppose we have five persons person number one two sibo five and these five person are the family members okay and now like we have to make a particular area a particular scale down array with that five percent okay so just see that like we have to meet different target errors for chart so initially my class id is zero so let's just see this line so basically like we have target variables for different phases like we have stored our facial data and now we are just simplifying that data into target variables okay so target are nothing just a simple array okay so basically what we are doing is like initially like we are having classes like initial classes 0 and after that like we are will be implement incrementing our class id with every iteration so a particular class id is associated with every phase data okay initially my class id is zero and now you see this like i have got my uh data item and this i'm just taking the shape of the data item like the length of my data items data items are basically the number of times our phase has been stored okay so like i'm just calculating the length of the data item and like converting that into an array of once so i'll be forming an area of once with the length of the data okay the size will be length of the rate item and i am multiplying it by class id so my class id is 0 so when 0 is multiplied with then one uh ones it will become an area of zeros okay so this will be my target number one after that like i will increase my class id i'll increment it i'll store the target in the labels area and i will just move to the next iteration after that i will go at my second phase okay and what i will do is like i will get another data item i will tell you the size of the data item because i'm having a different person now and i will convert i will make a np area of once and the size of that area will be data item size okay and i will multiply that by class id so whenever an area of once is multiplied with class id which is one so it will become the same area again so that will be my target number two and similarly when i class id are being incremented so basically like when class id is equal to when two is multiplied by area of once then it will become an area of two okay the basic difference is that the size of this these all the target arrays are different okay based on the particular faces the size of errors are different and the number on there is our difference okay so that's how like we are just modifying the facial data into target variables and storing all the data in this labels so labels is also an area we are storing all the different phase data and labels now and now we are just printing that data like whatever the face data we have what the face labels we have all that simple stuff and just combining that into np like np or concatenate paste data and phase labels space data is the phase data and phase labels are the labels of the faces associated with that okay and now just see like like we have printed that stuff and now it is my font this line doesn't care as of now and now just see that like like this this line okay so now let's see that like uh so now like we have like stored our facial data like we have extracted our data from the data set and we have stored okay now like we have to implement our like we have to call our gain function and we have to make a box around our face so what will happen is that when you store your face data okay and like one part is storing our face data in face data dot py file and now like when we are recognizing it like open your camera like play this file your camera will be open and then you will show your face in front of the camera a box will become around your face and your name will be written above that box okay suppose if i show you this we just just see this suppose like we are having a phase like this and like what what will happen is like when i open my webcam suppose this is my webcam okay this this whole screen is my webcam and this is my face like i'm having neck also and like so okay like rest of the body and a face so basically whatever got some will do is like when you open up the camera then a colored box will be covered on our face which will be like this which will cover only our face part okay like okay so a squared box will be formed around our face and our name will become here at the top okay so our like our algorithm will do that so like this is our webcam and this is me or anything or you and a particular green color box will be covered on this and this is due to this hard casket classifier which is like uh defining our face basically and our name will be written over that let me just show you that code so basically like like we are just doing the same thing again which we have seen earlier also like we will start a loop an infinite loop which will be recording our camera so we will start on the camera by cap dot read and this red and frame will basically be having our camera variables like webcam variables return is basically like telling us that whether the camera is on or off okay and frame is basically our main camera opening which we will be using okay so first like we have just converted our image to a grayscale image okay like whatever the video we are having we have just converted that into a grayscale video and after that like we are done same thing like we have just detect multiscale and these are the scaling factor and the number of neighbors like we have seen all this part in earlier video also okay just get over here so this spaces will basically contain our coordinates of the faces like x y w and h so basically uh if i change the plane so this this this corner like top left corner will contain the will having the point number x and y sorry i don't have the mouse so i'm talking like this so x and y are the coordinates and this is our width and this is our height basically okay so basically our this function will give us the x y with the height so that that all will be stored in in the face area okay and now like we are just starting a loop okay so like in the new loop we are having a face and with a phase we are having the coordinates and these coordinates are stored are got from the space okay so this space is basically the one phase on which we are iterating and everything this is same just we are just cutting the part of the camera frame and just converting that to 100 cross 100 frame size and now just say this out is equal to k n we are calling our function this c just concentrate here we are calling our function k in and now like train train set and face section flatten so face section dot plating so basically training set is our training set which we have which we are carrying over from the past time and phase section dot platinum means like we are defined as earlier also like we are having a face section just here we are having a face section basically the camera frame which is just uh uh calibrating our face so we're just flattening that to convert that into a area okay of one column in multiple rows we are just starting that because in canon our parameter should be a flattened array only a area of one call okay so with train set we have that we have our new face just we are just parting passing the two parameters and this will give us the output output will be that phase like training set we have trained and space section is the section we got here so it will basically guess the output that which frame we are having currently so trains that we got from data phase data okay all the phase data we have so it will just iterate over every data and just apply our k n algorithm with that okay which is having a training set and a test set okay on the base of that it will give us the prediction which is this and it will tell us that which point it belongs to okay it's like if we have this basically like this is a stuff like if you are having a face in between so it will just tell us that which face does our face belong to from this rest of the five phases basically okay and now like we have done that like we got the output face from here and now we we have to make a box on the face and a text on the face basically okay so c v2 dot rectangle dot rectangle is a function which is used to make a shape by opencv so what we are doing is just we are passing a frame the coordinates the top left coordinates and the bottom right coordinates okay top left xy and bottom right are exclusively y plus h we are passing the coordinates okay and it will make a face round it will make a rectangle in these coordinates and this is the color and this is the width width of the box color is 255 255 this is basically bgr color okay blue green red okay like if i do zero here and if i do so bgr means b g and r so if my b and g are 0 and my r is 255 it will make a box of a red color okay and if i make this like uh if i make this too much here so it will make a green color box okay this is just a color color format and now like to put a text like here like we write a function co2 co2.put text and it will just uh put a text here and this is the font of the text and this is the and this is the coordinates where i have to put my text and this is my color of the text like pgr format b is 255 so my text will be of blue color okay and this is the width okay and similarly like that we are just putting everything and now like we are just uh displaying our face okay like we are just showing our camera feed and all of that okay and after that everything just make a function like if i press a key q so my function will be entered and i can close my webcam and my webcam will not be closed by anything except pressing q okay and now like after all everything we just will just destroy the window and close the program that's all okay like this is a project of face cognition and i hope that like this is a very beginner level project in machine learning okay it's like i hope that this will this will be clear to all of you and just write like just give the comment below that how like like uh how much you like this playlist of this project okay and okay so let's end this video now okay bye
