With timestamps:

00:00 - hello everyone welcome to programming
00:02 - knowledge in this video we will discuss
00:04 - what is machine learning and what
00:07 - exactly is scikit-learn all about and
00:09 - why is it so important from the point of
00:12 - view of machine learning we will also
00:14 - see how to install it properly and get
00:16 - going first things first let's see
00:19 - briefly what machine learning actually
00:21 - is without going into formal textbook
00:24 - definition in very simple terms one can
00:27 - say that if performance of doing a
00:29 - particular task is improving with
00:32 - experience then we can say that the
00:34 - machine or the model is learning so
00:37 - there would be some tasks and we'll see
00:40 - how could the task is being done and
00:42 - this is done by measuring the
00:44 - performance if we see that the
00:46 - performance is if we see that the
00:49 - performance is increasing and getting
00:50 - better with experience then we can say
00:53 - that our program or our machine is
00:56 - learning mainly there are three types of
01:00 - machine learning or we can say that all
01:02 - machine learning algorithms basically
01:05 - fall under these three categories first
01:09 - one is supervised learning second
01:11 - consumer was running and the third one B
01:13 - reinforcement or in case of supervised
01:17 - learning the correct label or answer is
01:19 - provided with data as a classical
01:22 - example the size of houses are given
01:24 - with prices after feeding this data the
01:27 - program will learn and build up a
01:29 - hypothesis or model and then we will
01:31 - give our new size and the model will
01:34 - predict how much price should be based
01:36 - on the training data that is the data we
01:39 - fed earlier in supervised learning no
01:43 - levels are provided instead based on the
01:46 - similarity in data it creates groups by
01:48 - itself if you have visited Google News
01:51 - before then you must have noticed that a
01:53 - particular piece of news covered by
01:55 - several websites are grouped under one
01:58 - section these are not just grouped these
02:01 - are rather clustered in one section
02:03 - this can be one example of unsupervised
02:06 - learning in case of reinforcement
02:09 - learning there is an agent and
02:12 - he has to fulfill an objective or a
02:14 - reward this objective can be winning a
02:17 - game or building an opponent so the
02:19 - agent has to decide which step it needs
02:21 - to take or which option it needs to be
02:24 - selected out of the various available
02:26 - options based on the choice he gets
02:29 - either an award or a penalty also
02:32 - selecting a choice opens up new
02:34 - possibilities so you have to choose one
02:36 - state out of the many states available
02:38 - and this choice has to be made while
02:41 - keeping in view that the reward is to be
02:43 - maximized if you google about the most
02:48 - popular programming languages for
02:50 - machine learning and data science you
02:52 - will find out that Python tops the list
02:54 - and it would be great to know that
02:57 - scikit-learn is one of the most popular
02:59 - libraries to implement the machine
03:02 - learning algorithms it is easy it is
03:04 - clean and as a lot of efficient tools
03:06 - and features for classification
03:07 - regression clustering and so on it is
03:10 - built on number II Syfy and so up live
03:12 - but it is also recommended to install
03:15 - matplotlib method because the my program
03:19 - is an excellent blotting library and
03:21 - makes the data visualizations very easy
03:24 - let's move ahead to Python and see how
03:28 - to install the cyclone library we are
03:31 - assuming that you have installed the
03:32 - latest version of Python and set path
03:34 - and all the other variables correctly
03:37 - now let's type import s killer as you
03:46 - can see an error has been shown by
03:48 - python that team module has not been
03:50 - formed that means it has not been
03:51 - installed yet so let's head over to the
03:54 - command prompt and install it using leap
03:56 - it come at opened command prompt and
03:59 - here goes our first command so we have
04:03 - to install some libraries which are
04:05 - numpy Syfy job live and bad not here to
04:09 - install them we will type pip install
04:16 - numpy
04:20 - as we have pressed enter you can see the
04:26 - process has begun and it will be
04:29 - installed in a very short period of
04:32 - thing
04:35 - in a similar fashion we will write pip
04:38 - install followed by the name of the
04:42 - library which we wish to install for
04:45 - example Syfy or matplotlib or job leper
04:52 - or s killer after pressing the enter key
04:56 - the library will get installed and after
05:00 - that we can import it in Python and use
05:02 - it accordingly
05:04 - there are some queries and suggestions
05:06 - from the previous video for which we are
05:08 - extremely thankful and they are to be
05:11 - addressed so the first one is that this
05:14 - diagram is just used to bring the three
05:16 - keywords in one thing and hasn't really
05:19 - want much to do with the Union
05:21 - intersection and complement or any of
05:23 - those operations of mending a machine is
05:26 - said to learn affects performance of
05:28 - doing a certain task increases with
05:31 - experience also when it's 100 percent
05:35 - correct that there are three main types
05:37 - of machine learning many algorithm
05:40 - stones should be fall under only one
05:43 - there are many algorithms for example
05:45 - say on recommender system that may see
05:49 - parts of supervised and unsupervised and
05:51 - come up with an optimum sense now that
05:54 - you have installed scikit-learn and the
05:56 - other required activities let's type
05:58 - forward before we begin to tune or build
06:01 - our model then obviously first need to
06:04 - load the data to begin our work in the
06:06 - waifus please write also on a short note
06:09 - we don't use all the data available to
06:13 - play in our model we reserve some part
06:15 - of it for testing order now why can't we
06:18 - use the same set of data for the
06:20 - training and testing silver because the
06:22 - model would already know the correct
06:25 - level for those instances it is like
06:27 - getting same summon exercise as it was
06:30 - given in salt problem section example
06:32 - before or getting the exact same
06:35 - question a test which your teacher
06:36 - taught a day before I hope you get the
06:39 - picture
06:39 - well discuss more about splitting our
06:41 - data in training and testing set later
06:43 - but first let's get here with loading
06:46 - the data analyst
06:47 - the library itself comes with a few
06:50 - small data sets which are known as
06:52 - toilet isms and they are mostly used for
06:54 - seeing the manner in which
06:56 - classification and regression algorithms
06:59 - are being implemented here is a list of
07:01 - all the standard toilet assets mentioned
07:04 - in the documentation of this library
07:06 - since they are pre-built no downloading
07:09 - is required and we can load them as
07:11 - follows first of all we'll import
07:13 - circuit learn now from scikit-learn
07:20 - we will import data sets if you are
07:26 - interested in viewing the complete list
07:28 - you can always write the area and in the
07:30 - bracket you will have to write data sets
07:34 - okay so the complete list has been
07:37 - loaded but we are only interested in
07:39 - loading the void intercepts so let's
07:42 - just name a variable say iris and we'll
07:46 - write data sets dot load is so now iris
07:56 - contains the iris dataset and we want to
08:00 - know that how many example how many
08:03 - samples are there what kind of features
08:05 - are there how many features are there so
08:07 - let's start exploring this we can write
08:11 - friend iris thought after that we can
08:18 - write okay the list has been popped up
08:21 - we can use data of each year names file
08:23 - names if we use the feature names then
08:26 - it will actually show that what are the
08:28 - column headings so let us write feature
08:32 - names so you can see that there are four
08:37 - columns first one containing sepal
08:40 - length in centimeters second one
08:41 - containing several width in centimeter
08:43 - third one containing petal length and
08:45 - fourth one containing vital width so
08:48 - there are four columns let's see what's
08:50 - the data
08:53 - iris dot data
08:58 - so you can see it has been written that
09:01 - squeeze text 150 minutes so means that
09:04 - there are 150 examples let's double
09:07 - click on this to expand this okay so as
09:10 - you can see as we had seen earlier there
09:14 - were four columns and accordingly four
09:17 - columns have issued and 150 examples are
09:19 - there so now we know that there are 150
09:24 - flowers whose sepal width and length and
09:27 - petal width and length have been
09:29 - recorded now let us see what are the
09:31 - target values and under what name are
09:35 - deceived so we will again write print
09:39 - Irish dot target 0 1 & 2 are the values
09:48 - in which they are mapped so first 50 so
09:54 - there are 3 kinds of flowers that's why
09:56 - we have three values under the target
10:00 - section and let us say see their names
10:10 - names okay something's wrong I messed up
10:16 - the syntax a bit drained hi this dot
10:24 - names okay yes so the three values which
10:32 - had been mapped to zero one and two
10:33 - actually be flowers to sub for sequel o
10:36 - and virginica for more details about
10:40 - this dataset you can always write print
10:43 - iris dot followed by de SC are all in
10:47 - caps des this will give you a
10:51 - description about this data sector okay
10:57 - let's double click to expand this quiz
10:59 - text
11:04 - so as you can see everything has been
11:06 - specified over here for example number
11:09 - of instances are 150 15 each three
11:12 - classes number of attributes for which
11:14 - we had already seen earlier and these
11:16 - attribute informations are simple and
11:19 - simple width and exit row the classes
11:21 - are also specified over you so in the
11:24 - deac our attribute you can also find
11:27 - who's the author at what time was this
11:29 - dataset released and the specific
11:31 - details like we use to load iris a few
11:36 - moments ago you can similarly load any
11:38 - data by typing or for example load and
11:41 - the school digits or whichever you wish
11:42 - in the same way other than the toilet
11:46 - assets the other set of real world data
11:49 - sets can be fetched and downloaded if
11:51 - necessary these are the list of data
11:54 - sets that can be loaded using the fetch
11:57 - command again this complete list has
12:00 - been taken from the documentation itself
12:03 - open ml dot to RC is also a very popular
12:07 - public repository for machine learning
12:08 - in town here's how we can download a
12:11 - dataset from there suppose you want to
12:13 - download this data set of mice protein
12:15 - levels which are helpful in studying
12:17 - associative learning so just right we'll
12:21 - just search mice routine press the Enter
12:26 - key this result which pops up here all
12:32 - the description about the students that
12:33 - can be found and we have to download
12:36 - this data set on using psychic lock so
12:39 - we input SQL on and from SQL and we
12:41 - import our data sets and after that from
12:47 - SK learn dot ETA sets we import
12:59 - fetch mmm
13:06 - after this we need to name or give a
13:10 - name to a variable and we will write
13:13 - fetch underscore open and then and here
13:19 - we need to specify two things about the
13:21 - file name is equal to mice protein and
13:33 - separated by version mice has been
13:42 - loaded and we can see the details so
13:50 - that is how you can download any data
13:52 - set from the open ml dot orgy repository
13:56 - we can also generate our own leaders
13:59 - exit psychically using the commands
14:01 - which speaking with me that is make
14:03 - underscore for example and make
14:05 - underscore of classifications or make
14:07 - underscore regression but there we are
14:10 - skipping this for now as we believe that
14:12 - it is relatively less important from
14:14 - beginners point of view as compared to
14:17 - other discussed methods previously
14:20 - depending on the data and its format we
14:23 - can use different libraries for it for
14:25 - example it for your data is off the SQL
14:28 - JSON Excel or CSV file then it is best
14:32 - to use understand ready for active
14:34 - pandas comes with read CSV read excel
14:37 - reaches solid read SQL commands which
14:39 - make it a very easy and convenient if
14:41 - you have files in these formats also of
14:44 - the data frames can be created of
14:47 - dictionary type object using lists and
14:50 - tuples but if your file is of binary
14:54 - format for example if it has extension
14:56 - of dot mat or dot a r FF or WAV then it
15:01 - is recommended to use Sai politely and
15:04 - if there is column columnar data then
15:08 - vampire a should always be preferred and
15:10 - as well as the
15:11 - and video our concern then we can load
15:14 - them to an apparent using the SK image
15:17 - memory all these libraries interoperate
15:21 - very well with the psychic law and I
15:24 - leave the links related to these
15:26 - libraries in the description below so by
15:29 - this point of time we think that we have
15:32 - covered all the major Lee use data forms
15:34 - and how to use how to load the data in
15:37 - scikit-learn using them still if you
15:40 - think that there is a data form or any
15:43 - method which is missed and has not been
15:46 - covered yet then do mention us then do
15:50 - mention it in the comments below and we
15:52 - will try to cover it up we are now in a
15:55 - position to take a step forward and to
15:57 - build a model and that is exactly what
16:00 - we were going to do in this video we'll
16:02 - be downloading a data set from Canon and
16:04 - build a classification model and finally
16:07 - we will see how good a model is doing at
16:10 - predicting or classifying now let's
16:14 - proceed to have a look at the data set
16:16 - on which we are going to work so open
16:19 - Kaggle and search for seed from UCI and
16:22 - you will land up on this page let me
16:25 - zoom it so that it is more clearly
16:28 - visible so the file is of just nine KB
16:39 - and it gets downloaded in a city I have
16:42 - already downloaded it it contains three
16:45 - varieties of wheat yes it contains three
16:48 - varieties of wheat kymaro cyan canadian
16:51 - seventy elements each so that gives us a
16:53 - total of four 210 elements it has seven
16:57 - data columns describing each instance or
16:59 - example and finally there is a target
17:01 - variable which is the kind of weed and
17:04 - that is what is to be predicted
17:12 - and these are the column headings or the
17:15 - data which describe each instance of a
17:18 - particular reader after downloading the
17:22 - file if you open it through Excel this
17:24 - is how it will look as you can see there
17:27 - are a total of eight columns seven data
17:29 - columns and finally your target volume
17:32 - and there are a total of two hundred
17:36 - eleven rows and the first one was for
17:38 - heading so this gives us a total of 210
17:40 - rooms and it comes now let's head over
17:43 - to Python and get the real work started
17:47 - first of all we'll import ponders as PD
17:54 - following a convention well now you know
17:59 - variable say total data and read the
18:06 - contents of that file in this way this
18:08 - will be done using Li VD dot read
18:12 - underscore seriously command and if the
18:16 - file is present in the path then you can
18:19 - directly write the name of the file but
18:22 - in case the file is present at some
18:24 - other location you can write R followed
18:28 - by the path of the variable we can
18:43 - recall form about the radius of the data
18:45 - by using describe command that is total
18:50 - underscore theta dot this crime so as we
18:58 - had seen in that data set page itself
19:01 - there are exactly two hundred ten rows
19:04 - indexed from 0 to 209 and there are
19:07 - eight columns as expected so after this
19:12 - we'll take only data variables in no
19:14 - variable say X leaving aside the target
19:18 - variables so we'll let X equal to total
19:20 - underscore data and then we can use
19:24 - method and even the column names or just
19:27 - to drop the last volume but I prefer the
19:29 - I love method because of its clear-cut
19:31 - indexing so we will write tutor Anita
19:34 - 200 code it or dot I lock and we'll put
19:39 - the square brackets then I want all the
19:42 - rules and the column number zero to
19:47 - seventh and if you are familiar with is
19:50 - mixing then you know that by doing this
19:52 - we are not including the last column and
19:58 - again we can get reconfirmed about this
20:02 - selection by typing X dot info you can
20:07 - see the last column that is the column
20:10 - of target variables has been dropped and
20:12 - we are left with two only ten rows and
20:14 - seven columns as we had expected in a
20:19 - similar fashion we may use a variable
20:21 - semi to store all the target variables
20:24 - and will underscore data and give us the
20:29 - I lock method put the square brackets
20:32 - I want all the rows but only the last
20:35 - column again we can check out all the
20:39 - details but I think Y dot describe so
20:43 - yeah two hundred ten rows
20:46 - we will now import scikit-learn and
20:48 - other required memories for this project
20:59 - from scalar import SVM that is support
21:08 - vector machine and from SQL on dot SVM
21:16 - let's do put yes we see that is a
21:19 - support vector classifier okay looks
21:22 - like I messed up the syntax a bit to the
21:26 - capital
21:27 - SVC yeah that's good before we move
21:31 - ahead and work on the support vector
21:33 - classifier let's discuss a few terms
21:35 - about it and have a look about what it
21:37 - actually does
21:38 - so SVC is a class offense and it's of
21:42 - course super west because we have to
21:44 - supply label training later what is does
21:47 - is that it comes with depressed or
21:49 - optimal separating boundary or
21:51 - hyperplane which classifies new unseen
21:54 - examples the different kinds or
21:57 - categories of data are separated by a
21:59 - clear cap as well as possible there are
22:02 - a number of parameters that can be tuned
22:04 - if required and we will discuss them too
22:06 - once we train our model later in this
22:08 - video itself however for simple data
22:11 - sets like the one we are going to use
22:13 - the default settings work out to be just
22:15 - fine as we have already explained about
22:20 - the trained and tested on in previous
22:23 - videos we'll just partition the current
22:25 - dataset into the ratio specified by us
22:28 - so so from a scale or not model
22:44 - selection import rail test split test
22:56 - split if no analyse applied the default
23:04 - ratio is 3 is to 1 that is 75% of data
23:07 - for training and 25% for testing also
23:10 - it's not like the force 25% instances
23:13 - will be used for testing or last 25 the
23:17 - examples are selected randomly so we'll
23:20 - name some variables such as X 3 X test Y
23:28 - tween and Y test then we'll write 3 test
23:39 - split
23:43 - then we'll specify the data which is X
23:47 - then our target variable that is fine
23:50 - and after that we can write test size
23:55 - equals two point two that means 20% of
23:58 - the total data will be used as testing
24:00 - center if nothing is specified then as
24:02 - we had said previously 0.25 is selected
24:06 - by default and we can specify one more
24:10 - variable that is one more parameter
24:13 - actually that is random statement and we
24:16 - can give any number because this is
24:18 - actually the stir atom number generator
24:20 - and the number provided over here will
24:23 - be used to see the values so let's say
24:25 - of 30 now we will import standard scale
24:33 - of so we read from SQL or dot
24:38 - pre-processing input standard scale it
24:56 - is always a good practice to scale the
24:58 - data if you would have paid attention
25:00 - there were some data which were zero
25:03 - point something and somewhere near
25:05 - around 15 16 or so in other data sets
25:08 - this difference may be even bigger so
25:11 - what happens is that this use chance
25:13 - that the bigger number will outwit the
25:16 - parameters these parameters and the
25:19 - smaller number won't be cut in you
25:20 - pigments so to bring them on a
25:23 - relatively seems clear where the
25:24 - difference isn't much pronounced we
25:26 - scale it and provide a level playing
25:29 - field the gradient descent also takes a
25:32 - lot of time to converge at a minimum
25:34 - because the contours are skewed if not
25:37 - scaled but ok if we start discussing
25:40 - about that this video will become too
25:42 - let me do let us know if you want to
25:44 - cover if you want gradient descent to be
25:47 - covered in the next video but for now
25:49 - let's focus on the implementation so
25:52 - will now write SC equals 2 standard
25:55 - scalar
25:55 - followed by a couple of
25:56 - brackets after that will write extreme
26:00 - is equal to SC dot fit transform extreme
26:03 - and X test as just SC dot transform of X
26:07 - test now you must be wondering that why
26:10 - we are using fit transform method in
26:12 - case of train and transfer just the
26:14 - transform method in case of test so the
26:17 - thing is that we can use
26:19 - Budi fit and transform method one after
26:21 - the other or we can Club it together as
26:23 - we have done in the training set so when
26:27 - we scaled in it out we actually apply
26:28 - the mean normalization so the two
26:31 - parameters that are learned while
26:33 - scaling the training data the same
26:35 - parameters are used to scheme the test
26:37 - data and hence no fit method is used in
26:41 - case of test data so let's proceed now
26:46 - let's name a variable and call our
26:49 - classifier after pass after calling
26:57 - classifier we will now pass the training
27:01 - data in it that is will write CL f dot v
27:06 - that will write x let's go for my Y
27:11 - under school train there are lot of
27:17 - parameters but they will discuss about
27:20 - the most important ones so C is the
27:23 - penalty parameter if C is less decision
27:27 - boundaries will be smooth which is
27:29 - desirable but accuracy only that if C is
27:33 - worth then overfitting may occur but
27:36 - accuracy in business so there is a
27:38 - trade-off
27:39 - again we have different algorithms
27:41 - regarding this tool but for now let's
27:44 - stick to default Williams kernel is RBF
27:48 - RB a stands for radial basis function
27:52 - and it has a finite number of dimensions
27:56 - it it can have infinite number of
27:58 - dimensions and its value decreases as we
28:00 - move away from the center
28:03 - finally we have gamma which which is a
28:06 - parameter for nonlinear equity and the
28:10 - iron
28:10 - the higher it tries to fit the training
28:13 - data more exactly we are now left with
28:16 - the job of predicting using our
28:19 - classifier so we'll name a variable say
28:22 - red underscore CLS and we'll call the
28:27 - predict function see a left dot read it
28:30 - and we'll pass X underscore test to it
28:35 - but wait our job is not finished here oh
28:38 - we need to see how accurate our model is
28:41 - so we'll write the skill on dot matrix
28:54 - dot accuracy underscore we'll pass
29:04 - y underscore test that is the what it
29:08 - should have in and parity underscore CLF
29:13 - that is what has been predicted so its
29:19 - accuracy level is more than 95% and
29:21 - hence we can say that our SVM classifier
29:24 - is doing a pretty good job in predicting
29:26 - if you want to dig further deeper you
29:30 - can also generate a classification build
29:32 - report for that you will have to write a
29:36 - still on dot matrix don't classification
29:47 - report and again what it should have
29:54 - been
29:54 - y underscore test and what is fed is
29:56 - predicted third understood see LF it was
30:03 - not very readable so I use the Print
30:06 - command and now you can see that it is
30:09 - all being beautifully formatted in our
30:12 - table format here precision is the
30:17 - ability of classifier not to nibble an
30:19 - instance positive that is actually
30:21 - negative it is actually
30:24 - of positive predictions and the recall
30:27 - is ability of classifier to find all
30:29 - positive instances it is a fraction of
30:32 - positives that were correctly identified
30:35 - an f1 score is a harmonic mean of
30:38 - precision and recall so finally we made
30:42 - a model whose accuracy was more than 95%
30:44 - and there were quite a few new dumps
30:47 - methods and concepts in this video if
30:49 - you want to know about any particular
30:52 - topic in detail or you are unclear about
30:55 - any of the process so over here and make
30:58 - sure to leave it as a comment below and
30:59 - we will try to take it up in the next
31:02 - video thank you for your time this is
31:04 - the V shake sync from programming
31:05 - knowledge and check out other videos and
31:08 - subscribe to this channel for more such
31:09 - coordinate thank you in this video we
31:13 - will learn about the concept of pipeline
31:15 - now a typical pipeline consists of a
31:18 - pre-processing step that transforms or
31:21 - imputes the data and the final predictor
31:23 - that predicts target values so
31:26 - transformers and estimators that is the
31:29 - predictors can be combined together into
31:32 - a single unifying object it is a
31:34 - pipeline so pipeline is combining
31:37 - different Transformers array estimators
31:40 - to automate machine learning workflows
31:42 - as it sequentially applies a list of
31:45 - Trance a list of transforms and a final
31:49 - estimate now that we have now that we
31:53 - have a some theoretical idea about
31:57 - pipelining let's head over to Python and
31:59 - code it's simple implementation and see
32:02 - how it is actually done so first of all
32:06 - we'll import the required packages and
32:08 - datasets so let's first import the iris
32:12 - dataset scale on your data set import
32:24 - load is
32:30 - no we had used a standard scaler the
32:34 - last time so this time let's try the
32:36 - min/max kilo we will discuss about it in
32:39 - a while but first let's import the come
32:43 - in Mac scaler first scale on dot
32:52 - pre-processing input n max scale
33:05 - now we'll import a simple linear model
33:08 - logistic regression so from a scale on a
33:28 - stick
33:39 - and for splitting the data into training
33:42 - and testing set the model selection s
33:44 - accordingly imported s Kalon dot model
33:55 - selection import train test split and
34:13 - finally we'll import the pipeline so
34:17 - from this Kalon lured pipeline will
34:26 - import pipeline my new disc it's case
34:34 - sensitive so be alert and use a capital
34:37 - P later and a small P before most of the
34:40 - things that we have imported we have
34:42 - discussed about it in the previous
34:44 - videos so I'm not going to detail into
34:47 - all of these and in case you missed any
34:50 - of those concepts then do see our
34:53 - previous videos in this series and still
34:56 - if you are having any confusion then to
34:58 - comment it as a comment below here X is
35:03 - an element of a particular instance
35:06 - so what min/max killer does is that it
35:09 - first subtract the minimum of XS from X
35:12 - and divides the result by the difference
35:15 - of XS Max and min this intermediate
35:19 - result is now multiplied to the rage
35:21 - that is a Max and Max minus min and then
35:26 - the main is added min and Max are
35:29 - nothing but feature ages
35:32 - now let's write iris parecer variable
35:38 - which will load our iris dataset is
35:41 - equal to load iris
35:44 - after that we'll split our descent into
35:48 - training and test sent so X underscore
35:52 - three comma X underscore test comma y
35:59 - underscore train and y underscore test
36:07 - it's called trail test split method this
36:17 - will write iris dot theta for extreme
36:24 - then iris dot target these are for x and
36:33 - y respectively and then we'll specify
36:36 - that how much percentage of the whole
36:38 - dataset we want to P as our testing
36:41 - center so let's say our zero point two
36:44 - that is the twenty percent means twenty
36:47 - percent of our data will be used for
36:49 - testing and let's see the random
36:53 - variable so random state let's say 42
37:01 - now we will create the pipeline again a
37:04 - variable type underscore lr4 logistic
37:08 - regression pipeline my new year peace
37:14 - capital
37:22 - again a variables name min max 4 min max
37:31 - killer after this now that this killer
37:43 - has been added let's add is the logistic
37:49 - regression estimator or predictor a
38:01 - stick crash
38:18 - okay now let's fit our data in this
38:24 - pipeline so pipe underscore L r dot fit
38:32 - extreme and white rain extreme comma y
38:37 - three okay now let's see the score name
38:52 - now let's name we will score and store
38:55 - this coordinate so pipe underscore L our
39:00 - third score let's see how good our model
39:03 - is doing X underscore test comma Y
39:07 - underscore test so as you can see our
39:16 - model is 90% accurate in guessing the
39:20 - unknown examples or the examples that it
39:24 - has not seen before and hence we have
39:26 - created a simple pipeline in which we
39:29 - have first scaled the data using the
39:31 - vascular and then applied a simple
39:34 - linear Rod Steiger regression for
39:38 - classifying the iris dataset for any
39:43 - machine learning model there are some
39:45 - hyper parameters which can be adjusted
39:48 - and in turn they change the efficiency
39:51 - or the accuracy of the model so even if
39:55 - we fail finalize the machine learning
39:57 - model that we are going to work with a
39:59 - lot of job is still left as we have to
40:02 - adjust the hyper parameters one route
40:05 - force approach would be to start with
40:07 - default values and keep on adjusting
40:09 - manually according to our intuition but
40:12 - that would be very time-consuming and
40:14 - does not have any clear-cut education
40:16 - about whether we have reached an optimum
40:19 - value or not that's where grid search
40:22 - comes out to make our life a little bit
40:23 - easier we have already worked with the
40:26 - SV
40:26 - and the logistic regression so instead
40:29 - of repeating them again let's learn
40:31 - something new in this video first we'll
40:34 - learn about a new efficient classifier
40:36 - that is a random forest classifier and
40:38 - then we will use grid search to units
40:41 - hyper parameters for understanding
40:46 - random forest classifier let us take
40:48 - this interesting something suppose you
40:51 - want to visit any one of the following
40:53 - cities which are Delhi Mumbai Kolkata
40:55 - and Chennai but you are not sure which
40:58 - one so you make a list of these four
41:00 - cities and distribute to your 100
41:02 - friends and ask them to choose their
41:05 - favorite one out of these some choose
41:07 - Delhi some choose Mumbai but after the
41:09 - voting you found out that Kolkata has
41:11 - got the maximum number of foods so now
41:13 - you can be sure that you want to go to
41:15 - Kolkata
41:16 - that is how a random forest classifier
41:18 - works a random forest is a meta is a
41:22 - meta estimator that fits a number of
41:24 - four decision trees on various sub
41:27 - samples of data set and uses average to
41:30 - improve predictive accuracy drawing
41:33 - similarities we can say that these
41:36 - hundreds friends were like hundred
41:38 - decision trees and each one of them
41:39 - predicted a value and the final outcome
41:42 - was decided by a majority of voting that
41:45 - is the forest let's see how it is
41:49 - implemented in Python
41:51 - so as usual post a for will import
41:54 - circuit law from socket one dot n symbol
42:05 - we will import random forest classifier
42:19 - then from SQL Oh oops from Keylong dot
42:28 - datasets will report god iris dataset
42:36 - that is
42:38 - bring Woking load iris let's limit
42:44 - variable and load our iris data into it
42:54 - now we have to split on data set into
42:59 - training and testing set so for that
43:01 - will import s from a scale own dot model
43:11 - selection import train test
43:20 - split now let's split our data right
43:27 - extreme right X test then we'll read Y
43:33 - train then we'll write Y test now let's
43:42 - call this function train test split and
43:48 - then it will read iris dot a dot then is
43:54 - no target let's specify the tests nice
44:00 - say 20% and see the random value state
44:13 - equals to say 30 now let's call our
44:22 - classifier CLS equals to random forest
44:29 - classifier it can be calling this way
44:34 - also if we do not specify everything
44:36 - that only the default values will be
44:38 - supplied but for the purpose of this
44:40 - video we are going to work with three
44:43 - very basic hyperparameters which are
44:45 - number of estimators minimum samples net
44:47 - and minimum samples leaf so
44:51 - default value for number of estimators
44:53 - is 10 that is 10 this is a decision
44:56 - trees are used let's suppose we are
44:59 - going to work which is to number of
45:02 - estimators to and after that we can use
45:07 - min sample split then samples split
45:15 - equals to 3 the default value for this
45:18 - is 2 and then sample split is the
45:21 - minimum number of samples required to
45:23 - split an internal so of course it's
45:25 - difficult value is 2 and the third
45:28 - parameter that we are going to work with
45:30 - is minimum samples leave now what is
45:36 - minimum sample sleeve it is the minimum
45:38 - number of samples required at the pace
45:40 - that is a DB flow so its default value
45:43 - is 1 at least one sample is required
45:46 - over there so let's try to now let's fit
45:54 - our data into this classifier CLF not
45:57 - fit X train comma Y train ok the upper
46:12 - parameters are as we had specified them
46:16 - as you can see that minimum samples is -
46:18 - minimum sample sample leaf is - minimum
46:20 - sample split is 3 the number of
46:23 - estimators are - so yeah let us go ahead
46:27 - after fitting this we are left with the
46:30 - job of predicting third use red
46:33 - underscore CLF goes to CL f dot
46:37 - predicted and what we are going to
46:42 - predict X test now for checking the
46:49 - accuracy well-liked scale on dot metrics
46:54 - dot accuracy score and in the brackets
47:02 - will write y underscore test and red
47:07 - underscore CLF let's see how much
47:13 - accurate our model is with these hyper
47:15 - parameters okay it's already 93 percent
47:20 - accurate
47:20 - so now we'll see that can we improve
47:23 - this accuracy or not so in order to
47:27 - improve the accuracy will try to adjust
47:30 - or doing the other parameters using grid
47:33 - search so first of all let's import grid
47:35 - search from SQL on the dot model
47:44 - selection input grid search
47:55 - CV after that we have to specify a
48:00 - parameter grid parameter could what this
48:06 - parameter grid contains is it contains a
48:09 - various number of features and we have
48:12 - to specify them according to our wish so
48:15 - what will take each one of the
48:18 - combination and dry it out so in our
48:22 - case since we have chosen to work with
48:24 - these three features so first of all
48:26 - we'll specify any
48:28 - estimators now what are the values that
48:35 - we can walk that it can take so we have
48:39 - taken 3 initially so the - sorry so
48:45 - let's say - 5 10 20 suppose that my
48:52 - number of estimators can take any one of
48:54 - these 4 values after that I can write
49:00 - my next feature that is men samples
49:05 - split and this will be it was given
49:13 - three so let's say two and three after
49:18 - that which one features their principles
49:22 - many samples leave and main samples we
49:30 - can take up any of these any of the
49:32 - values like its default value was 1 so
49:36 - let's try 1 2 & 3 so basically what we
49:43 - have done is we have given it four
49:46 - options for number of estimators two
49:48 - options for minimum sample split and
49:50 - three options for minimum sample leave
49:52 - so a total of 4 into 2 into 3 it is 24
49:56 - option so it is going to dry on our
49:59 - model and the one with the best accuracy
50:02 - score will get selected and that is how
50:05 - grid search works so that is number of
50:07 - estimators can take up any of these 4
50:09 - values 2 5 10 or 20 minimum sample split
50:12 - can take any of these two value that is
50:13 - 2 or 3 and minimum circles leaf can take
50:16 - similarly any of these two values which
50:18 - are 1 2 & 3
50:20 - now let's write grid search and we will
50:29 - first call me grid search CV then
50:38 - specify our estimator or estimator was
50:42 - CLF which is the random forest
50:44 - classifier of course and after that we
50:48 - have to specify the parameter grid both
50:51 - have been saved by the same name I think
50:52 - so yeah param grid it will be after that
51:03 - after this we have to fit the data in
51:07 - our grid so grid underscore search
51:12 - dot fit lady under school serves not
51:16 - wait extry and white rain extreme my
51:22 - dream some kind of warning okay we'll
51:29 - ignore the warning and now that target
51:33 - search has been research has tenets
51:36 - working so we'll see what are the best
51:39 - parameters that it has come up with grid
51:43 - search dot best underscore para where I
51:54 - am status parameters so the grid search
51:59 - has told us that minimum sample sleeve
52:02 - should be one minimum sample spirit
52:04 - should be due and the number of
52:05 - estimator should be five for the optimum
52:08 - solution so let's do now random forest
52:10 - classifier that is our model accordingly
52:12 - according to these - parameters and then
52:15 - check our accuracy till this point the
52:19 - code is exactly the same as we had seen
52:21 - a few moments ago so from this point or
52:24 - will specify the hyper parameters that
52:27 - we have learned using grid search
52:29 - so number of estimators were five
52:41 - samples leave one and then samples split
52:52 - use - no you just have to filter data
52:59 - CLF not fit X train and then by train
53:09 - underscore train okay Rita has been fat
53:14 - now let's do the prediction thread
53:17 - underscore CL f equals to CL f dot
53:21 - predict and after that we'll write X
53:25 - underscore test the prediction has been
53:29 - done now the last step that is we're
53:31 - gonna check its accuracy Escalon dot
53:34 - matrix dot accuracy underscore school
53:41 - and accuracy we have to check Y
53:45 - underscore test as compared to red
53:48 - underscore CLF okay so as you can see
53:55 - the accuracy has gone to 100% this might
53:58 - be a case of overfitting but one thing
54:00 - is for sure that we do our hyper
54:03 - parameters using grid search and then
54:06 - using those hyper parameters and the
54:08 - accuracy for model definitely improved

Cleaned transcript:

hello everyone welcome to programming knowledge in this video we will discuss what is machine learning and what exactly is scikitlearn all about and why is it so important from the point of view of machine learning we will also see how to install it properly and get going first things first let's see briefly what machine learning actually is without going into formal textbook definition in very simple terms one can say that if performance of doing a particular task is improving with experience then we can say that the machine or the model is learning so there would be some tasks and we'll see how could the task is being done and this is done by measuring the performance if we see that the performance is if we see that the performance is increasing and getting better with experience then we can say that our program or our machine is learning mainly there are three types of machine learning or we can say that all machine learning algorithms basically fall under these three categories first one is supervised learning second consumer was running and the third one B reinforcement or in case of supervised learning the correct label or answer is provided with data as a classical example the size of houses are given with prices after feeding this data the program will learn and build up a hypothesis or model and then we will give our new size and the model will predict how much price should be based on the training data that is the data we fed earlier in supervised learning no levels are provided instead based on the similarity in data it creates groups by itself if you have visited Google News before then you must have noticed that a particular piece of news covered by several websites are grouped under one section these are not just grouped these are rather clustered in one section this can be one example of unsupervised learning in case of reinforcement learning there is an agent and he has to fulfill an objective or a reward this objective can be winning a game or building an opponent so the agent has to decide which step it needs to take or which option it needs to be selected out of the various available options based on the choice he gets either an award or a penalty also selecting a choice opens up new possibilities so you have to choose one state out of the many states available and this choice has to be made while keeping in view that the reward is to be maximized if you google about the most popular programming languages for machine learning and data science you will find out that Python tops the list and it would be great to know that scikitlearn is one of the most popular libraries to implement the machine learning algorithms it is easy it is clean and as a lot of efficient tools and features for classification regression clustering and so on it is built on number II Syfy and so up live but it is also recommended to install matplotlib method because the my program is an excellent blotting library and makes the data visualizations very easy let's move ahead to Python and see how to install the cyclone library we are assuming that you have installed the latest version of Python and set path and all the other variables correctly now let's type import s killer as you can see an error has been shown by python that team module has not been formed that means it has not been installed yet so let's head over to the command prompt and install it using leap it come at opened command prompt and here goes our first command so we have to install some libraries which are numpy Syfy job live and bad not here to install them we will type pip install numpy as we have pressed enter you can see the process has begun and it will be installed in a very short period of thing in a similar fashion we will write pip install followed by the name of the library which we wish to install for example Syfy or matplotlib or job leper or s killer after pressing the enter key the library will get installed and after that we can import it in Python and use it accordingly there are some queries and suggestions from the previous video for which we are extremely thankful and they are to be addressed so the first one is that this diagram is just used to bring the three keywords in one thing and hasn't really want much to do with the Union intersection and complement or any of those operations of mending a machine is said to learn affects performance of doing a certain task increases with experience also when it's 100 percent correct that there are three main types of machine learning many algorithm stones should be fall under only one there are many algorithms for example say on recommender system that may see parts of supervised and unsupervised and come up with an optimum sense now that you have installed scikitlearn and the other required activities let's type forward before we begin to tune or build our model then obviously first need to load the data to begin our work in the waifus please write also on a short note we don't use all the data available to play in our model we reserve some part of it for testing order now why can't we use the same set of data for the training and testing silver because the model would already know the correct level for those instances it is like getting same summon exercise as it was given in salt problem section example before or getting the exact same question a test which your teacher taught a day before I hope you get the picture well discuss more about splitting our data in training and testing set later but first let's get here with loading the data analyst the library itself comes with a few small data sets which are known as toilet isms and they are mostly used for seeing the manner in which classification and regression algorithms are being implemented here is a list of all the standard toilet assets mentioned in the documentation of this library since they are prebuilt no downloading is required and we can load them as follows first of all we'll import circuit learn now from scikitlearn we will import data sets if you are interested in viewing the complete list you can always write the area and in the bracket you will have to write data sets okay so the complete list has been loaded but we are only interested in loading the void intercepts so let's just name a variable say iris and we'll write data sets dot load is so now iris contains the iris dataset and we want to know that how many example how many samples are there what kind of features are there how many features are there so let's start exploring this we can write friend iris thought after that we can write okay the list has been popped up we can use data of each year names file names if we use the feature names then it will actually show that what are the column headings so let us write feature names so you can see that there are four columns first one containing sepal length in centimeters second one containing several width in centimeter third one containing petal length and fourth one containing vital width so there are four columns let's see what's the data iris dot data so you can see it has been written that squeeze text 150 minutes so means that there are 150 examples let's double click on this to expand this okay so as you can see as we had seen earlier there were four columns and accordingly four columns have issued and 150 examples are there so now we know that there are 150 flowers whose sepal width and length and petal width and length have been recorded now let us see what are the target values and under what name are deceived so we will again write print Irish dot target 0 1 & 2 are the values in which they are mapped so first 50 so there are 3 kinds of flowers that's why we have three values under the target section and let us say see their names names okay something's wrong I messed up the syntax a bit drained hi this dot names okay yes so the three values which had been mapped to zero one and two actually be flowers to sub for sequel o and virginica for more details about this dataset you can always write print iris dot followed by de SC are all in caps des this will give you a description about this data sector okay let's double click to expand this quiz text so as you can see everything has been specified over here for example number of instances are 150 15 each three classes number of attributes for which we had already seen earlier and these attribute informations are simple and simple width and exit row the classes are also specified over you so in the deac our attribute you can also find who's the author at what time was this dataset released and the specific details like we use to load iris a few moments ago you can similarly load any data by typing or for example load and the school digits or whichever you wish in the same way other than the toilet assets the other set of real world data sets can be fetched and downloaded if necessary these are the list of data sets that can be loaded using the fetch command again this complete list has been taken from the documentation itself open ml dot to RC is also a very popular public repository for machine learning in town here's how we can download a dataset from there suppose you want to download this data set of mice protein levels which are helpful in studying associative learning so just right we'll just search mice routine press the Enter key this result which pops up here all the description about the students that can be found and we have to download this data set on using psychic lock so we input SQL on and from SQL and we import our data sets and after that from SK learn dot ETA sets we import fetch mmm after this we need to name or give a name to a variable and we will write fetch underscore open and then and here we need to specify two things about the file name is equal to mice protein and separated by version mice has been loaded and we can see the details so that is how you can download any data set from the open ml dot orgy repository we can also generate our own leaders exit psychically using the commands which speaking with me that is make underscore for example and make underscore of classifications or make underscore regression but there we are skipping this for now as we believe that it is relatively less important from beginners point of view as compared to other discussed methods previously depending on the data and its format we can use different libraries for it for example it for your data is off the SQL JSON Excel or CSV file then it is best to use understand ready for active pandas comes with read CSV read excel reaches solid read SQL commands which make it a very easy and convenient if you have files in these formats also of the data frames can be created of dictionary type object using lists and tuples but if your file is of binary format for example if it has extension of dot mat or dot a r FF or WAV then it is recommended to use Sai politely and if there is column columnar data then vampire a should always be preferred and as well as the and video our concern then we can load them to an apparent using the SK image memory all these libraries interoperate very well with the psychic law and I leave the links related to these libraries in the description below so by this point of time we think that we have covered all the major Lee use data forms and how to use how to load the data in scikitlearn using them still if you think that there is a data form or any method which is missed and has not been covered yet then do mention us then do mention it in the comments below and we will try to cover it up we are now in a position to take a step forward and to build a model and that is exactly what we were going to do in this video we'll be downloading a data set from Canon and build a classification model and finally we will see how good a model is doing at predicting or classifying now let's proceed to have a look at the data set on which we are going to work so open Kaggle and search for seed from UCI and you will land up on this page let me zoom it so that it is more clearly visible so the file is of just nine KB and it gets downloaded in a city I have already downloaded it it contains three varieties of wheat yes it contains three varieties of wheat kymaro cyan canadian seventy elements each so that gives us a total of four 210 elements it has seven data columns describing each instance or example and finally there is a target variable which is the kind of weed and that is what is to be predicted and these are the column headings or the data which describe each instance of a particular reader after downloading the file if you open it through Excel this is how it will look as you can see there are a total of eight columns seven data columns and finally your target volume and there are a total of two hundred eleven rows and the first one was for heading so this gives us a total of 210 rooms and it comes now let's head over to Python and get the real work started first of all we'll import ponders as PD following a convention well now you know variable say total data and read the contents of that file in this way this will be done using Li VD dot read underscore seriously command and if the file is present in the path then you can directly write the name of the file but in case the file is present at some other location you can write R followed by the path of the variable we can recall form about the radius of the data by using describe command that is total underscore theta dot this crime so as we had seen in that data set page itself there are exactly two hundred ten rows indexed from 0 to 209 and there are eight columns as expected so after this we'll take only data variables in no variable say X leaving aside the target variables so we'll let X equal to total underscore data and then we can use method and even the column names or just to drop the last volume but I prefer the I love method because of its clearcut indexing so we will write tutor Anita 200 code it or dot I lock and we'll put the square brackets then I want all the rules and the column number zero to seventh and if you are familiar with is mixing then you know that by doing this we are not including the last column and again we can get reconfirmed about this selection by typing X dot info you can see the last column that is the column of target variables has been dropped and we are left with two only ten rows and seven columns as we had expected in a similar fashion we may use a variable semi to store all the target variables and will underscore data and give us the I lock method put the square brackets I want all the rows but only the last column again we can check out all the details but I think Y dot describe so yeah two hundred ten rows we will now import scikitlearn and other required memories for this project from scalar import SVM that is support vector machine and from SQL on dot SVM let's do put yes we see that is a support vector classifier okay looks like I messed up the syntax a bit to the capital SVC yeah that's good before we move ahead and work on the support vector classifier let's discuss a few terms about it and have a look about what it actually does so SVC is a class offense and it's of course super west because we have to supply label training later what is does is that it comes with depressed or optimal separating boundary or hyperplane which classifies new unseen examples the different kinds or categories of data are separated by a clear cap as well as possible there are a number of parameters that can be tuned if required and we will discuss them too once we train our model later in this video itself however for simple data sets like the one we are going to use the default settings work out to be just fine as we have already explained about the trained and tested on in previous videos we'll just partition the current dataset into the ratio specified by us so so from a scale or not model selection import rail test split test split if no analyse applied the default ratio is 3 is to 1 that is 75% of data for training and 25% for testing also it's not like the force 25% instances will be used for testing or last 25 the examples are selected randomly so we'll name some variables such as X 3 X test Y tween and Y test then we'll write 3 test split then we'll specify the data which is X then our target variable that is fine and after that we can write test size equals two point two that means 20% of the total data will be used as testing center if nothing is specified then as we had said previously 0.25 is selected by default and we can specify one more variable that is one more parameter actually that is random statement and we can give any number because this is actually the stir atom number generator and the number provided over here will be used to see the values so let's say of 30 now we will import standard scale of so we read from SQL or dot preprocessing input standard scale it is always a good practice to scale the data if you would have paid attention there were some data which were zero point something and somewhere near around 15 16 or so in other data sets this difference may be even bigger so what happens is that this use chance that the bigger number will outwit the parameters these parameters and the smaller number won't be cut in you pigments so to bring them on a relatively seems clear where the difference isn't much pronounced we scale it and provide a level playing field the gradient descent also takes a lot of time to converge at a minimum because the contours are skewed if not scaled but ok if we start discussing about that this video will become too let me do let us know if you want to cover if you want gradient descent to be covered in the next video but for now let's focus on the implementation so will now write SC equals 2 standard scalar followed by a couple of brackets after that will write extreme is equal to SC dot fit transform extreme and X test as just SC dot transform of X test now you must be wondering that why we are using fit transform method in case of train and transfer just the transform method in case of test so the thing is that we can use Budi fit and transform method one after the other or we can Club it together as we have done in the training set so when we scaled in it out we actually apply the mean normalization so the two parameters that are learned while scaling the training data the same parameters are used to scheme the test data and hence no fit method is used in case of test data so let's proceed now let's name a variable and call our classifier after pass after calling classifier we will now pass the training data in it that is will write CL f dot v that will write x let's go for my Y under school train there are lot of parameters but they will discuss about the most important ones so C is the penalty parameter if C is less decision boundaries will be smooth which is desirable but accuracy only that if C is worth then overfitting may occur but accuracy in business so there is a tradeoff again we have different algorithms regarding this tool but for now let's stick to default Williams kernel is RBF RB a stands for radial basis function and it has a finite number of dimensions it it can have infinite number of dimensions and its value decreases as we move away from the center finally we have gamma which which is a parameter for nonlinear equity and the iron the higher it tries to fit the training data more exactly we are now left with the job of predicting using our classifier so we'll name a variable say red underscore CLS and we'll call the predict function see a left dot read it and we'll pass X underscore test to it but wait our job is not finished here oh we need to see how accurate our model is so we'll write the skill on dot matrix dot accuracy underscore we'll pass y underscore test that is the what it should have in and parity underscore CLF that is what has been predicted so its accuracy level is more than 95% and hence we can say that our SVM classifier is doing a pretty good job in predicting if you want to dig further deeper you can also generate a classification build report for that you will have to write a still on dot matrix don't classification report and again what it should have been y underscore test and what is fed is predicted third understood see LF it was not very readable so I use the Print command and now you can see that it is all being beautifully formatted in our table format here precision is the ability of classifier not to nibble an instance positive that is actually negative it is actually of positive predictions and the recall is ability of classifier to find all positive instances it is a fraction of positives that were correctly identified an f1 score is a harmonic mean of precision and recall so finally we made a model whose accuracy was more than 95% and there were quite a few new dumps methods and concepts in this video if you want to know about any particular topic in detail or you are unclear about any of the process so over here and make sure to leave it as a comment below and we will try to take it up in the next video thank you for your time this is the V shake sync from programming knowledge and check out other videos and subscribe to this channel for more such coordinate thank you in this video we will learn about the concept of pipeline now a typical pipeline consists of a preprocessing step that transforms or imputes the data and the final predictor that predicts target values so transformers and estimators that is the predictors can be combined together into a single unifying object it is a pipeline so pipeline is combining different Transformers array estimators to automate machine learning workflows as it sequentially applies a list of Trance a list of transforms and a final estimate now that we have now that we have a some theoretical idea about pipelining let's head over to Python and code it's simple implementation and see how it is actually done so first of all we'll import the required packages and datasets so let's first import the iris dataset scale on your data set import load is no we had used a standard scaler the last time so this time let's try the min/max kilo we will discuss about it in a while but first let's import the come in Mac scaler first scale on dot preprocessing input n max scale now we'll import a simple linear model logistic regression so from a scale on a stick and for splitting the data into training and testing set the model selection s accordingly imported s Kalon dot model selection import train test split and finally we'll import the pipeline so from this Kalon lured pipeline will import pipeline my new disc it's case sensitive so be alert and use a capital P later and a small P before most of the things that we have imported we have discussed about it in the previous videos so I'm not going to detail into all of these and in case you missed any of those concepts then do see our previous videos in this series and still if you are having any confusion then to comment it as a comment below here X is an element of a particular instance so what min/max killer does is that it first subtract the minimum of XS from X and divides the result by the difference of XS Max and min this intermediate result is now multiplied to the rage that is a Max and Max minus min and then the main is added min and Max are nothing but feature ages now let's write iris parecer variable which will load our iris dataset is equal to load iris after that we'll split our descent into training and test sent so X underscore three comma X underscore test comma y underscore train and y underscore test it's called trail test split method this will write iris dot theta for extreme then iris dot target these are for x and y respectively and then we'll specify that how much percentage of the whole dataset we want to P as our testing center so let's say our zero point two that is the twenty percent means twenty percent of our data will be used for testing and let's see the random variable so random state let's say 42 now we will create the pipeline again a variable type underscore lr4 logistic regression pipeline my new year peace capital again a variables name min max 4 min max killer after this now that this killer has been added let's add is the logistic regression estimator or predictor a stick crash okay now let's fit our data in this pipeline so pipe underscore L r dot fit extreme and white rain extreme comma y three okay now let's see the score name now let's name we will score and store this coordinate so pipe underscore L our third score let's see how good our model is doing X underscore test comma Y underscore test so as you can see our model is 90% accurate in guessing the unknown examples or the examples that it has not seen before and hence we have created a simple pipeline in which we have first scaled the data using the vascular and then applied a simple linear Rod Steiger regression for classifying the iris dataset for any machine learning model there are some hyper parameters which can be adjusted and in turn they change the efficiency or the accuracy of the model so even if we fail finalize the machine learning model that we are going to work with a lot of job is still left as we have to adjust the hyper parameters one route force approach would be to start with default values and keep on adjusting manually according to our intuition but that would be very timeconsuming and does not have any clearcut education about whether we have reached an optimum value or not that's where grid search comes out to make our life a little bit easier we have already worked with the SV and the logistic regression so instead of repeating them again let's learn something new in this video first we'll learn about a new efficient classifier that is a random forest classifier and then we will use grid search to units hyper parameters for understanding random forest classifier let us take this interesting something suppose you want to visit any one of the following cities which are Delhi Mumbai Kolkata and Chennai but you are not sure which one so you make a list of these four cities and distribute to your 100 friends and ask them to choose their favorite one out of these some choose Delhi some choose Mumbai but after the voting you found out that Kolkata has got the maximum number of foods so now you can be sure that you want to go to Kolkata that is how a random forest classifier works a random forest is a meta is a meta estimator that fits a number of four decision trees on various sub samples of data set and uses average to improve predictive accuracy drawing similarities we can say that these hundreds friends were like hundred decision trees and each one of them predicted a value and the final outcome was decided by a majority of voting that is the forest let's see how it is implemented in Python so as usual post a for will import circuit law from socket one dot n symbol we will import random forest classifier then from SQL Oh oops from Keylong dot datasets will report god iris dataset that is bring Woking load iris let's limit variable and load our iris data into it now we have to split on data set into training and testing set so for that will import s from a scale own dot model selection import train test split now let's split our data right extreme right X test then we'll read Y train then we'll write Y test now let's call this function train test split and then it will read iris dot a dot then is no target let's specify the tests nice say 20% and see the random value state equals to say 30 now let's call our classifier CLS equals to random forest classifier it can be calling this way also if we do not specify everything that only the default values will be supplied but for the purpose of this video we are going to work with three very basic hyperparameters which are number of estimators minimum samples net and minimum samples leaf so default value for number of estimators is 10 that is 10 this is a decision trees are used let's suppose we are going to work which is to number of estimators to and after that we can use min sample split then samples split equals to 3 the default value for this is 2 and then sample split is the minimum number of samples required to split an internal so of course it's difficult value is 2 and the third parameter that we are going to work with is minimum samples leave now what is minimum sample sleeve it is the minimum number of samples required at the pace that is a DB flow so its default value is 1 at least one sample is required over there so let's try to now let's fit our data into this classifier CLF not fit X train comma Y train ok the upper parameters are as we had specified them as you can see that minimum samples is minimum sample sample leaf is minimum sample split is 3 the number of estimators are so yeah let us go ahead after fitting this we are left with the job of predicting third use red underscore CLF goes to CL f dot predicted and what we are going to predict X test now for checking the accuracy wellliked scale on dot metrics dot accuracy score and in the brackets will write y underscore test and red underscore CLF let's see how much accurate our model is with these hyper parameters okay it's already 93 percent accurate so now we'll see that can we improve this accuracy or not so in order to improve the accuracy will try to adjust or doing the other parameters using grid search so first of all let's import grid search from SQL on the dot model selection input grid search CV after that we have to specify a parameter grid parameter could what this parameter grid contains is it contains a various number of features and we have to specify them according to our wish so what will take each one of the combination and dry it out so in our case since we have chosen to work with these three features so first of all we'll specify any estimators now what are the values that we can walk that it can take so we have taken 3 initially so the sorry so let's say 5 10 20 suppose that my number of estimators can take any one of these 4 values after that I can write my next feature that is men samples split and this will be it was given three so let's say two and three after that which one features their principles many samples leave and main samples we can take up any of these any of the values like its default value was 1 so let's try 1 2 & 3 so basically what we have done is we have given it four options for number of estimators two options for minimum sample split and three options for minimum sample leave so a total of 4 into 2 into 3 it is 24 option so it is going to dry on our model and the one with the best accuracy score will get selected and that is how grid search works so that is number of estimators can take up any of these 4 values 2 5 10 or 20 minimum sample split can take any of these two value that is 2 or 3 and minimum circles leaf can take similarly any of these two values which are 1 2 & 3 now let's write grid search and we will first call me grid search CV then specify our estimator or estimator was CLF which is the random forest classifier of course and after that we have to specify the parameter grid both have been saved by the same name I think so yeah param grid it will be after that after this we have to fit the data in our grid so grid underscore search dot fit lady under school serves not wait extry and white rain extreme my dream some kind of warning okay we'll ignore the warning and now that target search has been research has tenets working so we'll see what are the best parameters that it has come up with grid search dot best underscore para where I am status parameters so the grid search has told us that minimum sample sleeve should be one minimum sample spirit should be due and the number of estimator should be five for the optimum solution so let's do now random forest classifier that is our model accordingly according to these parameters and then check our accuracy till this point the code is exactly the same as we had seen a few moments ago so from this point or will specify the hyper parameters that we have learned using grid search so number of estimators were five samples leave one and then samples split use no you just have to filter data CLF not fit X train and then by train underscore train okay Rita has been fat now let's do the prediction thread underscore CL f equals to CL f dot predict and after that we'll write X underscore test the prediction has been done now the last step that is we're gonna check its accuracy Escalon dot matrix dot accuracy underscore school and accuracy we have to check Y underscore test as compared to red underscore CLF okay so as you can see the accuracy has gone to 100% this might be a case of overfitting but one thing is for sure that we do our hyper parameters using grid search and then using those hyper parameters and the accuracy for model definitely improved
