hey welcome back everybody this is Ian and in this video I'm going to show you how to make a chat GPT like chat bot from scratch so we're going to use this guide over on medium.com this article that was written by the team over at prompt flayer and it's going to show us how to do this step by step so let's go ahead and get into it so at the top here Jared the author of the article talks about the history of chat gbt talks about how in the earliest versions it was just a simple system prompt and we have an example of that here Ur chat GB t a large language model trained by open AI based on the GPT 3.5 architecture this is your knowledge cuto off date 20221 and then the current date was 2023 1101 so it says here that technically the first version of chat GPT actually used gpt3 which was completion based not chat and the system prompt is a paradigm that began with GPT 3.5 not longer after chat gpt's initial release so you can learn more about that in this little link right here but we're going to go ahead and move on to the point here where he talks about how even though it started out really simple very quickly after getting some user input and also seeing how there were some malicious requests and injections and getting feedback ideas and feature requests that the prompts the little simple one that we saw above actually ballooned into this giant prompt that you see here so feel free to go through and read this on your own time there is a lot of stuff in here but essentially they're trying to create a bunch of safeguards that are saying like hey if you get this type of input make sure that you don't do this or you do do that and just trying to cover all their bases so there's a lot of stuff that got added there this is what Jared's referring to as prompt debt and even companies like open AI are not immune to this sort of thing so with a little bit of history behind us let's go ahead and dive into building my chat gbt which is going to be our little chat bot here all right so we have some steps here we're actually going to break this tutorial up into two parts and in today's part we're going to go over over writing our system prompt building the chat interface and connecting to logs for debugging and then in the future video we're going to provide my chat gbt with World information things like today's date and the location of the user then we're going to add calculator tools to offset where the llm falls short so we'll be able to connect to a function that can then do some operations for us lastly we're going to talk about different ways to make development easier so production deploys Snippets and reg ression test all right cool so let's go ahead and get started one thing that we want to point out here is that much has already been written about chat gbt this particular tutorial will focus on some of the best practices of building a chat bot like this my chat gbt that we're going to build Jared adds here that he believes the best way to approach prompt engineering is to treat the llm as a blackbox it's simply just writing English and checking the output we will build our development environment around this principle primarily optimizing for Speed of development where quick prompt iterations and feedback Cycles are a necessity the first thing we want to do is write our system prompt so let's start there basically we need to tell our chatbot about itself we need to give it some context so it understands what it's supposed to do okay so that's going to be achieved with a prompt layer in this case but we're going to set a system prompt so that's the starting point for this you can see the example here there's a link prompt layer if you don't have a free account you can go there and sign up and you can create your first prompt template so this shows you step by step what you need to do you're going to create a new prompt template it's going to default to chat you're going to paste in the system prompt and then you're going to set an initial user message with an input variable called question so we're going to scroll down here we actually get to copy this prompt for the system prompt which is pretty nice so we'll go ahead and copy that and if you want you can link The Prompt layer right here this is what prompt ler looks like when you have it opened up and you're signed in and you can follow instructions here or you can just follow along with me but essentially we want to create a new prom template so the way we can do that is head over to our registry and you can see I've already created one here so I'll go ahead and let's see if we can just rename this to number two and then we'll create a brand new one where's my button we'll move that here's this blue button here create template so we'll click on that and and then we have our system prompt so you'll notice that it's defaulted to chat completion is the older version the way that it used to be done now in newer versions of you know gbt 3 and a half turbo and gbt 4 gbt 4 Omni we use chat all right so on the leftand side here for the system we're going to paste in what we copied from the tutorial essentially telling the bot that you are an AI assistant called my chat gbt you are a large language model please be as helpful as possible if you don't know the answer don't be afraid to say I don't know and then we need to create a new message here and we're going to use single curly brackets here because you'll notice we have F string selected here if you did Ginga I think you would do double curly brackets but with FST string selected for our template we're going to add in a variable called question and you'll notice right away under input variables it actually identifies that variable named question and now it's going to expect us to pass in a variable called question with some Dynamic value so down here under parameters we'll click on set parameters and here you can decide which model provider you want which model you want from that provider and you can tweak the various settings so I'm going to leave all the defaults open AI gbt 3 and a half turbo temperature one maximum tokens 256 and so on so with that set we'll go ahead and click on create template here and here's where you can set your initial prompt version message like a commit message if you're coming from a programming background you're probably familiar with commits from something like git for Version Control this is optional you don't have to do it uh it'll create a message for you if you don't but since it's the initial version I just use the anit commit syntax and say next so it's going to ask if you want to run an evaluation we don't need to do that just yet so we're going to go ahead and save this by clicking on confirm and it's reminding me to set a title thank you so up here at the top there's the title and this part is very important because we're going to use this title to actually fetch this template whenever we're writing out our code here in a moment so we're going to call it my with a capital M chat with a Capital C and then all caps GPT my chat GPT all right cool so now if we try to create the template it should go through commit message again I'm just going to say AIT commit and click on next and then click on confirm and here we go I've got my template so we're going to return to this in a bit we'll actually be viewing the history of all of our requests on the left here clicking on requests as they come through and viewing information about those but for now let's head back over to the tutorial all right here we are back at the tutorial we've already copied this system roll message right here and we've put it into our prompt template the next step is going to be building the chat interface so it talks about creating a new GitHub repo you're welcome to do that if you want to track your project on GitHub I'm going to go ahead and skip that but essentially you'll want to create a new project and if you do it on GitHub you'll want to clone it down otherwise we'll just just want to create a new project inside of your code editor once you have your project ready we're going to go ahead and use poetry for the virtual environment if you want to use poetry great if you don't have it installed go ahead and look it up otherwise feel free to use whatever you want for your virtual environment these commands here if you have po installed are going to go ahead and initialize the project and then they're going to add our dependencies so let's head over to our code editor you'll notice that I have an app.py file already created and open but it's empty so if I LS and of my my chat gbt demo folder you'll see that I have that one file all right so let's run these commands real quick poetry andit poetry add prompt layer poetry add open aai and poetry add python. so running that it's actually going to start by asking you some questions about your poetry project you can just run through all these until you get to the end and then it's going to update install resolve all the dependencies great so once that's done you get back to your prompt you can check and see that you have a poetry lock file and a p project toml file great so let's head over to the tutorial and go ahead with the next step so now that we have all of our dependencies installed and our virtual environment set up we want to go ahead and create aemv file and this is where we're going to set our open AI API key and our prompt layer API key so if you're not familiar with where to get those with prompt layer you just go to your settings for your account and it'll have a a button for getting your API key click on that generate a new key and copy it and you can bring it over to your EMV file and paste it there likewise for open AI you go log into your account and then go to your settings and you can get your API key from there as well so once you have both of those inside of your EMV file and you've set them equal to these two variable names open a API key and prompt player API key then you're ready to go with the next step so inside of your app.py file we're going to set up some boiler plate code for bringing in EMV OS open Ai and prompt layer so we'll go ahead and copy this code right here and paste this in so again we're going to import OS and we're going to get EMV that's going to allow us to pull in our environment variable for promp layer the one for open AI gets pulled in for us automatically so we don't have to worry about that we import open Ai and we import prompt layer we load the EMV file and then we go ahead and set up our promp layer client by setting it equal to to prompt layer and then you can pass in your prompt layer API key so then we're going to set up our open aai client first thing we do is we actually pull open AI from prompt layer client and once we have that we go ahead and initialize our client by setting it equal to open AI open close parenthesis so that'll invoke the class there and create this new instance which is our open AI client and we're ready to go so let's head back over to the tutorial the first thing it wants us to do is get some user input so we're going to copy this here head back over here and basically we're saying user uncore input is equal to the input method this is built into Python and then inside the input method we just pass it the prompt that we want to display right before we actually get the input from the user so in this case welcome to my chat GPT how can I help and then a new line with a little carrot that indicates that the user can now type something so we'll go ahead and save that head back to the tutorial so now that we have collected to the user input the next thing we want to do is actually get our prom template so we can do that programmatically using the library inside of our code so of course we'll use promp layer to do that you can see the syntax here is just promp layer client. templates. getet and then you pass in the name of the template that's why I said it was important that we name it specifically capital M my Capital C chat Capital GPT the next thing you do is you pass in a dictionary with some values for the provider including your input variables that get injected into to that template the benefit of using prompt layer for the prompt template is that in addition to being able to track all of your responses in a dashboard automatically we can also tweak our template in one place and have it update anywhere where that template is being used so we don't have to go out and modify our code anywhere we literally just go to our dashboard change the template as we see fit and then we can continue running our code without any changes there pretty cool so let's go ahead and borrow this code right here head over to our code and paste this in and then let's talk about what it's doing so here we have the my chat GPT prompt variable set equal to prompt layer client. templates. getet and again this is why it's important that we name this my chat gbt with the exact spelling and the exact capitalization is because we're actually using that name of the prompt template to pull it in to our code so then after that we pass in a dictionary here with the provider in this case open Ai and then any input variables so we have the one user input that we got above we're going to inject that as the question variable so if you recall from our prompt template that we created in prompt layer earlier we did have for the initial user roll content we had a question variable that this is where we actually inject a value into all right so after that we want to actually send the request out to the API so we use the open client variable we use chat completions create and here we're taking all the values from this my chat GPT prompt llm quars and we are going to dump them into this create method call and so it's going to go through and it's going to take each key value pair and it's going to put it in here for us so that's what this double asterisk is doing here it's taking those key word arguments and it's injecting them into create and then lastly we're going to include plore tags and we're going to put a little tag in here you will note that this is a list in Python and so we can include any number of tags that we want so we will see when we go back to the dashboard for promp player in a little bit that will have this tag right here and this one is specifically for development but if you are doing things in production you could add a tag or change the tag out for production that way you could keep track of which requests and which response are what cool so we assign the response of that to the response variable and then we're going to Traverse down into that dictionary that comes back it has a choices list we take the first element from that list it has a message dictionary which has a Content key and that's going to have a string that points to the actual response so we're going to print that out and we should now be able to run our code we can check with the tutorial just to make sure that yep they are actually testing it so in order for us to test it we're going to use poetry so because poetry is our virtual environment we just run poetry shell it's going to put us into the virtual environment and then we can run python app.py let's go ahead and try that now make sure that your file is saved and then we'll open up our terminal down here so we'll just run poetry showell here it'll get us into our virtual environment and then we can run python app.py okay so it says welcome to my chat gbt how can I help and then we have our little carrot here with the space and then we have our cursor indicating that we can start typing so we'll just ask at something really simple we'll say uh what what is CSS so cascading stylesheets wait a second for the response and there it is CSS stands for cascading stylesheets blah blah blah blah blah awesome so we are able to interact with the bot simply by asking it a question and then it gives us a response that's cool but what would be really awesome is if we could ask it questions back to back and actually have a conversation a conversation such that it actually has some memory of not just our questions in the past but also its responses that's how most chat Bots operate and that's how we want to set this one up so there's a couple different ways to achieve this we'll see what the tutorial shows us let's go see what the next step is O making a loop Okay cool so we were able to ask one question we're able to get back one answer but what would be really cool is if we could have a loop that would allow us to continue having a conversation with this chat bot until we decide that we're done so the way that we're going to achieve this is with a W Loop every time the llm responds with a message we'll ask the user for a response and we'll pend both new messages so their response and our question into our prompt so there's a couple drawbacks of this of course our prompts can get really long really really quickly but this is just an MVP so in the future alternatively if you want to you could summarize the conversation you can inject it back into the system prompt context there's a couple other strategies for this we're not going to go into them in this tutorial for now what we're going to do is we're just going to append their response resp in our next question back into the messages list so let's see how to do that all right so we have some code here and it says just add this Loop that we've created with this wall Loop here right after we're done printing the first llm response from above so the code that we already have in there we're just going to add this right after it so we'll copy this here we'll head back to our code so we've printed the response this is our initial request initial response and now we're actually going to paste in this Loop here so I'll make this a little bit smaller so you can kind of see the whole thing here but basically it starts with prepare the new prompt a messages variable is equal to my chat GPT prompt now if you remember my chat GPT prompt is the variable we created whenever we pulled the template and we injected the very first user input value so it had that thing llm quars that we were able to dump into this create method for the client Jack completions call and we're going to access messages if you think back to our template that we created with prompt layer you'll recall that we had the system role and then we had our user role the user just had the question variable and the system had its initial prompt so that's what's going to be inside of the messages list at least the first time that we make our initial request it's going to have that first object with the system role and its content and then the next object is going to be the user role with the message for its content so now that we have that messages list the next thing we want to do is get the response message from the request that we just made so we have that response variable from the request and we're going to use choices bracket 0. message to get access to the first message this is going to be the assistant roles message that came back from our initial request okay so we're going to take the assistant response to our initial question and we're going to append it to the end of the existing messages list the one we just talked about that had the system rle and the initial user so at this point it goes system user assistant and then what follows is just going to be a back and forth between user and assistant all right so inside the loop is how we're going to facilitate that at this point we need to get more user input okay so we're going to say user uncore input the same variable that we used before is equal to again we're going to use the input method and then here we're just going to have the little bracket or the little carrot with a space indicating that the user can now type something now now that we have the user input we're going to append that after the assistant role that we just put into our messages list so here you can see messages. append and then we have this dictionary the role is now user the content is the user input that we just gathered from the user all right now we want to send all this to the llm so the first thing we do is we're going to access that my chat gbt prompt this is our prompt template we're going to go down into the llm quars and we're going to go over into our messages list here on this object and we're going to overwrite it with with the messages that we have been appending the assistant and user objects to so essentially we're just updating the my chat gbd prompt llm quarks and the reason we're doing that is because we're actually going to pass that into our next request to open AI API so you can see on the next line we're getting the response from sending a new request via client chat completions a creation of a chat completion so sending out this request to the openai uh API and again we're just dumping the contents of my chat gbt prompt llm quars and this is going to fill in things like which model we're using what the temperature is what the messages list is which at this point is updated and anything else that's included there so after that we have our plore tags again if you wanted to do this in you know staging or production or whatever the case you can put any kind of tags you want in here one or more it's up to you you can even leave it empty if you want for us specifically we're saying my chat gbt hyen Dev for our tag and now we're getting back the response message from that request so again we get the response we assigned it on 44 and we're just going down into the choices list and pulling that message dictionary and assigning that to response message that way we can go back to our messages list which is keeping track of all the different messages so again we start with our system we go to user then we go to assistant and then user and assistant at this point this response is going to be an assistant because it's a response from the API and we're going to append that dictionary stored in response message to the messages list now we're actually going to print that and then we're going to start the loop over again so at this point what we're printing is the content of the message dictionary from the assistant role which is going to have the response to the user's last question and now the user has an opportunity to continue the conversation because we start at the top of the loop again awesome all right let's go ahead and test this out in our terminal so with our virtual environment activated let's go ahead and run Python app.py and it says welcome to my chat GPT how can I help so here we're going to say what is and let's ask what is HTML all right cool so when it comes back with its answer you'll notice that we have this little carrot here it's waiting for our next question so we can continue on now we can keep asking it questions that's really cool but what's even cooler is that it has this shortterm memory and it recalls information about our past questions and its past responses so let's try this let's say my name is Ian what is your name and it says hello Ian my name is my chat gbt how can I assist you today all right cool so we've told it something about ourselves now let's say what was my name again and it recalls that our name is Ian it says your name is Ian how can I assist you today Ian awesome so it's actually able to recall information from the history of our conversation and now I can ask it something else you know what is CSS and it gives me my answer so when we're done here we don't have a mechanism for exiting just yet so you can just do control D to exit in that way and it'll take you back to your terminal all right cool let's jump back out here to the tutorial and we've already run it we've tested it out so you can see these examples here the last thing we want to do before we go here is add some logging so everything we've done is really neat but as is mentioned here any sufficiently complicated AI application can expect to go through a lot of prompt iterations so being able to track which ones work and which ones don't work is going to be critical so we can do that with prompt layer and actually the original inspiration behind prompt layer was because the author of this article Jared lost one of his good prompts after making a few edits so you may have experienced this similar fr ation where you're working with the prompt and it's working really well for you but then you edit it a couple times trying to get it to be better and now maybe it's performing worse but you can't remember exactly what that original prompt was and maybe you don't have a history of it anymore so with prompt layer we're getting the tracking for every single request that we send which is great that's built in if we head over here we can actually see all these requests that we've sent have actually been tracked here so you can see what is HTML and I said my name is Ian there's it response if you go to the next one it updates with the next part of the conversation and then the last one you can see the full conversation so system rooll user assistant user assistant user assistant user and then the final assistant all that worked great and in fact we can see our tag here my chat gbt hyen Dev we can even add some tags here if there weren't tags that we added whenever we first created the request but one thing that's not happening is it's not actually connected to the prompt template so you see no template here in the top left corner and we want to connect it with the template that we're using in this case my chat jpt so that's going to be what we're going to do now let's head over here and if we scroll down you can see there's a link to a way to track which logs were generated with which prompt versions so you can go ahead and click on the enrichment described here link if we do this it actually takes us over to the promp layer documentation has examples for python JavaScript and rest in this case we're going to use the one for python so we're going to go ahead and copy this head back over to our code what I'm going to do is I'm going to put it right after our request so we send our request we assign the response to the response variable and now right here we're going to paste in this prompt layer client track prompt and it's looking for a variable called plore request ID that's actually going to come from the request here because the client if you recall even though it's open AI it's being pulled from prompt layer and so it has some additional features so what we can do here is we can say response comma plore request ID and then this part I can't remember exactly it's like PL request ID is equal to true but I'm not 100% sure so we're going to jump over here and see if we can find that so under request ID is under logging if you scroll down here there it is it's actually return plore ID so we're going to go back to our code and we're going to say return plore ID so what that does if we add return plore ID equal true inside of this chat completions create method and then we set a comma plore requestor ID up here in the assignment for the variables is that this will actually return a tuple with two values the first being the response and the second being the ID of the prompt layer request and the good thing about that is now we can pull the response and we can get whatever information we need about that but we also have access to our prompt layer request ID so that we can pass it into this code and be able to track the usage of our prompt template so the next thing after setting the requestor ID equal to the pl request ID is the prompt name so you'll want to update this value to be the name of your prompt template in our case it's my chat GPT capital m capital c Capital GPT and then here you've got your prompt input variables so this could be a dictionary with your input varibles so for example we know we have one input variable question and then we know we set that equal to the user input now if you're using this anywhere else then you could just create a dictionary and you could add it there but for Simplicity sake here we're just going to say this is a literal dictionary with the key of question we know we only have one variable that we're using and then at this point the user input is whatever the user is inputed right here cool so then the last value here is this version equals 2 the two is just an example but you would set this to whichever version of your prom template you want to use for this tracking now if you don't have a specific version or if you want to use the latest version rather what you can do is you can just submit this and it'll always default to whatever the latest version is of that template so I don't think we have any versions yet you would get versions if you went over to your registry and you went inside of here you see we have version one and if you made an edit so you can see there's an edit button here and you go change some things and then you update and then you do a commit message and what it would do is it would create a second version here that you could then pass in the number two for version two if you had multiple versions you can do it that way there's also labels that can be used so if you want to do that you can refer to the documentation but it's pretty versatile again if you omit the version so if we take this out it's just going to use the latest version of The Prompt with the name in this case my chat GPT so we'll go ahead and remove it save that and run our code again and see if it works cool so it's saying hey welcome to my chat chbt how can I help and I'll say who was the first president of the United States and it says first president was George Washington with some information about when he served and then I'll do my name is Ian what's yours says Hi Ian I'm my chat GPT we can say what was my name again and it can say your name is Ian how can I help you today Ian awesome so let's head over to prompt ler here you can see our most recent tracking here but notice the difference here when we click on the latest one sure it has the system Ro and it's got the user and assistant user assistant user and then the final assistant so this may look the same as it was before down here but you'll notice where it says no template we actually have a template now you can see if I click on this it goes straight over to my registry for my chat gbt the template that we used and if we look down here we can actually see there's some tracking here and if you Mouse over the question variable you can actually see what the dynamic value was for the variable at the time of this specific request and a preview of the response from the assistant for that specific request and then if we go back over here you can see if you Mouse over the text in the most recent user message the content of it is blue and if you Mouse over it it actually does a little tool tip telling you the name of the variable assigned to that one in this case question so pretty cool now we have some more fine grain tracking of our prompt template and anytime we want to go over to it we can click on that we can make edits to it we can have different versions We can track which version we're using for which prompt requests that we have over inside of our code just a lot of Versatility added on to what you would normally be doing if you were writing a chatbot so really cool that promp layer gives us all those features makes it a lot easier for us to build these types of applications and then iterate on them quickly that's it for this video if you go back to the tutorial you can see that the next thing that we're going to do in the next video here is we're going to provide our chatbot with some real world information for example the current date and then the location of the user thanks a lot and we'll catch you all in the next video peace