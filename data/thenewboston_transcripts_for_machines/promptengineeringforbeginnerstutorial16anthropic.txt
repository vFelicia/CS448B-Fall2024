hey welcome back everybody this is Ian bringing you another video in this AI series with the New Boston so in the past videos we've talked a lot about open AI chat gbt 3 and a half turbo gbt 4 we've also discussed how to use prompt player in this video we're going to introduce a different llm from a company called anthropic or maybe it's anthropic I'm not sure in any event their llm or large language model is called Claude 2 in fact their latest version is 2.1 in this video we're going to introduce you to CLA 2 with the Legacy completion syntax just so you're familiar with it in case you come across it in a code base or if you're already using it in your own codebase and then in the next video we'll show you how to migrate to the newer version of the llm clae 2.1 using the newer API called the messages API so the implementation and usage of this API is really simple so simple in fact that we're going to wire it up into prompt ler right away so that we can of course keep track of all of our prompts and their responses so let's take a look at this this code let's see what's going on let's run it and then let's check prompt layer to make sure that the tracking is in order so the first thing to know is that you do have to have an API key for anthropic so you're going to go to their website sign up get API access generate that key and then you're going to export it and so the value that you're going to export it as is just going to be enthropy API key with underscores so anthropic uncore API uncore key so set that as an environment VAR able however you set up environment variables on your computer or in your project and point that to your secret key that you get from anthropic the next thing we're going to do is inside of our main.py file we're going to import prompt layer so then we're going to pull anthropic from prompt layer so we say anthropic is equal to prompt layer. anthropic now we're going to generate a client from anthropic do anthropic you'll notice that the second anthropic here has a capital A so make sure that you put that there and then we're going to invoke this so open close parentheses and the result of that the object that gets returned is going to be our client assigned to our client variable now we can do things like access the completions API through this SDK so we're going to say completion is equal to and then we're going to use that client and we're going to call its completions method or object which has a create method on it so client. completion. create and then it's just going to take a handful of arguments here the first argument is our prompt so you'll notice we use an F string here and we plug in something called enthropy human prompt and then at the end we have another string injected from enthropy a prompt I'll talk about what those are and why we need them here in a moment the next thing is just indicating which model we're going to use so in this video we're using cl2 but in future videos we'll migrate to Claud 2.1 then of course Max tokens in this case they call it Max tokens to sample so you should be familiar with that concept from our open AI videos but essentially we just want to control how many tokens are used with this this request response cycle at the end here we're going to print the whole completion object that comes back and then we're going to drill down into it a little bit and print the completion. completion object that comes back from that so let's go ahead and save this file and run our code so I have my virtual environment activated for this AI playground project and I have of course all my dependencies installed we're ready to go to run python main.py uh first I want to CD into where are we over here anthropic examples and then the first folder here 01 basic so inside of here I've got main.py I'm going to run main.py with Python 3 so the prompt was the same one that we used way back in the beginning weeks and weeks ago whenever we introduced uh chat gbt or open AI gbt API and that was we just asked which NHL team plays in Pittsburgh so National Hockey League and the response that we get back here is you can see here's the completion object the entire thing it's got a bit of information here like the ID and then this completion string and then some other things in here like type stop log ID and so on after that we're actually logging the completion completion which is just the string the content uh that comes back so the answer here is the Pittsburg Penguins are the NHL team that plays in Pittsburgh and it actually goes on and says quite a bit more they play their home games at the PBG paints Arena they have won five Stanley Cup championships and so on and so forth we didn't actually ask it to give us that information so you can already see that it's kind of overcompensating and it's just going to be up to us to prompt it in such a way that makes it to where it doesn't give us too much information if that's not what we want so we'll show you how to do that in future videos with the system prompt but for now we're just going to focus on getting this functionality working and making sure that it's being tracked over in prompt layer so let's head over to prompt layer and you can see at the top here I have this brand new history for Claude 2 and if I click on it you can see that the human prompt was which NHL team which NHL team plays in Pittsburgh and then the assistant response was the one that we just read you can see our Max tokens we can see how many tokens were used so it only used 79 and then you can see our cost and our latency here you can even see that it's using anthropic anthropic do completion. create so in the future when we migrate to the messaging API you'll see that it uses messages instead of completion also we can see that we're using CLA 2 model again that will change once we migrate to 2.1 so back over in our code the last thing I want to talk about before we get out of here is the prompting formatting so there is a validation that occurs when you send your completions request over to the anthropic API it's going to look at your prompt string and it's going to ensure that it's written a certain way so what it's looking for at the very beginning is either back slash in back slash in so the return statement double return statement and then human with a colon to indicate that what comes after that is the human prompt or likewise it's going to look for back slashin back slashin assistant all caps for human all caps for assistant and then a colon so what that'll look like is this it'll be back sln back sln and then human and then some prompt and then back slash in back slash in assistant and then some prompt and you're just going to alternate back and forth between human and assistant human and assistant well rather than having to type that out yourself and assuming that they might make changes to it in the future we actually pull that from the SDK itself so you see we have anthropic up here inside of this F string which just allows us to inject a variable or an expression into our string we have enthropy human prompt as a constant variable so that's actually going to give us that double back slash in and then human for the human prompt and likewise for the AI prompt there's back slashin backin assistant colon stored inside of enthropy doore prompt so if you don't do this correctly then you'll get an error back because it is passing through a layer of validation it's looking to ensure that you do it correctly now if you have you know a missing back slash in or maybe you have a space in there it will try to correct it for you but you definitely should not rely on that because it could change in the future and then you could find that your code is breaking the easiest solution is just to remember to pull those values from these constant variables that they have available within the client and to make sure that you put them at the beginning and at the end of your prompt so that's the formatting for sending your prompts and then of course you're just going to send this through get your response back parse through the response print out the actual completion which is the text that you're looking for and do whatever you want with it you can create an ongoing conversation if you want to it's a little bit easier to do when you're using the messages API versus the completions API so we'll talk about that in the next video but for now what we've talked about is how to get up and running really quickly with anthropics CLA 2 API and to link it into prompt layer so that we can track all our prompts and see whatever data we need about those prompts thanks a lot for joining us in this video can't wait to see you all in the next one until then peace