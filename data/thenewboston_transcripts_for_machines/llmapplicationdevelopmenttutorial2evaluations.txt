all righty y'all welcome back to another video and this one is going to be a fun one because what we're going to be doing is we are going to be improving ya we're going to be making sure that she is getting better smarter and to do that what I first want to do is kind of show you guys what a lot of people do whenever they try to improve their llms and why it may not be the best technique so I'm on the prompt template here and I'm just going to take this and open it in the playground because in instead of that question variable I want to give a some other questions now we already tested it with a very simple HTML question now let's go ahead and test a with a python question so I'll say what's the problem with this and then I'll just go ahead and make a simple print statement I'll print out hey however I will remove that second quotation mark and we'll see if I can pick up on this bug right here okay looks like you're missing the second quotation mark perfect um now let me give it a JavaScript related issue so I'll say VAR equals 3 and actually let me remove that semicolon too so got a couple issues here well first of all it should be VAR something a variable name and then of course it doesn't need to have that semi coolon but it's nice and okay so we see that a has some issues right here first of all um I was kind of giving this as a JavaScript example and she didn't know it was JavaScript I mean thinking back how could she she thought it was python so she definitely didn't get that missing semicolon huh now how would I tweak this so she answered the question correctly so let me remove that response and oh I know I can say you are a JavaScript AI coding assistant uh your job is to help people with programming questions let's see if we can get some better results this way oh there we go so I added the variable name and also added the missing semicolon okay good job a but um oh you know what I just added this JavaScript keyword so it probably isn't going to do that great for Python and HTML anymore so let me remove that and uh now it's not going to be great at JavaScript okay so already I'm noticing a couple things one whenever I say is I getting better or worse it's a little bit tricky because the question itself is vague now whenever your llm is getting better or worse it typically is getting better in some areas and worse in other areas now aside from that ambiguity I am also noticing that this entire method of testing it isn't very scientific I mean I'm pretty much just asking random questions and I'm trying to keep track in my head of is this better or is this worse nothing's quantifiable everything's subjective and it's just uh it feels messy off the bat so let me go ahead and show you a much better way to approach these kind of problems now let me go ahead and click on evaluate just so we have a nice clean page to look at and now let's go ahead and talk through some of the flaws we had in our logic so whenever the video first started I said that the goal was for us to improve ya or to make ya better but already that thought is a little bit subjective because what do I mean by better do I mean that ya is going to be giving shorter responses or do I mean that by better ya feels more like I'm talking to a human or do I mean that it's just giving more accurate responses just answering uh the questions correctly so I need to Define better now whenever we are developing l applications what we do is we need to assess a certain Behavior at a time and that means instead of using uh vague terms like better we need to give it a more clear um indicator of what we're assessing so instead of me saying I want to make a better instead what we can do in this example is we can say I want to make a better at answering common programming questions fixing easy programming bugs that's a lot more more quantifiable something that we can actually test now once we have that clarified the next thing we have to figure out is how can we test this and believe it or not the way that we test this for a llm is very similar to how we would test if we were a teacher and we were testing if a student is getting smarter or Dumber uh it sounds kind of mean but how we would do that is we would basically give them a test and then over time we can see if their test scores were going up or down so we're going to be doing the same thing with ya we're going to be creating a test and seeing if she is doing better or worse on it so now let me go ahead and pop open a spreadsheet and I'll show you guys the structure of how we're going to set up this test so what we're going to be doing is we are first going to be creating some type of question and we're going to ask it something like we did before let me just pop in a quick example here I'll say uh what's wrong with this and then we're going to be giving it some code let me just throw a little issue here and then in the next column we'll just have the answer and I'll say uh bad closing tag so I'm sure you guys can see where this is going um in this test we are actually going to have a lot of question and answers probably like 10 of them and the question what we're going to be doing with that is we are going to be throwing that into this prompt template and we're going to be sending it to the llm now when we get our response back actually let me say llm response is going to give us some type of answer right here and that when we get that answer back as the teacher we can just go ahead and see if it's right or if it's the wrong answer if it's the right answer oops if it's the right answer we'll just go ahead and give it a score of one if it's the wrong answer we'll give it a score of zero and that's basically our test the more questions that a gets right the more points she gets the more points she gets the smarter she's becoming and it's pretty much it the same way that you would test the kid like I said you were going to test a the same way and really gamify the whole experience now before this video what I actually did is I went and saved some time and I created a bunch of these questions and answers again these are just very simple um programming questions they usually just have a typo or missing semicolon or something and again that's the behavior that we want to assess the ability to answer common or easy programming questions and I'll go through each of these one by one real quick there's just 10 of them it seems kind of uh tedious but we're going to be working with the same data set a lot so it's important you understand the questions because then the responses are going to make a lot more sense so this first thing is just the HTML question you got a bad closing tag this second one is just this JavaScript question where the semicolon is missing the third one is just printing out hello world but it's missing those ending quotation marks now this fourth one is react code however instead of the um rendering the hello component I just had a typo and I called it jello so this fifth one right here this is just some D Jango code just instead of model it says schodel so that's the issue with that this SQL code right here is using two equal signs when it should use one for this one it's just python code however it's missing the colon right here after the for Loop there should be a colon now this JavaScript function this console log is not supposed to be included in this function body and there's also the function body never closes so that's another issue um this is SQL code 2 but instead of order by there's a little typo it says order re bu and the last question that we're going to be testing is some CSS code we can see that in this color property there's supposed to be a semicolon here and it's just missing so either way just a very simple 10 question test and from here what I want to do is I just want to download it so I'm going to download it as a CSV and then I just want to upload it to prompt layer so back in prompt layer what I'm going to do is over in the evaluate tab if I click new batch run then what I like to do is I like to name my batch runs or my tests basically the same as my data set just because I think it's um I don't know easy to keep things organized that way so my data set is named programming bugs easy so here I'm just going to call it programming bugs easy and now to upload this data set I'm going to click this button right here and you are going to see that I already have a data set in here and that's because I just recorded this video however for some reason my software the video and the audio got like mismatched and uh anyways more of the story I had to rerecord it which I'm doing right now so I'll show you guys how to upload the data set that we just created if you click create data set then you could upload that CSV file just go ahead and select it here and then choose your CSV file hit open and for the data set name I'm just going to name it programming bugs easy data set and I'll say data set 2 because I already have a data set name that name create one and now I can select it and just say select data set and now last but not least I'm going to hit create Pipeline and you can see that we now in prompt layer have a view that's very similar to the spreadsheet however since it's in prompt layer now we get a lot of extra functionality so from here what I want to do next is I want to add that other column that's going to be responsible for making that llm request and it's going to be doing it using our prompt template so let me go ahead and add another column by clicking this add step button and I'm going to be making it a prompt template column and then I just need to give it a name I'm going to name the column llm response and then I need to choose what prompt template I'm going to be using and of course we only have one so very easy decision and I just always want to use the latest version so by version I'm going to select default and that's going to always use the latest version of The Prompt template and now the last thing that this is saying is hey in your prompt template you have a variable called question now whenever I'm going through which of these columns do you want me to plug in for that question and of course it's the question column so then we're just going to map that question column to the question variable and now I'm going to hit run step and what's going to happen is I believe we're going to get some errors and I'll show you uh why we we are getting them and how to fix them and all right so it says that there's error please select llm provider and engine none found so what does this mean well let's go ahead and open our template again and it is in here University I assistant and check it out so right now we already know that we want to make an llm request using this prompt which we're going to be passing in a question variable and we already hooked it up to say that this is the question that you're going to be using first and then use this one so on so forth looks like everything is good to go but what we never specified to promp layer is which llm provider are we using are we using open AI are we using anthropic are we using another one so on and so forth now there are two different ways that we can configure this the first way is we can configure on a per column basis so anytime we run this column if we click this edit button and we can set the model engine parameters here so if I just want to say yes I want to use open AI with this model I can go ahead and save that however I don't like doing it this way and instead what I like doing is I like saving the type of provider and model I'm going to be using directly on my prompt template so how do I do that well if I go ahead and edit this prompt template then right here where it says parameters I can go ahead and choose set parameters and then I'll just stick with this right now open AI uh GPT 3.5 turbo those look good now I'm going to go ahead and just update my template to make sure those parameters are set and now back in my report all I need to do is really just um rerun this and now hopefully if everything went good since I have my parameters tied to my prompt template now it knows to use open AI so now let's see whenever it makes this request so now you see we start to get some answers back and let's go ahead and take a quick peek all right error in your HTML code that looks good nothing technically wrong still think it's python code but that's all right all right so this preview is looking good so far I got everything configured how I want it the only thing that I want to do before I kick off the full run again right now it's only doing the first four while I get everything configure but when I click this it's going to do the full 10 before I do that I want to add one more column so we can actually score these answers so I'm going to add another column and since it's just going to be me looking at the answers in manually giv a score I'm going to choose human input and then I'm going to click next and then for this column I'm just going to say score and for the data type it's just going to be a number again zero for wrong one for right and I'll just say text box it's the easiest thing so I'm just going to save this and now I can go ahead and enter whatever number I want and there you go so this is all good right now and uh yeah from here just go ahead and hit run full batch and then that's going to go ahead and run your full test Suite so after a couple seconds you see that our full test is now populated and then ya is going to be figuring out these answers and I'll give her some time and it will be just a few seconds but I'll give her a few more all right now after a couple seconds you see that ya has now responded with all our answers and what I'm going to do now is pretty much just go through like I'm a teacher grading a test just look at her answers and if it's right give her a one if it's wrong give her a zero and I'll do about like two or three of these and then I'll pause the video so you don't have to watch me do each one so the first one closing tag okay and it also says the title tag is missing but I don't think it was missing the closing title tag no it wasn't was it okay so she got like uh I don't know that's just like uh uh kind of got it so I'll give her half a point for that um let's see what else uh in this one nothing wrong but if you want to follow convention you should add semicolons okay that one's right and let's see this last one quotation mark she got that one right so that's another one and yeah so I'm basic basically just going to go through the rest of these I'll pause the video now and at the end of it I'll let you know the final score all right so I just finished grading the test and once I added the last value here prom player updated the average score so we can see that a got an 85% out of 100 she missed this first one where she said that there was no closing title or head tag and in fact there were and also another rea one where she said to import something from somewhere but uh it actually was the correct import so either way we got a score of 85 not too bad but I have a feeling that we could do some prompt engineering and make this a little bit better so what I'm going to do is I'm going to go back to my template right here and I'm going to edit it and how about we do something like this um you are a e coding assistant you are the best programmer in the world and another thing that I want to change is before we were using 3.5 turbo let's go ahead and see if GPT 4 is any better so now I made some changes to this I'm going to go ahead and update my template uh I don't need a commit message now here what I want to do is since I updated my template and I want to taste it test it out on that same test that we built I'm go ahead and select this do you want to run an evaluation basically do you want to run a test and for this I'm going to select the one that we just made programming bugs easy so now I'm going to go ahead and confirm this and when I do you can see if I hover over my latest version the updates that we just made I have this little link to eval report and if I click that you can see that the updated test is now running well in fact it's the same test just with the updated prompt templ and that brings me to another point which is whenever you are evaluating a prompt template what you want to make sure of is you're using a consistent test and the reason for that is because in this experiment you already have one of the variables changing which is the prompt template itself now if you are also changing it by giving it new test questions every time your experiment isn't going to be very scientific you're basically testing too much data or in other words what's the reason why the system is either improving or not improving so now it looks like I V2 let's call her uh finish answering this test so now let me go ahead and score it and see if she did any better all right and check it out this time after our adjustments it does indeed look like a improved she now had a perfect score all right so off the bat it feels like like this process is indeed better than just using the playground and trying to keep track of everything in your head and not being too scientific however there's still some things in this process that feel a little bit tedious first of all me going through in scoring each of these one by one I mean honestly it's taken a little bit of time it's kind of annoying and imagine if I have a bigger data set with a 100 or a thousand cases and I have a bunch of different prompt templates that I want to test I mean I'm going to get carpal tunnel in no time having to do this so I need to figure out a better way where I can actually score these responses and in addition to that another thing that I was thinking is check this out so before I just go ahead and release this to the new boston.com and let people start typing in any answer or excuse me any question that they want I want to think about this from a user's perspective so they are going to be on my website and they're going to be watching a video and what are they going to be typing in a lot of the time they're going to be typing in questions like we just saw programming related questions so on and so forth but let's say that they're watching the video of me teaching how to make beer or how to build a gokart they could also ask questions like uh this who is the dude in this video and well it may seem obvious just you know detect who's in the video and see but from A's perspective whenever she is embedded here she has no idea that she's on a web page she has no idea that there's a video right here or anything like that so she's not going to be able to respond to that question just because she doesn't have the context necessary now another thing is if we just put ourselves in the user shoes again they are going to be talking to a chat bot so they may ask some questions which they think is a simple question like what is your name and of course we know her name is a I mean I already said that in the other video and you know here you can see it says a on the left but for her huh her response would be as an i AI I don't have a personal name and we know that's not really true either so it seems like yes we are getting better but there's still some Kinks that we have to work out and in the next videos we're going to be taking a look at how to solve those problems but for now thank you guys for watching don't forget to subscribe and I'll see you next time