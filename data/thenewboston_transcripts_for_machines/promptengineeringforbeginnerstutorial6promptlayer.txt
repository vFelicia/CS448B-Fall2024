hey welcome back everyone this is Ian bringing you the sixth video in this AI series with the New Boston in this video I'm going to show you this platform called prompt layer promp layer is this really neat tool that has an API that we can use on top of open AI to do a lot of really cool things in particular we can track the history of all of our prompts that we're using we can rate those prompts we can add metadata to those prompts there's all kinds of cool things so we're going to go over that in this video and getting set up with it is super simple so let's go ahead and dive right into it this is the website right here prompt layer.com it'll be linked in the description below you'll want to log in or sign up and then log in and create a API key so generating an API key is really easy once you're logged in the first thing you're going to see at the homepage on the guide tab is this create new API key button also the instructions for installing pip install prompt layer really straightforward so just two steps here and then a couple lines of code code whenever you get into your actual codee base so let's do a quick kind of a tour of what we have here in the dashboard and then we'll jump over to our code and I'll show you how it works once you're logged in you have your guide now you're not going to have anything on the left hand side here until you actually integrate this into your codebase but once you start using it with open AI every time you make a request over to the chat completion API it's going to log that request over here so this is going to create a history you can the history from the history tab it's essentially this right here but then as you click on each of these items in the history it'll show you everything about that particular prompt and so in this case we have our system prompt and then we have our initial user prompt and then we have our initial response from the assistant one really nice thing about prompt player is that it tells you exactly how much a certain request cost so in this case this request cost 0025 58 super cheap which is great but if you're over in the open AI dashboard and you're looking at your usage you can see how much overall you've spent you can see if it was GPT 3.5 or if it was GPT 4 but for each you know granular request you don't get to see how much it costs you get to see the tokens which you can see the total tokens here as well 143 tokens so you would have to calculate that on your own in this case it's calculated for you now there is a playground over in open AI of course we've seen that in previous video but over here with prompt layer when we open this in the playground it's actually going to pick up where we left off with this request to the API so we can toggle back and forth between the completions Legacy API and the new chat completions API we're going to leave it on the chat completions API because that's how we initially sent this request but you can see here's the system message you know you're a helpful coding assistant you only answer coding related questions nothing else if you asked a question that is not related to coding you reply politely but declined to answer and then the user's first question is is HTML hard to learn and then over there we saw the response you could run this again and when you run it you'll get the assistant response here in a second so we wait a second for it to come back to us and here's the assistant's answer HTML or hypertext markup language is blah blah blah so it's answering the user R question now you can add a new message here so if you want to respond to the assistant you can do that so essentially you're getting granular control of what you would be doing in your code but now you're doing it in the playground that way you don't have to keep going back and changing your code over and over again you can play with it here and you can kind of get a feel for which prompts are going to work best for whatever it is you're trying to write and then you can go Implement that into your code base so you'll notice they have functions down here this is brand new this is something that open AI does not currently have in their playground this is similar to what we showed you in the function call video where you can actually have a list of functions that the chat completions API can see and it can see what the parameters are and if it decides it wants to call one of those or if you explicitly tell it to call one of those functions it'll send you back the arguments for that function that it wants you to call including the name of the function so that's really neat you can integrate that here as well there's also some additional stuff that does not exist inside of open aai things like tags so you can see here it says no tags so you can actually add some tags and some meta information to each of your requests and it'll allow you to keep better tracking of the requests so for example let's say you're building a chat bot for like Discord and your members of your Discord server are able to ask questions of this bot and then those questions will be piped through the GPT 3.5 turbo or gbt 4 apis and the responses will come back to the users in the Discord interface that's really cool except if you're offering this as a free service then people might take advantage of it so you'd want to track that and you'd want to see you know who's asking what how often are they asking what are they asking uh you know what is the the Cadence and everything so you could add some information to your Bot to where every request through to the chat completions API has some information some meta information about the user from your Discord server that asked it like their their username or their user ID that way if somebody is abusing the system you can see okay this user with this user ID is the one that's doing it you know they're overusing it or they're asking questions that aren't meant to be asked or whatever the case may be you can go in there and you can identify those things you can of course bake that into the application that you're writing but having it right here in a dashboard is really nice because it's automatic and you can share it with other team members so if you're working on larger projects and you're doing some prompt engineering then you can have anybody on your team in your startup or company or just friends that you're working with be able to log in you can share this with them and they can look at the different prompts and the different logs and histories and the different tags everything that's available from this tool prompt layer it can be available to everyone that you're working with so there's a lot of really cool stuff this is just the playground and the history over here we have the registry so the registry is going to allow you to create templates where you can inject variables into like premade prompts and use those in your code so that's really neat and then we have analytics so this is going to tell you you know the total cost of all the different requests the average latency how many requests there were all kinds of things and it's going to break it down into like pie charts and graphs and things like that so this is really helpful to have in addition to whatever you have going on over in your default open aai dashboard so let's head over to our code and see how we can integrate this because it's super easy it's so easy you're going to be like I can't believe it it's only two or three lines of code and then you're up and running in no time if you remember the first thing we want to do was a pip install prompt layer that way we can import it up here so if you're inside of your terminal make make sure that you have your virtual environment activated and then just do a pip install prompt layer once you run that it'll get installed and of course you can freeze that to your requirements.txt file if you're keeping track of all the different dependencies for your project everything after that is the same you know we're importing OS so that we can get access to our environment variables in this case you're going to want to take the API key and Export it so in your dashboard you know when you generate the key from your dashboard you're going to want to copy that and Export it as an environment variable in this case we named it prompt layer aior key you can name it whatever you like just make sure that it matches what you have here after that you'll notice that we no longer import open AI right that's how we were doing it previously we had import open AI now we're saying openai the variable is equal to prompt layer. openai and that's all you have to do to layer prompt layer on top of open AI so that now it's going to track all the requests to open a we of course have to add the API key for open AI same thing we just take the openai variable say API key is equal to and then pull that API key from the environment variable that we have set so that's the same as it was in all the previous videos you're going to make your request to open AI the same way that you would regularly and then you just go over to prompt layer your dashboard you log in and you look at the log for all the requests that you made so this doesn't actually change response is equal to open AI chat completion. create tell it which model you want tell it your messages so you start with your system role give it some content start with your user role after that give it some content in this case you know who was the first president of United States set your temperature and your max tokens but here we have an additional argument to the chat completion call that is not normally included in our previous API call so now that we have prompt player on top of this we can add some additional things that prompt ler knows about so in this case we have plore tags is equal to a list of strings and these are tags comma separated inside this list that we can attach to the request that way if we want to be able to categorize and search through all of our previous requests we can do that very easily by using these tag names so in this case I'm asking a question about who the first president of the United States was I'm going to tag this with US presidents so we'll see how that works more in a second after we send this request we take our response variable and we print it to the console so nothing new there so we can run real quick and wait for it to come back sure enough there's our response so we have all the typical things that we normally have in our response the answer to our question from the assistant role is the first president of the United States was George Washington we see how many tokens Etc so now if we jump back over to our dashboard for prompt layer we can see the most recent request that was logged is this one right here so it cost us 00625 we can see that it was 38 tokens if we go back we can see sure enough total tokens was 38 so that's good and you'll notice somewhere in here where is it there it is US presidents is the tag so this is helpful because if we go up here and we search for US presidents then it's going to pull up the two different requests this one we just did and one that I did earlier while I was testing this out that have those tags so you can see how useful that can be for you know categorizing and being able to you know organize and look at the different uh logs for your various requests that you made in the past you can even export this stuff you can finetune your model which is really crazy we'll talk more about that in a future video in this video we just want to show you how simple it was to get up and running with prompt player on top of the openai API and it's as simple as a couple lines of code a pip install with your virtual environment activated you know generating a key exporting that key as an environment variable and logging into your dashboard and viewing the logs of all your requests to the open API so that's it for this video thanks a lot and we'll catch you in the next one