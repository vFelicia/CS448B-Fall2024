hey welcome back everybody this is Ian bringing you another video in this AI series with the New Boston so in today's video we're going to talk about anthropics clae 3 API in the previous video we talked about CLA 2 we did mention that we were going to talk about clae 2.1 but CLA 3 is better faster came out right after we made that last video so we thought what the heck let's just go straight to CLA 3 everything that we're going to teach you here can actually translate over to claw 2.1 if you for whatever reason need to go back and use it but CL three is going to be the latest and greatest so we're going to go ahead and stay up to date with that by introducing it in this video this that we're looking at here on my screen is the blog post that talks about the release of Claude 3 the clae 3 Model family there's actually three different models that are included and that's because they've got a version called Haiku that is cheaper than the other two versions and faster in most cases but not quite as intelligent so it'll do a lot of really great stuff but it might not be what you need for all of your us cases and then kind of right in the middle ground they have one called sonnet which is a little more expensive and potentially a little slower but more intelligent and then Opus is probably the slowest of the three but is way more intelligent and also more expensive so it can do a huge range of things but it may not necessarily be the best for whatever your project is based on your project requirements so you'll want to go through this article and figure out based on the information in it which one is the best to use we are going to use hi cou because it is simple it's fast and it is the cheapest but you can change the values for which model to use and we'll show you how to do that in this video so yeah go ahead and go through this you can see like I described to you the kind of XY here for intelligence versus cost and where each of these three models in the model family kind of fall in you can see the one that we're going to use ha coup on the left being the least intelligent but also the cheapest and then Sonet right in the middle and then Opus over here on the right where it costs more but it's very intelligent bunch of cool stuff in here about benchmarking I'm not going to go through all that but you can go through and read all the cool stuff that they have available for you to read and then right here they talk about the specifics of each model so Opus being the most intelligent and the cost per million tokens for input and output the content W context window and potential uses so again if you have like a task automation thing then maybe Opus is the right thing for you if you have some R&D sort of thing for research then maybe it's Opus again you can go down to the next one you can read about Sonet and you can see okay this one strikes the ideal balance between intelligence and speed and particularly for Enterprise workloads so then go down to potential uses you can see oh it's actually pretty good for rag use cases or it's good for things like sales forecasting targeting marketing Etc so after that the faster one is going to be I think we skipped it is haiku which is the fastest most compact model for near instant responsiveness so as you might assume this one's going to be pretty good for like chat Bo bots so you can see down here for potential use cases customer interactions chat Bots content moderation more chat type stuff and uh cost saving tasks so we are going to use this one again just because it's affordable and it's really fast and it's still pretty powerful you can also see that it is uh the cheapest based on per million token usage okay great so feel free to look through this we will share a link to this in the description or the comments but let's hop over to our code mode great so this is the main pie file inside of the 02 dog toes folder which is inside of our anthropic examples folder and of course at the very top all this stuff is just boiler plate so uh I'm not going to go into that but basically that's just how we bring in anthropic how we include our API keys and we get it hooked up to prompt layer we are going to be using prompt layer in this video it's just so easy to hook it up to things like open Ai and anthropic where we can track all of our calls and track our prompts and create templates so it's just a nobrainer to have that included all right cool so here on line eight is where we're actually getting into the meat and potatoes of this lecture and that is how we can send a request using the messages API for clae 3 now previously I believe we used the text completions which is their older version that they started out with now for cloud 2.1 and Claude 3 they're using the messages API so that's what we're going to use here so we use the SDK to send the request to the API through through this client. messages. create method and the result of that is going to be the message the assistant rooll message that gets sent back from cloud 3 so we'll assign that message object to the completion variable so that we can then look at it here at the bottom of our file all right so which arguments does the create method take for calling this messages API the first one here is the messages list inside the list there's going to be some dictionaries now the main rule here that you have to remember is that the first dictionary has to be a role of user no matter what so you might be used to open AI where they had their first one being the system role that is not how they do it with anthropics apis in fact you can have a system argument but it's going to be a top level argument to the doc create method as opposed to being included within the messages list so within the messages list the first dictionary that you'll see here is going to have two properties the first one is content which will be the text that you're actually sending to the API with whatever prompt is the next property here is going to be role there's two of those so anything that's coming from us the user will be a user role anything that comes back from the API will be an assistant role once we get that message back if we want to append it to this messages list and some sort of Loop and create some type of chat interaction then we can do that so here for Simplicity sake we're just sending a one dictionary request one message request to the API and we're going to expect a single response message object back and that will be a assistant roll message so then we have our temperature this should look familiar to you coming from our previous videos with open AI but in anthropic they do things similarly where they do have this thing called temperature which is going to control the creativity of the response so it starts at 0.0 and goes up to 1.0 as it gets closer to 1.0 it gets more creative so you can mess with that and just judging on the responses that you get back you can decide where to put that value so so that is the value that you're going to use most commonly they have some other stuff like topor P that you can use alternatively to temperature you can read about that in the documentation it's a little more advanced you would use one or the other but not both in this case we're just going to do temperature so then you can see I have an argument commented out here called system and this is just like the system roll that we've used in previous videos with open AI apis anthropic has it here as a top level argument like I said before but we can uncomment this and we can add some additional context or Guide to the overall request so rather than just saying how many toes does a dog have or do dogs have rather we can say respond only in Yoda speak so yes it'll answer us this question how many toes do dogs have but it'll answer it as if it's coming from Yoda the Star Wars character so we'll see how that works and then we'll comment it out and we'll see how it works without it this is optional and you can use it depending on your various use cases okay so then the model here you can go to the documentation to get the string that you use for the various models like I said there's three for cloud 3 there's Haiku which we have here and there's going to be another one for Sonet and another one for Opus so then Max tokens you can put this to whatever you want but this of course is going to control the combination of your input and output tokens so that you don't spend too much money you can keep things within budget so the last thing that we have here is actually coming from prompt layer so PL stands for prompt layer and we have plore tags is equal to this list of strings here we only have a single string you could include more if you wanted to but we have it set to animal toes so this is just a tag for this specific request so that when we see it over in our promp layer dashboard we can more easily organize things and say okay yeah these were all the requests about animal toes versus these were all the requests about you know which presidents of the United States were who during which time and so on and so forth okay great so that's it for sending your request to the anthropic messages API so down here we're actually going to print out the response which is that message object and then we're going to drill down into it and get that first content object which is like a Content block object type something like that it doesn't really matter but it has two properties one is text and the other is type we want the text from that object the way we get that is we say completion. content which is a list we get the first item in the list using zero and then text so we'll see that here here in a second all these other print statements with nothing inside of them are just there for formatting purposes so let's jump over to our terminal and send our first request of course you want to make sure that you have your virtual environment activated all of your dependencies installed everything like that once you have that ready to go then you can just run python or Python 3 depending on how your python is set up and then main.py so this shouldn't take long because we're using hu sure enough came right back and you can see that the first thing we have here is the message object so if we go look at our code that's that completion variable and so here we have message so it has an ID property a Content property and then after content it's kind of hard to read this but there's one for model and then there's the role note that the role is no longer User it's assistant right we're going to alternate between user and assistant we send the user requests we get back the assistant response the stop reason is end turn that just means that it came to a natural stopping point stop sequence there's no stop sequence included that's a more advanced thing where you can include a list of strings and if it matches a string it will stop based on that match lastly we have the type message and usage and within usage we have the input tokens how many were used output tokens how many were used then of course a combination of that would be how many tokens you used Al together and that's going to be less than whatever your max tokens was less than or equal to okay great so then what I did is I drilled down into the complete got that first dictionary within the content list and then printed out the text so here we have this array and then we have this object and it has a text property we print it out here and what we get back is dogs typically have four toes on each paw so a total of 16 toes that's actually a great answer it actually goes on right and we gave it 256 tokens so it thought what the heck you know they didn't tell me explicitly to keep it to a certain number of characters or anything like that so here's my much lengthier response so based on this output you may decide yeah that's not exactly what I want and then you can just dial it back you can change the way that you prompted it maybe by adding something to the system or to the initial user content and make it to where it comes back formatted or limited to a certain number of characters based on whatever your needs are all right cool so it says include some other stuff here the Paw of a dog has the following toe configuration blah blah blah so the total number of toes on a dog is four toes time Four Paws equals 16 toes this is a standard number of toes for most dog Breeze blah blah blah BL blah actually interesting fact right so there are some rare exceptions where dogs may have extra toes polyl dogs uh I didn't even know that so that's kind of cool that we learned that but again it might not be what you need for your specific use case so then you can just modify your prompt to get back something a little closer to what you want So speaking of modifying our prompt let's go back to our request and you'll notice that system for respond only in Yoda speed is there uncommented but I haven't saved it yet so that's why we didn't see it the version when we ran the code this is what it saw where it was commented out so we have it uncommented here we have our system argument we're saying respond only in Yoda speak so Yoda being the character from Star Wars series and he talks in a very specific way so we are going to run this back again and the response should be the way that Yoda talks so it starts out with h which he's always doing that little thing before he says anything and then dogs have four toes on each pod they do a total of 16 toes a dog has so that is definitely how Yoda talks so great even CLA 3 Haiku which is the most inexpensive and least intelligent of the three models in the CLA 3 Model family was able to do great with those instructions so the last thing that we want to check out here of course is because we are dialed into prompt layer we have this all connected we should be able to go over to our prompt layer dashboard and and sure enough you can see the responses from all the requests that we've made so the most recent one is right here with the respond only in Yoda speak system here's the user prompt how many toes the dogs have and then here's the assistant response you can see that we've got our tag up here you can even add your own tags if you want to so we have our tags added programmatically but we can also add them through the graphical user interface here even after the fact and then it calculated the total tokens for us which is nice it even calculated the cost which is really cool and then you can take this over to the playground and keep messing with it from there if you want to do it from a graphical user interface or you can jump back into your code and you can do things programmatically likewise it has a history of all of our requests and the responses so you can see the one before it where we didn't have the system you can see that the system is omitted we just have the user prompt and then we have that longer assistant prompt that came back both of these the one that we just looked at with the Yoda speak and the one that we're looking at currently have the same tag of animal toes so they're going to be grouped in with the same tag we can organize things that way so that's it for this video teaching you the introductory Basics to sending a request with the messages API for Claude 3 model with specifically the hon model but they do have the Sonet and the Opus models that you can play around with for the anthropic API thanks so much for watching this video can't wait to see you in the next one until next time peace