all righty y'all welcome back to another video and in this one we are going to be taking a look at how to add longterm memory to your llm chatbot now let me go ahead and give you an example of what the heck I'm talking about so let's say talk in the chat GPT and I wanted to let's say tell me a joke about pizza all right so let's see what it comes up with why did the pizza apply for a job because it on it to make some dough not bad okay so funny joke uh now let's pretend I you know take a step away from my computer go grab a cup of water come back and okay talking to Jet gbt again uh tell me that joke about pizza again man I need a good laugh sure wait what's this why didn't the slice cuz it couldn't okay that's a pretty funny joke but no I wanted the joke that you told me last time that was my favorite joke well uh let me see no the one from last time okay that must be able to clear it up what type of okay so clearly chat GPT or whatever I'm talking to right now it has no recollection of our previous conversation and honestly it makes me feel a little bit hurt inside because it's like yeah we had this whole relationship going and now you're pretending like you don't even know about our past and well honestly I just don't like feeling this way so let's figure out how we can add some longterm memory to ya give her a little bit improvement over chat GPT so how are we going to be doing this well we are going to be using this little thingy called embeddings so what are embeddings embeddings are how we can turn text into numbers okay that sounds interesting not sure why we would want to do that but let's go ahead and scroll down see if we can figure this out so first of all how do we get these magical embeddings well it looks like we use this function right here and we give it some text and then after that let's see what we get back huh okay so it will contain the embedding Vector along with some other data and this embedding Vector it just looks like a big array of numbers okay so indeed these embeddings are a numerical representation of this text interesting but then the question Still Remains why do we want a big number that represents some text well let's put that thought on the back burner for just a second and hop over and talk about space for just a little bit so here what we're looking at is onedimensional space we only have one axis the x axis and if I were to say something like let's find something on at location 8 let's say uh let me get another shape here so we can have something to draw with all right just this red I'll make this uh a red circle it can be my little pointer I guess but let's say we wanted to uh like just signify something at location eight so this is zero one two 3 four five six 7 eight okay so in onedimensional space eight you just count over eight spots so let's you know make this a little bit fancier and now let's go to twodimensional space well now I can give you two coordinates like location 48 so what we do is count over four and then we would count up eight so we have two axises or two axes two coordinates okay simple enough same thing with three dimension iions so if we have three dimensions I could give you a three coordinate system an X location A Y location and a z location now honestly anything more than three dimensions it's kind of hard for me to comprehend however thankfully for computers it's not so what does all this have to do with vectors well this Vector representation right here believe it or not they're a lot more than these four numbers in this embedding there are like, 1500 right around there I think it's like 15,000 or 1567 whatever this actually maps to and I know this terminology is like not exactly right but just to uh kind of lay down the concepts this maps to a point in like 1500 dimensional space so basically whenever you have a piece of text right here at very high dimensional space that can be converted to a point that's out there somewhere uh actually let's say that some text is converted to this point in whatever dimensional space and we'll say another bit of text is converted to this point in whatever dimensional space all right that's interesting so I guess we're getting somewhere we know that different text kind of aligns with different points in space but I still don't see how this is going to help us with that longterm memory Pizza problem we were talking about earlier so check this out let me just go ahead and get rid of this and now that we had our little side quest talking about space let's go ahead and take a look at a more realistic example of what's going to happen so in this example instead of this text right here I am going to be converting these little messages to vectors so let's just go ahead and take this one real quick that says I love pizza and we'll kind of just start mapping this out whoa it's pretty big okay so let me zoom out a little bit and we'll say that this text right here I love pizza would you say that this ended up here in space again it doesn't really matter exactly where like up down left right I just want to show you something real quick so we'll say that this text I love pizza is in space right here now let's get another one of my comments I need to work out more and let's just make another shape for this so this is I need to work out more now the thing about Vector embeddings which make them very interesting is this if two pieces of text are similar they're going to end up close to each other in this whatever dimensional space now if these two pieces of text are not similar at all they don't have any concepts any words related they're going to be far apart so where would I really love pizza and I need to work out more be in relation to each other my guess is that they're going to be pretty dang far apart so we'll put them like this far apart so let's keep going through our examples and do this one I'm hungry a big cheese pizza sounds great okay so let me go ahead and add a little shape for this and where would this be I'm hungry hungry a big cheese pizza sounds great so this is pretty closely related to Pizza doesn't really have anything to do with working out so I would put it like right around here I would say and we can just do one more for fun does anyone know a good book about Bitcoin so okay let me just make the shape for this um has nothing to do with pizza nothing to do with working out really so maybe it would just be like out here somewhere okay so right now if I was storing these embeddings or these vectors then it would look something like this okay interesting but still not quite sure how this helps us with the pizza problem well check this out now things are going to start coming together let's say that we have a chat bot and all of our previous messages are already indexed they're already stored and we'll take a look at exactly how to do that in just a bit but they're stored in our data store just like we're seeing right here so now we got a new message that comes in and it says I'm hungry what kind of pizza should I eat so imagine here we come in and we say I'm hungry what kind of pizza should I eat right now chat PT or ya it doesn't have any context about our past it doesn't know if I talked to it before if I talked to it a million times before and if so what we talked about it basically has no idea of any memory of me and each message is like a brand new moment to it but check this out if I take this and I'm going to convert it to another shape because what we're going to be doing with this message the new one that comes in is we're also be going to be converting it to an embedding Vector just like we have been and then let me just give this a different outline so you guys can see what's going on just outline it uh green or something so this new message that comes in once it gets converted to a vector it's going to be placed somewhere in space so then what we can do is we can say okay this is where you are what we want to do from here is we want to go ahead and get the I'll just say x nearest messages and I say x because it could be like the two near nearest ones the 10 nearest ones this depends on how you want to configure it but let's just say we want to configure it to also get the two nearest messages so that's going to be this one and this one so this is going to be your original query that comes in and these are going to be the two little bits of information that found that is close to this message so what we can do from here is we can take all of these messages and we can include it in the prompt and then when we say I'm kind of hungry what kind of pizza should I eat it's going to know that one I really love pizza and two I particularly like cheese pizza right here so with that the desire is with this additional information these additional memories we can get a better response from the llm so sounds like a pretty cool concept now how do we actually do this well believe it or not you cannot store these embedded vectors on Lucid chart you have to use a another data store and one of the most popular ones is pine cone so with pine cone what you do is you first create an index now an index is basically just like a big bucket or data store that you can use to store vectors and then once you have your index you can pretty much just store vectors and it's pretty simple actually it's very simple actually so let me go ahead I already wrot a little bit of source code and I'll talk you through this so this little example right here is going to be using open Ai and pine cone again open AI is what we're going to be using to generate these vectors and then we're going to be storing them in Pine Cone basically our database so that's why I configured everything right here and now here what I'm doing is I'm just creating that index I already have it created as you guys can see I just named it post since it's going to store post vectors and in this example since I already have it created I just commented this out so you know I don't want to create it again it's already created but then what I'm going to do and again this would probably be coming from an actual database but because I didn't want to make this uh example too complicated I just made a quick dict and then I just gave it 10 random messages uh the example that I'm going to be using eventually is I'm hungry what kind of pizza should I eat same thing as we saw in Lucid chart and I have two messages about pizza right here I really love pizza and I'm hungry a Big Cheese Pizza I think that's the other one uh yeah so hopefully if everything works we should be getting these two messages back but either way continuing on what we're going to do next is I'm just looping through this messages dictionary and for each of those messages or posts I probably should have called um what I'm going to do is I'm pretty much just going to generate a vector from it right here and then once I have all those vectors or embedded embedding vectors generated I'm just going to insert them all into my post Index right here and that's how you see you have 10 matches or 10 posts in here because I had uh 10 matches and another thing that I want to point out before I go any further we'll get back to that code example in just a bit is you see whenever I create this index and maybe you didn't see that I skimmed over this but whenever I create this index I need to explicitly say how big are the vectors that I'm going to be storing in it because each Vector has a set size whenever I use this function from open AI it generates vectors in this size right here 1 53 36 now in this index I can only store vectors with that size and that's because of how the math works you can't compare a vector let's say of a length of three to a vector of a length of eight whenever you're comparing vectors and getting the distance between them to find out how similar the text is the vectors need to be the exact same size so I just want to point that out if you ever like come across another function that can get you a vector and it's not compatible just uh yeah something to watch out for but anyways heading back to this example now so what I did is I basically inserted converted all my posts to vectors and then I inserted them into my Vector Store Pine Cone and now with this this code right here is basically simulating what would happen whenever a new chat message came in so I'm a user and I just typed I'm hungry what kind of pizza should I eat now in order to find the most similar post related to this I first need to conver what the heck was that noise conver I need to convert this to an embedding now once I have that I can pretty much use this query right here to say okay from this Vector that I just created basically a numerical representation of this string right here get me the two nearest vectors and then it actually doesn't return the string it Returns the ID one of these numbers so from the ID all I have to do is pretty much just uh get the related messages from it but if all this works then what we should see is actually let's run this right here let me bump this up a little bit so you guys can see all right look at this so it printed out the two most closely related posts and it was this one it was the closest one I am hungry a big big cheese pizza sounds great and this one right here I really love pizza so there you go what you would do from here is of course take this original question and these two other related ones and throw this into your prompt template and then hopefully the llm can come back with a much more tailored accurate memorable response so yeah that is it for this tutorial I'm not exactly sure what we're going to be covering in the next one but I'm sure that is going to be awesome so I will see you then