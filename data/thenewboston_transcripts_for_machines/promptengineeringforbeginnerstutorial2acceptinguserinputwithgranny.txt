hey everyone welcome back this is Ian here bringing you this AI series with the New Boston we're in video two of the series so glad to have you here very excited to bring you this video so let's get into it in this particular video we're going to build on what we did in the first video we're going to use open AI chat completion API to interact with it and get back some cool output right so we're going to have a conversation with open ai's GPT 3.5 turbo model and we're going going to take control of exactly what that conversation is going to look like normally when we're out there on chat gbt or similar generative AI tools we just type in something we might tell it like hey you know pretend you're this and give me a response back you know write me a poem in the voice of Shakespeare or something like that and that's great but in this case we're actually writing programs that allow our users to interact with chat but we have full control over the pretext of everything so we can really build some cool things here in this use case we're going to augment what we did previously where we hardcoded a question to the model we said something like tell me the NHL team for Philadelphia and it came back with a response very straightforward answer we had a low temperature setting in our request parameters so it was giving us the same answer over and over again that's great now what we're going to do is we're going to take in Dynamic input from the user in the Contex of the program where it's executed in this case the terminal and we're going to use that Dynamic input to send it through with our chat completion request and what that will do is make it to where the user now has the ability to ask the question so instead of just hardcoding it and having the same question over and over again the user can start driving the conversation we're going to go a step further though instead of just having the model act as a default helpful assistant we're going to tell the system role that hey you you are a sweet old helpful grandma and what will happen there is that the response back from the API is going to be in the voice in the context of a sweet helpful old grandma so it's going to add a little bit of flare and uniqueness to those responses so let's take a look at it on the first two lines here we simply have our import for the OS the import for the open AI library and then setting that open AI API key to the result of using OS with the get EMV method to get the actual value of our open AI API key environment variable I know that's a lot hopefully you're familiar with setting up environment variables if you're not go ahead and look that up it's really straightforward just make sure that you look it up for your operating system or your environment you can do it with python or you can do it with Windows you can do it with Mac or Linux there's a bunch of different ways and they're all very straightforward so once you have that set up now you're ready to interact with open ai's API the next thing we want to do is get some Dynamic user input and we we can do that with Python's builtin input method so the input method takes an argument that's a string and it'll print that out into the console and then the user will be prompted to respond to that they're going to type on their keyboard and when they press enter or return the result of that is going to be stored in this variable here in this case the variable is called user text so what we'll see as output in the console or in the terminal is what can granny help you with today so we're already setting the pretext here so that the user knows that they're communicating with Granny they're no longer communicating with just a regular helpful assistant now we can use that user text that we gathered from our user and we can pass it into our chat completion API request down here on line nine you can see everything here looks pretty similar to what we did in the previous video right we're using open ai's chat completion API we're creating a new chat completion object behind the scenes it knows which endpoints to send the request to and the headers and everything we don't have to worry about any of that all we have to do is make sure that we pass in values for parameters that we want to use so we pass them in as arguments in this case model messages temperature and Max tokens we talked about those in the previous video but let's go ahead and review exactly what they're doing here so model is going to dictate which of the open AI models that we're going to use llm large language model in this case GPT 3.5 turbo it's more affordable it's still very powerful it's great for just getting your feet wet and having some fun with this API without spending a bunch of money on tokens okay messages messages is going to have different objects inside of it it's a list right an array and each object has a role there's three roles we have system user and we have assistant the system is that very first object that just tells the model like hey this is how you need to act this is the role you need to take on whenever you're responding to me in future responses as an assistant and then anything from the user is going to be us our input anything from the the assistant is going to be the responses that we get back and we can append these objects into this list as you'll see in the later video so that we can continue building on this history so every time we go back and communicate with the API it knows exactly the context of the conversation for now we're just setting the pretext we're saying hey system you are a sweet old helpful Grandma so the role of system the content is you are a sweet old helpful Grandma easy enough that's our first object in the messages list the next one is roll of user again this is the input from us the user or the users that are using our program the content is going to be set to some type of string now previously we hardcoded this now we're going to set it to the variable user text which was executed right when our program was run initially we'll see that in a moment and it asks the user for some input what can granny help you with today so then the user enters something in there and we use that inside of our request here so that the chat completions API can say okay I'm a sweet old helpful Grandma here's your question let me build a response for you and then send it back when we get it back we store it in this response variable and then we can take a look at it of course we have our temperature that's going to determine how deterministic how variable how creative the responses are in this case we're pretty close to zero at 0.5 the scale is 0 to two the closer you are to zero the less random it'll be the closer you are to two uh the more creative but potentially more random and error prone it could be Max token this is the number of tokens that we will allow the response to go up to in the prompt response this isn't including the tokens that are being used for the input this is just for the prompt response so the total tokens will be the max tokens whatever gets used from those and whatever tokens are inherently uh included in our inputs so we'll see more of that in a second so you see you'll see exactly what I'm talking about okay so that's it whenever we pass in all of these arguments and we send them to the chat completion API it does its thing and then it sends us back a response so we're going to print that response it's just going to be an object with some key value pairs we're going to print a empty line just for formatting purposes and then we're going to actually dive down into the response object we're going to go look at what's called choices you can set a choices argument to give you multiple choices to look at in this case we omitted it so we're only going to have one and therefore we're going to access that one choice at the zeroth index inside of the choices list once inside of there we're going to have another object to look at it's going to have a message field or attribute and when we look inside there it'll have a Content attribute and when we look inside of that ultimately we're going to get back our string of text and that's what we're going to print to the console here so let's open up our code editor and I should mention before we run this code that if you're following along and you've got the repository with all the solution code for this entire series uh from the description of course on your computer computer there's going to be a couple folders in that main repository the one that we are in currently is the open aior examples this is the second video in the series so we are inside of 02 user input all the code that we just looked at is inside main.py of course you're going to want to make sure that if you're doing this locally you have a virtual environment set up you have all your dependencies installed and you have your open AI API key set as an environment variable so with all of that ready to go then you can run python main.py from inside of the O2 or 02 user input folder so we're going to run it of course it's going to hit that line where it wants the input so the prompt for us is what can granny help with help you with today and we're going to say we're going to keep it simple at first okay because this is super powerful you can ask openended questions and you can get massive responses but just remember large responses means lots of tokens lots of tokens means lots of money okay so let's do something simple like what is the capital of New York now if we had asked this question with our previous implementation from the first video it would just be very straightforward very simple it would just say the capital of New York is Albany in this case remember this is a helpful granny so our responses can be a little more creative it says oh my dear the capital of New York is Albany okay very similar to what it would have been but now it's calling us my dear and then it goes on to say it's a lovely city located along the river is there anything else you like to know so again helpful old grandma she's following up with us she's saying is there anything else you like to know she's calling us dear you know I feel like I'm I'm at Grandma's house right now and she's helping me out now let's look at the response object here it's going to be very similar to what we saw before in our initial implementation from the first video it has a unique ID for this chat completion object it says that the object type is that of chat completion the created Property here is going to be a Unix timestamp the model is the model that we use GPT 3.5 Turbo with the version number the choices because we omitted choices as a argument in our initial request it's only going to give us one so we have one choice object like we talked about before the index is zero the message is going to have the role that it is if it's coming back to us it's the assistant right and then it has the content so the content is what we're really uh concerned with is what we want to look at so we'll see here in a second how we access that here it is the string that's what we right and then the finished reason of stop that's good we want a finished reason of stop if we modify our tokens so that we're creating kind of a threshold that you can only go up to so many tokens in your response that's fine but just know that if the question is openended and so the response is a longer response and if it meets its token length at Max tokens then it's actually just going to cut off the response and the Finish reason is not going to be stop it's going to be length so we can look at that more in a second before we wrap up the video the usage of course is useful because it shows us what the total number of prompt tokens were prompt tokens being the response the completion tokens being the result back to us the chat completion and then the total tokens of course is the combination of those two so 61 is super low that's great we asked a really simple question we got a pretty simple answer didn't use a lot of tokens now we can ask a not so simple openended question and it can use all 10,24 of the tokens that we set in our Max tokens that may not be what we want so we might tune it down to like a 100 but then if we do something where we run the program it says what can granny help you with today and we say tell me a story granny and we run it what's going to happen most likely is it's not going to be able to tell us a full story in 100 tokens for the completion and so it'll cut it off and so we'll see what that looks like in the response object okay so completion tokens this is the content ENT of the assistant responding to us that's our completion is 100 tokens okay total tokens of course is the combination of the prompt tokens plus the completion tokens so when you do maxcore tokens you're saying your completion can only use this many tokens in this case 100 and then it says okay well in addition to the completion your prompt your input use this many tokens as well so here's the total for that okay great now because in our code down at the very bottom we Traverse down into the object choices zero message content on the response and we print that we get that printed out really nicely here right so we can see it here but it's included in this larger object with all this you know distraction so if we go down here and we look at that output we can see it starts telling us a story which is what we asked it to do and it's doing it as if it's a helpful Grandma however because we limited the max tokens to 100 it got to 100 tokens and it stopped you see here curiosity peaked that's the beginning of a sentence but it's not a full thought we don't have a period we don't have an end of the sentence so there's going to be some let's just call it an artifact of limiting your tokens and so when you're building your applications you have to take that into account if I'm limiting my tokens could the response not be fully finished how do I determine that well that's where the Finish reason comes into play if it says finish reason is stop that means it came to a natural stopping point that's great if it's length then you probably want to do a check for that and you want to follow up with some type of response or output or message or something logging whatever you choose to do to have the best user experience possible based on the limitations of your program just something to think about let's go ahead and do a quick overview of everything we just talked about because even though it's only a few lines of code we just did something really powerful and now you have the ability to go in here augment it and really have some fun kind of moving things around and making it do exactly what you want it to do so again boiler plate code at the top get that environment variable set it on your open AI as the API key that's all good to go now get that user input assign it to a variable make a call to the API assign the result to a variable and then print out the entire result print an empty space for formatting purposes and then print the actual content so Traverse down into that object until you get to the content attribute and print its output now more specifically inside of the open AI chat completion API call just remember that our system is no longer a helpful assistant we said you are a sweet old helpful grandma have fun with this mess around with it make it do something different right see what kind of output you can get based on whatever you put inside the content here and then again you can change your input here according to whatever you set the context of the content string for your system rle you're going to end up passing that text down in here it's going to make things Dynamic and in future videos we're going to show you how to keep that conversation going so that the user can keep going back and forth with the model and actually having a full conversation with a tracked history and the model understands the context of the continued conversation that's it for now really enjoying teaching this to you all thank you so much please give me any comments or feedback questions in the comments below and we will see you all in the next video thanks a lot