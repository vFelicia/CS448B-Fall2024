hey welcome back everyone this is Ian bringing you another video in this AI series with the New Boston in today's video we're going to talk about something called embeddings so what are embeddings let's go straight to the source this is the open aai documentation and the answer to that question is open AI text embeddings measure the relatedness of text strings so it has some things that they're commonly used for listed here search clustering recommendations anomaly detection diversity measurement and classification specifically in this video we're going to use it for text similarity so we'll pass a word or multiple words into the embeddings function and then when it comes back we'll use some math to figure out the relatedness or the closeness of those two words it's going to give us a score and then based on the value of that score we'll be able to determine how similar two words are to each other so there's a lot of cool things you can do with embeddings I highly encourage you to go to the documentation go through here and see what all you can do we're specifically going to be using this example here to just create an embedding and then we'll use some math something called cosine similarity to calculate a score to determine how close two different words are to one another so when we send a request to the open AI embedding API what we get back is this embedding object here and so inside of it it has a list called data and that has one or more objects that include the embedding data each embedding is going to look like a list of floating Point numbers negative or positive and there will be a lot of them in there in this case specifically 1,536 but it could be less or more based on the model that you use there's some other information here like the index of that specific object inside of the data list and then what type of object it is in this case it's an embedding object which model is used with the request what type of object it is in this case it's a list and then how many tokens you used for your prompt and your total tokens with the overall usage of the request before we dive into the actual code let's talk a little bit about embeddings so that you can get a visual overview of exactly what they are and how they work so if we scroll back up here we can see that this embedding is a list of numbers and essentially numbers that are closer together in separate lists will help us determine how close the words that that those numbers are derived from are to one another how similar they are if we look at this right here you can see I have this kind of world mapped out and I've got these islands and on each island there's a word notice that words like cat and Feline and purr and lion are all very similar all very close to each other because they're similar and then you have something like dog or animal which isn't too far away because you know both a cat and a dog are animals and then we have things like eagle and elephant they're kind of a similar distance from animal as are cat and dog but then you have something like microscope which is way over here on the left far away from everything because it's not really similar to these other words we use something in this case we use cosine similarity which is a mathematical equation that helps us determine how close each of these islands are to one another that's what we're going to do we're going to take these words from these islands and we're going to pass them through this embedding function or we're going to send them up to the embedding API I should say and what we get back are these embeddings and so then we'll use the embeddings with that cosine similarity function which is just a mathematical equation it's going to give us a number and that number is going to help us determine how close these islands are to one another that'll help us know how similar the words are so cat should be closer to feline than cat is to microscope or cat should be closer to feline than elephant is to microscope things like that there's one last thing I want to show you before we jump into to the code and that is this YouTube video by stat quest with Josh starmer it's called cosine similarity clearly explained this is a great video it helped me better understand this concept it's really short only 10 minutes so if you want to learn more about the math behind figuring out the similarity between these different words with their embeddings then this is definitely a video for you let's go ahead and jump over to the code now and we'll see how this actually works with sending the requests up to open AI we are going to import something called num Pi that'll let us do the mathematical equation for the cosine similarity and then of course we need to bring in open AI library and create a new client from it now that we have that client we can actually send requests up to the embedding API okay so then we have a function here called cosine similarity so again if you want to understand the math happening behind the scenes for cosine similarity check out that video that I referenced but just know that what we do as we pass in two values the values here are going to be the actual embeddings so the vectors for each of the words and then we're going to get a score calculated for how close they are together a value between 0 and one if it's zero they're not similar at all if it's one they're very similar and any value in between will allow us to determine just how close or far away they are from one another the next function that we have here is get embedding so this is going to be our call to the client the open AI API embeddings endpoint so the reason I put this inside of a function here is because we're going to use it multiple times inside of this program I didn't want to repeat ourselves so I went ahead and wrapped it in a function called get embedding so get embedding is going to take an input if we pass in one single string value it's just going to give us one embedding if we pass in a list of multiple string values it'll give us an embedding for each of those strings and then we just want to pass in the model that we want to use so you'll see the model here in a second but essentially this function is going to return the response from calling client. eddings doc create it passes in our input and our model model and then it sends us back an object like the one that we looked at in the documentation a moment ago from that we can pull out the actual embedding so down here we have a constant variable called Model we're going to use the same model across all these requests and we only want to change it in one place if we decide to use a different one in the future so for now we're using text embedding Ada 002 this is the most recent model and it's said to perform better and be cheaper to use than all of the earlier models which are now deprecated so down here we have our first response that we get back from calling get embedding and passing in our input of corn and then the model text embedding a to 002 from that we're going to go ahead and print the response and you'll see it is a expanded version of the example output that we saw in the documentation a moment ago you'll notice whenever we actually get to that point that the embedding itself has a ton of values inside of it and that's because like I mentioned it's 1,536 total values in inside of that Vector so then we're going to print the length that'll confirm that number that I just said 1536 and that'll determine how many values are actually inside of that embedding and then the next one is going to be similar here but instead of passing in a single string for the input we're going to pass in a list with multiple strings what will happen here is we'll actually get back and embedding for each one of these strings so instead of just having data zero. embedding we'll have data zero and data one and then each of those will have an embedding inside of it then we can assign those embeddings to variables here A and B and we can pass them into our cosine similarity function which will then return a score ultimately we will print out the score helping us see the similarity between the words that were inputed in this case cat and Feline we'll do that again with words that aren't so similar elephant and microscope if you remember our Islands from earlier elephant was all the way over on the right microscope was way over on the left so the value that comes back from this get embedded setting the score that's calculated should be lower than the one that we get back for cat and Feline which were closer together in our Island example so again we're going to extract the embeddings from that response we're going to pass them to cosine similarity that's going to calculate a score and then we're going to print out that score here let's go ahead and open up our terminal we'll run this code and we'll see what it looks like in real time so here we are in our terminal we're inside of the AI playground repository the open AI examples folder and then inside of that the 13 embeddings folder there's a file inside of there called main.py so we'll go ahead and run that with python main. pi and that file is the one that holds all the code that we were just looking at so we'll let that run it'll send the request out and the first thing we should see is a giant list of these values so if we scroll up to the top here you can see there's just a ton of these and so these are all the values inside of our embedding and so here you can see data is a list and then it has an embedding inside of it which is another list that has a ton of values so if we scroll all the way down past all of these values we should be able to see 1,536 total values inside of that embedding now what we're really concerned with here is the output of the scoring for the last two function calls the one for cat and Feline and the one after that for elephant and microscope the similarity score between cat and Feline is 0.854 blah blah blah and then the similarity score between elephant and microscope is 0.79 37 yada yada and so you can see right out of the box elephant and microscope because they're further away they score lower than cat and Feline and that's basically it we got an embedding from our initial call so if we scroll up and we look here we got an embedding for corn and that's the one that we saw here with all these values and then we also did the same thing for cat and feline we didn't actually output the response but what we did do is take those two embeddings that we got back for each of those words pass them to our cosine similarity got our score and then printed it we repeated the same process for elephant and microscope and then we were able to determine how similar or how close those words were together with regards to their embeddings so that's how you get embeddings with open AI embedding API thanks a lot for watching this video can't wait to see you in the next one until next time peace