hey everyone how's it going my name is Ian and in today's tutorial I'm going to be teaching you all about the open AI API and how we can interact with it using the Python programming language so in this video we're going to specifically focus on interacting with the chat completions API from open aai which essentially allows us to engage with a GPT model like GPT 3.5 turbo or GPT 4 in a conversational format so this is something that you're probably already familiar with with regards to using tools like chat gbt this is essentially what's happening behind the scenes and now we're going to use a programming language like python to have more granular control and programmatic control of how we interact with that API so instead of just typing information into a graphical user interface we're actually going to write the code to send these questions to the model and get back responses so learning the skill can be super useful especially if you're interested in like building chat Bots image generators re recommendation engines uh code review tools there's just so many options in fact if you ever wanted to see kind of what kind of projects are out there you might go check out theirs and AI for that.com they have a whole host of different projects that people have built using these kinds of tools so you can kind of see what some of the possibilities are and and Spark your creativity in that way so this video is actually the first in a larger Series where we'll continue to introduce to you the ins and outs of using opena uh it's API as well as the anthropic API and the prompt layer apis so this series is really great for anyone who's interested in prompt engineering as a whole and so if you think that's you then definitely stick around because I think we're going to have some content here that you'll really enjoy and be able to learn from so before we get started if you want access to the GitHub repo the code that I'll be using in this video and the videos that follow be sure and get that link from the description below you can clone or download that repository on onto your local environment and follow along with the videos that way so yeah without further Ado let's go ahead and get into the code okay so I'm working inside of Visual Studio code which editor you use does not matter you just need an environment where you can write some python code and a terminal to be able to run that code so the first thing I'm going to do up here at the top of my program is introduce some boilerplate code and I've got a couple modules that I need access to in order to make this program work before we talk about that let's talk about what this program is going to do essentially we are going to send a request to open ai's chat completion API and we're going to create a new chat completion object so that is essentially what happens when you're using a tool like chat GPT and you type in a query or a prompt and then you get back a response so this is what's happening behind the scenes we're going to actually do this programmatically using this API using python so we're going to tell it which model we want to use we're going to create kind of a history of the conversation uh we're going to set the tone of the conversation we're going to determine how uh variable it is how creative it is or how concise it is with its responses and we're also going to dictate uh the max number of tokens we'll talk more about what some of those terms mean here in a second but essentially once we run this we're going to get an answer back from the API to our question which is which NHL team plays in Pittsburgh so down here at the bottom we can see an example response where it comes back to us and it says the NHL team that plays in Pittsburgh is the Pittsburgh Penguins but that's not all it gives us it gives us a whole bunch of other information that can be really useful when we're creating larger applications around this technology so let's back up to the very beginning now that we've done kind of an overview of what it is we're building and let's start getting into the actual syntax so at the top here on the first line I'm importing the OS module we need that because we are going to export a environment variable for the open AI API key which you'll need to get from the open AI API so you might want to pause the video or after you're done watching the video go to openai's website go find their API uh link we can put that in the description of course and sign up for an account if you don't already have one set up your billing buy some credits you can spend as little or as much as you want uh based on how many tokens you'll think you'll need and we'll talk about what tokens are and how they work here in a second but once you do that in your settings you can generate a API key which you can then export as an environment variable and you're going to want to name it open aore aior key if you're going to be using this code from the repo you can name it whatever you want if you're just following along and you want to use a different value for that environment variable all right so then the next line the kind of meat and potatoes of this entire program is importing the open AI module without that we cannot interact with open AI apis in this case the chatet completion API so we got to have that one now the first thing we do on line 4 after importing those modules is we set the API key property or attribute equal to the result of doing a os. get EMV on that environment variable that we set okay so that's going to set up your API key on open ai's API and now you are able to send and receive request response from that API so let's look at this doc string here it's going to outline exactly what's happening inside of the code that follows so in the code we have a model argument a messages argument a temperature argument and a Max tokens argument these are not all of the parameters that can be used with the chat completion apis create method these are just the bare minimum that we need to get a response back an answer to a question that we're going to ask so let's talk about what each of those do starting with the model so the model is an ID of the model to use and we've got some links here you can go to the documentation you can see which models are available to you essentially there's GPT 3.5 turbo and there's GPT 4 GPT 4 is is more creative and more powerful in many ways but it also comes with additional cost in terms of the tokens and then the credits that uh represent those tokens you can see what those costs are by visiting open ai's pricing page and you can determine which one you should use based on that the next argument is messages so messages is going to be a list of messes is comprising the conversation so far so you can think of this as the context we can start with our initial question and that's fine the GPT uh API can take that and it can answer it for us but then if it doesn't have a history of the conversation that we've had and we have a followup question that doesn't have any context to support it then it's not going to know the best way to answer that followup question so by creating this list of messages in a ordered format the API can go back and look at the history of our conversation thus far to determine the context of whatever the current question is that way it can give us the best possible answer and that allows us to have this continual back and forth chat type conversation so after that we've got temperature so temperature is going to be the argument for the sampling temperature that we want to use the values are between 0 and two higher values like 0.8 or even 1 or 1.2 will make the output more random it'll make it more creative uh lower values closer to zero are going to make it more focused and deterministic so essentially if you need something to be very concise and Technical then you might want to keep that value lower but if you want to give the API some more creative freedom then you might want to make it a little bit higher it just depends you can keep changing it and tweaking it until you get it right where you need it to be based on a number of respons es that you can look at so then Max tokens here at the bottom is going to be the maximum number of tokens to generate in the chat completion after we're done looking at the code I'm going to pull up a couple pages and show you exactly how tokens are uh created from the text that we use in our prompts and from the text that is given back to us as a response to our prompts but essentially you can think of tokens as the measurement of the inputs and outputs to this API so if we're sending a request with some text something like which NHL team plays in Pittsburgh then most fourl roughly fourletter words are going to comprise one token when you get two larger words like Pittsburgh then it's going to start breaking it up into multiple tokens okay so which this word right here it might just be one token it might break it up into two tokens and then the space leading to the next word including the next word in this case a threeletter word NHL will be our next token so this is roughly two tokens and then the space before the next word with the next four characters is going to be the next token it's going to repeat that pattern until it gets to a word that has multiple um more than four essentially maybe like seven or more characters and then it's going to break it up into multiple tokens and so the output to our request will actually tell us exactly how many tokens were used for that request so that's going to include everything that we piped in plus whatever we're getting back so we'll see that more in a minute in any event because all of this isn't free and it costs money uh and because these GPT models actually have token limits you want to set a Max tokens value so that you can dictate exactly how many of your tokens are being used or like what the maximum amount of tokens can be used before you max out your limits that way you don't spend too much money money and you don't surpass whatever the predefined limits are for the models again the link up top that we looked at a moment ago for the models is going to have more information about how many tokens are required or what the maximum tokens are for any one of these models that are listed there all right so down here in this other doc string and when I say dock string if you're not familiar with those they're just multiline comments with information about the program and so this is an example of the response data it has our chat completion object remember we are accessing the chat completion API and so if you want to learn more about all of the different fields that are included in this object you can visit this link right here but let's just do a brief overview of everything that we're looking at here so this is an example of what we would get back from executing our code right here so this would be the response variable pointing to the response from this API call so down here we have an ID that that's just a unique ID representing this particular uh chat completion object that we're getting back and then what type of object it is this is always going to be chat. completion when dealing with the chat completion API created is just a Unix timestamp in seconds the model is which model we used up above we used 3.5 for this particular example it was using gp4 so it's just telling you hey gp4 but it has some additional numbering with regards to the current version that was used these models are changed over time and so they have varying versions and they track those with these additional numbers we have choices here choices is a list of multiple objects or dictionaries when we made the request we did not add an additional parameter indicating that we wanted multiple choices we could using a parameter called in we can talk more about that in future videos but by default you only get back one choice if you don't indicate something other then n is equal to one or if you just omit in all together then you get one choice back so that's what we have here we have one choice inside this list it has an index indicating its order and all the choices if there were more than one and then it has a message object or a key that points to an object or what we can also call a dictionary that indicates the role of the message so who is it that is is giving this message in this case it's the assistant and in the context of the open aai API uh especially with regards to chat completions we have three different roles we have the system we have the assistant and we have the user so we'll talk more about those as we execute our code here in a moment uh but this one is from the assistant which is essentially the part of the API that has the back and forth conversation with us so the content is the actual textual content that is a response to our initial question or our followup question if we're having a continued conversation so in this case the content says the NHL team that plays in Pittsburgh is the Pittsburgh Penguins the last thing that we're going to see with regards to the choice object is the Finish reason in this case it's stop there are multiple values that can go inside of uh the value here for the Finish reason key stop is a good one to see because it lets us know that it did not stop because of like our Max token limit or anything like that it stopped because it reached a natural stopping point um it's essentially if you're thinking about like stat St codes it's kind of like the 200 of status codes it's just saying hey everything worked properly and we stopped naturally there was no errors or there was no limits that were reached or anything like that the last thing down here outside of the choices list is the usage key that points to another dictionary object where we have prompt tokens and completion tokens so prompt tokens are our prompts right that we inputed and then completion tokens are for the completion object that was generated uh back to us and so 24 + 12 is 36 so total tokens is going to be the combination of the prompt and completion tokens this is useful because as you're creating these prompts you can start seeing how much the usage is and then you can determine okay if I did this x number of times it would cost me this many tokens that would uh you know essentially be this much money and that's how much this type of program would cost to run over you know some time span so it gives you an idea during development uh what your usage looks like you can also go to the open AI dashboard the same place where you'll have generated your API key and set up your billing there's going to be a usage button in there you can take a look at a graph that shows you your usage day by day so that's helpful as well so at the bottom here we have a print statement where we pass in our response variable we have an empty print just to give us some nice formatting with that output and then we have an additional print where we actually Traverse down through that dictionary looking at the choices list the first element inside of it with the zeroth index and then the message attribute with the content attribute from there which ultimately traverses all the way down to the string value and that's what will get printed to the console here so we can actually run this let me move my little video out of the way and run this python file you can actually see where my previous run is in the history of my terminal but it does take a second or two to run right this is an API this is an API call and that means that there's the time the latency between our requests what happens out there on the open AI server and it coming back to us but in addition to that you know we're dealing with a GPT model that it has to take a little time to process all this information so uh it does take a second or two and there are ways to increase the uh user experience or make the user experience a little bit better whenever you're building applications around this sort of thing uh you can think like when you're using chat gbt and you type in a question it might give you like a little loading wheel or a dot dot dot sort of thing and then when it actually starts giving you the output back it does it uh kind of like typing out on the screen as opposed to just dumping it all at once so those are types of things you can think about once you get to the uiux part of developing your applications for now of course we're just kind of waiting we know it's going to give us a response of some sort and then after a second or two we get back our response so let's take a look at what we got back here we can see that this is very similar to what we were just looking at with that doc string where we had some example output so it's going to have the ID the object the created timestamp the model in this case because we Ed GPT 3.5 turbo it's going to say that that's the one that we used and then it has the versioning at the end there the choice is again because we didn't pass in any in argument to determine that there would be more more than one choice then it defaults to one so we get one object inside of here its index of course is zero it is the first element inside of this list it has a message which points to another dictionary or object where the role is assistant and the assistant is responding to our question with the content the NHL team that plays in Pittsburgh is the Pittsburgh Penguins this is actually the same exact response to the initial uh the example output I should say and that's because we have a pretty low um temperature right so again if the temperature value is low closer to zero uh then you're going to get more consistent responses not a lot of variation and as you go higher closer to one or even two then you're going to get more variation more creativity all right so the Finish reason is stop that's good that's what we're looking for and then the usage here is actually the same as the example that we saw in our doc string where we have a total of 36 tokens awesome so let's let's pull this down a little bit and go back and look at our code one thing that I want to address a little more here is the role of the messages list okay so or the the role of the roles of the messages left we're going to talk about the roles for each of these messages how about that so the first message that we have here the first object representing a message in our messages list has a roll key and you'll notice both of these objects have that and even in our response we have that so the role for the initial one is system and like I said before there are three right there is the uh what do they call it the assistant and then we have the system and we have the user so the system is only ever going to happen once at the very beginning This Is Us kind of providing the tone or um maybe the context for the assistant future responses we're saying hey you the system you are a and then some sort of something that they are so in this case we're saying you're a helpful assistant just so you know if you were to Omit this by default it's essentially going to be a helpful assistant that's kind of GPT 3.5 GPT 4 is like default mode but if you want to modify it to where like you are a expert in the field of some sort of science or mathematics then you can do that and it'll give you answers it'll do its best to form its answers in such a way uh that would respond to you as if it was that role so here we're just saying you are a helpful assistant that's great you can mess around with this and and set the mode or set the mood rather set the tone for the assistant by modifying the value to the content key inside of this object so the next one in line is the role for the user this is our initial question this is like what you would type into the input on chat gbt the actual website so we're saying hey here's our initial question which NHL team plays in Pittsburgh and and what we get back of course is going to be from the assistant so if we go down here to the response you can see that the message that comes back has a role of assistant and then the content is what the answer is to our question and so this is useful because then we can take that object and push it into or append it to the end of our messages list and include that in our next request back to the API if we have a followup question as the user role so what will happen is we're creating this history of the conversation between the user and the assistant of course with the initial setup as the system and then every additional request we make to the API will have that context and be able to answer us more efficiently and effectively so that is more or less it I'm going to pull up uh a couple more pages here to introduce you to a few more things I think are really useful before we wrap up this tutorial so let me jump over here to uh this browser window where I've got a couple different pages PES for the the API itself that are open Okay so this one is for the chat completions API if you want to dive a little bit deeper or you want a a different perspective on everything you've learned in today's video this is a great place to start you're going to read through what it tells you it's going to have examples you can change it to a different language if you prefer something like nodejs to Python and you can see the code for that and you can see some example results and how to Traverse through those objects to get whatever it is you're looking for in this case the content it's going to give you information about all the various parts of the response object and so on so this is helpful as review and if you want to dive a little bit deeper of course you have access to way more than what we talked about in this tutorial so this is a good place to bookmark and come back to as needed meanwhile over here we have access to the playground by the way all the links to these will be included in the description of this video but this is the the playground where let's just say you're in an environment where you don't have immediate access to node.js or python to be able to run this code in a editor or some type of IDE no problem you can still experiment with this if you have an API key set up you can go to the playground here you can tell the system what the system is that's that zeroi message and that messages list and then you can start with your user user rooll content just by adding a message here and then you'll start getting responses back back from the assistant role after you submit the user message so then you can modify the mode whether it's a chat and then there's a couple other Legacy options here that you can look into the model that you want we have things like 3.5 turbo which is what we used in our code example or we have GPT 4 GPT 3.5 turbo 16k you can learn more about what that is from the documentation or that you have even more models than that that you can access just a reminder there gb4 will charge you more tokens per request so you want to make sure that you're familiar with the pricing structure and everything and again we've linked to the pricing earlier in the tutorial you'll have that in the description of this video as well but you can see here you can modify things like the maximum length which is your tokens and the temperature and there's some other things in here that we didn't talk about that you're feel free to mess around with uh these are some of the parameters that are not essentially required but can give you a little more control as needed so then the last thing I have here is this tokenizer page P which I want to show you all because we talked about tokens and I kind of briefly explained that one token is roughly four characters of text and the common English language uh or common English text and so they translate to roughly 3/4 of a word so like let's say you have 100 tokens that's around 75 words but if you wanted a nice visualization of how this actually works then I want to show that to you here so if I click on show example here on this tokenizer page it's going to give me some text it even gives me me an emoji and it gives me some numbers things like that and you can see here how many tokens it pulled from this text or how many tokens uh this text is equal to and then how many characters were in the text overall and you can see with this color highlighting how exactly it breaks it up so for example a word like emojis is M and then o and then e or Unicode is Yun and then I and then OD but there's some other words that are long that still equal one token like underlying the exact science isn't necessarily four characters but that's just kind of the average that you can round it uh down to but if you want to get more of an understanding of kind of how this process works you can look you also notice that some of these these tokens are broken up by like common patterns so for example one two 3 is a common pattern so out of 1 2 3 4 5 6 7 8 90 it broke up 1 123 into its own token and then 4 five and then 678 and then 90 so you can look at the token IDs and you can see that essentially if there's two matching tokens they have the same token ID uh but each one of these tokens is given its own ID we're going to talk a lot more about this more advanced stuff in future videos but for now this is just a good introduction to kind of what's Happening behind the scenes what a token is and kind of how it's used as a measurement of your requests and the response that come back to you from the API so with that said thanks for watching this video if you have any questions please feel free to ask them in the comments below check the description out for all the resources the links to the repo for this series and be sure and be on the lookout for the next video in the series coming soon thanks a lot and we'll catch you all in the next video