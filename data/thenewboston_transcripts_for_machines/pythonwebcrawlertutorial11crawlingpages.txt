alright guys so in this video what we're going to do is we're going to take a look at crawling an actual page so this is actually a pretty fun function and again all this function is going to do is probably exactly like you think we're going to pass it in a URL or a web page and it can be any web page on your website and then it's going to go ahead and connect to that page get all of the links and once it has those links it's going to add them to the waiting list if you didn't curl the page already so once all of those links are in the waiting list and the page has been curled what it also needs to do is it needs to take that URL and add it to the crowd file and that ensures that you aren't crawling the same page over and over and over again so again that's what it's going to be doing and also after all that's done we need to update those data files because whenever we create another spider we need to make sure that it has an uptodate copy of you know the newest links the most uptodate links so let's go ahead and get started and this is also going to be a static method like before and what I named it crawl page all right so hold on my freaking knows not even going to pause my video you know I have this box of tissues by my computer that sounded kind of weird well I do just because I have a runny nose all the time and I actually have to like split them in half lengthwise because if I just try to stick an entire thing up my nostril then I can't fit all the tissue in there so yeah I'm sure you guys wanted to know the details of how I you know get snot out of my nose but anyways whenever we curl the page we need to display well we don't need to but it's good to display what page you're curling to the user so since this is a multithreaded program I'm going to say like thread two currently curling the forum home page thread five crawling the videos page thread one crawling Bucky's profile whatever and it's always good to display something because if they just boot up your program and it's just a black box with a cursor blinking meg um is this program running then my computer just freeze whatever so make sure you display something might as well display what URL we're on so the first one like I said is going to be the thread name the first time it's actually just called first spider but it's usually going to be the thread name and the second one is just a page URL pretty sweet so the first thing we're going to want to do inside this method is we just want to make sure that we didn't already crawl this page so we're going to say get that URL and make sure that it's not in spider crawled now make sure that you aren't using crowd file use the crawled set for faster operations and then before we start you know writing all the good stuff let's just go ahead and print our indicators to the user so they you know know that something's going on so the first thing I won't pronounce what name of the thread it is and what pages are currently curling so the thread name crawling page URL so it's going to say like thread two is I'll say now crawling arm you know the New Boston slash index dot PHP or whatever now I'm also going to print out some other cool stats so I thought it would be a good idea to print out how many links were in the waiting list and also how many links have already been crawled so that way the user can boot up this program go you know to McDonald's or whatever come back and back oh so it already crawled like ten thousand pages that's pretty cool so qu e Yui uhhuh all right so the amount of items in the waiting list is we first need to convert to a string since it's going to be a number we just need to get the length of spider Q so this is our set get however many items are in it and since that's going to be integer value just go ahead and convert it to a string simple enough and now let me just do the same thing for crawled and let me go ahead and separate it with a pipe symbol maybe I deal with a tab now pipes good why are my fingers like sausage fingers say so for crawl do we basically want to do the same thing and spider crawl all right so it's going to say like Q arm 20 crawled 6 something like that come on scrollbar all right so now we actually have to make the spider do something and a lot of these functions we didn't create yet so if we get a little issue don't worry we're going to be creating them in like the next two tutorials so the first one is spider add links to Q so I don't even need to explain what this is going to do once we retrieve a set of Link's we're just going to want to add them to the waiting list now where do we get those links from well spider gather links is essentially what we're going to do there and let me just throw this in and then I'll explain alright so this is a function we're going to make to actually connect to a web page and it's going to return a set of all of the links that it found on that web page now once we have that set of links we're just going to add it to the waiting list and this is the waiting list that all spiders can see and they can all be synchronized that way pretty easy stuff so after that what we need to do is we need to go back to the waiting list and we need to remove the page that we just curled so we're actually taking it from the waiting list and we need to put it on the crowd file so spider dots crawled and since this is set you can just use the built in add function and paid euro so again after you're done crawling a page all you're doing in these two lines is moving it from the waiting list to the crowd list simple enough and the last thing you need to do is sense this right here takes care of the sets we now need to update the actual files all right so these are for fast operations and then once it's done with all that that's when you actually update the file and I'll just say update files and all this function is going to do is it's going to call both of those sets and it's going to convert them to file easiest thing ever so there you go that is your curl page make sure I'm not missing anything all right so in the next couple toriel's we got three more functions to make we first need to connect a page gather all the links and again if that sounds tough 90% of the work is done for us right here so it's going to be real quick once we have the links we need to add them to the queue not that hard we already have that set and then we need to update files we already got that function created as well so it's going to be a piece of cake so thank you guys I will see you then