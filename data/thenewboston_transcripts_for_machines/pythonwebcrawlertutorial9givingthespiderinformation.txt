are already made so now that we understand the concept of how these spiders are going to be created let's go ahead and start creating them so right now we don't have a whole lot going on so whenever we make an instance of this class whenever we make a spider boom so let's take a look at our code all right so right now our spider is going to be created and he's going to be sitting there like um am I supposed to be doing something here I guess that's how the spider is going to sound so let's go ahead and give him some information and of course some things to do so of course for every spider the user needs to give us some information so it knows exactly what pages are supposed to be curling what project it's working on so on and so forth so we're going to allow the user to pass in the name of the project such as the new Boston also the base URL and this is just going to be the home page URL so the first spider knows where to start crawling and also the domain name so we can use some cool domain name functions to ensure that we're connecting to a valid web page without generating a bunch of errors so now what I want to do is now that we have some information that the user passed in we want to go ahead and set these to the class variables and that's going to ensure that all spiders are looking at the same information so that way all spiders are crawling the same site which is a good thing all right so for the class variable just set equal to whatever the user passed in for the project name and we're just going to do this for base URL base URL and spider domain name all right so again all spiders are going to have this shared information it's never going to change no matter if you have a hundred spiders or a thousand whatever now another thing I want to do is I want to set the file path for the cue file in the crowd file and again that's just so I don't have to type in the full path every time just going to save me a little bit of time so the cue file is equal to the project name so something like the New Boston plus Q file Q text and I'm Way too lazy to type all that again so I'm just going to say the file path for the crowd file is equal to crowd by the way I'm like I suddenly got a stuffy nose like oh no just happened today I was sleeping earlier and once I can I don't know every half an hour so one of my nostrils just starts running I'm like what the heck body come on so stupid nose alright so that had nothing to do with Python Python so let's get back on track here so once we pretty much set the information for you know whatever the user typed in all these spiders have a general idea of what they're supposed to be doing we now need to essentially create that project directory and create those two files those two text files now we can go ahead and just type them right in there but I actually want to stick this in a new method and it's just going to help it um you know my code look a little bit cleaner so I'm going to call the method boo so that's what we're going to be doing in there and also we're going to make another method later on called crawl page and again all this is going to do is it's going to connect a some web page I like this one and it's essentially going to you know do exactly we've been talking about this whole time crawl it gather of links whatever now check out one thing that I need to mention so the very first time we create a spider it's going to have only one page in the waiting list and that is the home page of the website so we can't really make a bunch of spiders right off the bat because it'll be pointless it only has one single page now once that spider let me where that is my there it is alright so once the spider gathers a bunch of links let's say gathers like 40 links from this home page that's when we can go ahead and make our program multithreaded and we're like hey spiders go do your thing so that's how it's going to work now whenever I crawl a page what I'm going to do is in the command prompt just so the user has something look at while this program is running is I'm going to say the name of the spider and this is going to be the name of the thread like um thread one through a to three so we'll save the name and what page it's currently crawling and that way the user just knows something's going on so again the first spider it doesn't run on a thread so we'll just go ahead and since this function is going to need a name we can either just say first spider um but I'm just going to say like a let me go ahead and say first later all right that looks pretty good alright now another thing that we're going to throw in here is of course whenever we call this function we need to give a page to crawl so what page is the first spider going to curl holy raviolis this base URL URL which is the home page and again this is only going to happen once because later on after it's done crawling it it's going to be in that crowd file so all the rest of the spiders are just going to ignore it simple enough sweet