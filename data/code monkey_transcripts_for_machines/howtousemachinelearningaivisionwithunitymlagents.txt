hello and welcome i'm your code monkey let's learn how to use machine learning ai in unity using ml agents to create an ai with vision that can actually see game engines are actually perfect for doing ai vision since you can actually create a virtual world in the real world in order to get a visual input you need an actual camera pointing at the world and capturing some photons however in a game engine you literally just press a button and create a camera that can render the image and what it sees now i've got two interesting demos here first a very simple one where birds jump around and the ai sees them and shoots and another interesting demo where the ai is hurting animals by moving around the map and identifying the sheep and the pigs the second demo also combines both classic ai and machine learning ai in an interesting way so stay tuned to the end of the video i also covered the complete getting started guide to machine learning with unity ml agents in another video so go watch that if you're not familiar with the tonkit machine learning in unity is actually very simple and easy to use once you understand the basics there's a complete playlist in the description and adding vision to your ais is also quite simple as we will soon see it's really just as easy as adding a component to your agent learn all about vr and ar with the patreon sponsor xr bootcamp it's a six to eight week bootcamp taught by industry professionals learn how to interact in vr optimize your rendering and learn about dots check them out at xrbootcamp.com and use the coupon cm10 to get 10 off any of the master classes so vision is awesome and really intuitive to understand for a human being but the one big downside that is inherent to using vision for machine learning is simply due to the scale of the pro for example in the very first machine learning video where i made an agent go to a go i simply needed 6 observations then on the flippy bird agent i used 9 raycasts to detect two types of objects for a total of about 40 observations and on the card driver which also used raycast it had a total of about 50 observations and here i'm using a camera sensor with a size of 50 by 50 meaning for a total of 2 500 observations so that's a lot higher than all of the other examples which means a lot more time is needed for training that's why those big visionbased algorithms have to be trained on massive gpu farms so with that in mind let's first see how we can use this awesome tool so here is my first demo we can see birds jumping around from both sides and right now i'm playing so i can click anywhere in order to shoot now naturally the goal is to actually shoot the bird so as the bird comes i click on it and if there you go i've got a hit and i can shoot all the birds so as you can see very simple demo so here i am in the editor and here's my agent everything as you can see is pretty basic so if you've seen the getting started video then all of this should be familiar so it's got the behavior parameters and the script which runs the agent a decision requester and then here is the new one we have the camera sensor so what it does is exactly what the name implies so it takes a camera input and uses that as the visual observation so i can see the camera that i'm using it's this one and here this camera is set up to view a simplified version of the scene so the main thing is over here on the culling mask this one is set to only render objects on the vision camera layer so over here we can see the camera preview and as you can see it's all in black except for white for where the bird is then on the burned object here we have just a basic sprite renderer with a white pixel and it's on that layer so if we play the game and look at the camera here we can see what the ai is actually seeing so as you can see it's constantly seeing as the bird jumps in and jumps out so this goes back to what i said about the total size and total amount of observations when dealing with visuals it's very important to simplify your vision as much as possible although of course as with anything related to machine learning brute force is always a possibility so if i had a massive amount of compute processing power i could just use the standard camera view in 1080p and full color and eventually after enough time the machine would actually learn but in here i just have my humble machine so simplifying the visual made it so that the training time didn't take hundreds of hours so back into the sensor let's just look at the other parameters so we can see the width and height for the visual size again remember the bigger the image the longer it won't take to train then we also have a toggle for grayscale which again if you sound like grayscale then what ends up is you have one channel so essentially just one float per pixel whereas if you go with color you end up with three floats per pixel so with a 50x50 image if it's in grayscale you have 2500 observations and if you change it to color then all of a sudden you have 7500 observations so it's a ton more with a ton more variation once again assuming you're trying to train your agents in a reasonable amount of time with a standard machine you should really simplify things as much as possible so here i'm giving the ai a simple 50x50 image with a simplified grayscale view of the scene then for the actions up here you can see that they are set up as discrete with just two possible actions and since the camera has a width and height of 50 and over here for the branch size i'm giving them 50 of each so essentially the ai is pretty much just working on a grin here on the agent code we can see what happens on the on action received and we have our two discrete actions so one of them is for the position x another one for the y so he simply gets the x and y and tries to shoot it and if it does hit the target then it gets a positive reward and ends the episode and if it does not hit the target then it gets a negative reward just to encourage you to hit the target as soon as possible so this is really all it takes in order to train this agent so i've got this very simple very small script and then here just the normal ml agents components as well as the extra camera sensor and again for training i use a manual curriculum essentially teaching it bit by bit just like i did for the flipping bird agent so i started off with a static very large target then when it learned that i increased the difficulty by making the target smaller here is the training graph as you can see it took quite a bit of time to learn even though i started with a very easy scenario with a large target so once again this is the difficulty when dealing with vision ai but over time it didn't learn and then when i added the movement it didn't even need to train anymore i don't really learn to hit the white target so movement doesn't really matter and over here is the result so as soon as the bird spawns it gets shot pretty much almost immediately so no matter where it comes from the ai always hits it now when training the ai i actually made a small mistake so here in the actions as you can see i just set up to receive two actions so just an x and y meaning that the ai has no chance to simply not shoot so whenever it takes a decision it always has to shoot somewhere so right now the way that it's set up with a decision requester on a period of 20 this works just fine that's about enough time for the bird to spawn and get inside of the view however if i pull it back and tell it to shoot on every single frame then yep there you go that's the issue since once again i didn't train the ai to be able to not shoot so it always shoots somewhere and when it doesn't see any white pixel then just start shooting randomly so what i should have done is instead add another action for either shoot or don't shoot okay so this is one way to add vision to your ai you add a camera sensor and assign a camera but there's one more way if you go into add a component and in here you browse inside the ml agents you can see we've got our behavior camera sensor decision and so on and all of here we have the render texture sensor so functionally this works pretty much exactly the same you would create a render texture then for example make this camera render onto that render texture and then use that render texture in here and as you can see we have the other parameters as well then just for these last two which i didn't mention previously for the observation stacks this one is how many frames won't be stacked before being fed into the network meaning if you set it to just one then network will take a decision based on only a single frame whereas if you put it to more for example to 10 the network will be asked to make a decision based on the information of these 10 frames so for example if you send more than one the ai won't be able to infer some sort of direction from the white pixels but once again that has a training cost so in my case i don't really care for the velocity so if i set it to one everything works just the same and then finally for the compression you can set it to no compression or png compression this is just to make it more efficient to send the data from unity onto the python trainer if you set it to png then essentially the final observation won't be smaller so that communication will go a bit faster than if you send it as completely uncompressed alright so that's one demo showcasing one way you can use machine learning vision in unity and over here i've got another interesting demo and by the way if you find the video helpful consider subscribing and hitting the like button it really helps out the channel so i've got this ai over here and it's moving around it goes towards a random animal looks at it identifies if it's a pig or a sheep and then sends it to the correct pen so sheep go in here and pigs go in here the models for these animals are from a great asset pack there's a link in the description in case you want to see it and this is also an interesting example because i'm combining both machine learning and classic ai at the same time and for another different thing it's also using manual decisions as to opposed to automatic decisions on every single frame so the movement is handled through a classic ai and the identification of the animal is using machine learning ai so let's look at how it's set up over here i've got my environment and it's got this main script and in here i just got references to all of the various prefabs and on awake we simply start spawning and spawn the initial animals on random positions then on the agent it has a rigid body in order to move and then it also has a basic mover script so here it is it is literally just the most basic classic ai you can think of it just kind of lights a move direction and moves the rigid body towards it so it's a very small very simple script we can set a target position and look at position and the agent goes there and looks towards a certain position so once again this is classic ai there's no machine learning ai used in here at all so over here on the environment script i can essentially tell the agent mover and i tell it to go to target position and look at a certain position and when it gets there that one fires off this event and runs this function which then caused it to take an action so again these actions are not happening automatically like we saw previously on the decision requester instead they are manually so as soon as the agent reaches a position it manually takes an action and then on this take action it simply manually requests a decision so the ai does its thing it processes the visual input and for the visual input it is pretty much the same thing that we saw previously so using a camera sensor and in here has a camera now this camera instead of being an overhead camera like we saw in the previous demo this one is attached to the agent and once again it's set up with a calling mask to only view the vision camera and if we run the game in c so in there we can see what the camera sees so as you can see the sheep and the pigs they have essentially cubes on top of them and they have a flat collar so as you can see the agent goes to a certain position rotates to face the animal and then it simply has that so then it takes a decision and the ai is simply looking at the color of the pixels pretty much so i'm looking at the pig here it's just the normal pig mesh and then it's got just a basic cube set up to an unlit color then for the action it's just a simple discrete action either zero or one and then it tries to select the animal and if it gets the correct one then gets a positive reward and if it's a negative one then a negative reward now for training i just place the agent in a fixed position with no movement then i spawn the random animal in front and sdai to take the decision so it was a very simple training process and now here for this example use case i don't need let's say the shape of the image all i want is pretty much just the color of the pixel in front so in the camera sensor i set it up with the smallest width and height which happens to be 20 by 20. so essentially you cannot have this lower than this now in theory you could obviously accomplish the scenario with just a raycast or by just using the center pixel of the screen as an observation but using vision is a bit more fun so here is the demo in action the agent goes towards a random animal looks towards it and then takes a decision it says that this one is a pig and the pig starts going towards a pig pen that one is a ship and goes in there so it just goes randomly from animal to animal constantly identifying them and it works out pretty much all the time so this is a great example of how you can mix and match both classic ai and machine learning ai obviously you could handle the movement through ml agents as well by using the camera sensor or just some recas but using classic ai works just fine and doesn't require tens of training so before you just decide to solve every problem using machine learning remember that you can always combine both alright so as you can see adding vision to your ai is awesome and very easy to do when using unity ml agents but it does have a pretty significant training cost think of it as a problem with regards to a noisy signal ratio visual observations have a ton of noise and very little signal so in the first demo the aic is this image but the only useful parts of this observation the only signal is just these handful of white pixels everything else is just noise that the ai needs to learn to ignore so vision is awesome but in most cases you really want to simplify your observations to exactly what your ai needs to know one of the benefits of working in a game engine as opposed to the real world is the fact that you can have the complete world state at your fingertips for example when training a robot in the real world in order to identify an object you don't have access to something like the economical state of the world so you need to process the visual input in order to identify the object but when working in a game engine you have complete access to that object and you can identify it easily just fire raycast into a component call that is much simpler to do and will result in significantly faster training time so adding vision is awesome but before you use it make sure there's really no other way to simplify your observations alright so that's machine learning vision ai in unity check the phone playlist link in the description where i'm adding all machine learning videos as long as you can download your project files and utilities from indiegogomonk.com this video is made possible thanks to these awesome supporters go to patreon.com unitycodemonkey to get some perks and help keep the videos free for everyone if you found this video home funk instead liking and subscribing wasn't fair to having comments and i'll see you next time