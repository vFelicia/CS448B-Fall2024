hello and welcome i'm your code monkey and the goal here is to get an ai to play flappy bird it's a simple game although it's quite challenging for a human so let's see if i can build an ai to beat my high score and go superhuman doing this using machine learning in unity is actually quite simple although as usual the training process was very interesting i had to essentially train the ai bit by bit so first make it learn just how to avoid the floor then slowly up the difficulty by adding randomness and smaller gaps as the i learned more and more i'll cover what i did to train it in more detail in a bit now the project that i have here is a fully functioning game this was actually created completely from scratch in another video a long time ago so go watch that if you want to know how the base game was made so i can play it and it works exactly as you would expect so just press the button to jump and for every pipe that i go through i get one score then over time the gaps become smaller and the pipe height becomes more and more random so everything becomes much more difficult so that's it's a very simple design now i wanted to make an ai to play the game flawlessly so for that i use unt ml agents to train the brain model i covered a complete getting start guide in another video so go watch that if you're not familiar with the toolkit machine learning in unity is actually very simple and easy to use once you understand the basics there's a complete plan list in description also quick note here unity is having a machine learning ai summit on december 10th it's a free oneday event with presentations panel discussions and handson workshops all about spatial simulation play testing robotics ml agents and more all of it presented by experts and industry leaders so if you're interested in machine learning or ai check out the link in the description as i've mentioned previously the real tricky thing about machine learning isn't really the code i'm going to showcase the agent class in a bit but the code is actually super simple the tricky part is setting up the training environment in a good way that allows for the agent to learn so the first question is always what does the ai need to know and what actions does it need to take in order to accomplish the task flippy bird is a pretty simple game there's only one action jump so adding the actions is very simple just set it to discrete with a single action with two possible values so jump or no jump then for the observations this is how the agent gathers information about its environment it needs to know where the various walls are so adding a ray perception sensor is perfect for this scenario so it adds a couple of rays they get fired in front of the bird on all sorts of angles and they are set up to detect checkpoints and walls so if i play the game side by side you can see yep look at that look at how the rays are behaving so there's an invisible checkpoint right down the middle and the rays correctly identify both the walls and the checkpoints then i also added a few more manual observations so here i gave it knowledge of its own transform position y so essentially it knows the height of the burnt and then also knowing how far the next pipe is as well as a normalized value for the current velocity so with all of that yeah i should have enough information in order to be able to complete the task then for the level setup itself like i said i made this whole thing from scratch in another video a long time ago so go watch that for a more detailed guide essentially as you can see the pipes are spawned on the right side and they move towards the left side soon as they get there they get despawn more spawn and so on and everything is pretty random and finally if the bird actually touches a pipe then yep we have our game over now as i mentioned in the beginning in order to train this ai i had to do it bit by bit i first tried to train it right with the final game but it just kept failing and failing so it kept hitting either the ceiling or the floor so constantly failing now in theory if i had enough processing power i could train it just like that through sheer brute force but i just have my humble machine not a massive gpu farm so i need to be a bit more clever with how i handle training so what i ended up doing here is essentially called curriculum learning devil agents does have a proper way of scripting a curriculum but here i did it manually essentially you teach the ai starting from an easy scenario and as it learns to complete the task you slowly increase the difficulty i will cover the proper way to script a curriculum so stay tuned for that but here like i said i made it manually so first i trained it pretty much with no pipes so just a massive huge gap and everything right down the middle so essentially just trying to teach the ai to avoid the top and the bottom also in order to get to this point quickly i used imitation learning which i covered in detail in another video i use it with a pretty high strength so it learned almost exclusively from my demos so on the config itself i enabled both bc and gale and using them both with a pretty good strength so essentially the goal is to get the ai to behave exactly as i told it to then once the ai figured out how to avoid both the top and the bottom i closed the gap for a little bit i also upped the strength of the extrinsic rewards up to 1.0 whilst reducing both gale and bc down to 0.4 so the goal was to make it start learning based on its own rewards and become better than me the result of that was quite interesting so first it had some trouble figuring it out so the reward dropped quite significantly so it was at 13 then dropped all the way under 5 but then it started to learn from its extrinsic rewards and the cumulative reward shot up to the maximum now up until that point the gap height was always down the middle so next up i added some randomness to the gap height even though with a rather large gap so with that there's a lot more randomness to how the levels are created and then i also lowered both types of imitation learning so both gale and bc all the way down to 0.1 so the demo barely impacts learning at this point really the extrinsic rewards is all i wanted to learn from and this time the ai didn't even stumble with this new scenario so it instantly adapted the changes and constantly got a perfect score now here the reward essentially has a maximum cap because of the value that i set for the max steps so if it survives for a total of 2000 subs and it automatically ends the episode and by the way if you find the video helpful consider subscribing and hitting the like button it really helps out the channel then for a pretty big jump in difficulty i tightened the gap size by quite a bit which also means that the random gap height is even more random so this is a pretty big jump in difficulty and it showed in the actual graphs so on the previous one it instantly went up to 40 which was the current maximum and as soon as i changed it it dropped all the way down to 13. so clearly it had a bit of trouble adapting to such randomness on the pipe height but once again the ai never quits on training so it pushed through its own difficulties and slowly adapted to its own environment so you can see right in here it was only getting 13 reward then it improved and got a bit worse and improved and slowly and surely it got quite a lot better and then all the way over here after 1.5 million steps it was reaching the maximum once again you can see the results are quite a lot more choppy due to how much randomness this environment has so with that the ai was already pretty good so i made two more levels of difficulty and again the results were the same so it instantly drops down and then slowly learns over time so on that one as well as on this one and then finally i set the max tab back into zero so now the episode only ends when the ai actually loses and then look at those results so as it started right away it got only 20 but it instantly shot up on the way up to 220 then for some reason it actually dropped down a bit so i guess the ai became so good that it kind of scared itself and then slowly started improving constantly making higher highs so the results are quite a lot more choppy but it was consistently improving and at this point in order to achieve perfect scores it would really just require more brute force training so as you can see ml agents can really learn anything if given enough time and if you take the proper time to construct a good training scenario then it learns actually pretty fast there's also one really interesting thing that i use for training this essentially i had to use a different method from what i covered in my other machine learning videos in those i usually put the training environment inside of prefab and just copy paste the environment a whole bunch of times so that's a great approach however here when i initially made this game i didn't make it with machine learning in mind so i didn't set up everything in order to easily support multiple environments the game expects a single bird and a single level so adding support for multiple environments would mean an almost complete rewrite of the whole game now thankfully ml agents has an awesome feature where you can run training on normal builds and in order to do that you really just have to set up your build to require no input so as i hit play it instantly starts playing the level so it doesn't require any input click on any play button or anything so that's all it takes then just make sure that your agent is set to default so that it actually learns and then you make a normal build so just go into build settings make sure your build is in there and just make an arm build now another thing is if you don't use any visuals for your own logic so for example if you don't use any camera sensors or any input from any camera then you can make it a server build which means that the game will run much faster without having to do any rendering so let me do it and just make the build over here i have my build and here it is the executable and if i run it yep it runs in a normal command prompt and here i've got some logs so as you can see the game is actually running right now so i have this build that i can use in order to run this for training i just open up the command prompt on my virtual environment again i covered all of this in detail in the getting started video then doing the normal command which is running ml agents dash learn then passing in the config file and then you add the parameter dash dash inf so this is the environment which is related build so over here i've got my project folder i've got the inside this underscore builds folder and inside flippy bird and inside it's named mlagents.xa so here i just put exactly that path so exactly like that without the final extension this is how we tell it what build to run and then adding dash dash numinifs so this is how many instances you want to run at once now running more instances takes quite a lot more resources like for example memory and hard drive space so actually initially i tried running 20 instances and my hard drive actually ran out of space because it used up almost 30 gigs so i ended up doing most of the training with just eight instances which is still quite enough to make it go quick then you just have dash dash run id so just the normal stuff and just press on enter and yep all eight builds are now currently running you can even inspect the task manager and see yep we've got eight sentences running then to stop training on windows you just hit control c and everything stops and it saves the model okay so that's how you run training on an external build so this is pretty much essential for most real use cases since when working on proper games you probably won't be able to easily create multiple environments whilst inside the game okay so let's try out our final ai so first of all here i am as a human player trying to get a nice high score so let's see how far i can push it okay there it is i did actually pretty well getting a high score of 39 so that's pretty good now let's see the final trend ai and see how it goes all right so there it is and it goes through the first few pipes okay so far so good let's see how much it goes and 37 38 39 and yep there goes my high score all right so that's it let's see how far it can go okay it's over 200 so at this point i guess it's safe to say that the machines have finally beaten the humans or at least this ai has been this human alright so this was another real example of machine learning in action as you can see it's actually really quite simple it took me only a few hours to write the agent and adapt the game to work with ml agents then it was just letting the training run over time while periodically increasing the difficulty so doing some classic ai would have probably taken quite a lot more time so here is another example use case of machine learning in action check the phone playlist linked in the description where i'm adding all machine learning videos as always you can download the project files and utilities from unitycodemonkey.com this video is made possible thanks to these awesome supporters go to patreon.com unitycodemonkey to get some perks and help keep the videos free for everyone if you found this video helpful consider liking and subscribing post any questions have any comments and i'll see you next time